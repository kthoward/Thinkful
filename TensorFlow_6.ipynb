{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_1 = unpickle(\"./cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2 = unpickle(\"./cifar-10-batches-py/data_batch_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_3 = unpickle(\"./cifar-10-batches-py/data_batch_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_4 = unpickle(\"./cifar-10-batches-py/data_batch_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_5 = unpickle(\"./cifar-10-batches-py/data_batch_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,  43,  50, ..., 140,  84,  72], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "shaped = np.reshape(data_1[b'labels'], (-1,1))\n",
    "onehot.fit(shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_2 = OneHotEncoder(sparse=False)\n",
    "shaped_2 = np.reshape(data_2[b'labels'], (-1,1))\n",
    "onehot_2.fit(shaped_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_3 = OneHotEncoder(sparse=False)\n",
    "shaped_3 = np.reshape(data_3[b'labels'], (-1,1))\n",
    "onehot_3.fit(shaped_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_4 = OneHotEncoder(sparse=False)\n",
    "shaped_4 = np.reshape(data_4[b'labels'], (-1,1))\n",
    "onehot_4.fit(shaped_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_5 = OneHotEncoder(sparse=False)\n",
    "shaped_5 = np.reshape(data_5[b'labels'], (-1,1))\n",
    "onehot_5.fit(shaped_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_var = onehot.transform(shaped)\n",
    "y_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_var_2 = onehot.transform(shaped_2)\n",
    "y_var_3 = onehot.transform(shaped_3)\n",
    "y_var_4 = onehot.transform(shaped_4)\n",
    "y_var_5 = onehot.transform(shaped_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_0 = data_1[b'data'][1000].reshape([3,32,32])\n",
    "np.rollaxis(image_0,0,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f483d68>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUeT8QiSKnF+IRJHzC5Eo/weNQcytcD1B0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be0fc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.rollaxis(image_0,0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for data in data_1[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll = []\n",
    "for image in images:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_2 = []\n",
    "for data in data_2[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_2.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_2 = []\n",
    "for image in images_2:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_2.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_3 = []\n",
    "for data in data_3[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_3.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_3 = []\n",
    "for image in images_3:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_3.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_4 = []\n",
    "for data in data_4[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_4.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_4 = []\n",
    "for image in images_4:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_4.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_5 = []\n",
    "for data in data_5[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_5.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_5 = []\n",
    "for image in images_5:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_5.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_roll = np.asarray(images_roll)\n",
    "image_one = images_roll[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_roll_2 = np.asarray(images_roll_2)\n",
    "images_roll_3 = np.asarray(images_roll_3)\n",
    "images_roll_4 = np.asarray(images_roll_4)\n",
    "images_roll_5 = np.asarray(images_roll_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.concatenate((images_roll,images_roll_3),axis=0)\n",
    "images = np.concatenate((images,images_roll_4),axis=0)\n",
    "images = np.concatenate((images,images_roll_5),axis=0)\n",
    "images = np.concatenate((images,images_roll_2),axis=0)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate((y_var,y_var_3),axis=0)\n",
    "y = np.concatenate((y,y_var_4),axis=0)\n",
    "y = np.concatenate((y,y_var_5),axis=0)\n",
    "y = np.concatenate((y,y_var_2),axis=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, y, test_size= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f7e0860>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12f4b5908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_1 = images_roll[1]\n",
    "plt.imshow(image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(3072,)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2000\n",
      "8000/8000 [==============================] - 3s 335us/step - loss: 3.4326 - acc: 0.1791 - val_loss: 2.2253 - val_acc: 0.2290\n",
      "Epoch 2/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 2.2689 - acc: 0.2146 - val_loss: 2.0181 - val_acc: 0.2565\n",
      "Epoch 3/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 2.2057 - acc: 0.2281 - val_loss: 2.0382 - val_acc: 0.2845\n",
      "Epoch 4/2000\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 2.1817 - acc: 0.2469 - val_loss: 1.9699 - val_acc: 0.2800\n",
      "Epoch 5/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 2.1106 - acc: 0.2590 - val_loss: 2.4490 - val_acc: 0.2065\n",
      "Epoch 6/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 2.0775 - acc: 0.2684 - val_loss: 1.8798 - val_acc: 0.3100\n",
      "Epoch 7/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 2.0227 - acc: 0.2801 - val_loss: 1.9939 - val_acc: 0.2850\n",
      "Epoch 8/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.9991 - acc: 0.2914 - val_loss: 2.0019 - val_acc: 0.3105\n",
      "Epoch 9/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.9807 - acc: 0.2979 - val_loss: 1.9090 - val_acc: 0.3320\n",
      "Epoch 10/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.9531 - acc: 0.3066 - val_loss: 2.1551 - val_acc: 0.2405\n",
      "Epoch 11/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.9350 - acc: 0.3123 - val_loss: 1.9382 - val_acc: 0.3180\n",
      "Epoch 12/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.9060 - acc: 0.3181 - val_loss: 2.0751 - val_acc: 0.2710\n",
      "Epoch 13/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.8928 - acc: 0.3289 - val_loss: 1.8188 - val_acc: 0.3580\n",
      "Epoch 14/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.8794 - acc: 0.3341 - val_loss: 1.8754 - val_acc: 0.3455\n",
      "Epoch 15/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.8635 - acc: 0.3421 - val_loss: 1.9018 - val_acc: 0.3215\n",
      "Epoch 16/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.8534 - acc: 0.3441 - val_loss: 1.8137 - val_acc: 0.3635\n",
      "Epoch 17/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.8399 - acc: 0.3488 - val_loss: 1.9362 - val_acc: 0.3270\n",
      "Epoch 18/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.8311 - acc: 0.3470 - val_loss: 1.8849 - val_acc: 0.3460\n",
      "Epoch 19/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.8221 - acc: 0.3580 - val_loss: 1.8883 - val_acc: 0.3470\n",
      "Epoch 20/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.8228 - acc: 0.3605 - val_loss: 1.9102 - val_acc: 0.3240\n",
      "Epoch 21/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.8187 - acc: 0.3579 - val_loss: 1.8704 - val_acc: 0.3340\n",
      "Epoch 22/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.8112 - acc: 0.3640 - val_loss: 1.8532 - val_acc: 0.3395\n",
      "Epoch 23/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.8123 - acc: 0.3614 - val_loss: 1.8350 - val_acc: 0.3575\n",
      "Epoch 24/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.7991 - acc: 0.3653 - val_loss: 1.8610 - val_acc: 0.3585\n",
      "Epoch 25/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.7866 - acc: 0.3654 - val_loss: 1.8099 - val_acc: 0.3850\n",
      "Epoch 26/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.7850 - acc: 0.3709 - val_loss: 1.8558 - val_acc: 0.3460\n",
      "Epoch 27/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.7769 - acc: 0.3725 - val_loss: 1.8468 - val_acc: 0.3555\n",
      "Epoch 28/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.7790 - acc: 0.3739 - val_loss: 1.7919 - val_acc: 0.3645\n",
      "Epoch 29/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.7701 - acc: 0.3762 - val_loss: 1.8467 - val_acc: 0.3495\n",
      "Epoch 30/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.7606 - acc: 0.3817 - val_loss: 1.9286 - val_acc: 0.3035\n",
      "Epoch 31/2000\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 1.7576 - acc: 0.3824 - val_loss: 1.8955 - val_acc: 0.3400\n",
      "Epoch 32/2000\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 1.7526 - acc: 0.3795 - val_loss: 1.8298 - val_acc: 0.3535\n",
      "Epoch 33/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.7539 - acc: 0.3806 - val_loss: 1.8963 - val_acc: 0.3295\n",
      "Epoch 34/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.7523 - acc: 0.3825 - val_loss: 1.8808 - val_acc: 0.3405\n",
      "Epoch 35/2000\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 1.7584 - acc: 0.3799 - val_loss: 1.9423 - val_acc: 0.3150\n",
      "Epoch 36/2000\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 1.7587 - acc: 0.3816 - val_loss: 1.9021 - val_acc: 0.3400\n",
      "Epoch 37/2000\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.7554 - acc: 0.3849 - val_loss: 1.7903 - val_acc: 0.3740\n",
      "Epoch 38/2000\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 1.7489 - acc: 0.3869 - val_loss: 1.8727 - val_acc: 0.3565\n",
      "Epoch 39/2000\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 1.7488 - acc: 0.3810 - val_loss: 1.8259 - val_acc: 0.3705\n",
      "Epoch 40/2000\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 1.7421 - acc: 0.3891 - val_loss: 1.8762 - val_acc: 0.3545\n",
      "Epoch 41/2000\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.7471 - acc: 0.3831 - val_loss: 1.8014 - val_acc: 0.3805\n",
      "Epoch 42/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.7345 - acc: 0.3920 - val_loss: 1.8355 - val_acc: 0.3540\n",
      "Epoch 43/2000\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 1.7237 - acc: 0.3946 - val_loss: 1.7785 - val_acc: 0.3775\n",
      "Epoch 44/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.7215 - acc: 0.3924 - val_loss: 1.8234 - val_acc: 0.3520\n",
      "Epoch 45/2000\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 1.7201 - acc: 0.3935 - val_loss: 1.8326 - val_acc: 0.3470\n",
      "Epoch 46/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.7162 - acc: 0.3934 - val_loss: 1.8042 - val_acc: 0.3745\n",
      "Epoch 47/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.7120 - acc: 0.3958 - val_loss: 1.8949 - val_acc: 0.3250\n",
      "Epoch 48/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.7081 - acc: 0.4032 - val_loss: 1.8967 - val_acc: 0.3380\n",
      "Epoch 49/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.7142 - acc: 0.4005 - val_loss: 1.8525 - val_acc: 0.3445\n",
      "Epoch 50/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.7033 - acc: 0.3996 - val_loss: 1.8275 - val_acc: 0.3710\n",
      "Epoch 51/2000\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 1.7075 - acc: 0.4027 - val_loss: 1.8557 - val_acc: 0.3685\n",
      "Epoch 52/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.6966 - acc: 0.4126 - val_loss: 1.9085 - val_acc: 0.3370\n",
      "Epoch 53/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.7010 - acc: 0.4031 - val_loss: 1.8615 - val_acc: 0.3495\n",
      "Epoch 54/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.6904 - acc: 0.4035 - val_loss: 1.8493 - val_acc: 0.3540\n",
      "Epoch 55/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.6899 - acc: 0.4088 - val_loss: 1.8119 - val_acc: 0.3775\n",
      "Epoch 56/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.6985 - acc: 0.4055 - val_loss: 1.8363 - val_acc: 0.3670\n",
      "Epoch 57/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.6893 - acc: 0.4042 - val_loss: 1.9105 - val_acc: 0.3440\n",
      "Epoch 58/2000\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 1.6907 - acc: 0.4076 - val_loss: 1.8330 - val_acc: 0.3745\n",
      "Epoch 59/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.6863 - acc: 0.4216 - val_loss: 1.8073 - val_acc: 0.3795\n",
      "Epoch 60/2000\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 1.6713 - acc: 0.4135 - val_loss: 1.8948 - val_acc: 0.3550\n",
      "Epoch 61/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.6743 - acc: 0.4153 - val_loss: 1.8088 - val_acc: 0.3750\n",
      "Epoch 62/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.6743 - acc: 0.4186 - val_loss: 1.9456 - val_acc: 0.3160\n",
      "Epoch 63/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.6725 - acc: 0.4183 - val_loss: 1.8117 - val_acc: 0.3680\n",
      "Epoch 64/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 1.6780 - acc: 0.4121 - val_loss: 1.9242 - val_acc: 0.3380\n",
      "Epoch 65/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.6705 - acc: 0.4201 - val_loss: 1.8365 - val_acc: 0.3670\n",
      "Epoch 66/2000\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 1.6614 - acc: 0.4200 - val_loss: 1.9150 - val_acc: 0.3225\n",
      "Epoch 67/2000\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 1.6621 - acc: 0.4260 - val_loss: 1.8524 - val_acc: 0.3670\n",
      "Epoch 68/2000\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 1.6537 - acc: 0.4229 - val_loss: 1.8755 - val_acc: 0.3470\n",
      "Epoch 69/2000\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 1.6678 - acc: 0.4206 - val_loss: 1.8313 - val_acc: 0.3635\n",
      "Epoch 70/2000\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 1.6652 - acc: 0.4218 - val_loss: 1.9448 - val_acc: 0.3315\n",
      "Epoch 71/2000\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 1.6546 - acc: 0.4200 - val_loss: 1.8159 - val_acc: 0.3720\n",
      "Epoch 72/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.6514 - acc: 0.4234 - val_loss: 1.8229 - val_acc: 0.3790\n",
      "Epoch 73/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.6557 - acc: 0.4224 - val_loss: 1.8594 - val_acc: 0.3675\n",
      "Epoch 74/2000\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 1.6581 - acc: 0.4219 - val_loss: 1.8450 - val_acc: 0.3715\n",
      "Epoch 75/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.6527 - acc: 0.4253 - val_loss: 1.8690 - val_acc: 0.3570\n",
      "Epoch 76/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.6513 - acc: 0.4269 - val_loss: 1.9631 - val_acc: 0.3270\n",
      "Epoch 77/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.6423 - acc: 0.4273 - val_loss: 1.8333 - val_acc: 0.3775\n",
      "Epoch 78/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.6392 - acc: 0.4321 - val_loss: 1.8668 - val_acc: 0.3540\n",
      "Epoch 79/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.6517 - acc: 0.4276 - val_loss: 1.8399 - val_acc: 0.3635\n",
      "Epoch 80/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.6515 - acc: 0.4255 - val_loss: 1.8363 - val_acc: 0.3595\n",
      "Epoch 81/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.6449 - acc: 0.4280 - val_loss: 1.8315 - val_acc: 0.3675\n",
      "Epoch 82/2000\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.6413 - acc: 0.4246 - val_loss: 1.8909 - val_acc: 0.3425\n",
      "Epoch 83/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.6510 - acc: 0.4271 - val_loss: 1.8252 - val_acc: 0.3500\n",
      "Epoch 84/2000\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 1.6434 - acc: 0.4313 - val_loss: 1.9259 - val_acc: 0.3465\n",
      "Epoch 85/2000\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 1.6483 - acc: 0.4259 - val_loss: 1.8774 - val_acc: 0.3625\n",
      "Epoch 86/2000\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 1.6437 - acc: 0.4268 - val_loss: 1.8720 - val_acc: 0.3535\n",
      "Epoch 87/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.6395 - acc: 0.4289 - val_loss: 1.8930 - val_acc: 0.3345\n",
      "Epoch 88/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.6223 - acc: 0.4331 - val_loss: 1.9077 - val_acc: 0.3465\n",
      "Epoch 89/2000\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 1.6244 - acc: 0.4360 - val_loss: 1.8795 - val_acc: 0.3420\n",
      "Epoch 90/2000\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 1.6256 - acc: 0.4365 - val_loss: 1.8913 - val_acc: 0.3560\n",
      "Epoch 91/2000\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 1.6255 - acc: 0.4298 - val_loss: 1.9340 - val_acc: 0.3285\n",
      "Epoch 92/2000\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 1.6310 - acc: 0.4314 - val_loss: 1.8481 - val_acc: 0.3685\n",
      "Epoch 93/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.6241 - acc: 0.4366 - val_loss: 1.8839 - val_acc: 0.3465\n",
      "Epoch 94/2000\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 1.6213 - acc: 0.4338 - val_loss: 1.8311 - val_acc: 0.3630\n",
      "Epoch 95/2000\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 1.6223 - acc: 0.4387 - val_loss: 1.9095 - val_acc: 0.3605\n",
      "Epoch 96/2000\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 1.6243 - acc: 0.4345 - val_loss: 1.8445 - val_acc: 0.3670\n",
      "Epoch 97/2000\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 1.6240 - acc: 0.4301 - val_loss: 1.8656 - val_acc: 0.3620\n",
      "Epoch 98/2000\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 1.6172 - acc: 0.4400 - val_loss: 1.8451 - val_acc: 0.3770\n",
      "Epoch 99/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.6180 - acc: 0.4373 - val_loss: 1.8967 - val_acc: 0.3560\n",
      "Epoch 100/2000\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 1.6221 - acc: 0.4390 - val_loss: 1.8484 - val_acc: 0.3635\n",
      "Epoch 101/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.6101 - acc: 0.4373 - val_loss: 1.9543 - val_acc: 0.3390\n",
      "Epoch 102/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.6087 - acc: 0.4417 - val_loss: 2.0214 - val_acc: 0.3415\n",
      "Epoch 103/2000\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 1.6075 - acc: 0.4449 - val_loss: 1.8982 - val_acc: 0.3525\n",
      "Epoch 104/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.6121 - acc: 0.4385 - val_loss: 1.8543 - val_acc: 0.3570\n",
      "Epoch 105/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5986 - acc: 0.4373 - val_loss: 2.3413 - val_acc: 0.2925\n",
      "Epoch 106/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.6017 - acc: 0.4379 - val_loss: 1.9472 - val_acc: 0.3475\n",
      "Epoch 107/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.6059 - acc: 0.4415 - val_loss: 1.8917 - val_acc: 0.3535\n",
      "Epoch 108/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.5965 - acc: 0.4429 - val_loss: 1.8666 - val_acc: 0.3650\n",
      "Epoch 109/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.5958 - acc: 0.4458 - val_loss: 1.8645 - val_acc: 0.3740\n",
      "Epoch 110/2000\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 1.5969 - acc: 0.4442 - val_loss: 1.8892 - val_acc: 0.3680\n",
      "Epoch 111/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.5948 - acc: 0.4501 - val_loss: 1.8998 - val_acc: 0.3630\n",
      "Epoch 112/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.5985 - acc: 0.4466 - val_loss: 1.8667 - val_acc: 0.3630\n",
      "Epoch 113/2000\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 1.5953 - acc: 0.4447 - val_loss: 1.8847 - val_acc: 0.3475\n",
      "Epoch 114/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.5956 - acc: 0.4459 - val_loss: 1.9326 - val_acc: 0.3510\n",
      "Epoch 115/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.5937 - acc: 0.4467 - val_loss: 1.9576 - val_acc: 0.3485\n",
      "Epoch 116/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5916 - acc: 0.4452 - val_loss: 1.8785 - val_acc: 0.3730\n",
      "Epoch 117/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.5861 - acc: 0.4493 - val_loss: 1.8977 - val_acc: 0.3555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5840 - acc: 0.4550 - val_loss: 1.9895 - val_acc: 0.3365\n",
      "Epoch 119/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5727 - acc: 0.4507 - val_loss: 2.1343 - val_acc: 0.3035\n",
      "Epoch 120/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5873 - acc: 0.4465 - val_loss: 1.8912 - val_acc: 0.3490\n",
      "Epoch 121/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5769 - acc: 0.4527 - val_loss: 1.9333 - val_acc: 0.3670\n",
      "Epoch 122/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5869 - acc: 0.4441 - val_loss: 1.8616 - val_acc: 0.3650\n",
      "Epoch 123/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5804 - acc: 0.4504 - val_loss: 1.9397 - val_acc: 0.3505\n",
      "Epoch 124/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.5806 - acc: 0.4549 - val_loss: 1.9403 - val_acc: 0.3540\n",
      "Epoch 125/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.5722 - acc: 0.4506 - val_loss: 1.8922 - val_acc: 0.3715\n",
      "Epoch 126/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5730 - acc: 0.4609 - val_loss: 1.9964 - val_acc: 0.3280\n",
      "Epoch 127/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5798 - acc: 0.4502 - val_loss: 1.8899 - val_acc: 0.3660\n",
      "Epoch 128/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.5726 - acc: 0.4550 - val_loss: 1.9653 - val_acc: 0.3485\n",
      "Epoch 129/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.5738 - acc: 0.4521 - val_loss: 1.8982 - val_acc: 0.3575\n",
      "Epoch 130/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.5771 - acc: 0.4502 - val_loss: 1.9437 - val_acc: 0.3355\n",
      "Epoch 131/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.5681 - acc: 0.4583 - val_loss: 1.8487 - val_acc: 0.3825\n",
      "Epoch 132/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5683 - acc: 0.4534 - val_loss: 1.8507 - val_acc: 0.3755\n",
      "Epoch 133/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5706 - acc: 0.4475 - val_loss: 1.9539 - val_acc: 0.3455\n",
      "Epoch 134/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5640 - acc: 0.4524 - val_loss: 1.9061 - val_acc: 0.3485\n",
      "Epoch 135/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5702 - acc: 0.4549 - val_loss: 1.9195 - val_acc: 0.3740\n",
      "Epoch 136/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5626 - acc: 0.4562 - val_loss: 1.9013 - val_acc: 0.3635\n",
      "Epoch 137/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5603 - acc: 0.4606 - val_loss: 1.9157 - val_acc: 0.3605\n",
      "Epoch 138/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5700 - acc: 0.4542 - val_loss: 1.8791 - val_acc: 0.3625\n",
      "Epoch 139/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.5648 - acc: 0.4489 - val_loss: 1.9444 - val_acc: 0.3590\n",
      "Epoch 140/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.5618 - acc: 0.4544 - val_loss: 1.9030 - val_acc: 0.3535\n",
      "Epoch 141/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.5616 - acc: 0.4600 - val_loss: 2.0377 - val_acc: 0.3315\n",
      "Epoch 142/2000\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 1.5651 - acc: 0.4631 - val_loss: 2.0389 - val_acc: 0.3250\n",
      "Epoch 143/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5567 - acc: 0.4616 - val_loss: 1.8547 - val_acc: 0.3600\n",
      "Epoch 144/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.5563 - acc: 0.4597 - val_loss: 1.9774 - val_acc: 0.3385\n",
      "Epoch 145/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5556 - acc: 0.4619 - val_loss: 1.9048 - val_acc: 0.3815\n",
      "Epoch 146/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5590 - acc: 0.4650 - val_loss: 1.9035 - val_acc: 0.3580\n",
      "Epoch 147/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5625 - acc: 0.4595 - val_loss: 1.8817 - val_acc: 0.3620\n",
      "Epoch 148/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5509 - acc: 0.4575 - val_loss: 1.8964 - val_acc: 0.3595\n",
      "Epoch 149/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5525 - acc: 0.4639 - val_loss: 1.9110 - val_acc: 0.3605\n",
      "Epoch 150/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.5549 - acc: 0.4636 - val_loss: 1.8969 - val_acc: 0.3545\n",
      "Epoch 151/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.5517 - acc: 0.4625 - val_loss: 2.0094 - val_acc: 0.3375\n",
      "Epoch 152/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5493 - acc: 0.4611 - val_loss: 1.9564 - val_acc: 0.3605\n",
      "Epoch 153/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.5460 - acc: 0.4654 - val_loss: 1.9831 - val_acc: 0.3330\n",
      "Epoch 154/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.5465 - acc: 0.4604 - val_loss: 1.9531 - val_acc: 0.3435\n",
      "Epoch 155/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.5489 - acc: 0.4676 - val_loss: 1.9710 - val_acc: 0.3495\n",
      "Epoch 156/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.5481 - acc: 0.4641 - val_loss: 1.9331 - val_acc: 0.3490\n",
      "Epoch 157/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5439 - acc: 0.4660 - val_loss: 1.9047 - val_acc: 0.3540\n",
      "Epoch 158/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5526 - acc: 0.4576 - val_loss: 1.9299 - val_acc: 0.3520\n",
      "Epoch 159/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5465 - acc: 0.4549 - val_loss: 1.9331 - val_acc: 0.3520\n",
      "Epoch 160/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.5456 - acc: 0.4571 - val_loss: 1.9958 - val_acc: 0.3315\n",
      "Epoch 161/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5520 - acc: 0.4646 - val_loss: 1.9069 - val_acc: 0.3610\n",
      "Epoch 162/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.5426 - acc: 0.4602 - val_loss: 1.9621 - val_acc: 0.3320\n",
      "Epoch 163/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.5463 - acc: 0.4656 - val_loss: 1.8840 - val_acc: 0.3540\n",
      "Epoch 164/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5394 - acc: 0.4606 - val_loss: 1.8901 - val_acc: 0.3570\n",
      "Epoch 165/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5363 - acc: 0.4641 - val_loss: 1.8755 - val_acc: 0.3645\n",
      "Epoch 166/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5393 - acc: 0.4627 - val_loss: 1.8991 - val_acc: 0.3695\n",
      "Epoch 167/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5425 - acc: 0.4723 - val_loss: 1.9482 - val_acc: 0.3540\n",
      "Epoch 168/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5393 - acc: 0.4705 - val_loss: 1.8987 - val_acc: 0.3515\n",
      "Epoch 169/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.5421 - acc: 0.4650 - val_loss: 1.9039 - val_acc: 0.3565\n",
      "Epoch 170/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5351 - acc: 0.4688 - val_loss: 2.0122 - val_acc: 0.3425\n",
      "Epoch 171/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5350 - acc: 0.4664 - val_loss: 1.9677 - val_acc: 0.3365\n",
      "Epoch 172/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5328 - acc: 0.4644 - val_loss: 1.8859 - val_acc: 0.3740\n",
      "Epoch 173/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.5299 - acc: 0.4704 - val_loss: 1.9800 - val_acc: 0.3440\n",
      "Epoch 174/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5377 - acc: 0.4676 - val_loss: 1.8876 - val_acc: 0.3600\n",
      "Epoch 175/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5252 - acc: 0.4702 - val_loss: 1.9100 - val_acc: 0.3545\n",
      "Epoch 176/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5305 - acc: 0.4713 - val_loss: 1.8855 - val_acc: 0.3635\n",
      "Epoch 177/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5241 - acc: 0.4710 - val_loss: 1.9822 - val_acc: 0.3440\n",
      "Epoch 178/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5274 - acc: 0.4718 - val_loss: 1.9250 - val_acc: 0.3640\n",
      "Epoch 179/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5214 - acc: 0.4688 - val_loss: 1.9744 - val_acc: 0.3485\n",
      "Epoch 180/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5278 - acc: 0.4744 - val_loss: 1.9119 - val_acc: 0.3480\n",
      "Epoch 181/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5369 - acc: 0.4654 - val_loss: 1.9401 - val_acc: 0.3500\n",
      "Epoch 182/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.5175 - acc: 0.4756 - val_loss: 1.9467 - val_acc: 0.3520\n",
      "Epoch 183/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5099 - acc: 0.4786 - val_loss: 1.9531 - val_acc: 0.3575\n",
      "Epoch 184/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5283 - acc: 0.4695 - val_loss: 1.9564 - val_acc: 0.3495\n",
      "Epoch 185/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5161 - acc: 0.4795 - val_loss: 1.9798 - val_acc: 0.3445\n",
      "Epoch 186/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5152 - acc: 0.4789 - val_loss: 1.9636 - val_acc: 0.3395\n",
      "Epoch 187/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5193 - acc: 0.4754 - val_loss: 1.9203 - val_acc: 0.3720\n",
      "Epoch 188/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5103 - acc: 0.4769 - val_loss: 1.8850 - val_acc: 0.3695\n",
      "Epoch 189/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5182 - acc: 0.4734 - val_loss: 1.9105 - val_acc: 0.3520\n",
      "Epoch 190/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5200 - acc: 0.4765 - val_loss: 1.9185 - val_acc: 0.3655\n",
      "Epoch 191/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5139 - acc: 0.4761 - val_loss: 1.9720 - val_acc: 0.3560\n",
      "Epoch 192/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5181 - acc: 0.4695 - val_loss: 1.9818 - val_acc: 0.3485\n",
      "Epoch 193/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.5194 - acc: 0.4711 - val_loss: 1.9439 - val_acc: 0.3485\n",
      "Epoch 194/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5180 - acc: 0.4700 - val_loss: 1.9732 - val_acc: 0.3510\n",
      "Epoch 195/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5174 - acc: 0.4760 - val_loss: 1.9492 - val_acc: 0.3395\n",
      "Epoch 196/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5114 - acc: 0.4800 - val_loss: 1.9705 - val_acc: 0.3355\n",
      "Epoch 197/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5028 - acc: 0.4824 - val_loss: 2.0000 - val_acc: 0.3510\n",
      "Epoch 198/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.5178 - acc: 0.4718 - val_loss: 1.9104 - val_acc: 0.3635\n",
      "Epoch 199/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.5149 - acc: 0.4746 - val_loss: 1.9319 - val_acc: 0.3540\n",
      "Epoch 200/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.5133 - acc: 0.4729 - val_loss: 1.9248 - val_acc: 0.3580\n",
      "Epoch 201/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.5152 - acc: 0.4743 - val_loss: 1.9024 - val_acc: 0.3615\n",
      "Epoch 202/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5065 - acc: 0.4725 - val_loss: 1.8962 - val_acc: 0.3740\n",
      "Epoch 203/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5111 - acc: 0.4720 - val_loss: 2.0854 - val_acc: 0.3260\n",
      "Epoch 204/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.5027 - acc: 0.4799 - val_loss: 1.9300 - val_acc: 0.3680\n",
      "Epoch 205/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.4957 - acc: 0.4805 - val_loss: 1.9258 - val_acc: 0.3600\n",
      "Epoch 206/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4975 - acc: 0.4843 - val_loss: 1.9791 - val_acc: 0.3440\n",
      "Epoch 207/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.5132 - acc: 0.4773 - val_loss: 1.9361 - val_acc: 0.3655\n",
      "Epoch 208/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5123 - acc: 0.4764 - val_loss: 1.8998 - val_acc: 0.3535\n",
      "Epoch 209/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4998 - acc: 0.4839 - val_loss: 2.0332 - val_acc: 0.3285\n",
      "Epoch 210/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4977 - acc: 0.4830 - val_loss: 1.9871 - val_acc: 0.3455\n",
      "Epoch 211/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4932 - acc: 0.4840 - val_loss: 1.9934 - val_acc: 0.3390\n",
      "Epoch 212/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4944 - acc: 0.4739 - val_loss: 1.9392 - val_acc: 0.3610\n",
      "Epoch 213/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4988 - acc: 0.4818 - val_loss: 2.0704 - val_acc: 0.3175\n",
      "Epoch 214/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5034 - acc: 0.4803 - val_loss: 1.9708 - val_acc: 0.3450\n",
      "Epoch 215/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4990 - acc: 0.4835 - val_loss: 1.9262 - val_acc: 0.3600\n",
      "Epoch 216/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5006 - acc: 0.4848 - val_loss: 1.9890 - val_acc: 0.3435\n",
      "Epoch 217/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4953 - acc: 0.4854 - val_loss: 1.9348 - val_acc: 0.3585\n",
      "Epoch 218/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5015 - acc: 0.4785 - val_loss: 1.9219 - val_acc: 0.3655\n",
      "Epoch 219/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4953 - acc: 0.4819 - val_loss: 1.9492 - val_acc: 0.3525\n",
      "Epoch 220/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4966 - acc: 0.4864 - val_loss: 2.0604 - val_acc: 0.3180\n",
      "Epoch 221/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.4993 - acc: 0.4808 - val_loss: 1.9833 - val_acc: 0.3425\n",
      "Epoch 222/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4993 - acc: 0.4785 - val_loss: 2.1078 - val_acc: 0.3210\n",
      "Epoch 223/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4912 - acc: 0.4773 - val_loss: 1.9635 - val_acc: 0.3380\n",
      "Epoch 224/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4861 - acc: 0.4860 - val_loss: 1.9825 - val_acc: 0.3525\n",
      "Epoch 225/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4940 - acc: 0.4863 - val_loss: 1.9187 - val_acc: 0.3635\n",
      "Epoch 226/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4910 - acc: 0.4851 - val_loss: 1.9410 - val_acc: 0.3555\n",
      "Epoch 227/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4875 - acc: 0.4844 - val_loss: 1.9186 - val_acc: 0.3535\n",
      "Epoch 228/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4837 - acc: 0.4895 - val_loss: 1.9883 - val_acc: 0.3415\n",
      "Epoch 229/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4837 - acc: 0.4895 - val_loss: 1.8964 - val_acc: 0.3590\n",
      "Epoch 230/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4953 - acc: 0.4863 - val_loss: 1.9996 - val_acc: 0.3420\n",
      "Epoch 231/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4899 - acc: 0.4822 - val_loss: 1.9843 - val_acc: 0.3445\n",
      "Epoch 232/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4960 - acc: 0.4783 - val_loss: 2.0036 - val_acc: 0.3365\n",
      "Epoch 233/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4781 - acc: 0.4854 - val_loss: 1.9728 - val_acc: 0.3575\n",
      "Epoch 234/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4848 - acc: 0.4870 - val_loss: 1.9796 - val_acc: 0.3550\n",
      "Epoch 235/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4757 - acc: 0.4880 - val_loss: 2.0934 - val_acc: 0.3120\n",
      "Epoch 236/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4862 - acc: 0.4806 - val_loss: 1.9754 - val_acc: 0.3500\n",
      "Epoch 237/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4816 - acc: 0.4834 - val_loss: 2.0079 - val_acc: 0.3415\n",
      "Epoch 238/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4775 - acc: 0.4890 - val_loss: 1.9388 - val_acc: 0.3645\n",
      "Epoch 239/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4884 - acc: 0.4775 - val_loss: 2.0760 - val_acc: 0.3240\n",
      "Epoch 240/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4818 - acc: 0.4873 - val_loss: 2.0012 - val_acc: 0.3360\n",
      "Epoch 241/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4860 - acc: 0.4827 - val_loss: 1.9593 - val_acc: 0.3600\n",
      "Epoch 242/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4751 - acc: 0.4875 - val_loss: 2.0525 - val_acc: 0.3370\n",
      "Epoch 243/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4858 - acc: 0.4864 - val_loss: 1.9840 - val_acc: 0.3465\n",
      "Epoch 244/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4772 - acc: 0.4939 - val_loss: 1.9761 - val_acc: 0.3530\n",
      "Epoch 245/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4844 - acc: 0.4933 - val_loss: 1.9226 - val_acc: 0.3670\n",
      "Epoch 246/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4793 - acc: 0.4854 - val_loss: 1.9865 - val_acc: 0.3530\n",
      "Epoch 247/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4667 - acc: 0.4954 - val_loss: 1.9858 - val_acc: 0.3520\n",
      "Epoch 248/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4688 - acc: 0.4885 - val_loss: 1.9988 - val_acc: 0.3420\n",
      "Epoch 249/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4767 - acc: 0.4886 - val_loss: 1.9919 - val_acc: 0.3465\n",
      "Epoch 250/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4722 - acc: 0.4898 - val_loss: 1.9756 - val_acc: 0.3490\n",
      "Epoch 251/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4680 - acc: 0.4912 - val_loss: 1.9723 - val_acc: 0.3490\n",
      "Epoch 252/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4754 - acc: 0.4974 - val_loss: 1.9990 - val_acc: 0.3510\n",
      "Epoch 253/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4702 - acc: 0.4901 - val_loss: 1.9797 - val_acc: 0.3585\n",
      "Epoch 254/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4653 - acc: 0.4911 - val_loss: 2.1604 - val_acc: 0.3250\n",
      "Epoch 255/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4665 - acc: 0.4970 - val_loss: 1.9970 - val_acc: 0.3600\n",
      "Epoch 256/2000\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 1.4696 - acc: 0.4904 - val_loss: 2.0547 - val_acc: 0.3525\n",
      "Epoch 257/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.4683 - acc: 0.4904 - val_loss: 2.0248 - val_acc: 0.3505\n",
      "Epoch 258/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4652 - acc: 0.4947 - val_loss: 1.9639 - val_acc: 0.3605\n",
      "Epoch 259/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.4720 - acc: 0.4931 - val_loss: 1.9868 - val_acc: 0.3540\n",
      "Epoch 260/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4692 - acc: 0.4861 - val_loss: 2.0471 - val_acc: 0.3380\n",
      "Epoch 261/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4675 - acc: 0.4915 - val_loss: 2.0409 - val_acc: 0.3450\n",
      "Epoch 262/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4613 - acc: 0.4909 - val_loss: 2.2580 - val_acc: 0.3125\n",
      "Epoch 263/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4792 - acc: 0.4885 - val_loss: 1.9838 - val_acc: 0.3510\n",
      "Epoch 264/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4624 - acc: 0.4926 - val_loss: 1.9772 - val_acc: 0.3625\n",
      "Epoch 265/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.4671 - acc: 0.4841 - val_loss: 1.9253 - val_acc: 0.3575\n",
      "Epoch 266/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 1.4605 - acc: 0.4975 - val_loss: 1.9988 - val_acc: 0.3510\n",
      "Epoch 267/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.4604 - acc: 0.4863 - val_loss: 1.9674 - val_acc: 0.3440\n",
      "Epoch 268/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.4621 - acc: 0.4901 - val_loss: 1.9687 - val_acc: 0.3645\n",
      "Epoch 269/2000\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 1.4681 - acc: 0.4910 - val_loss: 1.9933 - val_acc: 0.3465\n",
      "Epoch 270/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4712 - acc: 0.4794 - val_loss: 1.9984 - val_acc: 0.3485\n",
      "Epoch 271/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4739 - acc: 0.4845 - val_loss: 1.9670 - val_acc: 0.3515\n",
      "Epoch 272/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4721 - acc: 0.4966 - val_loss: 1.9870 - val_acc: 0.3420\n",
      "Epoch 273/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4564 - acc: 0.4970 - val_loss: 2.0417 - val_acc: 0.3380\n",
      "Epoch 274/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4655 - acc: 0.4956 - val_loss: 1.9849 - val_acc: 0.3505\n",
      "Epoch 275/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.4669 - acc: 0.4930 - val_loss: 1.9779 - val_acc: 0.3500\n",
      "Epoch 276/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4554 - acc: 0.4991 - val_loss: 1.9821 - val_acc: 0.3445\n",
      "Epoch 277/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4569 - acc: 0.4928 - val_loss: 1.9813 - val_acc: 0.3555\n",
      "Epoch 278/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4543 - acc: 0.4977 - val_loss: 1.9920 - val_acc: 0.3375\n",
      "Epoch 279/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4576 - acc: 0.4905 - val_loss: 2.0634 - val_acc: 0.3575\n",
      "Epoch 280/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4575 - acc: 0.4949 - val_loss: 1.9634 - val_acc: 0.3570\n",
      "Epoch 281/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4491 - acc: 0.4972 - val_loss: 2.0493 - val_acc: 0.3295\n",
      "Epoch 282/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4567 - acc: 0.4926 - val_loss: 2.0671 - val_acc: 0.3405\n",
      "Epoch 283/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4434 - acc: 0.5076 - val_loss: 2.0217 - val_acc: 0.3385\n",
      "Epoch 284/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4560 - acc: 0.5050 - val_loss: 2.0481 - val_acc: 0.3480\n",
      "Epoch 285/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4524 - acc: 0.4985 - val_loss: 2.0253 - val_acc: 0.3530\n",
      "Epoch 286/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4522 - acc: 0.4926 - val_loss: 2.0811 - val_acc: 0.3390\n",
      "Epoch 287/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4453 - acc: 0.5021 - val_loss: 2.0150 - val_acc: 0.3400\n",
      "Epoch 288/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4483 - acc: 0.5001 - val_loss: 2.0586 - val_acc: 0.3305\n",
      "Epoch 289/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4509 - acc: 0.5008 - val_loss: 2.0361 - val_acc: 0.3465\n",
      "Epoch 290/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4596 - acc: 0.4989 - val_loss: 2.1533 - val_acc: 0.3195\n",
      "Epoch 291/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4479 - acc: 0.4976 - val_loss: 2.0816 - val_acc: 0.3315\n",
      "Epoch 292/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4519 - acc: 0.4954 - val_loss: 2.0436 - val_acc: 0.3425\n",
      "Epoch 293/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4532 - acc: 0.4929 - val_loss: 1.9744 - val_acc: 0.3600\n",
      "Epoch 294/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4559 - acc: 0.4986 - val_loss: 2.0707 - val_acc: 0.3310\n",
      "Epoch 295/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4424 - acc: 0.4989 - val_loss: 2.0461 - val_acc: 0.3270\n",
      "Epoch 296/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4405 - acc: 0.5015 - val_loss: 2.0526 - val_acc: 0.3330\n",
      "Epoch 297/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4431 - acc: 0.4976 - val_loss: 2.0983 - val_acc: 0.3425\n",
      "Epoch 298/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4497 - acc: 0.5052 - val_loss: 1.9793 - val_acc: 0.3515\n",
      "Epoch 299/2000\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 1.4410 - acc: 0.5042 - val_loss: 2.0630 - val_acc: 0.3380\n",
      "Epoch 300/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4449 - acc: 0.5004 - val_loss: 1.9646 - val_acc: 0.3570\n",
      "Epoch 301/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4530 - acc: 0.5014 - val_loss: 2.0553 - val_acc: 0.3250\n",
      "Epoch 302/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4407 - acc: 0.5076 - val_loss: 2.1221 - val_acc: 0.3310\n",
      "Epoch 303/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.4445 - acc: 0.4961 - val_loss: 2.0376 - val_acc: 0.3460\n",
      "Epoch 304/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4360 - acc: 0.4994 - val_loss: 2.2332 - val_acc: 0.3155\n",
      "Epoch 305/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4421 - acc: 0.5092 - val_loss: 2.1078 - val_acc: 0.3275\n",
      "Epoch 306/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4400 - acc: 0.5012 - val_loss: 1.9868 - val_acc: 0.3510\n",
      "Epoch 307/2000\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 1.4465 - acc: 0.5041 - val_loss: 2.0739 - val_acc: 0.3375\n",
      "Epoch 308/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.4305 - acc: 0.5048 - val_loss: 2.0452 - val_acc: 0.3365\n",
      "Epoch 309/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.4457 - acc: 0.4989 - val_loss: 2.1060 - val_acc: 0.3370\n",
      "Epoch 310/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4496 - acc: 0.4909 - val_loss: 2.0249 - val_acc: 0.3470\n",
      "Epoch 311/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.4333 - acc: 0.5039 - val_loss: 2.0971 - val_acc: 0.3440\n",
      "Epoch 312/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.4337 - acc: 0.5014 - val_loss: 2.1174 - val_acc: 0.3480\n",
      "Epoch 313/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.4352 - acc: 0.5002 - val_loss: 2.0003 - val_acc: 0.3375\n",
      "Epoch 314/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.4401 - acc: 0.5015 - val_loss: 2.0188 - val_acc: 0.3335\n",
      "Epoch 315/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.4414 - acc: 0.5039 - val_loss: 1.9904 - val_acc: 0.3515\n",
      "Epoch 316/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.4399 - acc: 0.5068 - val_loss: 2.0410 - val_acc: 0.3370\n",
      "Epoch 317/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4385 - acc: 0.5008 - val_loss: 2.1204 - val_acc: 0.3310\n",
      "Epoch 318/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4321 - acc: 0.5065 - val_loss: 2.1270 - val_acc: 0.3310\n",
      "Epoch 319/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4357 - acc: 0.5080 - val_loss: 2.0549 - val_acc: 0.3410\n",
      "Epoch 320/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4371 - acc: 0.5002 - val_loss: 2.0712 - val_acc: 0.3340\n",
      "Epoch 321/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4393 - acc: 0.5022 - val_loss: 2.0256 - val_acc: 0.3380\n",
      "Epoch 322/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4282 - acc: 0.5080 - val_loss: 2.0097 - val_acc: 0.3580\n",
      "Epoch 323/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4315 - acc: 0.5072 - val_loss: 2.0493 - val_acc: 0.3420\n",
      "Epoch 324/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4352 - acc: 0.5054 - val_loss: 1.9925 - val_acc: 0.3440\n",
      "Epoch 325/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4345 - acc: 0.4976 - val_loss: 2.0416 - val_acc: 0.3435\n",
      "Epoch 326/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4314 - acc: 0.5054 - val_loss: 2.0709 - val_acc: 0.3365\n",
      "Epoch 327/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4264 - acc: 0.5076 - val_loss: 2.1118 - val_acc: 0.3365\n",
      "Epoch 328/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4271 - acc: 0.5098 - val_loss: 2.1323 - val_acc: 0.3395\n",
      "Epoch 329/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4293 - acc: 0.5075 - val_loss: 2.0335 - val_acc: 0.3475\n",
      "Epoch 330/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4376 - acc: 0.5046 - val_loss: 2.0720 - val_acc: 0.3445\n",
      "Epoch 331/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4452 - acc: 0.5052 - val_loss: 2.1488 - val_acc: 0.3170\n",
      "Epoch 332/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4219 - acc: 0.5054 - val_loss: 2.0217 - val_acc: 0.3500\n",
      "Epoch 333/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4294 - acc: 0.5059 - val_loss: 2.0463 - val_acc: 0.3310\n",
      "Epoch 334/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4373 - acc: 0.5019 - val_loss: 2.0241 - val_acc: 0.3435\n",
      "Epoch 335/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4261 - acc: 0.5103 - val_loss: 2.1583 - val_acc: 0.3385\n",
      "Epoch 336/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4293 - acc: 0.5031 - val_loss: 2.0066 - val_acc: 0.3555\n",
      "Epoch 337/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4231 - acc: 0.5076 - val_loss: 2.1577 - val_acc: 0.3200\n",
      "Epoch 338/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4232 - acc: 0.5086 - val_loss: 1.9997 - val_acc: 0.3590\n",
      "Epoch 339/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4252 - acc: 0.5076 - val_loss: 2.0453 - val_acc: 0.3410\n",
      "Epoch 340/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4183 - acc: 0.5119 - val_loss: 2.2437 - val_acc: 0.3040\n",
      "Epoch 341/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4229 - acc: 0.5119 - val_loss: 2.0354 - val_acc: 0.3460\n",
      "Epoch 342/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4204 - acc: 0.5099 - val_loss: 2.0732 - val_acc: 0.3395\n",
      "Epoch 343/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4266 - acc: 0.5011 - val_loss: 2.1107 - val_acc: 0.3390\n",
      "Epoch 344/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4173 - acc: 0.5094 - val_loss: 2.1305 - val_acc: 0.3265\n",
      "Epoch 345/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4181 - acc: 0.5075 - val_loss: 2.0025 - val_acc: 0.3435\n",
      "Epoch 346/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4174 - acc: 0.5090 - val_loss: 2.2816 - val_acc: 0.3050\n",
      "Epoch 347/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.4231 - acc: 0.5051 - val_loss: 2.0365 - val_acc: 0.3385\n",
      "Epoch 348/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.4333 - acc: 0.5052 - val_loss: 2.0531 - val_acc: 0.3510\n",
      "Epoch 349/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.4210 - acc: 0.5081 - val_loss: 2.0038 - val_acc: 0.3520\n",
      "Epoch 350/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.4234 - acc: 0.5109 - val_loss: 2.2539 - val_acc: 0.3095\n",
      "Epoch 351/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4263 - acc: 0.5028 - val_loss: 2.0033 - val_acc: 0.3530\n",
      "Epoch 352/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4296 - acc: 0.5074 - val_loss: 2.0785 - val_acc: 0.3405\n",
      "Epoch 353/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4281 - acc: 0.5142 - val_loss: 1.9978 - val_acc: 0.3475\n",
      "Epoch 354/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4192 - acc: 0.5154 - val_loss: 2.1611 - val_acc: 0.3380\n",
      "Epoch 355/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4195 - acc: 0.5108 - val_loss: 2.0355 - val_acc: 0.3515\n",
      "Epoch 356/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4162 - acc: 0.5121 - val_loss: 2.1395 - val_acc: 0.3375\n",
      "Epoch 357/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4151 - acc: 0.5118 - val_loss: 2.0530 - val_acc: 0.3450\n",
      "Epoch 358/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4107 - acc: 0.5131 - val_loss: 2.0674 - val_acc: 0.3395\n",
      "Epoch 359/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4208 - acc: 0.5084 - val_loss: 2.0261 - val_acc: 0.3465\n",
      "Epoch 360/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4160 - acc: 0.5108 - val_loss: 2.0466 - val_acc: 0.3540\n",
      "Epoch 361/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4176 - acc: 0.5065 - val_loss: 2.1032 - val_acc: 0.3270\n",
      "Epoch 362/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4173 - acc: 0.5074 - val_loss: 2.1049 - val_acc: 0.3390\n",
      "Epoch 363/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4219 - acc: 0.5145 - val_loss: 2.0129 - val_acc: 0.3525\n",
      "Epoch 364/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4213 - acc: 0.5109 - val_loss: 2.1514 - val_acc: 0.3095\n",
      "Epoch 365/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4120 - acc: 0.5049 - val_loss: 2.1350 - val_acc: 0.3190\n",
      "Epoch 366/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4024 - acc: 0.5130 - val_loss: 2.0590 - val_acc: 0.3300\n",
      "Epoch 367/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4146 - acc: 0.5152 - val_loss: 2.0299 - val_acc: 0.3435\n",
      "Epoch 368/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.4034 - acc: 0.5204 - val_loss: 2.0022 - val_acc: 0.3525\n",
      "Epoch 369/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4099 - acc: 0.5114 - val_loss: 2.0172 - val_acc: 0.3515\n",
      "Epoch 370/2000\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 1.4090 - acc: 0.5147 - val_loss: 2.0433 - val_acc: 0.3435\n",
      "Epoch 371/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.4167 - acc: 0.5103 - val_loss: 2.0701 - val_acc: 0.3420\n",
      "Epoch 372/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.4132 - acc: 0.5124 - val_loss: 2.0256 - val_acc: 0.3565\n",
      "Epoch 373/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4128 - acc: 0.5140 - val_loss: 2.0950 - val_acc: 0.3320\n",
      "Epoch 374/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4148 - acc: 0.5112 - val_loss: 2.0518 - val_acc: 0.3400\n",
      "Epoch 375/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4114 - acc: 0.5160 - val_loss: 2.1524 - val_acc: 0.3250\n",
      "Epoch 376/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4023 - acc: 0.5134 - val_loss: 2.0433 - val_acc: 0.3500\n",
      "Epoch 377/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.4091 - acc: 0.5111 - val_loss: 2.0471 - val_acc: 0.3460\n",
      "Epoch 378/2000\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 1.4116 - acc: 0.5125 - val_loss: 2.1642 - val_acc: 0.3355\n",
      "Epoch 379/2000\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 1.4163 - acc: 0.5166 - val_loss: 2.0420 - val_acc: 0.3410\n",
      "Epoch 380/2000\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 1.4077 - acc: 0.5119 - val_loss: 2.2041 - val_acc: 0.3160\n",
      "Epoch 381/2000\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 1.4164 - acc: 0.5080 - val_loss: 2.0913 - val_acc: 0.3450\n",
      "Epoch 382/2000\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 1.4164 - acc: 0.5125 - val_loss: 2.0611 - val_acc: 0.3430\n",
      "Epoch 383/2000\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 1.4224 - acc: 0.5101 - val_loss: 2.0024 - val_acc: 0.3620\n",
      "Epoch 384/2000\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 1.4097 - acc: 0.5095 - val_loss: 2.1386 - val_acc: 0.3310\n",
      "Epoch 385/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.4052 - acc: 0.5130 - val_loss: 2.1784 - val_acc: 0.3170\n",
      "Epoch 386/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3929 - acc: 0.5191 - val_loss: 1.9951 - val_acc: 0.3615\n",
      "Epoch 387/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4012 - acc: 0.5159 - val_loss: 2.0078 - val_acc: 0.3665\n",
      "Epoch 388/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.4105 - acc: 0.5118 - val_loss: 2.1350 - val_acc: 0.3395\n",
      "Epoch 389/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4073 - acc: 0.5163 - val_loss: 2.0349 - val_acc: 0.3440\n",
      "Epoch 390/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.4078 - acc: 0.5069 - val_loss: 2.2477 - val_acc: 0.3060\n",
      "Epoch 391/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4045 - acc: 0.5108 - val_loss: 2.0776 - val_acc: 0.3360\n",
      "Epoch 392/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.4057 - acc: 0.5166 - val_loss: 2.0959 - val_acc: 0.3345\n",
      "Epoch 393/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.4036 - acc: 0.5129 - val_loss: 2.1529 - val_acc: 0.3205\n",
      "Epoch 394/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4024 - acc: 0.5120 - val_loss: 2.1185 - val_acc: 0.3325\n",
      "Epoch 395/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4042 - acc: 0.5137 - val_loss: 2.0406 - val_acc: 0.3535\n",
      "Epoch 396/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4063 - acc: 0.5144 - val_loss: 2.1054 - val_acc: 0.3345\n",
      "Epoch 397/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4051 - acc: 0.5119 - val_loss: 2.2202 - val_acc: 0.3085\n",
      "Epoch 398/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4002 - acc: 0.5158 - val_loss: 2.1316 - val_acc: 0.3285\n",
      "Epoch 399/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3991 - acc: 0.5109 - val_loss: 2.0585 - val_acc: 0.3380\n",
      "Epoch 400/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4026 - acc: 0.5191 - val_loss: 2.1321 - val_acc: 0.3380\n",
      "Epoch 401/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4036 - acc: 0.5159 - val_loss: 2.1064 - val_acc: 0.3375\n",
      "Epoch 402/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4031 - acc: 0.5152 - val_loss: 2.0783 - val_acc: 0.3440\n",
      "Epoch 403/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3914 - acc: 0.5179 - val_loss: 2.1032 - val_acc: 0.3395\n",
      "Epoch 404/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3920 - acc: 0.5159 - val_loss: 2.0844 - val_acc: 0.3410\n",
      "Epoch 405/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4044 - acc: 0.5114 - val_loss: 2.0627 - val_acc: 0.3500\n",
      "Epoch 406/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3959 - acc: 0.5147 - val_loss: 2.0704 - val_acc: 0.3570\n",
      "Epoch 407/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3878 - acc: 0.5212 - val_loss: 2.1126 - val_acc: 0.3375\n",
      "Epoch 408/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3939 - acc: 0.5186 - val_loss: 2.0844 - val_acc: 0.3260\n",
      "Epoch 409/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3958 - acc: 0.5166 - val_loss: 2.1866 - val_acc: 0.3205\n",
      "Epoch 410/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3983 - acc: 0.5197 - val_loss: 2.1633 - val_acc: 0.3180\n",
      "Epoch 411/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3996 - acc: 0.5119 - val_loss: 2.0783 - val_acc: 0.3480\n",
      "Epoch 412/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4036 - acc: 0.5186 - val_loss: 2.0305 - val_acc: 0.3575\n",
      "Epoch 413/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4045 - acc: 0.5121 - val_loss: 2.0772 - val_acc: 0.3430\n",
      "Epoch 414/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3863 - acc: 0.5242 - val_loss: 2.0881 - val_acc: 0.3350\n",
      "Epoch 415/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3858 - acc: 0.5181 - val_loss: 2.1681 - val_acc: 0.3290\n",
      "Epoch 416/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3911 - acc: 0.5180 - val_loss: 2.1383 - val_acc: 0.3335\n",
      "Epoch 417/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3908 - acc: 0.5224 - val_loss: 2.0818 - val_acc: 0.3400\n",
      "Epoch 418/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3945 - acc: 0.5221 - val_loss: 2.2899 - val_acc: 0.3095\n",
      "Epoch 419/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3894 - acc: 0.5163 - val_loss: 2.0850 - val_acc: 0.3365\n",
      "Epoch 420/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3886 - acc: 0.5185 - val_loss: 2.0826 - val_acc: 0.3390\n",
      "Epoch 421/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3995 - acc: 0.5150 - val_loss: 2.1627 - val_acc: 0.3365\n",
      "Epoch 422/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3980 - acc: 0.5202 - val_loss: 2.0421 - val_acc: 0.3475\n",
      "Epoch 423/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3912 - acc: 0.5266 - val_loss: 2.0920 - val_acc: 0.3305\n",
      "Epoch 424/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3958 - acc: 0.5131 - val_loss: 2.1164 - val_acc: 0.3355\n",
      "Epoch 425/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3834 - acc: 0.5240 - val_loss: 2.3462 - val_acc: 0.3100\n",
      "Epoch 426/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3877 - acc: 0.5195 - val_loss: 2.1727 - val_acc: 0.3360\n",
      "Epoch 427/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3784 - acc: 0.5161 - val_loss: 2.1427 - val_acc: 0.3385\n",
      "Epoch 428/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3907 - acc: 0.5226 - val_loss: 2.1629 - val_acc: 0.3220\n",
      "Epoch 429/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3871 - acc: 0.5220 - val_loss: 2.1030 - val_acc: 0.3335\n",
      "Epoch 430/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3858 - acc: 0.5181 - val_loss: 2.1193 - val_acc: 0.3270\n",
      "Epoch 431/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3748 - acc: 0.5276 - val_loss: 2.1272 - val_acc: 0.3370\n",
      "Epoch 432/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3798 - acc: 0.5223 - val_loss: 2.1397 - val_acc: 0.3375\n",
      "Epoch 433/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3917 - acc: 0.5235 - val_loss: 2.0617 - val_acc: 0.3385\n",
      "Epoch 434/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3815 - acc: 0.5207 - val_loss: 2.0875 - val_acc: 0.3445\n",
      "Epoch 435/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3844 - acc: 0.5214 - val_loss: 2.2730 - val_acc: 0.3225\n",
      "Epoch 436/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3891 - acc: 0.5274 - val_loss: 2.1316 - val_acc: 0.3360\n",
      "Epoch 437/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3824 - acc: 0.5165 - val_loss: 2.0999 - val_acc: 0.3380\n",
      "Epoch 438/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3714 - acc: 0.5214 - val_loss: 2.3600 - val_acc: 0.3030\n",
      "Epoch 439/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.3876 - acc: 0.5204 - val_loss: 2.1276 - val_acc: 0.3350\n",
      "Epoch 440/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3792 - acc: 0.5266 - val_loss: 2.3752 - val_acc: 0.3055\n",
      "Epoch 441/2000\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 1.3858 - acc: 0.5179 - val_loss: 2.1304 - val_acc: 0.3265\n",
      "Epoch 442/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.3842 - acc: 0.5274 - val_loss: 2.0803 - val_acc: 0.3520\n",
      "Epoch 443/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3867 - acc: 0.5254 - val_loss: 2.1876 - val_acc: 0.3270\n",
      "Epoch 444/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3938 - acc: 0.5200 - val_loss: 2.1595 - val_acc: 0.3250\n",
      "Epoch 445/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3735 - acc: 0.5267 - val_loss: 2.1120 - val_acc: 0.3410\n",
      "Epoch 446/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3794 - acc: 0.5234 - val_loss: 2.1234 - val_acc: 0.3200\n",
      "Epoch 447/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3755 - acc: 0.5241 - val_loss: 2.1213 - val_acc: 0.3305\n",
      "Epoch 448/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3703 - acc: 0.5288 - val_loss: 2.0605 - val_acc: 0.3425\n",
      "Epoch 449/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3783 - acc: 0.5216 - val_loss: 2.0890 - val_acc: 0.3315\n",
      "Epoch 450/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3743 - acc: 0.5232 - val_loss: 2.0850 - val_acc: 0.3460\n",
      "Epoch 451/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3862 - acc: 0.5151 - val_loss: 2.1493 - val_acc: 0.3390\n",
      "Epoch 452/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3723 - acc: 0.5217 - val_loss: 2.1695 - val_acc: 0.3320\n",
      "Epoch 453/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3705 - acc: 0.5272 - val_loss: 2.1369 - val_acc: 0.3290\n",
      "Epoch 454/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3811 - acc: 0.5159 - val_loss: 2.1200 - val_acc: 0.3320\n",
      "Epoch 455/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3731 - acc: 0.5315 - val_loss: 2.0812 - val_acc: 0.3370\n",
      "Epoch 456/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3641 - acc: 0.5336 - val_loss: 2.0990 - val_acc: 0.3345\n",
      "Epoch 457/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3680 - acc: 0.5236 - val_loss: 2.2567 - val_acc: 0.3195\n",
      "Epoch 458/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3680 - acc: 0.5240 - val_loss: 2.1005 - val_acc: 0.3355\n",
      "Epoch 459/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3673 - acc: 0.5235 - val_loss: 2.2341 - val_acc: 0.3080\n",
      "Epoch 460/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3752 - acc: 0.5226 - val_loss: 2.1419 - val_acc: 0.3240\n",
      "Epoch 461/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3725 - acc: 0.5211 - val_loss: 2.0848 - val_acc: 0.3355\n",
      "Epoch 462/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3660 - acc: 0.5248 - val_loss: 2.1662 - val_acc: 0.3325\n",
      "Epoch 463/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3856 - acc: 0.5194 - val_loss: 2.0853 - val_acc: 0.3455\n",
      "Epoch 464/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3694 - acc: 0.5284 - val_loss: 2.1677 - val_acc: 0.3295\n",
      "Epoch 465/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3707 - acc: 0.5269 - val_loss: 2.1551 - val_acc: 0.3340\n",
      "Epoch 466/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3735 - acc: 0.5186 - val_loss: 2.2642 - val_acc: 0.3030\n",
      "Epoch 467/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3744 - acc: 0.5280 - val_loss: 2.1274 - val_acc: 0.3430\n",
      "Epoch 468/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3841 - acc: 0.5160 - val_loss: 2.0516 - val_acc: 0.3445\n",
      "Epoch 469/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3564 - acc: 0.5276 - val_loss: 2.0809 - val_acc: 0.3435\n",
      "Epoch 470/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3716 - acc: 0.5285 - val_loss: 2.1116 - val_acc: 0.3340\n",
      "Epoch 471/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3646 - acc: 0.5336 - val_loss: 2.2252 - val_acc: 0.3190\n",
      "Epoch 472/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3615 - acc: 0.5272 - val_loss: 2.1335 - val_acc: 0.3420\n",
      "Epoch 473/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3643 - acc: 0.5351 - val_loss: 2.1906 - val_acc: 0.3335\n",
      "Epoch 474/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3684 - acc: 0.5224 - val_loss: 2.1667 - val_acc: 0.3305\n",
      "Epoch 475/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3661 - acc: 0.5283 - val_loss: 2.0673 - val_acc: 0.3515\n",
      "Epoch 476/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3580 - acc: 0.5272 - val_loss: 2.1956 - val_acc: 0.3265\n",
      "Epoch 477/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3642 - acc: 0.5341 - val_loss: 2.1292 - val_acc: 0.3315\n",
      "Epoch 478/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3635 - acc: 0.5269 - val_loss: 2.1970 - val_acc: 0.3345\n",
      "Epoch 479/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3723 - acc: 0.5261 - val_loss: 2.0668 - val_acc: 0.3480\n",
      "Epoch 480/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3623 - acc: 0.5358 - val_loss: 2.2111 - val_acc: 0.3140\n",
      "Epoch 481/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3765 - acc: 0.5271 - val_loss: 2.1332 - val_acc: 0.3430\n",
      "Epoch 482/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3674 - acc: 0.5314 - val_loss: 2.0600 - val_acc: 0.3480\n",
      "Epoch 483/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3530 - acc: 0.5280 - val_loss: 2.1473 - val_acc: 0.3320\n",
      "Epoch 484/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3619 - acc: 0.5264 - val_loss: 2.0949 - val_acc: 0.3400\n",
      "Epoch 485/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3632 - acc: 0.5244 - val_loss: 2.2035 - val_acc: 0.3360\n",
      "Epoch 486/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3728 - acc: 0.5246 - val_loss: 2.0720 - val_acc: 0.3405\n",
      "Epoch 487/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3572 - acc: 0.5283 - val_loss: 2.1258 - val_acc: 0.3410\n",
      "Epoch 488/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3649 - acc: 0.5271 - val_loss: 2.1337 - val_acc: 0.3320\n",
      "Epoch 489/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3666 - acc: 0.5274 - val_loss: 2.1730 - val_acc: 0.3240\n",
      "Epoch 490/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3642 - acc: 0.5317 - val_loss: 2.0963 - val_acc: 0.3415\n",
      "Epoch 491/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3639 - acc: 0.5320 - val_loss: 2.2348 - val_acc: 0.3270\n",
      "Epoch 492/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3669 - acc: 0.5279 - val_loss: 2.2067 - val_acc: 0.3255\n",
      "Epoch 493/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3613 - acc: 0.5271 - val_loss: 2.1510 - val_acc: 0.3390\n",
      "Epoch 494/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3660 - acc: 0.5302 - val_loss: 2.1335 - val_acc: 0.3325\n",
      "Epoch 495/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3630 - acc: 0.5276 - val_loss: 2.1531 - val_acc: 0.3390\n",
      "Epoch 496/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3661 - acc: 0.5251 - val_loss: 2.1520 - val_acc: 0.3295\n",
      "Epoch 497/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3618 - acc: 0.5250 - val_loss: 2.0859 - val_acc: 0.3375\n",
      "Epoch 498/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3644 - acc: 0.5295 - val_loss: 2.1583 - val_acc: 0.3435\n",
      "Epoch 499/2000\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 1.3589 - acc: 0.5335 - val_loss: 2.1036 - val_acc: 0.3295\n",
      "Epoch 500/2000\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 1.3606 - acc: 0.5295 - val_loss: 2.1973 - val_acc: 0.3385\n",
      "Epoch 501/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3565 - acc: 0.5306 - val_loss: 2.1524 - val_acc: 0.3390\n",
      "Epoch 502/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3536 - acc: 0.5356 - val_loss: 2.0728 - val_acc: 0.3575\n",
      "Epoch 503/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3574 - acc: 0.5342 - val_loss: 2.1883 - val_acc: 0.3285\n",
      "Epoch 504/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3598 - acc: 0.5291 - val_loss: 2.1412 - val_acc: 0.3490\n",
      "Epoch 505/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.3686 - acc: 0.5274 - val_loss: 2.2070 - val_acc: 0.3240\n",
      "Epoch 506/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3620 - acc: 0.5344 - val_loss: 2.1344 - val_acc: 0.3360\n",
      "Epoch 507/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3571 - acc: 0.5277 - val_loss: 2.1745 - val_acc: 0.3340\n",
      "Epoch 508/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3560 - acc: 0.5276 - val_loss: 2.1274 - val_acc: 0.3545\n",
      "Epoch 509/2000\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 1.3528 - acc: 0.5250 - val_loss: 2.3359 - val_acc: 0.3090\n",
      "Epoch 510/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.3538 - acc: 0.5331 - val_loss: 2.2682 - val_acc: 0.3230\n",
      "Epoch 511/2000\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 1.3583 - acc: 0.5304 - val_loss: 2.1284 - val_acc: 0.3295\n",
      "Epoch 512/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3533 - acc: 0.5325 - val_loss: 2.1663 - val_acc: 0.3305\n",
      "Epoch 513/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3522 - acc: 0.5358 - val_loss: 2.1163 - val_acc: 0.3335\n",
      "Epoch 514/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3523 - acc: 0.5391 - val_loss: 2.1399 - val_acc: 0.3355\n",
      "Epoch 515/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3486 - acc: 0.5391 - val_loss: 2.1327 - val_acc: 0.3285\n",
      "Epoch 516/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3605 - acc: 0.5245 - val_loss: 2.1656 - val_acc: 0.3160\n",
      "Epoch 517/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3527 - acc: 0.5302 - val_loss: 2.1398 - val_acc: 0.3285\n",
      "Epoch 518/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3555 - acc: 0.5301 - val_loss: 2.2067 - val_acc: 0.3150\n",
      "Epoch 519/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3509 - acc: 0.5389 - val_loss: 2.2778 - val_acc: 0.3185\n",
      "Epoch 520/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3543 - acc: 0.5302 - val_loss: 2.2334 - val_acc: 0.3345\n",
      "Epoch 521/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3541 - acc: 0.5330 - val_loss: 2.1544 - val_acc: 0.3450\n",
      "Epoch 522/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3469 - acc: 0.5309 - val_loss: 2.1280 - val_acc: 0.3405\n",
      "Epoch 523/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3557 - acc: 0.5308 - val_loss: 2.1808 - val_acc: 0.3285\n",
      "Epoch 524/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3482 - acc: 0.5340 - val_loss: 2.1998 - val_acc: 0.3115\n",
      "Epoch 525/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3482 - acc: 0.5309 - val_loss: 2.1236 - val_acc: 0.3475\n",
      "Epoch 526/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3508 - acc: 0.5261 - val_loss: 2.1664 - val_acc: 0.3245\n",
      "Epoch 527/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3555 - acc: 0.5286 - val_loss: 2.1013 - val_acc: 0.3380\n",
      "Epoch 528/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3499 - acc: 0.5389 - val_loss: 2.1403 - val_acc: 0.3480\n",
      "Epoch 529/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3479 - acc: 0.5337 - val_loss: 2.1680 - val_acc: 0.3295\n",
      "Epoch 530/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3489 - acc: 0.5353 - val_loss: 2.2404 - val_acc: 0.3120\n",
      "Epoch 531/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3508 - acc: 0.5335 - val_loss: 2.2023 - val_acc: 0.3320\n",
      "Epoch 532/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3383 - acc: 0.5349 - val_loss: 2.1403 - val_acc: 0.3330\n",
      "Epoch 533/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3503 - acc: 0.5345 - val_loss: 2.1956 - val_acc: 0.3355\n",
      "Epoch 534/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3522 - acc: 0.5379 - val_loss: 2.1983 - val_acc: 0.3340\n",
      "Epoch 535/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3411 - acc: 0.5385 - val_loss: 2.1741 - val_acc: 0.3340\n",
      "Epoch 536/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3462 - acc: 0.5405 - val_loss: 2.0914 - val_acc: 0.3445\n",
      "Epoch 537/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3459 - acc: 0.5364 - val_loss: 2.0894 - val_acc: 0.3385\n",
      "Epoch 538/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3441 - acc: 0.5405 - val_loss: 2.2251 - val_acc: 0.3235\n",
      "Epoch 539/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3400 - acc: 0.5413 - val_loss: 2.2094 - val_acc: 0.3295\n",
      "Epoch 540/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3544 - acc: 0.5304 - val_loss: 2.1827 - val_acc: 0.3315\n",
      "Epoch 541/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3484 - acc: 0.5337 - val_loss: 2.1865 - val_acc: 0.3355\n",
      "Epoch 542/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3452 - acc: 0.5306 - val_loss: 2.1238 - val_acc: 0.3435\n",
      "Epoch 543/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3528 - acc: 0.5298 - val_loss: 2.1050 - val_acc: 0.3365\n",
      "Epoch 544/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3435 - acc: 0.5377 - val_loss: 2.0642 - val_acc: 0.3645\n",
      "Epoch 545/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3392 - acc: 0.5364 - val_loss: 2.2103 - val_acc: 0.3290\n",
      "Epoch 546/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3365 - acc: 0.5373 - val_loss: 2.3193 - val_acc: 0.3245\n",
      "Epoch 547/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3491 - acc: 0.5377 - val_loss: 2.2006 - val_acc: 0.3405\n",
      "Epoch 548/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3425 - acc: 0.5390 - val_loss: 2.1403 - val_acc: 0.3230\n",
      "Epoch 549/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3457 - acc: 0.5365 - val_loss: 2.1851 - val_acc: 0.3290\n",
      "Epoch 550/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3502 - acc: 0.5327 - val_loss: 2.1966 - val_acc: 0.3245\n",
      "Epoch 551/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3361 - acc: 0.5373 - val_loss: 2.1734 - val_acc: 0.3230\n",
      "Epoch 552/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3426 - acc: 0.5435 - val_loss: 2.0943 - val_acc: 0.3485\n",
      "Epoch 553/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3393 - acc: 0.5428 - val_loss: 2.2499 - val_acc: 0.3120\n",
      "Epoch 554/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3356 - acc: 0.5413 - val_loss: 2.1626 - val_acc: 0.3270\n",
      "Epoch 555/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.3359 - acc: 0.5363 - val_loss: 2.1479 - val_acc: 0.3395\n",
      "Epoch 556/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.3402 - acc: 0.5383 - val_loss: 2.1046 - val_acc: 0.3455\n",
      "Epoch 557/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.3378 - acc: 0.5371 - val_loss: 2.1180 - val_acc: 0.3475\n",
      "Epoch 558/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3434 - acc: 0.5294 - val_loss: 2.3587 - val_acc: 0.3065\n",
      "Epoch 559/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3411 - acc: 0.5393 - val_loss: 2.1086 - val_acc: 0.3485\n",
      "Epoch 560/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.3366 - acc: 0.5393 - val_loss: 2.1641 - val_acc: 0.3325\n",
      "Epoch 561/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.3393 - acc: 0.5381 - val_loss: 2.1499 - val_acc: 0.3325\n",
      "Epoch 562/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3322 - acc: 0.5423 - val_loss: 2.1743 - val_acc: 0.3390\n",
      "Epoch 563/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3420 - acc: 0.5330 - val_loss: 2.1360 - val_acc: 0.3450\n",
      "Epoch 564/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3359 - acc: 0.5426 - val_loss: 2.1644 - val_acc: 0.3370\n",
      "Epoch 565/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3348 - acc: 0.5394 - val_loss: 2.2180 - val_acc: 0.3285\n",
      "Epoch 566/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.3350 - acc: 0.5387 - val_loss: 2.2316 - val_acc: 0.3300\n",
      "Epoch 567/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3347 - acc: 0.5399 - val_loss: 2.2885 - val_acc: 0.3265\n",
      "Epoch 568/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3346 - acc: 0.5360 - val_loss: 2.3316 - val_acc: 0.3130\n",
      "Epoch 569/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3319 - acc: 0.5364 - val_loss: 2.3152 - val_acc: 0.3275\n",
      "Epoch 570/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3277 - acc: 0.5358 - val_loss: 2.3127 - val_acc: 0.2980\n",
      "Epoch 571/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3388 - acc: 0.5396 - val_loss: 2.1639 - val_acc: 0.3450\n",
      "Epoch 572/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3349 - acc: 0.5429 - val_loss: 2.2067 - val_acc: 0.3350\n",
      "Epoch 573/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3365 - acc: 0.5373 - val_loss: 2.1585 - val_acc: 0.3385\n",
      "Epoch 574/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3322 - acc: 0.5426 - val_loss: 2.2837 - val_acc: 0.3265\n",
      "Epoch 575/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3347 - acc: 0.5406 - val_loss: 2.2215 - val_acc: 0.3315\n",
      "Epoch 576/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3323 - acc: 0.5446 - val_loss: 2.1307 - val_acc: 0.3340\n",
      "Epoch 577/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3326 - acc: 0.5426 - val_loss: 2.1668 - val_acc: 0.3185\n",
      "Epoch 578/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3301 - acc: 0.5400 - val_loss: 2.2339 - val_acc: 0.3380\n",
      "Epoch 579/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3437 - acc: 0.5390 - val_loss: 2.2216 - val_acc: 0.3300\n",
      "Epoch 580/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3293 - acc: 0.5333 - val_loss: 2.2046 - val_acc: 0.3265\n",
      "Epoch 581/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3395 - acc: 0.5379 - val_loss: 2.1792 - val_acc: 0.3355\n",
      "Epoch 582/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3334 - acc: 0.5404 - val_loss: 2.1812 - val_acc: 0.3335\n",
      "Epoch 583/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3210 - acc: 0.5402 - val_loss: 2.1685 - val_acc: 0.3355\n",
      "Epoch 584/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3294 - acc: 0.5410 - val_loss: 2.1574 - val_acc: 0.3350\n",
      "Epoch 585/2000\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 1.3260 - acc: 0.5435 - val_loss: 2.3266 - val_acc: 0.2990\n",
      "Epoch 586/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3230 - acc: 0.5478 - val_loss: 2.1580 - val_acc: 0.3265\n",
      "Epoch 587/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3282 - acc: 0.5455 - val_loss: 2.1789 - val_acc: 0.3350\n",
      "Epoch 588/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3301 - acc: 0.5387 - val_loss: 2.1193 - val_acc: 0.3385\n",
      "Epoch 589/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3273 - acc: 0.5461 - val_loss: 2.3477 - val_acc: 0.3130\n",
      "Epoch 590/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3325 - acc: 0.5425 - val_loss: 2.2343 - val_acc: 0.3175\n",
      "Epoch 591/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3217 - acc: 0.5389 - val_loss: 2.1116 - val_acc: 0.3380\n",
      "Epoch 592/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3218 - acc: 0.5443 - val_loss: 2.1794 - val_acc: 0.3260\n",
      "Epoch 593/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3301 - acc: 0.5383 - val_loss: 2.1784 - val_acc: 0.3295\n",
      "Epoch 594/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3289 - acc: 0.5421 - val_loss: 2.2201 - val_acc: 0.3435\n",
      "Epoch 595/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3383 - acc: 0.5364 - val_loss: 2.1125 - val_acc: 0.3385\n",
      "Epoch 596/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3204 - acc: 0.5449 - val_loss: 2.1823 - val_acc: 0.3255\n",
      "Epoch 597/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3307 - acc: 0.5400 - val_loss: 2.1981 - val_acc: 0.3380\n",
      "Epoch 598/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3350 - acc: 0.5423 - val_loss: 2.3636 - val_acc: 0.3165\n",
      "Epoch 599/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3330 - acc: 0.5356 - val_loss: 2.1655 - val_acc: 0.3330\n",
      "Epoch 600/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3275 - acc: 0.5396 - val_loss: 2.1056 - val_acc: 0.3500\n",
      "Epoch 601/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3243 - acc: 0.5389 - val_loss: 2.2585 - val_acc: 0.3245\n",
      "Epoch 602/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3303 - acc: 0.5377 - val_loss: 2.2539 - val_acc: 0.3190\n",
      "Epoch 603/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3267 - acc: 0.5381 - val_loss: 2.2101 - val_acc: 0.3240\n",
      "Epoch 604/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3343 - acc: 0.5326 - val_loss: 2.1730 - val_acc: 0.3330\n",
      "Epoch 605/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3239 - acc: 0.5419 - val_loss: 2.2728 - val_acc: 0.3280\n",
      "Epoch 606/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3241 - acc: 0.5451 - val_loss: 2.1640 - val_acc: 0.3250\n",
      "Epoch 607/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3260 - acc: 0.5413 - val_loss: 2.1870 - val_acc: 0.3385\n",
      "Epoch 608/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3334 - acc: 0.5394 - val_loss: 2.1588 - val_acc: 0.3360\n",
      "Epoch 609/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3227 - acc: 0.5481 - val_loss: 2.1742 - val_acc: 0.3375\n",
      "Epoch 610/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3251 - acc: 0.5400 - val_loss: 2.2173 - val_acc: 0.3300\n",
      "Epoch 611/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3344 - acc: 0.5364 - val_loss: 2.2672 - val_acc: 0.3210\n",
      "Epoch 612/2000\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 1.3378 - acc: 0.5421 - val_loss: 2.2969 - val_acc: 0.3150\n",
      "Epoch 613/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3204 - acc: 0.5443 - val_loss: 2.1605 - val_acc: 0.3455\n",
      "Epoch 614/2000\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 1.3286 - acc: 0.5389 - val_loss: 2.1600 - val_acc: 0.3355\n",
      "Epoch 615/2000\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 1.3407 - acc: 0.5359 - val_loss: 2.2142 - val_acc: 0.3300\n",
      "Epoch 616/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3283 - acc: 0.5397 - val_loss: 2.1324 - val_acc: 0.3385\n",
      "Epoch 617/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3116 - acc: 0.5509 - val_loss: 2.1808 - val_acc: 0.3350\n",
      "Epoch 618/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3237 - acc: 0.5462 - val_loss: 2.1792 - val_acc: 0.3375\n",
      "Epoch 619/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3255 - acc: 0.5459 - val_loss: 2.1291 - val_acc: 0.3485\n",
      "Epoch 620/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3257 - acc: 0.5402 - val_loss: 2.2565 - val_acc: 0.3145\n",
      "Epoch 621/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.3260 - acc: 0.5369 - val_loss: 2.2003 - val_acc: 0.3320\n",
      "Epoch 622/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.3244 - acc: 0.5430 - val_loss: 2.2546 - val_acc: 0.3205\n",
      "Epoch 623/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.3213 - acc: 0.5445 - val_loss: 2.4099 - val_acc: 0.3140\n",
      "Epoch 624/2000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 1.3317 - acc: 0.5431 - val_loss: 2.1966 - val_acc: 0.3300\n",
      "Epoch 625/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.3155 - acc: 0.5435 - val_loss: 2.1863 - val_acc: 0.3215\n",
      "Epoch 626/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3210 - acc: 0.5401 - val_loss: 2.1531 - val_acc: 0.3400\n",
      "Epoch 627/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3208 - acc: 0.5465 - val_loss: 2.2452 - val_acc: 0.3190\n",
      "Epoch 628/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3309 - acc: 0.5385 - val_loss: 2.1917 - val_acc: 0.3395\n",
      "Epoch 629/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3112 - acc: 0.5459 - val_loss: 2.2806 - val_acc: 0.3250\n",
      "Epoch 630/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3148 - acc: 0.5479 - val_loss: 2.2376 - val_acc: 0.3355\n",
      "Epoch 631/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3168 - acc: 0.5383 - val_loss: 2.2049 - val_acc: 0.3305\n",
      "Epoch 632/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3176 - acc: 0.5474 - val_loss: 2.2363 - val_acc: 0.3295\n",
      "Epoch 633/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3153 - acc: 0.5437 - val_loss: 2.1891 - val_acc: 0.3300\n",
      "Epoch 634/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3140 - acc: 0.5499 - val_loss: 2.2107 - val_acc: 0.3250\n",
      "Epoch 635/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3268 - acc: 0.5445 - val_loss: 2.1579 - val_acc: 0.3260\n",
      "Epoch 636/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3116 - acc: 0.5500 - val_loss: 2.4185 - val_acc: 0.2975\n",
      "Epoch 637/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3159 - acc: 0.5405 - val_loss: 2.4027 - val_acc: 0.2965\n",
      "Epoch 638/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3166 - acc: 0.5455 - val_loss: 2.2030 - val_acc: 0.3245\n",
      "Epoch 639/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3179 - acc: 0.5415 - val_loss: 2.2831 - val_acc: 0.3280\n",
      "Epoch 640/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3167 - acc: 0.5503 - val_loss: 2.2548 - val_acc: 0.3165\n",
      "Epoch 641/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3198 - acc: 0.5491 - val_loss: 2.1956 - val_acc: 0.3280\n",
      "Epoch 642/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3108 - acc: 0.5439 - val_loss: 2.3205 - val_acc: 0.3115\n",
      "Epoch 643/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3205 - acc: 0.5419 - val_loss: 2.2600 - val_acc: 0.3215\n",
      "Epoch 644/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3131 - acc: 0.5462 - val_loss: 2.1858 - val_acc: 0.3305\n",
      "Epoch 645/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3155 - acc: 0.5464 - val_loss: 2.2075 - val_acc: 0.3360\n",
      "Epoch 646/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3235 - acc: 0.5408 - val_loss: 2.1896 - val_acc: 0.3380\n",
      "Epoch 647/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3140 - acc: 0.5431 - val_loss: 2.2997 - val_acc: 0.3150\n",
      "Epoch 648/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3177 - acc: 0.5486 - val_loss: 2.2194 - val_acc: 0.3440\n",
      "Epoch 649/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3168 - acc: 0.5381 - val_loss: 2.2389 - val_acc: 0.3315\n",
      "Epoch 650/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3155 - acc: 0.5471 - val_loss: 2.2755 - val_acc: 0.3135\n",
      "Epoch 651/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3149 - acc: 0.5499 - val_loss: 2.1682 - val_acc: 0.3420\n",
      "Epoch 652/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3160 - acc: 0.5479 - val_loss: 2.2185 - val_acc: 0.3250\n",
      "Epoch 653/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3058 - acc: 0.5473 - val_loss: 2.2939 - val_acc: 0.3180\n",
      "Epoch 654/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3037 - acc: 0.5467 - val_loss: 2.2263 - val_acc: 0.3305\n",
      "Epoch 655/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3145 - acc: 0.5462 - val_loss: 2.1668 - val_acc: 0.3390\n",
      "Epoch 656/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3151 - acc: 0.5485 - val_loss: 2.1645 - val_acc: 0.3410\n",
      "Epoch 657/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3166 - acc: 0.5425 - val_loss: 2.2438 - val_acc: 0.3215\n",
      "Epoch 658/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3071 - acc: 0.5489 - val_loss: 2.1551 - val_acc: 0.3415\n",
      "Epoch 659/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3063 - acc: 0.5450 - val_loss: 2.2223 - val_acc: 0.3265\n",
      "Epoch 660/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3144 - acc: 0.5460 - val_loss: 2.2647 - val_acc: 0.3225\n",
      "Epoch 661/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3125 - acc: 0.5418 - val_loss: 2.2985 - val_acc: 0.3155\n",
      "Epoch 662/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3090 - acc: 0.5461 - val_loss: 2.2687 - val_acc: 0.3210\n",
      "Epoch 663/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3008 - acc: 0.5533 - val_loss: 2.2342 - val_acc: 0.3245\n",
      "Epoch 664/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3110 - acc: 0.5381 - val_loss: 2.2635 - val_acc: 0.3145\n",
      "Epoch 665/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3128 - acc: 0.5431 - val_loss: 2.2049 - val_acc: 0.3385\n",
      "Epoch 666/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3046 - acc: 0.5489 - val_loss: 2.2723 - val_acc: 0.3270\n",
      "Epoch 667/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3149 - acc: 0.5445 - val_loss: 2.1836 - val_acc: 0.3300\n",
      "Epoch 668/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3099 - acc: 0.5501 - val_loss: 2.2055 - val_acc: 0.3290\n",
      "Epoch 669/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2965 - acc: 0.5505 - val_loss: 2.1664 - val_acc: 0.3315\n",
      "Epoch 670/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3020 - acc: 0.5474 - val_loss: 2.2992 - val_acc: 0.3220\n",
      "Epoch 671/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.3041 - acc: 0.5513 - val_loss: 2.2050 - val_acc: 0.3405\n",
      "Epoch 672/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.3134 - acc: 0.5504 - val_loss: 2.2232 - val_acc: 0.3350\n",
      "Epoch 673/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3096 - acc: 0.5510 - val_loss: 2.2341 - val_acc: 0.3205\n",
      "Epoch 674/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3041 - acc: 0.5550 - val_loss: 2.2086 - val_acc: 0.3400\n",
      "Epoch 675/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3075 - acc: 0.5479 - val_loss: 2.1624 - val_acc: 0.3400\n",
      "Epoch 676/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3126 - acc: 0.5473 - val_loss: 2.4207 - val_acc: 0.2955\n",
      "Epoch 677/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3048 - acc: 0.5533 - val_loss: 2.1866 - val_acc: 0.3355\n",
      "Epoch 678/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3021 - acc: 0.5514 - val_loss: 2.2711 - val_acc: 0.3155\n",
      "Epoch 679/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3046 - acc: 0.5499 - val_loss: 2.1974 - val_acc: 0.3355\n",
      "Epoch 680/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3088 - acc: 0.5513 - val_loss: 2.2842 - val_acc: 0.3310\n",
      "Epoch 681/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3104 - acc: 0.5488 - val_loss: 2.2701 - val_acc: 0.3270\n",
      "Epoch 682/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2998 - acc: 0.5500 - val_loss: 2.2258 - val_acc: 0.3290\n",
      "Epoch 683/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2992 - acc: 0.5524 - val_loss: 2.2131 - val_acc: 0.3265\n",
      "Epoch 684/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3004 - acc: 0.5496 - val_loss: 2.2502 - val_acc: 0.3140\n",
      "Epoch 685/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3025 - acc: 0.5481 - val_loss: 2.1752 - val_acc: 0.3370\n",
      "Epoch 686/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2981 - acc: 0.5521 - val_loss: 2.1919 - val_acc: 0.3250\n",
      "Epoch 687/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2938 - acc: 0.5527 - val_loss: 2.2620 - val_acc: 0.3275\n",
      "Epoch 688/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3011 - acc: 0.5513 - val_loss: 2.2058 - val_acc: 0.3305\n",
      "Epoch 689/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3015 - acc: 0.5553 - val_loss: 2.2299 - val_acc: 0.3270\n",
      "Epoch 690/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2942 - acc: 0.5520 - val_loss: 2.2104 - val_acc: 0.3295\n",
      "Epoch 691/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3064 - acc: 0.5464 - val_loss: 2.1975 - val_acc: 0.3325\n",
      "Epoch 692/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3106 - acc: 0.5449 - val_loss: 2.2173 - val_acc: 0.3060\n",
      "Epoch 693/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2963 - acc: 0.5545 - val_loss: 2.3002 - val_acc: 0.3170\n",
      "Epoch 694/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3026 - acc: 0.5497 - val_loss: 2.4520 - val_acc: 0.2940\n",
      "Epoch 695/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3074 - acc: 0.5516 - val_loss: 2.3430 - val_acc: 0.3180\n",
      "Epoch 696/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2991 - acc: 0.5534 - val_loss: 2.3403 - val_acc: 0.3290\n",
      "Epoch 697/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3059 - acc: 0.5488 - val_loss: 2.3039 - val_acc: 0.3240\n",
      "Epoch 698/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3024 - acc: 0.5465 - val_loss: 2.3188 - val_acc: 0.3130\n",
      "Epoch 699/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2998 - acc: 0.5520 - val_loss: 2.2637 - val_acc: 0.3165\n",
      "Epoch 700/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3144 - acc: 0.5389 - val_loss: 2.4367 - val_acc: 0.3110\n",
      "Epoch 701/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2975 - acc: 0.5514 - val_loss: 2.2607 - val_acc: 0.3175\n",
      "Epoch 702/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2893 - acc: 0.5514 - val_loss: 2.2457 - val_acc: 0.3335\n",
      "Epoch 703/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2944 - acc: 0.5497 - val_loss: 2.3741 - val_acc: 0.3305\n",
      "Epoch 704/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2843 - acc: 0.5614 - val_loss: 2.4092 - val_acc: 0.3215\n",
      "Epoch 705/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2995 - acc: 0.5467 - val_loss: 2.2872 - val_acc: 0.3255\n",
      "Epoch 706/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2954 - acc: 0.5525 - val_loss: 2.2152 - val_acc: 0.3480\n",
      "Epoch 707/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3004 - acc: 0.5490 - val_loss: 2.1808 - val_acc: 0.3390\n",
      "Epoch 708/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2968 - acc: 0.5571 - val_loss: 2.2223 - val_acc: 0.3245\n",
      "Epoch 709/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3007 - acc: 0.5464 - val_loss: 2.2897 - val_acc: 0.3250\n",
      "Epoch 710/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3005 - acc: 0.5496 - val_loss: 2.2093 - val_acc: 0.3330\n",
      "Epoch 711/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2876 - acc: 0.5548 - val_loss: 2.3634 - val_acc: 0.3225\n",
      "Epoch 712/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2978 - acc: 0.5522 - val_loss: 2.2610 - val_acc: 0.3210\n",
      "Epoch 713/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2952 - acc: 0.5506 - val_loss: 2.3451 - val_acc: 0.3175\n",
      "Epoch 714/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3071 - acc: 0.5496 - val_loss: 2.1792 - val_acc: 0.3375\n",
      "Epoch 715/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2982 - acc: 0.5505 - val_loss: 2.2194 - val_acc: 0.3325\n",
      "Epoch 716/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2970 - acc: 0.5501 - val_loss: 2.2508 - val_acc: 0.3310\n",
      "Epoch 717/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2899 - acc: 0.5460 - val_loss: 2.2882 - val_acc: 0.3345\n",
      "Epoch 718/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2969 - acc: 0.5504 - val_loss: 2.2723 - val_acc: 0.3360\n",
      "Epoch 719/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2831 - acc: 0.5586 - val_loss: 2.2674 - val_acc: 0.3215\n",
      "Epoch 720/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3011 - acc: 0.5514 - val_loss: 2.4208 - val_acc: 0.2960\n",
      "Epoch 721/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3024 - acc: 0.5524 - val_loss: 2.2722 - val_acc: 0.3250\n",
      "Epoch 722/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2986 - acc: 0.5514 - val_loss: 2.3675 - val_acc: 0.3140\n",
      "Epoch 723/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2929 - acc: 0.5554 - val_loss: 2.1560 - val_acc: 0.3390\n",
      "Epoch 724/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2942 - acc: 0.5598 - val_loss: 2.2363 - val_acc: 0.3355\n",
      "Epoch 725/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3063 - acc: 0.5446 - val_loss: 2.5256 - val_acc: 0.2765\n",
      "Epoch 726/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2923 - acc: 0.5540 - val_loss: 2.4055 - val_acc: 0.2890\n",
      "Epoch 727/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2949 - acc: 0.5506 - val_loss: 2.2048 - val_acc: 0.3305\n",
      "Epoch 728/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2877 - acc: 0.5526 - val_loss: 2.2447 - val_acc: 0.3240\n",
      "Epoch 729/2000\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 1.2974 - acc: 0.5524 - val_loss: 2.1589 - val_acc: 0.3385\n",
      "Epoch 730/2000\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 1.3070 - acc: 0.5481 - val_loss: 2.1763 - val_acc: 0.3245\n",
      "Epoch 731/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2997 - acc: 0.5522 - val_loss: 2.3769 - val_acc: 0.3080\n",
      "Epoch 732/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2982 - acc: 0.5481 - val_loss: 2.5141 - val_acc: 0.2955\n",
      "Epoch 733/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2990 - acc: 0.5457 - val_loss: 2.2209 - val_acc: 0.3205\n",
      "Epoch 734/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2865 - acc: 0.5529 - val_loss: 2.2000 - val_acc: 0.3325\n",
      "Epoch 735/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2911 - acc: 0.5594 - val_loss: 2.2390 - val_acc: 0.3370\n",
      "Epoch 736/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.2905 - acc: 0.5577 - val_loss: 2.3069 - val_acc: 0.3150\n",
      "Epoch 737/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.2895 - acc: 0.5560 - val_loss: 2.2288 - val_acc: 0.3365\n",
      "Epoch 738/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3014 - acc: 0.5495 - val_loss: 2.2270 - val_acc: 0.3320\n",
      "Epoch 739/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2897 - acc: 0.5548 - val_loss: 2.4125 - val_acc: 0.2975\n",
      "Epoch 740/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2906 - acc: 0.5579 - val_loss: 2.3064 - val_acc: 0.3300\n",
      "Epoch 741/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2929 - acc: 0.5527 - val_loss: 2.2258 - val_acc: 0.3325\n",
      "Epoch 742/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2890 - acc: 0.5604 - val_loss: 2.2515 - val_acc: 0.3260\n",
      "Epoch 743/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2938 - acc: 0.5515 - val_loss: 2.2753 - val_acc: 0.3230\n",
      "Epoch 744/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2811 - acc: 0.5555 - val_loss: 2.4124 - val_acc: 0.3190\n",
      "Epoch 745/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2973 - acc: 0.5540 - val_loss: 2.2130 - val_acc: 0.3265\n",
      "Epoch 746/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2832 - acc: 0.5589 - val_loss: 2.2806 - val_acc: 0.3225\n",
      "Epoch 747/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2959 - acc: 0.5530 - val_loss: 2.2438 - val_acc: 0.3300\n",
      "Epoch 748/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2851 - acc: 0.5554 - val_loss: 2.2750 - val_acc: 0.3225\n",
      "Epoch 749/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2899 - acc: 0.5569 - val_loss: 2.3862 - val_acc: 0.3075\n",
      "Epoch 750/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2857 - acc: 0.5575 - val_loss: 2.3277 - val_acc: 0.3175\n",
      "Epoch 751/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2885 - acc: 0.5554 - val_loss: 2.2650 - val_acc: 0.3105\n",
      "Epoch 752/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2986 - acc: 0.5503 - val_loss: 2.2113 - val_acc: 0.3395\n",
      "Epoch 753/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2819 - acc: 0.5570 - val_loss: 2.2174 - val_acc: 0.3330\n",
      "Epoch 754/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2951 - acc: 0.5511 - val_loss: 2.4676 - val_acc: 0.3030\n",
      "Epoch 755/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2856 - acc: 0.5529 - val_loss: 2.3230 - val_acc: 0.3290\n",
      "Epoch 756/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2888 - acc: 0.5580 - val_loss: 2.2631 - val_acc: 0.3255\n",
      "Epoch 757/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2806 - acc: 0.5550 - val_loss: 2.2274 - val_acc: 0.3335\n",
      "Epoch 758/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2907 - acc: 0.5519 - val_loss: 2.2066 - val_acc: 0.3405\n",
      "Epoch 759/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2845 - acc: 0.5499 - val_loss: 2.2459 - val_acc: 0.3390\n",
      "Epoch 760/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2740 - acc: 0.5541 - val_loss: 2.5545 - val_acc: 0.3125\n",
      "Epoch 761/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2920 - acc: 0.5534 - val_loss: 2.2154 - val_acc: 0.3350\n",
      "Epoch 762/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2874 - acc: 0.5546 - val_loss: 2.2727 - val_acc: 0.3165\n",
      "Epoch 763/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2922 - acc: 0.5507 - val_loss: 2.2604 - val_acc: 0.3165\n",
      "Epoch 764/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2899 - acc: 0.5517 - val_loss: 2.1853 - val_acc: 0.3270\n",
      "Epoch 765/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2872 - acc: 0.5550 - val_loss: 2.3813 - val_acc: 0.3055\n",
      "Epoch 766/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2944 - acc: 0.5507 - val_loss: 2.2284 - val_acc: 0.3290\n",
      "Epoch 767/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2804 - acc: 0.5580 - val_loss: 2.3577 - val_acc: 0.3120\n",
      "Epoch 768/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2832 - acc: 0.5534 - val_loss: 2.2120 - val_acc: 0.3305\n",
      "Epoch 769/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2803 - acc: 0.5611 - val_loss: 2.2137 - val_acc: 0.3275\n",
      "Epoch 770/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2800 - acc: 0.5582 - val_loss: 2.3530 - val_acc: 0.3255\n",
      "Epoch 771/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2826 - acc: 0.5586 - val_loss: 2.2313 - val_acc: 0.3270\n",
      "Epoch 772/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2748 - acc: 0.5634 - val_loss: 2.2927 - val_acc: 0.3145\n",
      "Epoch 773/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2854 - acc: 0.5561 - val_loss: 2.2557 - val_acc: 0.3240\n",
      "Epoch 774/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2808 - acc: 0.5623 - val_loss: 2.3129 - val_acc: 0.3120\n",
      "Epoch 775/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2836 - acc: 0.5615 - val_loss: 2.1945 - val_acc: 0.3430\n",
      "Epoch 776/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2768 - acc: 0.5577 - val_loss: 2.3141 - val_acc: 0.3265\n",
      "Epoch 777/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2800 - acc: 0.5538 - val_loss: 2.2446 - val_acc: 0.3350\n",
      "Epoch 778/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2865 - acc: 0.5522 - val_loss: 2.3237 - val_acc: 0.3225\n",
      "Epoch 779/2000\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 1.2906 - acc: 0.5516 - val_loss: 2.2776 - val_acc: 0.3140\n",
      "Epoch 780/2000\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 1.2836 - acc: 0.5560 - val_loss: 2.4144 - val_acc: 0.3005\n",
      "Epoch 781/2000\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.2815 - acc: 0.5541 - val_loss: 2.2488 - val_acc: 0.3240\n",
      "Epoch 782/2000\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 1.2965 - acc: 0.5469 - val_loss: 2.2509 - val_acc: 0.3415\n",
      "Epoch 783/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.2932 - acc: 0.5501 - val_loss: 2.2771 - val_acc: 0.3265\n",
      "Epoch 784/2000\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 1.2857 - acc: 0.5530 - val_loss: 2.2814 - val_acc: 0.3160\n",
      "Epoch 785/2000\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: 1.2946 - acc: 0.5493 - val_loss: 2.2620 - val_acc: 0.3185\n",
      "Epoch 786/2000\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 1.2912 - acc: 0.5519 - val_loss: 2.2954 - val_acc: 0.3100\n",
      "Epoch 787/2000\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 1.2890 - acc: 0.5493 - val_loss: 2.2913 - val_acc: 0.3290\n",
      "Epoch 788/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.2822 - acc: 0.5550 - val_loss: 2.2577 - val_acc: 0.3260\n",
      "Epoch 789/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2837 - acc: 0.5507 - val_loss: 2.3004 - val_acc: 0.3210\n",
      "Epoch 790/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2813 - acc: 0.5560 - val_loss: 2.2559 - val_acc: 0.3175\n",
      "Epoch 791/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2734 - acc: 0.5654 - val_loss: 2.3333 - val_acc: 0.3265\n",
      "Epoch 792/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2818 - acc: 0.5616 - val_loss: 2.2819 - val_acc: 0.3205\n",
      "Epoch 793/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 1.2860 - acc: 0.5561 - val_loss: 2.2436 - val_acc: 0.3330\n",
      "Epoch 794/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2762 - acc: 0.5587 - val_loss: 2.3467 - val_acc: 0.3240\n",
      "Epoch 795/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2826 - acc: 0.5543 - val_loss: 2.2102 - val_acc: 0.3395\n",
      "Epoch 796/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2734 - acc: 0.5565 - val_loss: 2.3509 - val_acc: 0.3170\n",
      "Epoch 797/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2773 - acc: 0.5540 - val_loss: 2.2662 - val_acc: 0.3175\n",
      "Epoch 798/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2747 - acc: 0.5586 - val_loss: 2.2890 - val_acc: 0.3045\n",
      "Epoch 799/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2837 - acc: 0.5543 - val_loss: 2.2229 - val_acc: 0.3345\n",
      "Epoch 800/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2707 - acc: 0.5606 - val_loss: 2.3133 - val_acc: 0.3245\n",
      "Epoch 801/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2783 - acc: 0.5551 - val_loss: 2.2403 - val_acc: 0.3365\n",
      "Epoch 802/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2834 - acc: 0.5557 - val_loss: 2.3271 - val_acc: 0.3230\n",
      "Epoch 803/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2765 - acc: 0.5604 - val_loss: 2.3751 - val_acc: 0.3085\n",
      "Epoch 804/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2704 - acc: 0.5575 - val_loss: 2.2908 - val_acc: 0.3330\n",
      "Epoch 805/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2797 - acc: 0.5544 - val_loss: 2.3374 - val_acc: 0.3040\n",
      "Epoch 806/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2591 - acc: 0.5650 - val_loss: 2.2642 - val_acc: 0.3290\n",
      "Epoch 807/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2737 - acc: 0.5584 - val_loss: 2.3820 - val_acc: 0.3210\n",
      "Epoch 808/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2748 - acc: 0.5623 - val_loss: 2.3512 - val_acc: 0.3140\n",
      "Epoch 809/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2820 - acc: 0.5609 - val_loss: 2.3109 - val_acc: 0.3265\n",
      "Epoch 810/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2688 - acc: 0.5656 - val_loss: 2.2780 - val_acc: 0.3305\n",
      "Epoch 811/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2629 - acc: 0.5616 - val_loss: 2.2653 - val_acc: 0.3290\n",
      "Epoch 812/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2847 - acc: 0.5550 - val_loss: 2.3452 - val_acc: 0.3245\n",
      "Epoch 813/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2761 - acc: 0.5605 - val_loss: 2.5039 - val_acc: 0.2930\n",
      "Epoch 814/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2738 - acc: 0.5634 - val_loss: 2.2684 - val_acc: 0.3205\n",
      "Epoch 815/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2761 - acc: 0.5534 - val_loss: 2.2461 - val_acc: 0.3220\n",
      "Epoch 816/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2760 - acc: 0.5574 - val_loss: 2.3608 - val_acc: 0.3135\n",
      "Epoch 817/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2684 - acc: 0.5592 - val_loss: 2.5304 - val_acc: 0.2910\n",
      "Epoch 818/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2675 - acc: 0.5623 - val_loss: 2.2027 - val_acc: 0.3430\n",
      "Epoch 819/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2787 - acc: 0.5584 - val_loss: 2.2857 - val_acc: 0.3110\n",
      "Epoch 820/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2689 - acc: 0.5660 - val_loss: 2.1974 - val_acc: 0.3325\n",
      "Epoch 821/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2689 - acc: 0.5594 - val_loss: 2.2460 - val_acc: 0.3295\n",
      "Epoch 822/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2630 - acc: 0.5576 - val_loss: 2.3220 - val_acc: 0.3150\n",
      "Epoch 823/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2743 - acc: 0.5641 - val_loss: 2.3274 - val_acc: 0.3210\n",
      "Epoch 824/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2598 - acc: 0.5614 - val_loss: 2.2715 - val_acc: 0.3205\n",
      "Epoch 825/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2717 - acc: 0.5594 - val_loss: 2.2527 - val_acc: 0.3305\n",
      "Epoch 826/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2582 - acc: 0.5635 - val_loss: 2.2687 - val_acc: 0.3385\n",
      "Epoch 827/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2690 - acc: 0.5590 - val_loss: 2.4138 - val_acc: 0.3210\n",
      "Epoch 828/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2664 - acc: 0.5611 - val_loss: 2.2755 - val_acc: 0.3220\n",
      "Epoch 829/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.2684 - acc: 0.5625 - val_loss: 2.3046 - val_acc: 0.3180\n",
      "Epoch 830/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2591 - acc: 0.5654 - val_loss: 2.3380 - val_acc: 0.3285\n",
      "Epoch 831/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2595 - acc: 0.5652 - val_loss: 2.4782 - val_acc: 0.3030\n",
      "Epoch 832/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2755 - acc: 0.5557 - val_loss: 2.2519 - val_acc: 0.3465\n",
      "Epoch 833/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2584 - acc: 0.5624 - val_loss: 2.2882 - val_acc: 0.3195\n",
      "Epoch 834/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2688 - acc: 0.5576 - val_loss: 2.2867 - val_acc: 0.3270\n",
      "Epoch 835/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2560 - acc: 0.5694 - val_loss: 2.2999 - val_acc: 0.3340\n",
      "Epoch 836/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2661 - acc: 0.5563 - val_loss: 2.2733 - val_acc: 0.3245\n",
      "Epoch 837/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2688 - acc: 0.5616 - val_loss: 2.4364 - val_acc: 0.2965\n",
      "Epoch 838/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2724 - acc: 0.5651 - val_loss: 2.3361 - val_acc: 0.3210\n",
      "Epoch 839/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2723 - acc: 0.5640 - val_loss: 2.3432 - val_acc: 0.3045\n",
      "Epoch 840/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2624 - acc: 0.5609 - val_loss: 2.2213 - val_acc: 0.3455\n",
      "Epoch 841/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2658 - acc: 0.5615 - val_loss: 2.3199 - val_acc: 0.3205\n",
      "Epoch 842/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2752 - acc: 0.5619 - val_loss: 2.3154 - val_acc: 0.3110\n",
      "Epoch 843/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2664 - acc: 0.5611 - val_loss: 2.5404 - val_acc: 0.2815\n",
      "Epoch 844/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2623 - acc: 0.5669 - val_loss: 2.3608 - val_acc: 0.3150\n",
      "Epoch 845/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2724 - acc: 0.5527 - val_loss: 2.3552 - val_acc: 0.3225\n",
      "Epoch 846/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2646 - acc: 0.5661 - val_loss: 2.3738 - val_acc: 0.3050\n",
      "Epoch 847/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2667 - acc: 0.5551 - val_loss: 2.2951 - val_acc: 0.3195\n",
      "Epoch 848/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2713 - acc: 0.5598 - val_loss: 2.5365 - val_acc: 0.2855\n",
      "Epoch 849/2000\n",
      "8000/8000 [==============================] - 10804s 1s/step - loss: 1.2605 - acc: 0.5661 - val_loss: 2.4475 - val_acc: 0.3050\n",
      "Epoch 850/2000\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 1.2673 - acc: 0.5645 - val_loss: 2.5276 - val_acc: 0.3060\n",
      "Epoch 851/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2608 - acc: 0.5574 - val_loss: 2.2852 - val_acc: 0.3265\n",
      "Epoch 852/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2615 - acc: 0.5579 - val_loss: 2.3101 - val_acc: 0.3175\n",
      "Epoch 853/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.2639 - acc: 0.5663 - val_loss: 2.2686 - val_acc: 0.3230\n",
      "Epoch 854/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2549 - acc: 0.5671 - val_loss: 2.2905 - val_acc: 0.3255\n",
      "Epoch 855/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2670 - acc: 0.5701 - val_loss: 2.2990 - val_acc: 0.3145\n",
      "Epoch 856/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.2554 - acc: 0.5702 - val_loss: 2.2428 - val_acc: 0.3310\n",
      "Epoch 857/2000\n",
      "8000/8000 [==============================] - 17994s 2s/step - loss: 1.2669 - acc: 0.5673 - val_loss: 2.2791 - val_acc: 0.3360\n",
      "Epoch 858/2000\n",
      "8000/8000 [==============================] - 2s 311us/step - loss: 1.2787 - acc: 0.5616 - val_loss: 2.2274 - val_acc: 0.3260\n",
      "Epoch 859/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.2666 - acc: 0.5587 - val_loss: 2.4735 - val_acc: 0.3070\n",
      "Epoch 860/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.2681 - acc: 0.5621 - val_loss: 2.2752 - val_acc: 0.3155\n",
      "Epoch 861/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.2643 - acc: 0.5630 - val_loss: 2.3404 - val_acc: 0.3075\n",
      "Epoch 862/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2618 - acc: 0.5626 - val_loss: 2.2770 - val_acc: 0.3120\n",
      "Epoch 863/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2626 - acc: 0.5625 - val_loss: 2.4430 - val_acc: 0.3025\n",
      "Epoch 864/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2574 - acc: 0.5642 - val_loss: 2.4007 - val_acc: 0.3225\n",
      "Epoch 865/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2643 - acc: 0.5637 - val_loss: 2.2408 - val_acc: 0.3325\n",
      "Epoch 866/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2655 - acc: 0.5594 - val_loss: 2.3057 - val_acc: 0.3230\n",
      "Epoch 867/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2558 - acc: 0.5631 - val_loss: 2.3621 - val_acc: 0.3115\n",
      "Epoch 868/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2624 - acc: 0.5644 - val_loss: 2.3137 - val_acc: 0.3220\n",
      "Epoch 869/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2653 - acc: 0.5645 - val_loss: 2.3441 - val_acc: 0.3245\n",
      "Epoch 870/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2601 - acc: 0.5681 - val_loss: 2.2522 - val_acc: 0.3355\n",
      "Epoch 871/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2612 - acc: 0.5617 - val_loss: 2.2577 - val_acc: 0.3225\n",
      "Epoch 872/2000\n",
      "8000/8000 [==============================] - 36890s 5s/step - loss: 1.2581 - acc: 0.5584 - val_loss: 2.3566 - val_acc: 0.3285\n",
      "Epoch 873/2000\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 1.2608 - acc: 0.5695 - val_loss: 2.2652 - val_acc: 0.3255\n",
      "Epoch 874/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.2660 - acc: 0.5656 - val_loss: 2.4031 - val_acc: 0.3095\n",
      "Epoch 875/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.2576 - acc: 0.5665 - val_loss: 2.3750 - val_acc: 0.3210\n",
      "Epoch 876/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2563 - acc: 0.5661 - val_loss: 2.2805 - val_acc: 0.3315\n",
      "Epoch 877/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2667 - acc: 0.5635 - val_loss: 2.2296 - val_acc: 0.3390\n",
      "Epoch 878/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2593 - acc: 0.5665 - val_loss: 2.3413 - val_acc: 0.3195\n",
      "Epoch 879/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2547 - acc: 0.5666 - val_loss: 2.2802 - val_acc: 0.3295\n",
      "Epoch 880/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2566 - acc: 0.5660 - val_loss: 2.2800 - val_acc: 0.3145\n",
      "Epoch 881/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2651 - acc: 0.5685 - val_loss: 2.3360 - val_acc: 0.3160\n",
      "Epoch 882/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2508 - acc: 0.5723 - val_loss: 2.3169 - val_acc: 0.3305\n",
      "Epoch 883/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2517 - acc: 0.5684 - val_loss: 2.2918 - val_acc: 0.3075\n",
      "Epoch 884/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2482 - acc: 0.5717 - val_loss: 2.3345 - val_acc: 0.3160\n",
      "Epoch 885/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2571 - acc: 0.5644 - val_loss: 2.3555 - val_acc: 0.3040\n",
      "Epoch 886/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2579 - acc: 0.5637 - val_loss: 2.2463 - val_acc: 0.3270\n",
      "Epoch 887/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2538 - acc: 0.5636 - val_loss: 2.2430 - val_acc: 0.3275\n",
      "Epoch 888/2000\n",
      "8000/8000 [==============================] - 7190s 899ms/step - loss: 1.2576 - acc: 0.5650 - val_loss: 2.3101 - val_acc: 0.3245\n",
      "Epoch 889/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.2544 - acc: 0.5647 - val_loss: 2.3203 - val_acc: 0.3270\n",
      "Epoch 890/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2479 - acc: 0.5728 - val_loss: 2.4289 - val_acc: 0.3050\n",
      "Epoch 891/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2628 - acc: 0.5631 - val_loss: 2.2961 - val_acc: 0.3205\n",
      "Epoch 892/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.2544 - acc: 0.5677 - val_loss: 2.2922 - val_acc: 0.3250\n",
      "Epoch 893/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.2607 - acc: 0.5673 - val_loss: 2.2868 - val_acc: 0.3310\n",
      "Epoch 894/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2605 - acc: 0.5614 - val_loss: 2.2489 - val_acc: 0.3390\n",
      "Epoch 895/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2551 - acc: 0.5650 - val_loss: 2.3523 - val_acc: 0.3140\n",
      "Epoch 896/2000\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 1.2534 - acc: 0.5680 - val_loss: 2.2798 - val_acc: 0.3400\n",
      "Epoch 897/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2540 - acc: 0.5661 - val_loss: 2.3161 - val_acc: 0.3230\n",
      "Epoch 898/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2537 - acc: 0.5659 - val_loss: 2.4087 - val_acc: 0.3110\n",
      "Epoch 899/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2577 - acc: 0.5660 - val_loss: 2.3301 - val_acc: 0.3175\n",
      "Epoch 900/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2613 - acc: 0.5635 - val_loss: 2.3506 - val_acc: 0.3175\n",
      "Epoch 901/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2541 - acc: 0.5658 - val_loss: 2.3395 - val_acc: 0.3250\n",
      "Epoch 902/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2607 - acc: 0.5640 - val_loss: 2.4164 - val_acc: 0.3085\n",
      "Epoch 903/2000\n",
      "8000/8000 [==============================] - 7818s 977ms/step - loss: 1.2497 - acc: 0.5640 - val_loss: 2.3321 - val_acc: 0.3210\n",
      "Epoch 904/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2478 - acc: 0.5709 - val_loss: 2.3262 - val_acc: 0.3195\n",
      "Epoch 905/2000\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.2623 - acc: 0.5656 - val_loss: 2.4954 - val_acc: 0.3130\n",
      "Epoch 906/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.2601 - acc: 0.5658 - val_loss: 2.3366 - val_acc: 0.3220\n",
      "Epoch 907/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.2482 - acc: 0.5747 - val_loss: 2.3760 - val_acc: 0.3060\n",
      "Epoch 908/2000\n",
      "8000/8000 [==============================] - 3s 375us/step - loss: 1.2613 - acc: 0.5665 - val_loss: 2.2740 - val_acc: 0.3360\n",
      "Epoch 909/2000\n",
      "8000/8000 [==============================] - 3s 330us/step - loss: 1.2622 - acc: 0.5645 - val_loss: 2.3284 - val_acc: 0.3230\n",
      "Epoch 910/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2609 - acc: 0.5658 - val_loss: 2.3448 - val_acc: 0.3240\n",
      "Epoch 911/2000\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 1.2569 - acc: 0.5629 - val_loss: 2.3154 - val_acc: 0.3280\n",
      "Epoch 912/2000\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 1.2566 - acc: 0.5677 - val_loss: 2.3059 - val_acc: 0.3225\n",
      "Epoch 913/2000\n",
      "8000/8000 [==============================] - 3s 341us/step - loss: 1.2621 - acc: 0.5641 - val_loss: 2.5209 - val_acc: 0.2890\n",
      "Epoch 914/2000\n",
      "8000/8000 [==============================] - 2s 311us/step - loss: 1.2716 - acc: 0.5581 - val_loss: 2.4197 - val_acc: 0.2995\n",
      "Epoch 915/2000\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 1.2630 - acc: 0.5671 - val_loss: 2.3253 - val_acc: 0.3295\n",
      "Epoch 916/2000\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 1.2609 - acc: 0.5658 - val_loss: 2.2703 - val_acc: 0.3315\n",
      "Epoch 917/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2463 - acc: 0.5728 - val_loss: 2.3160 - val_acc: 0.3225\n",
      "Epoch 918/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2581 - acc: 0.5651 - val_loss: 2.3493 - val_acc: 0.3285\n",
      "Epoch 919/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2556 - acc: 0.5611 - val_loss: 2.4504 - val_acc: 0.3130\n",
      "Epoch 920/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2442 - acc: 0.5707 - val_loss: 2.3799 - val_acc: 0.3165\n",
      "Epoch 921/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2491 - acc: 0.5728 - val_loss: 2.4372 - val_acc: 0.3085\n",
      "Epoch 922/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2532 - acc: 0.5702 - val_loss: 2.2914 - val_acc: 0.3275\n",
      "Epoch 923/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.2291 - acc: 0.5691 - val_loss: 2.4192 - val_acc: 0.3205\n",
      "Epoch 924/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2525 - acc: 0.5614 - val_loss: 2.3657 - val_acc: 0.3175\n",
      "Epoch 925/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2459 - acc: 0.5659 - val_loss: 2.3472 - val_acc: 0.3135\n",
      "Epoch 926/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2566 - acc: 0.5623 - val_loss: 2.3007 - val_acc: 0.3280\n",
      "Epoch 927/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2479 - acc: 0.5660 - val_loss: 2.2944 - val_acc: 0.3160\n",
      "Epoch 928/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2609 - acc: 0.5659 - val_loss: 2.4793 - val_acc: 0.3145\n",
      "Epoch 929/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2492 - acc: 0.5649 - val_loss: 2.4277 - val_acc: 0.3230\n",
      "Epoch 930/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.2560 - acc: 0.5660 - val_loss: 2.2662 - val_acc: 0.3345\n",
      "Epoch 931/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2335 - acc: 0.5735 - val_loss: 2.2599 - val_acc: 0.3350\n",
      "Epoch 932/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2607 - acc: 0.5640 - val_loss: 2.2559 - val_acc: 0.3325\n",
      "Epoch 933/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2512 - acc: 0.5689 - val_loss: 2.3381 - val_acc: 0.3170\n",
      "Epoch 934/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.2458 - acc: 0.5737 - val_loss: 2.4757 - val_acc: 0.3070\n",
      "Epoch 935/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2462 - acc: 0.5717 - val_loss: 2.3551 - val_acc: 0.3145\n",
      "Epoch 936/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2441 - acc: 0.5737 - val_loss: 2.2590 - val_acc: 0.3400\n",
      "Epoch 937/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.2445 - acc: 0.5683 - val_loss: 2.3450 - val_acc: 0.3240\n",
      "Epoch 938/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2461 - acc: 0.5729 - val_loss: 2.3933 - val_acc: 0.3000\n",
      "Epoch 939/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2453 - acc: 0.5712 - val_loss: 2.3765 - val_acc: 0.3060\n",
      "Epoch 940/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2307 - acc: 0.5707 - val_loss: 2.3482 - val_acc: 0.3200\n",
      "Epoch 941/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2452 - acc: 0.5740 - val_loss: 2.3170 - val_acc: 0.3145\n",
      "Epoch 942/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.2405 - acc: 0.5753 - val_loss: 2.3262 - val_acc: 0.3320\n",
      "Epoch 943/2000\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 1.2593 - acc: 0.5608 - val_loss: 2.3549 - val_acc: 0.3065\n",
      "Epoch 944/2000\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 1.2532 - acc: 0.5699 - val_loss: 2.2545 - val_acc: 0.3420\n",
      "Epoch 945/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2376 - acc: 0.5717 - val_loss: 2.4215 - val_acc: 0.3155\n",
      "Epoch 946/2000\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 1.2484 - acc: 0.5711 - val_loss: 2.3771 - val_acc: 0.3190\n",
      "Epoch 947/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2553 - acc: 0.5647 - val_loss: 2.2597 - val_acc: 0.3200\n",
      "Epoch 948/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2516 - acc: 0.5671 - val_loss: 2.3333 - val_acc: 0.3215\n",
      "Epoch 949/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2493 - acc: 0.5674 - val_loss: 2.3888 - val_acc: 0.3095\n",
      "Epoch 950/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2407 - acc: 0.5677 - val_loss: 2.3985 - val_acc: 0.3165\n",
      "Epoch 951/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.2411 - acc: 0.5729 - val_loss: 2.4028 - val_acc: 0.3175\n",
      "Epoch 952/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.2460 - acc: 0.5656 - val_loss: 2.3313 - val_acc: 0.3235\n",
      "Epoch 953/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2486 - acc: 0.5646 - val_loss: 2.3501 - val_acc: 0.3260\n",
      "Epoch 954/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2484 - acc: 0.5598 - val_loss: 2.3373 - val_acc: 0.3125\n",
      "Epoch 955/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2410 - acc: 0.5724 - val_loss: 2.3512 - val_acc: 0.3160\n",
      "Epoch 956/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.2461 - acc: 0.5679 - val_loss: 2.3601 - val_acc: 0.3190\n",
      "Epoch 957/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.2294 - acc: 0.5726 - val_loss: 2.2796 - val_acc: 0.3310\n",
      "Epoch 958/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.2483 - acc: 0.5660 - val_loss: 2.2710 - val_acc: 0.3340\n",
      "Epoch 959/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2363 - acc: 0.5716 - val_loss: 2.4853 - val_acc: 0.3005\n",
      "Epoch 960/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.2349 - acc: 0.5793 - val_loss: 2.3060 - val_acc: 0.3215\n",
      "Epoch 961/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2318 - acc: 0.5774 - val_loss: 2.5232 - val_acc: 0.2935\n",
      "Epoch 962/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2371 - acc: 0.5668 - val_loss: 2.4556 - val_acc: 0.3095\n",
      "Epoch 963/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2536 - acc: 0.5652 - val_loss: 2.3740 - val_acc: 0.3295\n",
      "Epoch 964/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.2326 - acc: 0.5710 - val_loss: 2.3321 - val_acc: 0.3225\n",
      "Epoch 965/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.2507 - acc: 0.5677 - val_loss: 2.3349 - val_acc: 0.3210\n",
      "Epoch 966/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.2439 - acc: 0.5741 - val_loss: 2.5235 - val_acc: 0.3015\n",
      "Epoch 967/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.2429 - acc: 0.5745 - val_loss: 2.3470 - val_acc: 0.3180\n",
      "Epoch 968/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.2284 - acc: 0.5734 - val_loss: 2.2610 - val_acc: 0.3395\n",
      "Epoch 969/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.2341 - acc: 0.5754 - val_loss: 2.3788 - val_acc: 0.3100\n",
      "Epoch 970/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2421 - acc: 0.5696 - val_loss: 2.3516 - val_acc: 0.3165\n",
      "Epoch 971/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.2331 - acc: 0.5734 - val_loss: 2.5869 - val_acc: 0.2970\n",
      "Epoch 972/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2397 - acc: 0.5700 - val_loss: 2.3000 - val_acc: 0.3265\n",
      "Epoch 973/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2424 - acc: 0.5681 - val_loss: 2.3394 - val_acc: 0.3170\n",
      "Epoch 974/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.2444 - acc: 0.5673 - val_loss: 2.3109 - val_acc: 0.3240\n",
      "Epoch 975/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2446 - acc: 0.5700 - val_loss: 2.2750 - val_acc: 0.3305\n",
      "Epoch 976/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.2408 - acc: 0.5699 - val_loss: 2.3156 - val_acc: 0.3330\n",
      "Epoch 977/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2339 - acc: 0.5724 - val_loss: 2.3729 - val_acc: 0.3205\n",
      "Epoch 978/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2353 - acc: 0.5735 - val_loss: 2.4946 - val_acc: 0.3095\n",
      "Epoch 979/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2401 - acc: 0.5673 - val_loss: 2.3545 - val_acc: 0.3235\n",
      "Epoch 980/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.2422 - acc: 0.5735 - val_loss: 2.2944 - val_acc: 0.3365\n",
      "Epoch 981/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2258 - acc: 0.5766 - val_loss: 2.3637 - val_acc: 0.3240\n",
      "Epoch 982/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2358 - acc: 0.5725 - val_loss: 2.3775 - val_acc: 0.3200\n",
      "Epoch 983/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2362 - acc: 0.5739 - val_loss: 2.3305 - val_acc: 0.3305\n",
      "Epoch 984/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2339 - acc: 0.5785 - val_loss: 2.5039 - val_acc: 0.3165\n",
      "Epoch 985/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2376 - acc: 0.5642 - val_loss: 2.2858 - val_acc: 0.3345\n",
      "Epoch 986/2000\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: 1.2438 - acc: 0.5759 - val_loss: 2.3412 - val_acc: 0.3250\n",
      "Epoch 987/2000\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 1.2412 - acc: 0.5720 - val_loss: 2.3348 - val_acc: 0.3220\n",
      "Epoch 988/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.2330 - acc: 0.5830 - val_loss: 2.4914 - val_acc: 0.3020\n",
      "Epoch 989/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.2369 - acc: 0.5670 - val_loss: 2.3984 - val_acc: 0.3200\n",
      "Epoch 990/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2352 - acc: 0.5781 - val_loss: 2.3551 - val_acc: 0.3075\n",
      "Epoch 991/2000\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 1.2413 - acc: 0.5714 - val_loss: 2.5162 - val_acc: 0.3065\n",
      "Epoch 992/2000\n",
      "8000/8000 [==============================] - 2s 287us/step - loss: 1.2367 - acc: 0.5753 - val_loss: 2.3881 - val_acc: 0.3255\n",
      "Epoch 993/2000\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 1.2402 - acc: 0.5747 - val_loss: 2.2852 - val_acc: 0.3225\n",
      "Epoch 994/2000\n",
      "8000/8000 [==============================] - 2s 289us/step - loss: 1.2559 - acc: 0.5640 - val_loss: 2.3287 - val_acc: 0.3245\n",
      "Epoch 995/2000\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 1.2397 - acc: 0.5684 - val_loss: 2.3947 - val_acc: 0.3200\n",
      "Epoch 996/2000\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 1.2449 - acc: 0.5684 - val_loss: 2.2941 - val_acc: 0.3255\n",
      "Epoch 997/2000\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 1.2419 - acc: 0.5704 - val_loss: 2.3499 - val_acc: 0.3265\n",
      "Epoch 998/2000\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 1.2418 - acc: 0.5707 - val_loss: 2.3281 - val_acc: 0.3270\n",
      "Epoch 999/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.2420 - acc: 0.5737 - val_loss: 2.3506 - val_acc: 0.3235\n",
      "Epoch 1000/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.2315 - acc: 0.5733 - val_loss: 2.3523 - val_acc: 0.3330\n",
      "Epoch 1001/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.2391 - acc: 0.5728 - val_loss: 2.3531 - val_acc: 0.3190\n",
      "Epoch 1002/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.2329 - acc: 0.5728 - val_loss: 2.3296 - val_acc: 0.3125\n",
      "Epoch 1003/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.2439 - acc: 0.5656 - val_loss: 2.5402 - val_acc: 0.2845\n",
      "Epoch 1004/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2344 - acc: 0.5795 - val_loss: 2.3573 - val_acc: 0.3315\n",
      "Epoch 1005/2000\n",
      "3328/8000 [===========>..................] - ETA: 1s - loss: 1.2416 - acc: 0.5730"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4afe6417a6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2357\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=data_1[b'data']/255, y=y_var, batch_size=None, epochs=2000, verbose=1, callbacks=None, validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 2.1050 - acc: 0.4802 - val_loss: 1.3734 - val_acc: 0.5052\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.5476 - acc: 0.5199 - val_loss: 5.8688 - val_acc: 0.5062\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.5787 - acc: 0.5350 - val_loss: 1.6339 - val_acc: 0.5105\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.6131 - acc: 0.5229 - val_loss: 1.2297 - val_acc: 0.5809\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7151 - acc: 0.5084 - val_loss: 1.4373 - val_acc: 0.4518\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.7538 - acc: 0.4986 - val_loss: 1.5796 - val_acc: 0.4571\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7382 - acc: 0.4879 - val_loss: 1.8798 - val_acc: 0.2684\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.6989 - acc: 0.4963 - val_loss: 2.0891 - val_acc: 0.3480\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.6956 - acc: 0.5211 - val_loss: 1.7156 - val_acc: 0.3940\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.7547 - acc: 0.4922 - val_loss: 1.3395 - val_acc: 0.5195\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 1.7661 - acc: 0.5012 - val_loss: 1.7739 - val_acc: 0.2805\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.8077 - acc: 0.4673 - val_loss: 1.4116 - val_acc: 0.4942\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.8026 - acc: 0.4785 - val_loss: 1.4955 - val_acc: 0.4779\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.8967 - acc: 0.4761 - val_loss: 1.8687 - val_acc: 0.3281\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.9481 - acc: 0.4679 - val_loss: 1.8014 - val_acc: 0.3645\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 221s 6ms/step - loss: 2.4817 - acc: 0.4419 - val_loss: 1.6660 - val_acc: 0.4063\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.9411 - acc: 0.4573 - val_loss: 1.7128 - val_acc: 0.3617\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 2.2605 - acc: 0.4466 - val_loss: 1.6590 - val_acc: 0.3708\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.7144 - acc: 0.4765 - val_loss: 1.8816 - val_acc: 0.2659\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 2.0784 - acc: 0.4275 - val_loss: 1.9408 - val_acc: 0.4034\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 2.0291 - acc: 0.4380 - val_loss: 1.7777 - val_acc: 0.4524\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.9495 - acc: 0.4303 - val_loss: 1.6223 - val_acc: 0.4278\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.8092 - acc: 0.4511 - val_loss: 1.4502 - val_acc: 0.4840\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.8653 - acc: 0.4738 - val_loss: 1.6171 - val_acc: 0.4849\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 2.0193 - acc: 0.4425 - val_loss: 1.6359 - val_acc: 0.4086\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 2.0636 - acc: 0.4261 - val_loss: 2.9927 - val_acc: 0.4399\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 408s 10ms/step - loss: 2.1964 - acc: 0.4229 - val_loss: 1.9654 - val_acc: 0.3162\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 2.1502 - acc: 0.4252 - val_loss: 1.9772 - val_acc: 0.2576\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.9303 - acc: 0.4393 - val_loss: 1.3103 - val_acc: 0.5745\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.8924 - acc: 0.4714 - val_loss: 1.4748 - val_acc: 0.5372\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 2.0894 - acc: 0.4319 - val_loss: 1.8843 - val_acc: 0.3789\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 2.0122 - acc: 0.4529 - val_loss: 2.3088 - val_acc: 0.3761\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 2.1835 - acc: 0.4330 - val_loss: 1.6600 - val_acc: 0.3835\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 3729s 93ms/step - loss: 1.9396 - acc: 0.4494 - val_loss: 1.7549 - val_acc: 0.3278\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 2.7617 - acc: 0.4203 - val_loss: 1.7028 - val_acc: 0.4192\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 2.0746 - acc: 0.4168 - val_loss: 2.1835 - val_acc: 0.2588\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 2.1953 - acc: 0.4030 - val_loss: 1.7638 - val_acc: 0.4073\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.9653 - acc: 0.4148 - val_loss: 1.6773 - val_acc: 0.4159\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 2.0763 - acc: 0.3992 - val_loss: 1.7608 - val_acc: 0.3453\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 2.3681 - acc: 0.3882 - val_loss: 2.1601 - val_acc: 0.3388\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 2.3227 - acc: 0.3807 - val_loss: 1.8729 - val_acc: 0.3844\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 2.0176 - acc: 0.4029 - val_loss: 1.8208 - val_acc: 0.3508\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.8917 - acc: 0.4181 - val_loss: 1.8933 - val_acc: 0.4392\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 2.0310 - acc: 0.4239 - val_loss: 1.7828 - val_acc: 0.3705\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 2.0941 - acc: 0.4192 - val_loss: 1.7004 - val_acc: 0.4016\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 2.1479 - acc: 0.4186 - val_loss: 1.7269 - val_acc: 0.4732\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 2.1523 - acc: 0.4071 - val_loss: 2.0474 - val_acc: 0.2596\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 2.0342 - acc: 0.4231 - val_loss: 2.0409 - val_acc: 0.4764\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 179s 4ms/step - loss: 2.1339 - acc: 0.4070 - val_loss: 2.0909 - val_acc: 0.3665\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 182s 5ms/step - loss: 2.4025 - acc: 0.3711 - val_loss: 2.9320 - val_acc: 0.2792\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 183s 5ms/step - loss: 2.2890 - acc: 0.3360 - val_loss: 2.0724 - val_acc: 0.3111\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 186s 5ms/step - loss: 2.4307 - acc: 0.3026 - val_loss: 1.8431 - val_acc: 0.2661\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 185s 5ms/step - loss: 2.4998 - acc: 0.2781 - val_loss: 1.9799 - val_acc: 0.2797\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 185s 5ms/step - loss: 2.2547 - acc: 0.2810 - val_loss: 1.8377 - val_acc: 0.3058\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 186s 5ms/step - loss: 2.4199 - acc: 0.2846 - val_loss: 2.4363 - val_acc: 0.1206\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 188s 5ms/step - loss: 2.3417 - acc: 0.2770 - val_loss: 1.9490 - val_acc: 0.2203\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 188s 5ms/step - loss: 2.2974 - acc: 0.2698 - val_loss: 1.8632 - val_acc: 0.2800\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 188s 5ms/step - loss: 2.3186 - acc: 0.2669 - val_loss: 2.1144 - val_acc: 0.1805\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 187s 5ms/step - loss: 2.3289 - acc: 0.2753 - val_loss: 2.0222 - val_acc: 0.2859\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 189s 5ms/step - loss: 2.7283 - acc: 0.2731 - val_loss: 2.0965 - val_acc: 0.2167\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.6295 - acc: 0.2674 - val_loss: 1.9213 - val_acc: 0.2428\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.6608 - acc: 0.2449 - val_loss: 2.3490 - val_acc: 0.2654\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1977 - acc: 0.2340 - val_loss: 1.9140 - val_acc: 0.2595\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 2.4246 - acc: 0.2758 - val_loss: 1.9860 - val_acc: 0.2194\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1235 - acc: 0.2491 - val_loss: 1.9829 - val_acc: 0.1883\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.2299 - acc: 0.2569 - val_loss: 2.0297 - val_acc: 0.2169\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2490 - acc: 0.2571 - val_loss: 1.9178 - val_acc: 0.2145\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.4809 - acc: 0.2519 - val_loss: 1.8401 - val_acc: 0.2277\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.5394 - acc: 0.2621 - val_loss: 1.8394 - val_acc: 0.2393\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2434 - acc: 0.2522 - val_loss: 2.0345 - val_acc: 0.2783\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.4479 - acc: 0.2424 - val_loss: 1.9913 - val_acc: 0.2054\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2557 - acc: 0.2450 - val_loss: 1.9143 - val_acc: 0.2136\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1050 - acc: 0.2397 - val_loss: 1.9882 - val_acc: 0.1981\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1782 - acc: 0.2493 - val_loss: 1.8489 - val_acc: 0.2086\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2433 - acc: 0.2370 - val_loss: 2.1775 - val_acc: 0.1801\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.4372 - acc: 0.2359 - val_loss: 1.9136 - val_acc: 0.2455\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2290 - acc: 0.2297 - val_loss: 2.0497 - val_acc: 0.1940\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.4096 - acc: 0.2465 - val_loss: 6.7898 - val_acc: 0.2193\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.1589 - acc: 0.2384 - val_loss: 1.8803 - val_acc: 0.2130\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.3094 - acc: 0.2437 - val_loss: 1.8720 - val_acc: 0.2222\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2869 - acc: 0.2330 - val_loss: 2.3607 - val_acc: 0.2764\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.6932 - acc: 0.2410 - val_loss: 8.4946 - val_acc: 0.2406\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.4560 - acc: 0.2478 - val_loss: 1.8235 - val_acc: 0.2426\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2951 - acc: 0.2419 - val_loss: 5.7313 - val_acc: 0.2402\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2002 - acc: 0.2349 - val_loss: 1.9686 - val_acc: 0.2108\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1979 - acc: 0.2440 - val_loss: 1.9354 - val_acc: 0.2129\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.5035 - acc: 0.2502 - val_loss: 1.9720 - val_acc: 0.2108\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2558 - acc: 0.2458 - val_loss: 1.8358 - val_acc: 0.2498\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 2.0791 - acc: 0.2371 - val_loss: 2.4974 - val_acc: 0.1211\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.3747 - acc: 0.2452 - val_loss: 1.8505 - val_acc: 0.2238\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.6309 - acc: 0.2457 - val_loss: 1.8541 - val_acc: 0.2598\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.5613 - acc: 0.2466 - val_loss: 1.9033 - val_acc: 0.2416\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.0503 - acc: 0.2387 - val_loss: 1.9952 - val_acc: 0.2087\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.4787 - acc: 0.2538 - val_loss: 1.9338 - val_acc: 0.2091\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.3766 - acc: 0.2506 - val_loss: 2.0775 - val_acc: 0.1881\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2905 - acc: 0.2427 - val_loss: 1.8910 - val_acc: 0.2292\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1075 - acc: 0.2403 - val_loss: 1.8610 - val_acc: 0.2231\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 4.1635 - acc: 0.2380 - val_loss: 2.0757 - val_acc: 0.2580\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 2.8488 - acc: 0.2477 - val_loss: 2.0811 - val_acc: 0.1750\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.5321 - acc: 0.2486 - val_loss: 1.8712 - val_acc: 0.2615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fce96e10>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(images/255, y,\n",
    "          batch_size=None,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_split=0.2,\n",
    "          validation_data=None,\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "40000/40000 [==============================] - 821s 21ms/step - loss: 1.9476 - acc: 0.2991 - val_loss: 1.6137 - val_acc: 0.4214\n",
      "Epoch 2/250\n",
      "40000/40000 [==============================] - 4084s 102ms/step - loss: 1.5424 - acc: 0.4561 - val_loss: 1.6081 - val_acc: 0.4469\n",
      "Epoch 3/250\n",
      "40000/40000 [==============================] - 822s 21ms/step - loss: 1.4483 - acc: 0.4977 - val_loss: 1.7613 - val_acc: 0.4171\n",
      "Epoch 4/250\n",
      "40000/40000 [==============================] - 3998s 100ms/step - loss: 1.3592 - acc: 0.5301 - val_loss: 1.4824 - val_acc: 0.4685\n",
      "Epoch 5/250\n",
      "40000/40000 [==============================] - 878s 22ms/step - loss: 1.3005 - acc: 0.5519 - val_loss: 1.3672 - val_acc: 0.5141\n",
      "Epoch 6/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.2575 - acc: 0.5727 - val_loss: 1.3564 - val_acc: 0.5286\n",
      "Epoch 7/250\n",
      "40000/40000 [==============================] - 803s 20ms/step - loss: 1.2309 - acc: 0.5837 - val_loss: 1.3713 - val_acc: 0.5441\n",
      "Epoch 8/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.2169 - acc: 0.5906 - val_loss: 1.6085 - val_acc: 0.5235\n",
      "Epoch 9/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.2009 - acc: 0.5996 - val_loss: 1.4393 - val_acc: 0.5037\n",
      "Epoch 10/250\n",
      "40000/40000 [==============================] - 803s 20ms/step - loss: 1.1894 - acc: 0.6046 - val_loss: 1.4135 - val_acc: 0.5229\n",
      "Epoch 11/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1726 - acc: 0.6098 - val_loss: 1.4046 - val_acc: 0.5361\n",
      "Epoch 12/250\n",
      "40000/40000 [==============================] - 812s 20ms/step - loss: 1.1596 - acc: 0.6148 - val_loss: 1.6012 - val_acc: 0.4856\n",
      "Epoch 13/250\n",
      "40000/40000 [==============================] - 4463s 112ms/step - loss: 1.1543 - acc: 0.6224 - val_loss: 1.4904 - val_acc: 0.5373\n",
      "Epoch 14/250\n",
      "40000/40000 [==============================] - 849s 21ms/step - loss: 1.1414 - acc: 0.6251 - val_loss: 1.6811 - val_acc: 0.5019\n",
      "Epoch 15/250\n",
      "40000/40000 [==============================] - 805s 20ms/step - loss: 1.1297 - acc: 0.6271 - val_loss: 1.5663 - val_acc: 0.5429\n",
      "Epoch 16/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1444 - acc: 0.6270 - val_loss: 1.5416 - val_acc: 0.4717\n",
      "Epoch 17/250\n",
      "40000/40000 [==============================] - 809s 20ms/step - loss: 1.1226 - acc: 0.6337 - val_loss: 1.7183 - val_acc: 0.4762\n",
      "Epoch 18/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1303 - acc: 0.6350 - val_loss: 1.5404 - val_acc: 0.4847\n",
      "Epoch 19/250\n",
      "40000/40000 [==============================] - 806s 20ms/step - loss: 1.1210 - acc: 0.6386 - val_loss: 1.6264 - val_acc: 0.5232\n",
      "Epoch 20/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.0991 - acc: 0.6436 - val_loss: 1.6753 - val_acc: 0.5091\n",
      "Epoch 21/250\n",
      "40000/40000 [==============================] - 806s 20ms/step - loss: 1.1291 - acc: 0.6377 - val_loss: 1.6221 - val_acc: 0.5226\n",
      "Epoch 22/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1265 - acc: 0.6403 - val_loss: 1.5659 - val_acc: 0.4925\n",
      "Epoch 23/250\n",
      "40000/40000 [==============================] - 810s 20ms/step - loss: 1.1291 - acc: 0.6375 - val_loss: 1.7380 - val_acc: 0.4623\n",
      "Epoch 24/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1389 - acc: 0.6369 - val_loss: 1.5464 - val_acc: 0.4984\n",
      "Epoch 25/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.1359 - acc: 0.6412 - val_loss: 1.6633 - val_acc: 0.5042\n",
      "Epoch 26/250\n",
      "40000/40000 [==============================] - 809s 20ms/step - loss: 1.1243 - acc: 0.6381 - val_loss: 1.7601 - val_acc: 0.5280\n",
      "Epoch 27/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1304 - acc: 0.6374 - val_loss: 1.7603 - val_acc: 0.4927\n",
      "Epoch 28/250\n",
      "40000/40000 [==============================] - 810s 20ms/step - loss: 1.1162 - acc: 0.6414 - val_loss: 1.5943 - val_acc: 0.4707\n",
      "Epoch 29/250\n",
      "40000/40000 [==============================] - 809s 20ms/step - loss: 1.1285 - acc: 0.6412 - val_loss: 1.6480 - val_acc: 0.4875\n",
      "Epoch 30/250\n",
      "40000/40000 [==============================] - 811s 20ms/step - loss: 1.1364 - acc: 0.6435 - val_loss: 1.5721 - val_acc: 0.4653\n",
      "Epoch 31/250\n",
      "40000/40000 [==============================] - 811s 20ms/step - loss: 1.1778 - acc: 0.6290 - val_loss: 1.6640 - val_acc: 0.4871\n",
      "Epoch 32/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.1513 - acc: 0.6349 - val_loss: 1.8360 - val_acc: 0.4592\n",
      "Epoch 33/250\n",
      "40000/40000 [==============================] - 812s 20ms/step - loss: 1.1524 - acc: 0.6350 - val_loss: 1.5751 - val_acc: 0.4971\n",
      "Epoch 34/250\n",
      "40000/40000 [==============================] - 825s 21ms/step - loss: 1.1260 - acc: 0.6429 - val_loss: 2.2385 - val_acc: 0.4894\n",
      "Epoch 35/250\n",
      "40000/40000 [==============================] - 1438s 36ms/step - loss: 1.1484 - acc: 0.6372 - val_loss: 1.6542 - val_acc: 0.4669\n",
      "Epoch 36/250\n",
      " 8448/40000 [=====>........................] - ETA: 19:55 - loss: 1.1451 - acc: 0.6377"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-9df74fc259d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(images/255, y,\n",
    "          batch_size=None,\n",
    "          epochs=250,\n",
    "          verbose=1,\n",
    "          validation_data=None,\n",
    "          validation_split=0.2,\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv.compile(loss=rms,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 239s 5ms/step - loss: 1.9623 - acc: 0.2923 - val_loss: 1.7316 - val_acc: 0.3836\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 246s 5ms/step - loss: 1.5681 - acc: 0.4399 - val_loss: 1.4504 - val_acc: 0.4677\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 247s 5ms/step - loss: 1.3896 - acc: 0.5044 - val_loss: 1.3548 - val_acc: 0.5244\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 242s 5ms/step - loss: 1.2748 - acc: 0.5495 - val_loss: 1.4874 - val_acc: 0.4896\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 245s 5ms/step - loss: 1.1773 - acc: 0.5857 - val_loss: 1.1314 - val_acc: 0.6022\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 301s 6ms/step - loss: 1.1035 - acc: 0.6129 - val_loss: 1.1418 - val_acc: 0.5987\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 221s 4ms/step - loss: 1.0520 - acc: 0.6316 - val_loss: 1.0523 - val_acc: 0.6327\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 226s 5ms/step - loss: 0.9970 - acc: 0.6512 - val_loss: 0.9412 - val_acc: 0.6686\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 226s 5ms/step - loss: 0.9498 - acc: 0.6674 - val_loss: 0.8525 - val_acc: 0.7010\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 0.9069 - acc: 0.6822 - val_loss: 0.7986 - val_acc: 0.7209\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 275s 5ms/step - loss: 0.8765 - acc: 0.6925 - val_loss: 0.7815 - val_acc: 0.7311\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 261s 5ms/step - loss: 0.8391 - acc: 0.7060 - val_loss: 0.7223 - val_acc: 0.7484\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.8076 - acc: 0.7148 - val_loss: 0.7196 - val_acc: 0.7454\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 136s 3ms/step - loss: 0.7766 - acc: 0.7259 - val_loss: 0.6551 - val_acc: 0.7682\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 0.7485 - acc: 0.7367 - val_loss: 0.6277 - val_acc: 0.7819\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.7211 - acc: 0.7459 - val_loss: 0.6360 - val_acc: 0.7746\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 141s 3ms/step - loss: 0.6961 - acc: 0.7547 - val_loss: 0.5745 - val_acc: 0.7992\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 1598s 32ms/step - loss: 0.6741 - acc: 0.7636 - val_loss: 0.5511 - val_acc: 0.8100\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 133s 3ms/step - loss: 0.6526 - acc: 0.7698 - val_loss: 0.5504 - val_acc: 0.8106\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 126s 3ms/step - loss: 0.6250 - acc: 0.7797 - val_loss: 0.4727 - val_acc: 0.8385\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 0.5995 - acc: 0.7882 - val_loss: 0.5371 - val_acc: 0.8055\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 0.5883 - acc: 0.7941 - val_loss: 0.5593 - val_acc: 0.7954\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 450s 9ms/step - loss: 0.5688 - acc: 0.7964 - val_loss: 0.4406 - val_acc: 0.8427\n",
      "Epoch 24/100\n",
      "49250/50000 [============================>.] - ETA: 1s - loss: 0.5461 - acc: 0.8076"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-34a1d24f0c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_roll_2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_var_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(images/255, y,\n",
    "          batch_size=250,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(images_roll_2/255, y_var_2),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 170s 4ms/step - loss: 0.8410 - acc: 0.7156 - val_loss: 0.7593 - val_acc: 0.7388\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8481 - acc: 0.7155 - val_loss: 0.7761 - val_acc: 0.7332\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8478 - acc: 0.7174 - val_loss: 0.7857 - val_acc: 0.7339\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 2766s 69ms/step - loss: 0.8430 - acc: 0.7164 - val_loss: 0.8695 - val_acc: 0.7079\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8461 - acc: 0.7163 - val_loss: 0.7957 - val_acc: 0.7230\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8442 - acc: 0.7174 - val_loss: 0.7734 - val_acc: 0.7361\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8413 - acc: 0.7183 - val_loss: 0.7726 - val_acc: 0.7366\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8363 - acc: 0.7169 - val_loss: 0.9544 - val_acc: 0.6641\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.8457 - acc: 0.7187 - val_loss: 0.7116 - val_acc: 0.7581\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 178s 4ms/step - loss: 0.8430 - acc: 0.7166 - val_loss: 0.7568 - val_acc: 0.7402\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 3762s 94ms/step - loss: 0.8402 - acc: 0.7172 - val_loss: 0.7785 - val_acc: 0.7290\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8396 - acc: 0.7172 - val_loss: 0.8264 - val_acc: 0.7198\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 184s 5ms/step - loss: 0.8381 - acc: 0.7179 - val_loss: 0.7331 - val_acc: 0.7547\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 187s 5ms/step - loss: 0.8425 - acc: 0.7176 - val_loss: 0.7389 - val_acc: 0.7494\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 190s 5ms/step - loss: 0.8441 - acc: 0.7182 - val_loss: 0.7652 - val_acc: 0.7400\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 192s 5ms/step - loss: 0.8329 - acc: 0.7190 - val_loss: 0.7680 - val_acc: 0.7369\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 195s 5ms/step - loss: 0.8429 - acc: 0.7175 - val_loss: 0.7197 - val_acc: 0.7567\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 208s 5ms/step - loss: 0.8353 - acc: 0.7214 - val_loss: 0.8626 - val_acc: 0.7061\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 212s 5ms/step - loss: 0.8335 - acc: 0.7224 - val_loss: 0.7740 - val_acc: 0.7325\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 217s 5ms/step - loss: 0.8265 - acc: 0.7195 - val_loss: 0.8028 - val_acc: 0.7336\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 222s 6ms/step - loss: 0.8359 - acc: 0.7186 - val_loss: 0.8003 - val_acc: 0.7320\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 226s 6ms/step - loss: 0.8395 - acc: 0.7193 - val_loss: 0.7721 - val_acc: 0.7378\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 220s 6ms/step - loss: 0.8373 - acc: 0.7205 - val_loss: 0.8592 - val_acc: 0.7052\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 220s 5ms/step - loss: 0.8412 - acc: 0.7196 - val_loss: 0.6938 - val_acc: 0.7672\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 174s 4ms/step - loss: 0.8329 - acc: 0.7198 - val_loss: 0.7110 - val_acc: 0.7609\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8383 - acc: 0.7177 - val_loss: 0.8968 - val_acc: 0.6969\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8324 - acc: 0.7224 - val_loss: 0.7930 - val_acc: 0.7244\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8334 - acc: 0.7219 - val_loss: 0.7780 - val_acc: 0.7362\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8341 - acc: 0.7219 - val_loss: 0.8611 - val_acc: 0.7110\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 180s 5ms/step - loss: 0.8306 - acc: 0.7227 - val_loss: 0.7436 - val_acc: 0.7450\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.8339 - acc: 0.7203 - val_loss: 0.7056 - val_acc: 0.7621\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8279 - acc: 0.7219 - val_loss: 0.8374 - val_acc: 0.7162\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8344 - acc: 0.7232 - val_loss: 0.8136 - val_acc: 0.7184\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8349 - acc: 0.7210 - val_loss: 0.7277 - val_acc: 0.7529\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8310 - acc: 0.7230 - val_loss: 0.8544 - val_acc: 0.7095\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8225 - acc: 0.7254 - val_loss: 0.7066 - val_acc: 0.7588\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8278 - acc: 0.7206 - val_loss: 0.7831 - val_acc: 0.7329\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8274 - acc: 0.7228 - val_loss: 0.7903 - val_acc: 0.7351\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8255 - acc: 0.7235 - val_loss: 0.8814 - val_acc: 0.7165\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8274 - acc: 0.7260 - val_loss: 0.8096 - val_acc: 0.7296\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8292 - acc: 0.7264 - val_loss: 0.7106 - val_acc: 0.7588\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8262 - acc: 0.7268 - val_loss: 0.8164 - val_acc: 0.7242\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8325 - acc: 0.7223 - val_loss: 0.7264 - val_acc: 0.7558\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8281 - acc: 0.7231 - val_loss: 0.8070 - val_acc: 0.7261\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8291 - acc: 0.7225 - val_loss: 0.7831 - val_acc: 0.7324\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8339 - acc: 0.7230 - val_loss: 0.8058 - val_acc: 0.7271\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8338 - acc: 0.7232 - val_loss: 0.7262 - val_acc: 0.7511\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8385 - acc: 0.7231 - val_loss: 0.7896 - val_acc: 0.7343\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8262 - acc: 0.7243 - val_loss: 0.7859 - val_acc: 0.7362\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8286 - acc: 0.7268 - val_loss: 0.8372 - val_acc: 0.7132\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8237 - acc: 0.7272 - val_loss: 1.0142 - val_acc: 0.6636\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 173s 4ms/step - loss: 0.8287 - acc: 0.7290 - val_loss: 0.6981 - val_acc: 0.7622\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 180s 4ms/step - loss: 0.8340 - acc: 0.7242 - val_loss: 0.8697 - val_acc: 0.7130\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8257 - acc: 0.7246 - val_loss: 0.7123 - val_acc: 0.7557\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8317 - acc: 0.7235 - val_loss: 0.7184 - val_acc: 0.7545\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8300 - acc: 0.7258 - val_loss: 0.7804 - val_acc: 0.7372\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8196 - acc: 0.7266 - val_loss: 0.8502 - val_acc: 0.7257\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8318 - acc: 0.7253 - val_loss: 0.6898 - val_acc: 0.7708\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8432 - acc: 0.7217 - val_loss: 0.7323 - val_acc: 0.7553\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8342 - acc: 0.7260 - val_loss: 0.6976 - val_acc: 0.7681\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8425 - acc: 0.7236 - val_loss: 0.7403 - val_acc: 0.7541\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8350 - acc: 0.7244 - val_loss: 0.8478 - val_acc: 0.7341\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8350 - acc: 0.7258 - val_loss: 0.7840 - val_acc: 0.7379\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8297 - acc: 0.7253 - val_loss: 0.8624 - val_acc: 0.7120\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8200 - acc: 0.7297 - val_loss: 0.9685 - val_acc: 0.6868\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8337 - acc: 0.7250 - val_loss: 0.7464 - val_acc: 0.7524\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8405 - acc: 0.7240 - val_loss: 0.7385 - val_acc: 0.7545\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8438 - acc: 0.7239 - val_loss: 0.7323 - val_acc: 0.7492\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8470 - acc: 0.7215 - val_loss: 0.7368 - val_acc: 0.7569\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8439 - acc: 0.7239 - val_loss: 0.8599 - val_acc: 0.7145\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8498 - acc: 0.7218 - val_loss: 0.8931 - val_acc: 0.7040\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8483 - acc: 0.7209 - val_loss: 0.7795 - val_acc: 0.7392\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.8418 - acc: 0.7251 - val_loss: 0.7369 - val_acc: 0.7483\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.8295 - acc: 0.7274 - val_loss: 0.7402 - val_acc: 0.7598\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8419 - acc: 0.7224 - val_loss: 0.7899 - val_acc: 0.7378\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8556 - acc: 0.7219 - val_loss: 0.8182 - val_acc: 0.7276\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8480 - acc: 0.7224 - val_loss: 0.7305 - val_acc: 0.7597\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8521 - acc: 0.7223 - val_loss: 0.7452 - val_acc: 0.7523\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8337 - acc: 0.7227 - val_loss: 0.7854 - val_acc: 0.7315\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8372 - acc: 0.7265 - val_loss: 0.7514 - val_acc: 0.7467\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8224 - acc: 0.7285 - val_loss: 0.7271 - val_acc: 0.7596\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8367 - acc: 0.7252 - val_loss: 0.7578 - val_acc: 0.7484\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8548 - acc: 0.7229 - val_loss: 0.7471 - val_acc: 0.7611\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8461 - acc: 0.7249 - val_loss: 0.7089 - val_acc: 0.7649\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8444 - acc: 0.7267 - val_loss: 0.7306 - val_acc: 0.7553\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8457 - acc: 0.7258 - val_loss: 0.8161 - val_acc: 0.7253\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8479 - acc: 0.7245 - val_loss: 0.7939 - val_acc: 0.7351\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8582 - acc: 0.7225 - val_loss: 0.8227 - val_acc: 0.7173\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8510 - acc: 0.7224 - val_loss: 0.7785 - val_acc: 0.7471\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8665 - acc: 0.7210 - val_loss: 0.7447 - val_acc: 0.7515\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 0.8562 - acc: 0.7217 - val_loss: 0.7886 - val_acc: 0.7388\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 0.8495 - acc: 0.7244 - val_loss: 0.8480 - val_acc: 0.7226\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8527 - acc: 0.7211 - val_loss: 0.8353 - val_acc: 0.7175\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8533 - acc: 0.7207 - val_loss: 0.7371 - val_acc: 0.7564\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8863 - acc: 0.7161 - val_loss: 0.7326 - val_acc: 0.7556\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8626 - acc: 0.7199 - val_loss: 0.7089 - val_acc: 0.7636\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8633 - acc: 0.7199 - val_loss: 0.7195 - val_acc: 0.7555\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8717 - acc: 0.7186 - val_loss: 0.7412 - val_acc: 0.7611\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8448 - acc: 0.7229 - val_loss: 0.7775 - val_acc: 0.7456\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8893 - acc: 0.7160 - val_loss: 0.7590 - val_acc: 0.7575\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8808 - acc: 0.7179 - val_loss: 0.7208 - val_acc: 0.7565\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8798 - acc: 0.7186 - val_loss: 0.7788 - val_acc: 0.7508\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8805 - acc: 0.7195 - val_loss: 0.8732 - val_acc: 0.7156\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8896 - acc: 0.7176 - val_loss: 0.8646 - val_acc: 0.7162\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8895 - acc: 0.7163 - val_loss: 0.7448 - val_acc: 0.7498\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8782 - acc: 0.7190 - val_loss: 0.9819 - val_acc: 0.6568\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8613 - acc: 0.7224 - val_loss: 0.8584 - val_acc: 0.7226\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8582 - acc: 0.7210 - val_loss: 0.7103 - val_acc: 0.7672\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8546 - acc: 0.7262 - val_loss: 0.7385 - val_acc: 0.7533\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.9160 - acc: 0.7120 - val_loss: 0.8340 - val_acc: 0.7114\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8974 - acc: 0.7135 - val_loss: 0.7709 - val_acc: 0.7404\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8727 - acc: 0.7225 - val_loss: 0.7335 - val_acc: 0.7484\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 0.8926 - acc: 0.7152 - val_loss: 0.7477 - val_acc: 0.7506\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.8882 - acc: 0.7151 - val_loss: 0.7803 - val_acc: 0.7346\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8859 - acc: 0.7150 - val_loss: 0.9072 - val_acc: 0.7046\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8811 - acc: 0.7179 - val_loss: 0.8418 - val_acc: 0.7284\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8959 - acc: 0.7136 - val_loss: 0.7649 - val_acc: 0.7495\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8954 - acc: 0.7118 - val_loss: 0.7740 - val_acc: 0.7423\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8903 - acc: 0.7196 - val_loss: 0.7515 - val_acc: 0.7539\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8905 - acc: 0.7189 - val_loss: 0.7041 - val_acc: 0.7659\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.9099 - acc: 0.7138 - val_loss: 0.8550 - val_acc: 0.7139\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8776 - acc: 0.7173 - val_loss: 0.7939 - val_acc: 0.7352\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.9130 - acc: 0.7140 - val_loss: 0.7246 - val_acc: 0.7612\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8995 - acc: 0.7151 - val_loss: 0.7540 - val_acc: 0.7544\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 178s 4ms/step - loss: 0.9267 - acc: 0.7095 - val_loss: 0.7524 - val_acc: 0.7512\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.9084 - acc: 0.7109 - val_loss: 0.7446 - val_acc: 0.7511\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.9076 - acc: 0.7137 - val_loss: 0.7859 - val_acc: 0.7434\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8916 - acc: 0.7122 - val_loss: 0.7767 - val_acc: 0.7329\n",
      "Epoch 129/200\n",
      "11280/40000 [=======>......................] - ETA: 1:52 - loss: 0.9064 - acc: 0.7073"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-0fc953a3c7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First update Wednesday morning\n",
    "model_conv_1 = Sequential()\n",
    "model_conv_1.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Flatten())\n",
    "model_conv_1.add(Dense(1000, activation='relu'))\n",
    "model_conv_1.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/75\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.8497 - acc: 0.7212 - val_loss: 0.9540 - val_acc: 0.6875\n",
      "Epoch 2/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8744 - acc: 0.7144 - val_loss: 0.9094 - val_acc: 0.6896\n",
      "Epoch 3/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8243 - acc: 0.7298 - val_loss: 0.8652 - val_acc: 0.7098\n",
      "Epoch 4/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8172 - acc: 0.7277 - val_loss: 0.8621 - val_acc: 0.7045\n",
      "Epoch 5/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7945 - acc: 0.7355 - val_loss: 0.9558 - val_acc: 0.6869\n",
      "Epoch 6/75\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8075 - acc: 0.7325 - val_loss: 0.8788 - val_acc: 0.7159\n",
      "Epoch 7/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7924 - acc: 0.7379 - val_loss: 0.8331 - val_acc: 0.7262\n",
      "Epoch 8/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7884 - acc: 0.7391 - val_loss: 0.8176 - val_acc: 0.7278\n",
      "Epoch 9/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7676 - acc: 0.7426 - val_loss: 0.8086 - val_acc: 0.7238\n",
      "Epoch 10/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7543 - acc: 0.7466 - val_loss: 1.2667 - val_acc: 0.5682\n",
      "Epoch 11/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7656 - acc: 0.7432 - val_loss: 0.8486 - val_acc: 0.7159\n",
      "Epoch 12/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7584 - acc: 0.7467 - val_loss: 0.7765 - val_acc: 0.7364\n",
      "Epoch 13/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7515 - acc: 0.7479 - val_loss: 0.7737 - val_acc: 0.7367\n",
      "Epoch 14/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7444 - acc: 0.7494 - val_loss: 0.7964 - val_acc: 0.7251\n",
      "Epoch 15/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7389 - acc: 0.7530 - val_loss: 0.8101 - val_acc: 0.7217\n",
      "Epoch 16/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7379 - acc: 0.7532 - val_loss: 0.8164 - val_acc: 0.7268\n",
      "Epoch 17/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7362 - acc: 0.7545 - val_loss: 0.8126 - val_acc: 0.7309\n",
      "Epoch 18/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7175 - acc: 0.7590 - val_loss: 0.8376 - val_acc: 0.7241\n",
      "Epoch 19/75\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.7216 - acc: 0.7577 - val_loss: 0.7893 - val_acc: 0.7336\n",
      "Epoch 20/75\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.7270 - acc: 0.7564 - val_loss: 0.7788 - val_acc: 0.7400\n",
      "Epoch 21/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7210 - acc: 0.7582 - val_loss: 0.7908 - val_acc: 0.7300\n",
      "Epoch 22/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7157 - acc: 0.7611 - val_loss: 0.7914 - val_acc: 0.7359\n",
      "Epoch 23/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7142 - acc: 0.7608 - val_loss: 0.8370 - val_acc: 0.7206\n",
      "Epoch 24/75\n",
      "40000/40000 [==============================] - 3721s 93ms/step - loss: 0.7017 - acc: 0.7654 - val_loss: 0.7496 - val_acc: 0.7503\n",
      "Epoch 25/75\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.7046 - acc: 0.7612 - val_loss: 0.7539 - val_acc: 0.7492\n",
      "Epoch 26/75\n",
      "40000/40000 [==============================] - 7313s 183ms/step - loss: 0.6910 - acc: 0.7674 - val_loss: 0.9094 - val_acc: 0.6922\n",
      "Epoch 27/75\n",
      "40000/40000 [==============================] - 7310s 183ms/step - loss: 0.6989 - acc: 0.7669 - val_loss: 0.8447 - val_acc: 0.7157\n",
      "Epoch 28/75\n",
      "40000/40000 [==============================] - 3552s 89ms/step - loss: 0.7117 - acc: 0.7603 - val_loss: 0.7325 - val_acc: 0.7607\n",
      "Epoch 29/75\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.6881 - acc: 0.7687 - val_loss: 0.7376 - val_acc: 0.7509\n",
      "Epoch 30/75\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.6895 - acc: 0.7682 - val_loss: 0.7777 - val_acc: 0.7453\n",
      "Epoch 31/75\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.6953 - acc: 0.7638 - val_loss: 0.7871 - val_acc: 0.7345\n",
      "Epoch 32/75\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.6923 - acc: 0.7673 - val_loss: 0.7756 - val_acc: 0.7340\n",
      "Epoch 33/75\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6833 - acc: 0.7695 - val_loss: 0.7490 - val_acc: 0.7423\n",
      "Epoch 34/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6915 - acc: 0.7677 - val_loss: 0.6991 - val_acc: 0.7658\n",
      "Epoch 35/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6621 - acc: 0.7756 - val_loss: 0.8530 - val_acc: 0.7204\n",
      "Epoch 36/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6802 - acc: 0.7703 - val_loss: 0.7956 - val_acc: 0.7301\n",
      "Epoch 37/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6747 - acc: 0.7719 - val_loss: 0.7452 - val_acc: 0.7444\n",
      "Epoch 38/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6792 - acc: 0.7715 - val_loss: 0.8386 - val_acc: 0.7316\n",
      "Epoch 39/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6666 - acc: 0.7742 - val_loss: 0.7238 - val_acc: 0.7525\n",
      "Epoch 40/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6695 - acc: 0.7762 - val_loss: 0.7250 - val_acc: 0.7488\n",
      "Epoch 41/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6708 - acc: 0.7729 - val_loss: 0.7342 - val_acc: 0.7428\n",
      "Epoch 42/75\n",
      "40000/40000 [==============================] - 615s 15ms/step - loss: 0.6712 - acc: 0.7747 - val_loss: 0.7703 - val_acc: 0.7365\n",
      "Epoch 43/75\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6601 - acc: 0.7793 - val_loss: 0.7210 - val_acc: 0.7585\n",
      "Epoch 44/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6629 - acc: 0.7795 - val_loss: 0.7637 - val_acc: 0.7490\n",
      "Epoch 45/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6632 - acc: 0.7748 - val_loss: 0.7487 - val_acc: 0.7393\n",
      "Epoch 46/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6566 - acc: 0.7778 - val_loss: 0.7821 - val_acc: 0.7399\n",
      "Epoch 47/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6586 - acc: 0.7770 - val_loss: 0.7297 - val_acc: 0.7556\n",
      "Epoch 48/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6516 - acc: 0.7787 - val_loss: 0.7427 - val_acc: 0.7490\n",
      "Epoch 49/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6572 - acc: 0.7771 - val_loss: 0.7340 - val_acc: 0.7548\n",
      "Epoch 50/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6467 - acc: 0.7816 - val_loss: 0.8902 - val_acc: 0.7129\n",
      "Epoch 51/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6460 - acc: 0.7809 - val_loss: 0.7321 - val_acc: 0.7542\n",
      "Epoch 52/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6468 - acc: 0.7833 - val_loss: 0.7582 - val_acc: 0.7442\n",
      "Epoch 53/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6503 - acc: 0.7807 - val_loss: 0.7045 - val_acc: 0.7576\n",
      "Epoch 54/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6521 - acc: 0.7799 - val_loss: 0.7741 - val_acc: 0.7475\n",
      "Epoch 55/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6431 - acc: 0.7838 - val_loss: 0.6771 - val_acc: 0.7728\n",
      "Epoch 56/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6429 - acc: 0.7775 - val_loss: 0.7157 - val_acc: 0.7569\n",
      "Epoch 57/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6381 - acc: 0.7858 - val_loss: 0.9231 - val_acc: 0.6964\n",
      "Epoch 58/75\n",
      "40000/40000 [==============================] - 1456s 36ms/step - loss: 0.6516 - acc: 0.7799 - val_loss: 0.7719 - val_acc: 0.7436\n",
      "Epoch 59/75\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.6392 - acc: 0.7829 - val_loss: 0.7139 - val_acc: 0.7616\n",
      "Epoch 60/75\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6364 - acc: 0.7833 - val_loss: 0.7075 - val_acc: 0.7565\n",
      "Epoch 61/75\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6441 - acc: 0.7804 - val_loss: 0.7214 - val_acc: 0.7557\n",
      "Epoch 62/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6368 - acc: 0.7847 - val_loss: 0.8484 - val_acc: 0.7300\n",
      "Epoch 63/75\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.6314 - acc: 0.7855 - val_loss: 0.8232 - val_acc: 0.7302\n",
      "Epoch 64/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6411 - acc: 0.7824 - val_loss: 0.7154 - val_acc: 0.7586\n",
      "Epoch 65/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6323 - acc: 0.7845 - val_loss: 0.6895 - val_acc: 0.7658\n",
      "Epoch 66/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6293 - acc: 0.7868 - val_loss: 0.6906 - val_acc: 0.7643\n",
      "Epoch 67/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6317 - acc: 0.7853 - val_loss: 0.7455 - val_acc: 0.7476\n",
      "Epoch 68/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6313 - acc: 0.7866 - val_loss: 0.8246 - val_acc: 0.7336\n",
      "Epoch 69/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6276 - acc: 0.7859 - val_loss: 0.7439 - val_acc: 0.7463\n",
      "Epoch 70/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6257 - acc: 0.7872 - val_loss: 0.6914 - val_acc: 0.7651\n",
      "Epoch 71/75\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6366 - acc: 0.7846 - val_loss: 0.7515 - val_acc: 0.7447\n",
      "Epoch 72/75\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6265 - acc: 0.7863 - val_loss: 0.8186 - val_acc: 0.7345\n",
      "Epoch 73/75\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6180 - acc: 0.7889 - val_loss: 0.7401 - val_acc: 0.7495\n",
      "Epoch 74/75\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6306 - acc: 0.7864 - val_loss: 0.7281 - val_acc: 0.7589\n",
      "Epoch 75/75\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6177 - acc: 0.7925 - val_loss: 0.6878 - val_acc: 0.7688\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_conv_1.fit(X_train/255, y_train,\n",
    "          batch_size=1000,\n",
    "          epochs=75,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b169390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['acc'])\n",
    "plt.plot(history_2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a17a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv_1 = Sequential()\n",
    "model_conv_1.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Flatten())\n",
    "model_conv_1.add(Dense(1000, activation='relu'))\n",
    "model_conv_1.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 2.1989 - acc: 0.1864 - val_loss: 1.9906 - val_acc: 0.2653\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.9762 - acc: 0.2741 - val_loss: 2.0385 - val_acc: 0.2501\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.9137 - acc: 0.3008 - val_loss: 1.8261 - val_acc: 0.3516\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.8257 - acc: 0.3360 - val_loss: 1.7124 - val_acc: 0.3757\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.7861 - acc: 0.3564 - val_loss: 1.6912 - val_acc: 0.3873\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.7399 - acc: 0.3655 - val_loss: 1.7154 - val_acc: 0.3842\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.6944 - acc: 0.3817 - val_loss: 1.8445 - val_acc: 0.3459\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.6709 - acc: 0.3892 - val_loss: 1.5556 - val_acc: 0.4433\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.6180 - acc: 0.4103 - val_loss: 1.6183 - val_acc: 0.4185\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.5952 - acc: 0.4186 - val_loss: 1.4752 - val_acc: 0.4734\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.5609 - acc: 0.4373 - val_loss: 1.4792 - val_acc: 0.4750\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.5401 - acc: 0.4409 - val_loss: 1.5393 - val_acc: 0.4604\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.5073 - acc: 0.4536 - val_loss: 1.3667 - val_acc: 0.5100\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.4818 - acc: 0.4663 - val_loss: 1.4678 - val_acc: 0.4761\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.4652 - acc: 0.4710 - val_loss: 1.5603 - val_acc: 0.4534\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.4543 - acc: 0.4770 - val_loss: 1.3947 - val_acc: 0.5053\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.4266 - acc: 0.4870 - val_loss: 1.3733 - val_acc: 0.5118\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.4081 - acc: 0.4956 - val_loss: 1.2787 - val_acc: 0.5443\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.3932 - acc: 0.5024 - val_loss: 1.3165 - val_acc: 0.5297\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 965s 24ms/step - loss: 1.3766 - acc: 0.5058 - val_loss: 1.3099 - val_acc: 0.5310\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.3535 - acc: 0.5162 - val_loss: 1.3257 - val_acc: 0.5227\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.3488 - acc: 0.5171 - val_loss: 1.2913 - val_acc: 0.5452\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.3170 - acc: 0.5292 - val_loss: 1.3861 - val_acc: 0.5171\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.3191 - acc: 0.5305 - val_loss: 1.2931 - val_acc: 0.5385\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 571s 14ms/step - loss: 1.2914 - acc: 0.5391 - val_loss: 1.1926 - val_acc: 0.5764\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 454s 11ms/step - loss: 1.2965 - acc: 0.5388 - val_loss: 1.2209 - val_acc: 0.5691\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.2646 - acc: 0.5488 - val_loss: 1.1752 - val_acc: 0.5865\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 272s 7ms/step - loss: 1.2720 - acc: 0.5468 - val_loss: 1.2301 - val_acc: 0.5669\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 182s 5ms/step - loss: 1.2545 - acc: 0.5527 - val_loss: 1.2157 - val_acc: 0.5655\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.2248 - acc: 0.5633 - val_loss: 1.1886 - val_acc: 0.5844\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.2314 - acc: 0.5609 - val_loss: 1.2207 - val_acc: 0.5630\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.2067 - acc: 0.5697 - val_loss: 1.0921 - val_acc: 0.6171\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 345s 9ms/step - loss: 1.2071 - acc: 0.5707 - val_loss: 1.1317 - val_acc: 0.6000\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.1927 - acc: 0.5760 - val_loss: 1.1281 - val_acc: 0.5993\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.1863 - acc: 0.5800 - val_loss: 1.1000 - val_acc: 0.6132\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.1665 - acc: 0.5861 - val_loss: 1.1627 - val_acc: 0.5916\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.1642 - acc: 0.5859 - val_loss: 1.1193 - val_acc: 0.6067\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.1567 - acc: 0.5887 - val_loss: 1.1163 - val_acc: 0.6046\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.1414 - acc: 0.5962 - val_loss: 1.1642 - val_acc: 0.5785\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 896s 22ms/step - loss: 1.1329 - acc: 0.5988 - val_loss: 1.1538 - val_acc: 0.5982\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 1.1241 - acc: 0.6014 - val_loss: 1.1498 - val_acc: 0.5985\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.1154 - acc: 0.6050 - val_loss: 1.1072 - val_acc: 0.6111\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.1046 - acc: 0.6076 - val_loss: 1.0388 - val_acc: 0.6297\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.0929 - acc: 0.6107 - val_loss: 1.0868 - val_acc: 0.6181\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.0864 - acc: 0.6149 - val_loss: 1.1268 - val_acc: 0.6004\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 224s 6ms/step - loss: 1.0760 - acc: 0.6183 - val_loss: 1.1019 - val_acc: 0.6133\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.0713 - acc: 0.6193 - val_loss: 1.0746 - val_acc: 0.6227\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.0563 - acc: 0.6286 - val_loss: 1.0594 - val_acc: 0.6332\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.0542 - acc: 0.6273 - val_loss: 0.9991 - val_acc: 0.6481\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 1087s 27ms/step - loss: 1.0399 - acc: 0.6303 - val_loss: 1.0869 - val_acc: 0.6271\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0387 - acc: 0.6319 - val_loss: 0.9822 - val_acc: 0.6521\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.0323 - acc: 0.6317 - val_loss: 1.0082 - val_acc: 0.6457\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.0179 - acc: 0.6380 - val_loss: 1.0081 - val_acc: 0.6496\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.0170 - acc: 0.6399 - val_loss: 0.9622 - val_acc: 0.6587\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.0018 - acc: 0.6421 - val_loss: 1.2192 - val_acc: 0.5819\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.0134 - acc: 0.6407 - val_loss: 0.9298 - val_acc: 0.6719\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9920 - acc: 0.6488 - val_loss: 1.0264 - val_acc: 0.6440\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.9893 - acc: 0.6501 - val_loss: 0.9792 - val_acc: 0.6565\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.9755 - acc: 0.6539 - val_loss: 0.9431 - val_acc: 0.6666\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 158s 4ms/step - loss: 0.9760 - acc: 0.6529 - val_loss: 0.9195 - val_acc: 0.6775\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.9672 - acc: 0.6581 - val_loss: 1.0408 - val_acc: 0.6332\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.9615 - acc: 0.6601 - val_loss: 0.9710 - val_acc: 0.6604\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 7680s 192ms/step - loss: 0.9586 - acc: 0.6609 - val_loss: 0.9665 - val_acc: 0.6627\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.9498 - acc: 0.6631 - val_loss: 0.9970 - val_acc: 0.6577\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.9442 - acc: 0.6667 - val_loss: 0.9452 - val_acc: 0.6677\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.9352 - acc: 0.6706 - val_loss: 0.9426 - val_acc: 0.6727\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.9311 - acc: 0.6651 - val_loss: 0.9585 - val_acc: 0.6718\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.9278 - acc: 0.6712 - val_loss: 0.9651 - val_acc: 0.6631\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 0.9108 - acc: 0.6799 - val_loss: 0.9727 - val_acc: 0.6651\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.9118 - acc: 0.6777 - val_loss: 0.9394 - val_acc: 0.6697\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.9083 - acc: 0.6776 - val_loss: 0.8918 - val_acc: 0.6861\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.9052 - acc: 0.6796 - val_loss: 0.9323 - val_acc: 0.6756\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.8878 - acc: 0.6864 - val_loss: 0.9188 - val_acc: 0.6745\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.8880 - acc: 0.6844 - val_loss: 0.9229 - val_acc: 0.6765\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.8857 - acc: 0.6872 - val_loss: 0.8809 - val_acc: 0.6938\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.8826 - acc: 0.6873 - val_loss: 0.9003 - val_acc: 0.6876\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.8693 - acc: 0.6915 - val_loss: 0.9395 - val_acc: 0.6757\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.8659 - acc: 0.6923 - val_loss: 0.8889 - val_acc: 0.6875\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.8662 - acc: 0.6949 - val_loss: 0.9058 - val_acc: 0.6878\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.8555 - acc: 0.6956 - val_loss: 0.8765 - val_acc: 0.6922\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.8515 - acc: 0.6966 - val_loss: 0.8415 - val_acc: 0.7055\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8474 - acc: 0.7002 - val_loss: 0.9808 - val_acc: 0.6643\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.8464 - acc: 0.7003 - val_loss: 0.9554 - val_acc: 0.6695\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.8495 - acc: 0.6991 - val_loss: 0.8311 - val_acc: 0.7055\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.8285 - acc: 0.7068 - val_loss: 0.8709 - val_acc: 0.6978\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.8325 - acc: 0.7040 - val_loss: 0.9351 - val_acc: 0.6792\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.8164 - acc: 0.7111 - val_loss: 0.8601 - val_acc: 0.7025\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.8227 - acc: 0.7103 - val_loss: 0.8836 - val_acc: 0.6928\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.8106 - acc: 0.7120 - val_loss: 0.9034 - val_acc: 0.6922\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.8102 - acc: 0.7128 - val_loss: 0.8317 - val_acc: 0.7082\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.8076 - acc: 0.7137 - val_loss: 0.8952 - val_acc: 0.6879\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.8007 - acc: 0.7142 - val_loss: 0.8935 - val_acc: 0.6919\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.7915 - acc: 0.7196 - val_loss: 0.8834 - val_acc: 0.6912\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7919 - acc: 0.7186 - val_loss: 0.8686 - val_acc: 0.6991\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7802 - acc: 0.7233 - val_loss: 0.8201 - val_acc: 0.7113\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7872 - acc: 0.7229 - val_loss: 0.8724 - val_acc: 0.6977\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.7781 - acc: 0.7244 - val_loss: 0.8558 - val_acc: 0.7064\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.7843 - acc: 0.7209 - val_loss: 0.8833 - val_acc: 0.6928\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.7650 - acc: 0.7271 - val_loss: 0.8008 - val_acc: 0.7201\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.7707 - acc: 0.7246 - val_loss: 0.8257 - val_acc: 0.7116\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.7605 - acc: 0.7306 - val_loss: 0.8019 - val_acc: 0.7177\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.7615 - acc: 0.7286 - val_loss: 0.8053 - val_acc: 0.7186\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.7597 - acc: 0.7294 - val_loss: 0.8199 - val_acc: 0.7151\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7614 - acc: 0.7297 - val_loss: 0.8863 - val_acc: 0.6984\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7490 - acc: 0.7354 - val_loss: 0.8055 - val_acc: 0.7237\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.7439 - acc: 0.7357 - val_loss: 0.8309 - val_acc: 0.7101\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.7408 - acc: 0.7361 - val_loss: 0.8445 - val_acc: 0.6999\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 6279s 157ms/step - loss: 0.7408 - acc: 0.7378 - val_loss: 0.7958 - val_acc: 0.7218\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.7370 - acc: 0.7363 - val_loss: 0.8026 - val_acc: 0.7213\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7309 - acc: 0.7386 - val_loss: 0.8674 - val_acc: 0.7008\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7210 - acc: 0.7453 - val_loss: 0.8137 - val_acc: 0.7125\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.7207 - acc: 0.7457 - val_loss: 0.8314 - val_acc: 0.7095\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7180 - acc: 0.7439 - val_loss: 0.7967 - val_acc: 0.7244\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 174s 4ms/step - loss: 0.7213 - acc: 0.7423 - val_loss: 0.8455 - val_acc: 0.7147\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.7078 - acc: 0.7494 - val_loss: 0.7960 - val_acc: 0.7236\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7102 - acc: 0.7449 - val_loss: 0.8021 - val_acc: 0.7250\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7033 - acc: 0.7494 - val_loss: 0.7788 - val_acc: 0.7294\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6949 - acc: 0.7517 - val_loss: 0.7950 - val_acc: 0.7260\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6923 - acc: 0.7556 - val_loss: 0.8195 - val_acc: 0.7181\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6945 - acc: 0.7516 - val_loss: 0.8329 - val_acc: 0.7152\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6909 - acc: 0.7553 - val_loss: 0.7853 - val_acc: 0.7290\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6864 - acc: 0.7555 - val_loss: 0.7729 - val_acc: 0.7360\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6832 - acc: 0.7574 - val_loss: 0.7650 - val_acc: 0.7326\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.6786 - acc: 0.7590 - val_loss: 0.8456 - val_acc: 0.7098\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6811 - acc: 0.7586 - val_loss: 0.7698 - val_acc: 0.7379\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6697 - acc: 0.7627 - val_loss: 0.7683 - val_acc: 0.7381\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6769 - acc: 0.7617 - val_loss: 0.7796 - val_acc: 0.7297\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.6577 - acc: 0.7651 - val_loss: 0.7986 - val_acc: 0.7212\n",
      "Epoch 129/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6686 - acc: 0.7614 - val_loss: 0.8308 - val_acc: 0.7195\n",
      "Epoch 130/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.6682 - acc: 0.7606 - val_loss: 0.8143 - val_acc: 0.7218\n",
      "Epoch 131/200\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.6562 - acc: 0.7645 - val_loss: 0.7837 - val_acc: 0.7338\n",
      "Epoch 132/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.6564 - acc: 0.7678 - val_loss: 0.7670 - val_acc: 0.7338\n",
      "Epoch 133/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6523 - acc: 0.7672 - val_loss: 0.7935 - val_acc: 0.7260\n",
      "Epoch 134/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6480 - acc: 0.7684 - val_loss: 0.7859 - val_acc: 0.7326\n",
      "Epoch 135/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6532 - acc: 0.7676 - val_loss: 0.7653 - val_acc: 0.7353\n",
      "Epoch 136/200\n",
      "40000/40000 [==============================] - 451s 11ms/step - loss: 0.6397 - acc: 0.7718 - val_loss: 0.7890 - val_acc: 0.7315\n",
      "Epoch 137/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6369 - acc: 0.7726 - val_loss: 0.7826 - val_acc: 0.7312\n",
      "Epoch 138/200\n",
      "40000/40000 [==============================] - 5032s 126ms/step - loss: 0.6415 - acc: 0.7719 - val_loss: 0.7551 - val_acc: 0.7459\n",
      "Epoch 139/200\n",
      "40000/40000 [==============================] - 1868s 47ms/step - loss: 0.6422 - acc: 0.7707 - val_loss: 0.7520 - val_acc: 0.7399\n",
      "Epoch 140/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6272 - acc: 0.7780 - val_loss: 0.7356 - val_acc: 0.7490\n",
      "Epoch 141/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.6294 - acc: 0.7735 - val_loss: 0.7769 - val_acc: 0.7328\n",
      "Epoch 142/200\n",
      "40000/40000 [==============================] - 178s 4ms/step - loss: 0.6262 - acc: 0.7783 - val_loss: 0.7516 - val_acc: 0.7451\n",
      "Epoch 143/200\n",
      "40000/40000 [==============================] - 204s 5ms/step - loss: 0.6191 - acc: 0.7782 - val_loss: 0.7520 - val_acc: 0.7431\n",
      "Epoch 144/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.6201 - acc: 0.7780 - val_loss: 0.7529 - val_acc: 0.7427\n",
      "Epoch 145/200\n",
      "40000/40000 [==============================] - 219s 5ms/step - loss: 0.6226 - acc: 0.7796 - val_loss: 0.7705 - val_acc: 0.7316\n",
      "Epoch 146/200\n",
      "40000/40000 [==============================] - 239s 6ms/step - loss: 0.6232 - acc: 0.7784 - val_loss: 0.7740 - val_acc: 0.7335\n",
      "Epoch 147/200\n",
      "40000/40000 [==============================] - 249s 6ms/step - loss: 0.6118 - acc: 0.7788 - val_loss: 0.7612 - val_acc: 0.7392\n",
      "Epoch 148/200\n",
      "40000/40000 [==============================] - 252s 6ms/step - loss: 0.6071 - acc: 0.7828 - val_loss: 0.7919 - val_acc: 0.7266\n",
      "Epoch 149/200\n",
      "40000/40000 [==============================] - 254s 6ms/step - loss: 0.6032 - acc: 0.7867 - val_loss: 0.7769 - val_acc: 0.7366\n",
      "Epoch 150/200\n",
      "40000/40000 [==============================] - 230s 6ms/step - loss: 0.5961 - acc: 0.7887 - val_loss: 0.7642 - val_acc: 0.7384\n",
      "Epoch 151/200\n",
      "40000/40000 [==============================] - 236s 6ms/step - loss: 0.5999 - acc: 0.7870 - val_loss: 0.7545 - val_acc: 0.7407\n",
      "Epoch 152/200\n",
      "40000/40000 [==============================] - 250s 6ms/step - loss: 0.6077 - acc: 0.7829 - val_loss: 0.7667 - val_acc: 0.7407\n",
      "Epoch 153/200\n",
      "40000/40000 [==============================] - 245s 6ms/step - loss: 0.5927 - acc: 0.7896 - val_loss: 0.7413 - val_acc: 0.7479\n",
      "Epoch 154/200\n",
      "40000/40000 [==============================] - 249s 6ms/step - loss: 0.5901 - acc: 0.7907 - val_loss: 0.7184 - val_acc: 0.7524\n",
      "Epoch 155/200\n",
      "40000/40000 [==============================] - 254s 6ms/step - loss: 0.5870 - acc: 0.7901 - val_loss: 0.7323 - val_acc: 0.7482\n",
      "Epoch 156/200\n",
      "40000/40000 [==============================] - 247s 6ms/step - loss: 0.5825 - acc: 0.7906 - val_loss: 0.7599 - val_acc: 0.7438\n",
      "Epoch 157/200\n",
      "40000/40000 [==============================] - 218s 5ms/step - loss: 0.5848 - acc: 0.7918 - val_loss: 0.7420 - val_acc: 0.7495\n",
      "Epoch 158/200\n",
      "40000/40000 [==============================] - 220s 5ms/step - loss: 0.5734 - acc: 0.7945 - val_loss: 0.7411 - val_acc: 0.7469\n",
      "Epoch 159/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5866 - acc: 0.7903 - val_loss: 0.7929 - val_acc: 0.7299\n",
      "Epoch 160/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5860 - acc: 0.7922 - val_loss: 0.7368 - val_acc: 0.7508\n",
      "Epoch 161/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.5713 - acc: 0.7939 - val_loss: 0.7562 - val_acc: 0.7441\n",
      "Epoch 162/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.5736 - acc: 0.7962 - val_loss: 0.7250 - val_acc: 0.7546\n",
      "Epoch 163/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.5717 - acc: 0.7956 - val_loss: 0.7442 - val_acc: 0.7507\n",
      "Epoch 164/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5656 - acc: 0.7964 - val_loss: 0.7623 - val_acc: 0.7456\n",
      "Epoch 165/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.5642 - acc: 0.7999 - val_loss: 0.7383 - val_acc: 0.7496\n",
      "Epoch 166/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.5736 - acc: 0.7940 - val_loss: 0.7555 - val_acc: 0.7467\n",
      "Epoch 167/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.5600 - acc: 0.7992 - val_loss: 0.7389 - val_acc: 0.7518\n",
      "Epoch 168/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.5519 - acc: 0.8003 - val_loss: 0.7562 - val_acc: 0.7471\n",
      "Epoch 169/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5575 - acc: 0.8000 - val_loss: 0.7334 - val_acc: 0.7526\n",
      "Epoch 170/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.5441 - acc: 0.8061 - val_loss: 0.7429 - val_acc: 0.7528\n",
      "Epoch 171/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.5505 - acc: 0.8014 - val_loss: 0.7223 - val_acc: 0.7559\n",
      "Epoch 172/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5454 - acc: 0.8044 - val_loss: 0.7302 - val_acc: 0.7546\n",
      "Epoch 173/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.5476 - acc: 0.8034 - val_loss: 0.7402 - val_acc: 0.7531\n",
      "Epoch 174/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.5334 - acc: 0.8078 - val_loss: 0.7347 - val_acc: 0.7557\n",
      "Epoch 175/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.5442 - acc: 0.8055 - val_loss: 0.7622 - val_acc: 0.7439\n",
      "Epoch 176/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5425 - acc: 0.8052 - val_loss: 0.7345 - val_acc: 0.7531\n",
      "Epoch 177/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.5333 - acc: 0.8087 - val_loss: 0.7418 - val_acc: 0.7532\n",
      "Epoch 178/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5265 - acc: 0.8089 - val_loss: 0.7253 - val_acc: 0.7560\n",
      "Epoch 179/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5255 - acc: 0.8145 - val_loss: 0.7563 - val_acc: 0.7533\n",
      "Epoch 180/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.5335 - acc: 0.8123 - val_loss: 0.7859 - val_acc: 0.7438\n",
      "Epoch 181/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5268 - acc: 0.8099 - val_loss: 0.7889 - val_acc: 0.7405\n",
      "Epoch 182/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5183 - acc: 0.8141 - val_loss: 0.7711 - val_acc: 0.7449\n",
      "Epoch 183/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5232 - acc: 0.8113 - val_loss: 0.7486 - val_acc: 0.7555\n",
      "Epoch 184/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5183 - acc: 0.8146 - val_loss: 0.7877 - val_acc: 0.7373\n",
      "Epoch 185/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5108 - acc: 0.8168 - val_loss: 0.7239 - val_acc: 0.7613\n",
      "Epoch 186/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5166 - acc: 0.8142 - val_loss: 0.8467 - val_acc: 0.7291\n",
      "Epoch 187/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.5085 - acc: 0.8157 - val_loss: 0.7507 - val_acc: 0.7491\n",
      "Epoch 188/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5116 - acc: 0.8153 - val_loss: 0.7344 - val_acc: 0.7586\n",
      "Epoch 189/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5062 - acc: 0.8187 - val_loss: 0.7550 - val_acc: 0.7471\n",
      "Epoch 190/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5070 - acc: 0.8177 - val_loss: 0.7437 - val_acc: 0.7523\n",
      "Epoch 191/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5039 - acc: 0.8191 - val_loss: 0.7256 - val_acc: 0.7588\n",
      "Epoch 192/200\n",
      "40000/40000 [==============================] - 7311s 183ms/step - loss: 0.4965 - acc: 0.8214 - val_loss: 0.8054 - val_acc: 0.7314\n",
      "Epoch 193/200\n",
      "40000/40000 [==============================] - 7310s 183ms/step - loss: 0.5033 - acc: 0.8206 - val_loss: 0.7455 - val_acc: 0.7571\n",
      "Epoch 194/200\n",
      "40000/40000 [==============================] - 3715s 93ms/step - loss: 0.4993 - acc: 0.8199 - val_loss: 0.7690 - val_acc: 0.7489\n",
      "Epoch 195/200\n",
      "40000/40000 [==============================] - 7812s 195ms/step - loss: 0.5023 - acc: 0.8199 - val_loss: 0.7297 - val_acc: 0.7599\n",
      "Epoch 196/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4895 - acc: 0.8210 - val_loss: 0.7860 - val_acc: 0.7374\n",
      "Epoch 197/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4855 - acc: 0.8262 - val_loss: 0.7271 - val_acc: 0.7601\n",
      "Epoch 198/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4908 - acc: 0.8242 - val_loss: 0.7426 - val_acc: 0.7522\n",
      "Epoch 199/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4815 - acc: 0.8266 - val_loss: 0.7724 - val_acc: 0.7497\n",
      "Epoch 200/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4829 - acc: 0.8272 - val_loss: 0.7734 - val_acc: 0.7469\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_conv_1.fit(X_train/255, y_train,\n",
    "          batch_size=1000,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b181390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['acc'])\n",
    "plt.plot(history_2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b1735f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4890 - acc: 0.8258 - val_loss: 0.7825 - val_acc: 0.7452\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4856 - acc: 0.8259 - val_loss: 0.7237 - val_acc: 0.7597\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4753 - acc: 0.8284 - val_loss: 0.7454 - val_acc: 0.7565\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4740 - acc: 0.8301 - val_loss: 0.7464 - val_acc: 0.7531\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4822 - acc: 0.8255 - val_loss: 0.7680 - val_acc: 0.7459\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4743 - acc: 0.8292 - val_loss: 0.7286 - val_acc: 0.7624\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4722 - acc: 0.8292 - val_loss: 0.7679 - val_acc: 0.7485\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4662 - acc: 0.8311 - val_loss: 0.7801 - val_acc: 0.7476\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.4711 - acc: 0.8299 - val_loss: 0.7232 - val_acc: 0.7620\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.4656 - acc: 0.8341 - val_loss: 0.7387 - val_acc: 0.7595\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.4599 - acc: 0.8361 - val_loss: 0.8068 - val_acc: 0.7429\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.4620 - acc: 0.8347 - val_loss: 0.7360 - val_acc: 0.7572\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4608 - acc: 0.8340 - val_loss: 0.7829 - val_acc: 0.7543\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4585 - acc: 0.8330 - val_loss: 0.7881 - val_acc: 0.7482\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4492 - acc: 0.8388 - val_loss: 0.7838 - val_acc: 0.7511\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4536 - acc: 0.8363 - val_loss: 0.7622 - val_acc: 0.7570\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4521 - acc: 0.8383 - val_loss: 0.7196 - val_acc: 0.7661\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.4445 - acc: 0.8383 - val_loss: 0.8071 - val_acc: 0.7469\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.4575 - acc: 0.8349 - val_loss: 0.7642 - val_acc: 0.7490\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.4417 - acc: 0.8379 - val_loss: 0.7582 - val_acc: 0.7523\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.4494 - acc: 0.8380 - val_loss: 0.7679 - val_acc: 0.7514\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.4385 - acc: 0.8421 - val_loss: 0.7376 - val_acc: 0.7624\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.4419 - acc: 0.8403 - val_loss: 0.7631 - val_acc: 0.7560\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.4419 - acc: 0.8436 - val_loss: 0.7440 - val_acc: 0.7630\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.4386 - acc: 0.8411 - val_loss: 0.7320 - val_acc: 0.7636\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4299 - acc: 0.8448 - val_loss: 0.7970 - val_acc: 0.7453\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4453 - acc: 0.8402 - val_loss: 0.7590 - val_acc: 0.7605\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4268 - acc: 0.8447 - val_loss: 0.8141 - val_acc: 0.7418\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4262 - acc: 0.8448 - val_loss: 0.7665 - val_acc: 0.7572\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4303 - acc: 0.8457 - val_loss: 0.7967 - val_acc: 0.7506\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.4288 - acc: 0.8447 - val_loss: 0.7539 - val_acc: 0.7603\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4258 - acc: 0.8474 - val_loss: 0.7313 - val_acc: 0.7660\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4230 - acc: 0.8462 - val_loss: 0.7766 - val_acc: 0.7474\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4235 - acc: 0.8464 - val_loss: 0.7320 - val_acc: 0.7693\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4295 - acc: 0.8444 - val_loss: 0.7475 - val_acc: 0.7608\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4172 - acc: 0.8492 - val_loss: 0.7850 - val_acc: 0.7545\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.4118 - acc: 0.8495 - val_loss: 0.7837 - val_acc: 0.7531\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4160 - acc: 0.8494 - val_loss: 0.7566 - val_acc: 0.7618\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4080 - acc: 0.8531 - val_loss: 0.7587 - val_acc: 0.7618\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4110 - acc: 0.8519 - val_loss: 0.7691 - val_acc: 0.7591\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4037 - acc: 0.8527 - val_loss: 0.7796 - val_acc: 0.7546\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4149 - acc: 0.8485 - val_loss: 0.8139 - val_acc: 0.7463\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4029 - acc: 0.8542 - val_loss: 0.8027 - val_acc: 0.7539\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4057 - acc: 0.8533 - val_loss: 0.7490 - val_acc: 0.7644\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3970 - acc: 0.8554 - val_loss: 0.8117 - val_acc: 0.7514\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4019 - acc: 0.8549 - val_loss: 0.7729 - val_acc: 0.7568\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4002 - acc: 0.8539 - val_loss: 0.7523 - val_acc: 0.7638\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.4018 - acc: 0.8553 - val_loss: 0.7905 - val_acc: 0.7527\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3927 - acc: 0.8564 - val_loss: 0.7781 - val_acc: 0.7551\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.4000 - acc: 0.8547 - val_loss: 0.7754 - val_acc: 0.7571\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3910 - acc: 0.8587 - val_loss: 0.8237 - val_acc: 0.7494\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.3948 - acc: 0.8565 - val_loss: 0.8086 - val_acc: 0.7521\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3910 - acc: 0.8570 - val_loss: 0.7980 - val_acc: 0.7476\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3888 - acc: 0.8585 - val_loss: 0.7825 - val_acc: 0.7589\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3873 - acc: 0.8593 - val_loss: 0.7875 - val_acc: 0.7520\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3885 - acc: 0.8594 - val_loss: 0.7692 - val_acc: 0.7661\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3795 - acc: 0.8634 - val_loss: 0.7727 - val_acc: 0.7607\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3892 - acc: 0.8610 - val_loss: 0.7796 - val_acc: 0.7631\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3760 - acc: 0.8645 - val_loss: 0.8079 - val_acc: 0.7494\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3829 - acc: 0.8617 - val_loss: 0.7837 - val_acc: 0.7574\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3820 - acc: 0.8644 - val_loss: 0.7455 - val_acc: 0.7687\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3823 - acc: 0.8617 - val_loss: 0.8123 - val_acc: 0.7500\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3756 - acc: 0.8646 - val_loss: 0.7375 - val_acc: 0.7714\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3698 - acc: 0.8665 - val_loss: 0.8387 - val_acc: 0.7491\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3749 - acc: 0.8635 - val_loss: 0.7835 - val_acc: 0.7619\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3727 - acc: 0.8658 - val_loss: 0.8382 - val_acc: 0.7496\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3685 - acc: 0.8674 - val_loss: 0.8106 - val_acc: 0.7519\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3690 - acc: 0.8673 - val_loss: 0.7642 - val_acc: 0.7642\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3683 - acc: 0.8686 - val_loss: 0.7531 - val_acc: 0.7677\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.3686 - acc: 0.8687 - val_loss: 0.7704 - val_acc: 0.7629\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.3635 - acc: 0.8678 - val_loss: 0.7707 - val_acc: 0.7618\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.3700 - acc: 0.8666 - val_loss: 0.7446 - val_acc: 0.7690\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3609 - acc: 0.8696 - val_loss: 0.8438 - val_acc: 0.7479\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3642 - acc: 0.8694 - val_loss: 0.7808 - val_acc: 0.7635\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3616 - acc: 0.8694 - val_loss: 0.7711 - val_acc: 0.7594\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.3536 - acc: 0.8719 - val_loss: 0.8419 - val_acc: 0.7457\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.3620 - acc: 0.8672 - val_loss: 0.7909 - val_acc: 0.7542\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.3593 - acc: 0.8706 - val_loss: 0.7668 - val_acc: 0.7644\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.3628 - acc: 0.8685 - val_loss: 0.7593 - val_acc: 0.7679\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.3453 - acc: 0.8753 - val_loss: 0.8860 - val_acc: 0.7387\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.3575 - acc: 0.8697 - val_loss: 0.8142 - val_acc: 0.7550\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.3498 - acc: 0.8739 - val_loss: 0.8226 - val_acc: 0.7476\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.3466 - acc: 0.8730 - val_loss: 0.8106 - val_acc: 0.7575\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3446 - acc: 0.8755 - val_loss: 0.7985 - val_acc: 0.7568\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3441 - acc: 0.8744 - val_loss: 0.8120 - val_acc: 0.7568\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3489 - acc: 0.8746 - val_loss: 0.7822 - val_acc: 0.7619\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3368 - acc: 0.8790 - val_loss: 0.7908 - val_acc: 0.7637\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3455 - acc: 0.8768 - val_loss: 0.7893 - val_acc: 0.7590\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3431 - acc: 0.8766 - val_loss: 0.7860 - val_acc: 0.7592\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3329 - acc: 0.8806 - val_loss: 0.7917 - val_acc: 0.7598\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3385 - acc: 0.8784 - val_loss: 0.7957 - val_acc: 0.7642\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3356 - acc: 0.8783 - val_loss: 0.8139 - val_acc: 0.7572\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3365 - acc: 0.8782 - val_loss: 0.8238 - val_acc: 0.7514\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3372 - acc: 0.8782 - val_loss: 0.7926 - val_acc: 0.7615\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3321 - acc: 0.8793 - val_loss: 0.8021 - val_acc: 0.7636\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3395 - acc: 0.8780 - val_loss: 0.7717 - val_acc: 0.7653\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3265 - acc: 0.8812 - val_loss: 0.8258 - val_acc: 0.7558\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3309 - acc: 0.8795 - val_loss: 0.7825 - val_acc: 0.7621\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 594s 15ms/step - loss: 0.3223 - acc: 0.8838 - val_loss: 0.7923 - val_acc: 0.7652\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.3242 - acc: 0.8827 - val_loss: 0.7916 - val_acc: 0.7640\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3358 - acc: 0.8802 - val_loss: 0.7788 - val_acc: 0.7684\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3215 - acc: 0.8825 - val_loss: 0.8161 - val_acc: 0.7523\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3263 - acc: 0.8814 - val_loss: 0.8495 - val_acc: 0.7476\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3253 - acc: 0.8834 - val_loss: 0.8296 - val_acc: 0.7548\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3253 - acc: 0.8837 - val_loss: 0.8274 - val_acc: 0.7607\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3215 - acc: 0.8838 - val_loss: 0.8088 - val_acc: 0.7587\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3225 - acc: 0.8837 - val_loss: 0.7998 - val_acc: 0.7623\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3181 - acc: 0.8842 - val_loss: 0.8352 - val_acc: 0.7502\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3195 - acc: 0.8853 - val_loss: 0.8543 - val_acc: 0.7513\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3197 - acc: 0.8839 - val_loss: 0.8117 - val_acc: 0.7583\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.3182 - acc: 0.8855 - val_loss: 0.8167 - val_acc: 0.7579\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3117 - acc: 0.8880 - val_loss: 0.8124 - val_acc: 0.7568\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3103 - acc: 0.8888 - val_loss: 0.8154 - val_acc: 0.7618\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.3191 - acc: 0.8865 - val_loss: 0.8599 - val_acc: 0.7495\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.3225 - acc: 0.8845 - val_loss: 0.8267 - val_acc: 0.7528\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.3095 - acc: 0.8882 - val_loss: 0.8168 - val_acc: 0.7592\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.3097 - acc: 0.8882 - val_loss: 0.8206 - val_acc: 0.7568\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.3020 - acc: 0.8937 - val_loss: 0.8569 - val_acc: 0.7547\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.3104 - acc: 0.8885 - val_loss: 0.8034 - val_acc: 0.7593\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.3038 - acc: 0.8910 - val_loss: 0.8102 - val_acc: 0.7647\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.3013 - acc: 0.8916 - val_loss: 0.8613 - val_acc: 0.7453\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3095 - acc: 0.8901 - val_loss: 0.8225 - val_acc: 0.7645\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.3005 - acc: 0.8912 - val_loss: 0.8735 - val_acc: 0.7501\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.2909 - acc: 0.8958 - val_loss: 0.8542 - val_acc: 0.7581\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.3047 - acc: 0.8906 - val_loss: 0.8711 - val_acc: 0.7511\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3004 - acc: 0.8900 - val_loss: 0.8243 - val_acc: 0.7574\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2957 - acc: 0.8957 - val_loss: 0.8292 - val_acc: 0.7610\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.2934 - acc: 0.8931 - val_loss: 0.8666 - val_acc: 0.7513\n",
      "Epoch 129/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.3061 - acc: 0.8903 - val_loss: 0.8095 - val_acc: 0.7599\n",
      "Epoch 130/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.2971 - acc: 0.8931 - val_loss: 0.8399 - val_acc: 0.7478\n",
      "Epoch 131/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.2953 - acc: 0.8946 - val_loss: 0.8276 - val_acc: 0.7590\n",
      "Epoch 132/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.3028 - acc: 0.8920 - val_loss: 0.7921 - val_acc: 0.7660\n",
      "Epoch 133/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2912 - acc: 0.8954 - val_loss: 0.8451 - val_acc: 0.7541\n",
      "Epoch 134/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2951 - acc: 0.8933 - val_loss: 0.8387 - val_acc: 0.7535\n",
      "Epoch 135/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2962 - acc: 0.8920 - val_loss: 0.8362 - val_acc: 0.7581\n",
      "Epoch 136/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2951 - acc: 0.8931 - val_loss: 0.8513 - val_acc: 0.7571\n",
      "Epoch 137/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2934 - acc: 0.8942 - val_loss: 0.8371 - val_acc: 0.7568\n",
      "Epoch 138/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2907 - acc: 0.8943 - val_loss: 0.8295 - val_acc: 0.7582\n",
      "Epoch 139/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.2857 - acc: 0.8967 - val_loss: 0.8311 - val_acc: 0.7641\n",
      "Epoch 140/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2912 - acc: 0.8939 - val_loss: 0.8441 - val_acc: 0.7550\n",
      "Epoch 141/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2969 - acc: 0.8936 - val_loss: 0.9027 - val_acc: 0.7534\n",
      "Epoch 142/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2788 - acc: 0.9002 - val_loss: 0.8239 - val_acc: 0.7610\n",
      "Epoch 143/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2812 - acc: 0.8981 - val_loss: 0.8310 - val_acc: 0.7637\n",
      "Epoch 144/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2882 - acc: 0.8963 - val_loss: 0.8556 - val_acc: 0.7593\n",
      "Epoch 145/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2835 - acc: 0.8973 - val_loss: 0.8705 - val_acc: 0.7487\n",
      "Epoch 146/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2843 - acc: 0.8981 - val_loss: 0.8343 - val_acc: 0.7626\n",
      "Epoch 147/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2846 - acc: 0.8985 - val_loss: 0.8925 - val_acc: 0.7397\n",
      "Epoch 148/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2755 - acc: 0.9034 - val_loss: 0.9043 - val_acc: 0.7409\n",
      "Epoch 149/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2930 - acc: 0.8954 - val_loss: 0.8837 - val_acc: 0.7557\n",
      "Epoch 150/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2817 - acc: 0.8997 - val_loss: 0.8478 - val_acc: 0.7556\n",
      "Epoch 151/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2857 - acc: 0.8961 - val_loss: 0.8223 - val_acc: 0.7642\n",
      "Epoch 152/200\n",
      "40000/40000 [==============================] - 2791s 70ms/step - loss: 0.2705 - acc: 0.9026 - val_loss: 0.8276 - val_acc: 0.7675\n",
      "Epoch 153/200\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.2755 - acc: 0.9015 - val_loss: 0.8665 - val_acc: 0.7557\n",
      "Epoch 154/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.2728 - acc: 0.9008 - val_loss: 0.8828 - val_acc: 0.7552\n",
      "Epoch 155/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.2718 - acc: 0.9015 - val_loss: 0.8195 - val_acc: 0.7637\n",
      "Epoch 156/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.2761 - acc: 0.9014 - val_loss: 0.8373 - val_acc: 0.7594\n",
      "Epoch 157/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.2672 - acc: 0.9044 - val_loss: 0.8958 - val_acc: 0.7506\n",
      "Epoch 158/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.2709 - acc: 0.9025 - val_loss: 0.8701 - val_acc: 0.7521\n",
      "Epoch 159/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.2746 - acc: 0.9008 - val_loss: 0.8472 - val_acc: 0.7580\n",
      "Epoch 160/200\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.2779 - acc: 0.9009 - val_loss: 0.8419 - val_acc: 0.7588\n",
      "Epoch 161/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.2761 - acc: 0.9009 - val_loss: 0.8206 - val_acc: 0.7622\n",
      "Epoch 162/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.2797 - acc: 0.8991 - val_loss: 0.8417 - val_acc: 0.7609\n",
      "Epoch 163/200\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.2721 - acc: 0.9041 - val_loss: 0.8419 - val_acc: 0.7639\n",
      "Epoch 164/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.2683 - acc: 0.9042 - val_loss: 0.8352 - val_acc: 0.7644\n",
      "Epoch 165/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.2662 - acc: 0.9040 - val_loss: 0.8906 - val_acc: 0.7427\n",
      "Epoch 166/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.2670 - acc: 0.9053 - val_loss: 0.8379 - val_acc: 0.7610\n",
      "Epoch 167/200\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.2604 - acc: 0.9048 - val_loss: 0.8566 - val_acc: 0.7572\n",
      "Epoch 168/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.2758 - acc: 0.9019 - val_loss: 0.8396 - val_acc: 0.7609\n",
      "Epoch 169/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.2624 - acc: 0.9054 - val_loss: 0.8591 - val_acc: 0.7654\n",
      "Epoch 170/200\n",
      "40000/40000 [==============================] - 158s 4ms/step - loss: 0.2660 - acc: 0.9051 - val_loss: 0.8466 - val_acc: 0.7623\n",
      "Epoch 171/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.2625 - acc: 0.9054 - val_loss: 0.8697 - val_acc: 0.7595\n",
      "Epoch 172/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.2666 - acc: 0.9053 - val_loss: 0.8804 - val_acc: 0.7515\n",
      "Epoch 173/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.2604 - acc: 0.9071 - val_loss: 0.8480 - val_acc: 0.7638\n",
      "Epoch 174/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.2692 - acc: 0.9018 - val_loss: 0.8648 - val_acc: 0.7623\n",
      "Epoch 175/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.2536 - acc: 0.9087 - val_loss: 0.8627 - val_acc: 0.7560\n",
      "Epoch 176/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2730 - acc: 0.9018 - val_loss: 0.8596 - val_acc: 0.7584\n",
      "Epoch 177/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2589 - acc: 0.9079 - val_loss: 0.8852 - val_acc: 0.7538\n",
      "Epoch 178/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2590 - acc: 0.9081 - val_loss: 0.8990 - val_acc: 0.7534\n",
      "Epoch 179/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2617 - acc: 0.9071 - val_loss: 0.9061 - val_acc: 0.7559\n",
      "Epoch 180/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2625 - acc: 0.9057 - val_loss: 0.8596 - val_acc: 0.7594\n",
      "Epoch 181/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.2502 - acc: 0.9107 - val_loss: 0.8564 - val_acc: 0.7632\n",
      "Epoch 182/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2587 - acc: 0.9069 - val_loss: 0.9571 - val_acc: 0.7356\n",
      "Epoch 183/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2617 - acc: 0.9071 - val_loss: 0.8415 - val_acc: 0.7623\n",
      "Epoch 184/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2536 - acc: 0.9094 - val_loss: 0.8585 - val_acc: 0.7677\n",
      "Epoch 185/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2469 - acc: 0.9116 - val_loss: 0.8513 - val_acc: 0.7621\n",
      "Epoch 186/200\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.2559 - acc: 0.9083 - val_loss: 0.8813 - val_acc: 0.7593\n",
      "Epoch 187/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2520 - acc: 0.9084 - val_loss: 0.8736 - val_acc: 0.7569\n",
      "Epoch 188/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2489 - acc: 0.9124 - val_loss: 0.8621 - val_acc: 0.7588\n",
      "Epoch 189/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2564 - acc: 0.9092 - val_loss: 0.8554 - val_acc: 0.7603\n",
      "Epoch 190/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2470 - acc: 0.9112 - val_loss: 0.8789 - val_acc: 0.7516\n",
      "Epoch 191/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2544 - acc: 0.9096 - val_loss: 0.9104 - val_acc: 0.7557\n",
      "Epoch 192/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2495 - acc: 0.9103 - val_loss: 0.8731 - val_acc: 0.7601\n",
      "Epoch 193/200\n",
      "40000/40000 [==============================] - 977s 24ms/step - loss: 0.2527 - acc: 0.9099 - val_loss: 0.8774 - val_acc: 0.7612\n",
      "Epoch 194/200\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.2494 - acc: 0.9105 - val_loss: 0.9041 - val_acc: 0.7530\n",
      "Epoch 195/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2476 - acc: 0.9127 - val_loss: 0.9419 - val_acc: 0.7528\n",
      "Epoch 196/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2529 - acc: 0.9095 - val_loss: 0.8793 - val_acc: 0.7567\n",
      "Epoch 197/200\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.2420 - acc: 0.9132 - val_loss: 0.9051 - val_acc: 0.7567\n",
      "Epoch 198/200\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.2524 - acc: 0.9094 - val_loss: 0.8787 - val_acc: 0.7548\n",
      "Epoch 199/200\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2461 - acc: 0.9132 - val_loss: 0.8660 - val_acc: 0.7613\n",
      "Epoch 200/200\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.2403 - acc: 0.9143 - val_loss: 0.8530 - val_acc: 0.7598\n"
     ]
    }
   ],
   "source": [
    "history_3 = model_conv_1.fit(X_train/255, y_train,\n",
    "          batch_size=1000,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d6e98d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_3.history['acc'])\n",
    "plt.plot(history_3.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13d5fe2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_3.history['loss'])\n",
    "plt.plot(history_3.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 2.3065 - acc: 0.1678 - val_loss: 2.1296 - val_acc: 0.2324\n",
      "Epoch 2/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 2.1556 - acc: 0.2108 - val_loss: 2.1358 - val_acc: 0.2219\n",
      "Epoch 3/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 2.1061 - acc: 0.2340 - val_loss: 2.0106 - val_acc: 0.2987\n",
      "Epoch 4/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 2.0143 - acc: 0.2706 - val_loss: 1.9162 - val_acc: 0.3291\n",
      "Epoch 5/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.9549 - acc: 0.2903 - val_loss: 1.9182 - val_acc: 0.3057\n",
      "Epoch 6/600\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.9236 - acc: 0.3022 - val_loss: 1.8917 - val_acc: 0.3077\n",
      "Epoch 7/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.9378 - acc: 0.3046 - val_loss: 1.7708 - val_acc: 0.3677\n",
      "Epoch 8/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.8579 - acc: 0.3270 - val_loss: 1.7376 - val_acc: 0.3788\n",
      "Epoch 9/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.8203 - acc: 0.3403 - val_loss: 1.7218 - val_acc: 0.3718\n",
      "Epoch 10/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.7911 - acc: 0.3529 - val_loss: 1.8853 - val_acc: 0.3363\n",
      "Epoch 11/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.7654 - acc: 0.3604 - val_loss: 1.9841 - val_acc: 0.3165\n",
      "Epoch 12/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 1.7332 - acc: 0.3761 - val_loss: 1.6499 - val_acc: 0.4081\n",
      "Epoch 13/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7100 - acc: 0.3800 - val_loss: 1.6835 - val_acc: 0.3825\n",
      "Epoch 14/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 1.6865 - acc: 0.3908 - val_loss: 1.6495 - val_acc: 0.3999\n",
      "Epoch 15/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.6621 - acc: 0.4001 - val_loss: 1.5680 - val_acc: 0.4343\n",
      "Epoch 16/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.6319 - acc: 0.4102 - val_loss: 1.5802 - val_acc: 0.4266\n",
      "Epoch 17/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.6197 - acc: 0.4128 - val_loss: 1.6328 - val_acc: 0.4108\n",
      "Epoch 18/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.6230 - acc: 0.4128 - val_loss: 1.5128 - val_acc: 0.4552\n",
      "Epoch 19/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.5836 - acc: 0.4272 - val_loss: 1.6174 - val_acc: 0.4091\n",
      "Epoch 20/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.5810 - acc: 0.4293 - val_loss: 1.4744 - val_acc: 0.4608\n",
      "Epoch 21/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.5514 - acc: 0.4373 - val_loss: 1.4577 - val_acc: 0.4613\n",
      "Epoch 22/600\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.5393 - acc: 0.4447 - val_loss: 1.4822 - val_acc: 0.4619\n",
      "Epoch 23/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.5293 - acc: 0.4469 - val_loss: 1.5324 - val_acc: 0.4529\n",
      "Epoch 24/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.5257 - acc: 0.4508 - val_loss: 1.4365 - val_acc: 0.4763\n",
      "Epoch 25/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.5062 - acc: 0.4580 - val_loss: 1.4593 - val_acc: 0.4689\n",
      "Epoch 26/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.5022 - acc: 0.4604 - val_loss: 1.4323 - val_acc: 0.4732\n",
      "Epoch 27/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.4723 - acc: 0.4668 - val_loss: 1.3928 - val_acc: 0.5024\n",
      "Epoch 28/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.4807 - acc: 0.4674 - val_loss: 1.5934 - val_acc: 0.4284\n",
      "Epoch 29/600\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.4594 - acc: 0.4758 - val_loss: 1.3724 - val_acc: 0.4966\n",
      "Epoch 30/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.4568 - acc: 0.4750 - val_loss: 1.3656 - val_acc: 0.5092\n",
      "Epoch 31/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.4370 - acc: 0.4868 - val_loss: 1.4962 - val_acc: 0.4573\n",
      "Epoch 32/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.4428 - acc: 0.4840 - val_loss: 1.3411 - val_acc: 0.5162\n",
      "Epoch 33/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.4070 - acc: 0.4962 - val_loss: 1.4261 - val_acc: 0.4989\n",
      "Epoch 34/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.4205 - acc: 0.4940 - val_loss: 1.2943 - val_acc: 0.5395\n",
      "Epoch 35/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.4033 - acc: 0.4936 - val_loss: 1.2782 - val_acc: 0.5389\n",
      "Epoch 36/600\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.3904 - acc: 0.4998 - val_loss: 1.2896 - val_acc: 0.5329\n",
      "Epoch 37/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.3811 - acc: 0.5035 - val_loss: 1.5128 - val_acc: 0.4515\n",
      "Epoch 38/600\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.3894 - acc: 0.5007 - val_loss: 1.2991 - val_acc: 0.5355\n",
      "Epoch 39/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.3650 - acc: 0.5129 - val_loss: 1.3356 - val_acc: 0.5195\n",
      "Epoch 40/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.3573 - acc: 0.5162 - val_loss: 1.3881 - val_acc: 0.5024\n",
      "Epoch 41/600\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.3522 - acc: 0.5175 - val_loss: 1.3093 - val_acc: 0.5308\n",
      "Epoch 42/600\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.3474 - acc: 0.5177 - val_loss: 1.2526 - val_acc: 0.5521\n",
      "Epoch 43/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.3227 - acc: 0.5283 - val_loss: 1.4988 - val_acc: 0.4772\n",
      "Epoch 44/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.3180 - acc: 0.5307 - val_loss: 1.2770 - val_acc: 0.5394\n",
      "Epoch 45/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.3359 - acc: 0.5261 - val_loss: 1.3108 - val_acc: 0.5285\n",
      "Epoch 46/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.3203 - acc: 0.5258 - val_loss: 1.2046 - val_acc: 0.5663\n",
      "Epoch 47/600\n",
      "40000/40000 [==============================] - 3719s 93ms/step - loss: 1.3076 - acc: 0.5337 - val_loss: 1.2545 - val_acc: 0.5448\n",
      "Epoch 48/600\n",
      "40000/40000 [==============================] - 10908s 273ms/step - loss: 1.2952 - acc: 0.5371 - val_loss: 1.2211 - val_acc: 0.5593\n",
      "Epoch 49/600\n",
      "40000/40000 [==============================] - 7315s 183ms/step - loss: 1.2963 - acc: 0.5369 - val_loss: 1.2533 - val_acc: 0.5533\n",
      "Epoch 50/600\n",
      "40000/40000 [==============================] - 7299s 182ms/step - loss: 1.2847 - acc: 0.5425 - val_loss: 1.2117 - val_acc: 0.5647\n",
      "Epoch 51/600\n",
      "40000/40000 [==============================] - 7309s 183ms/step - loss: 1.2859 - acc: 0.5413 - val_loss: 1.3046 - val_acc: 0.5236\n",
      "Epoch 52/600\n",
      "40000/40000 [==============================] - 10906s 273ms/step - loss: 1.2649 - acc: 0.5496 - val_loss: 1.3030 - val_acc: 0.5402\n",
      "Epoch 53/600\n",
      "40000/40000 [==============================] - 7309s 183ms/step - loss: 1.2748 - acc: 0.5449 - val_loss: 1.2283 - val_acc: 0.5583\n",
      "Epoch 54/600\n",
      "40000/40000 [==============================] - 7308s 183ms/step - loss: 1.2562 - acc: 0.5502 - val_loss: 1.2734 - val_acc: 0.5484\n",
      "Epoch 55/600\n",
      "40000/40000 [==============================] - 1996s 50ms/step - loss: 1.2548 - acc: 0.5501 - val_loss: 1.1993 - val_acc: 0.5719\n",
      "Epoch 56/600\n",
      "40000/40000 [==============================] - 267s 7ms/step - loss: 1.2652 - acc: 0.5500 - val_loss: 1.2144 - val_acc: 0.5753\n",
      "Epoch 57/600\n",
      "40000/40000 [==============================] - 256s 6ms/step - loss: 1.2361 - acc: 0.5599 - val_loss: 1.2428 - val_acc: 0.5602\n",
      "Epoch 58/600\n",
      "40000/40000 [==============================] - 257s 6ms/step - loss: 1.2394 - acc: 0.5582 - val_loss: 1.2361 - val_acc: 0.5510\n",
      "Epoch 59/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 253s 6ms/step - loss: 1.2390 - acc: 0.5569 - val_loss: 1.2072 - val_acc: 0.5725\n",
      "Epoch 60/600\n",
      "40000/40000 [==============================] - 263s 7ms/step - loss: 1.2226 - acc: 0.5615 - val_loss: 1.1677 - val_acc: 0.5869\n",
      "Epoch 61/600\n",
      "40000/40000 [==============================] - 261s 7ms/step - loss: 1.2233 - acc: 0.5637 - val_loss: 1.2373 - val_acc: 0.5628\n",
      "Epoch 62/600\n",
      "40000/40000 [==============================] - 211s 5ms/step - loss: 1.2123 - acc: 0.5691 - val_loss: 1.2082 - val_acc: 0.5710\n",
      "Epoch 63/600\n",
      "40000/40000 [==============================] - 223s 6ms/step - loss: 1.2210 - acc: 0.5649 - val_loss: 1.1963 - val_acc: 0.5662\n",
      "Epoch 64/600\n",
      "40000/40000 [==============================] - 229s 6ms/step - loss: 1.1949 - acc: 0.5724 - val_loss: 1.1712 - val_acc: 0.5816\n",
      "Epoch 65/600\n",
      "40000/40000 [==============================] - 210s 5ms/step - loss: 1.2054 - acc: 0.5707 - val_loss: 1.1545 - val_acc: 0.5876\n",
      "Epoch 66/600\n",
      "40000/40000 [==============================] - 224s 6ms/step - loss: 1.1884 - acc: 0.5765 - val_loss: 1.1207 - val_acc: 0.5949\n",
      "Epoch 67/600\n",
      "40000/40000 [==============================] - 226s 6ms/step - loss: 1.1965 - acc: 0.5761 - val_loss: 1.1234 - val_acc: 0.5984\n",
      "Epoch 68/600\n",
      "40000/40000 [==============================] - 216s 5ms/step - loss: 1.1726 - acc: 0.5833 - val_loss: 1.2099 - val_acc: 0.5768\n",
      "Epoch 69/600\n",
      "40000/40000 [==============================] - 220s 6ms/step - loss: 1.1785 - acc: 0.5817 - val_loss: 1.0997 - val_acc: 0.6081\n",
      "Epoch 70/600\n",
      "40000/40000 [==============================] - 223s 6ms/step - loss: 1.1726 - acc: 0.5845 - val_loss: 1.0736 - val_acc: 0.6169\n",
      "Epoch 71/600\n",
      "40000/40000 [==============================] - 5488s 137ms/step - loss: 1.1680 - acc: 0.5860 - val_loss: 1.0978 - val_acc: 0.6107\n",
      "Epoch 72/600\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.1723 - acc: 0.5820 - val_loss: 1.1017 - val_acc: 0.6106\n",
      "Epoch 73/600\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.1362 - acc: 0.5968 - val_loss: 1.1301 - val_acc: 0.5990\n",
      "Epoch 74/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 1.1600 - acc: 0.5862 - val_loss: 1.2039 - val_acc: 0.5759\n",
      "Epoch 75/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.1534 - acc: 0.5896 - val_loss: 1.0655 - val_acc: 0.6209\n",
      "Epoch 76/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.1492 - acc: 0.5917 - val_loss: 1.0915 - val_acc: 0.6141\n",
      "Epoch 77/600\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.1352 - acc: 0.5956 - val_loss: 1.1242 - val_acc: 0.6016\n",
      "Epoch 78/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.1435 - acc: 0.5948 - val_loss: 1.0252 - val_acc: 0.6358\n",
      "Epoch 79/600\n",
      "40000/40000 [==============================] - 197s 5ms/step - loss: 1.1294 - acc: 0.5999 - val_loss: 1.0679 - val_acc: 0.6201\n",
      "Epoch 80/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.1133 - acc: 0.6038 - val_loss: 1.1030 - val_acc: 0.6122\n",
      "Epoch 81/600\n",
      "40000/40000 [==============================] - 732s 18ms/step - loss: 1.1157 - acc: 0.6040 - val_loss: 1.0961 - val_acc: 0.6072\n",
      "Epoch 82/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.1247 - acc: 0.6034 - val_loss: 1.0578 - val_acc: 0.6253\n",
      "Epoch 83/600\n",
      "40000/40000 [==============================] - 2937s 73ms/step - loss: 1.1217 - acc: 0.6019 - val_loss: 1.0601 - val_acc: 0.6252\n",
      "Epoch 84/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.1057 - acc: 0.6070 - val_loss: 1.2453 - val_acc: 0.5798\n",
      "Epoch 85/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.0875 - acc: 0.6161 - val_loss: 1.1086 - val_acc: 0.6077\n",
      "Epoch 86/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.1065 - acc: 0.6056 - val_loss: 1.1000 - val_acc: 0.6226\n",
      "Epoch 87/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 1.0988 - acc: 0.6089 - val_loss: 1.0657 - val_acc: 0.6249\n",
      "Epoch 88/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0897 - acc: 0.6141 - val_loss: 1.0214 - val_acc: 0.6391\n",
      "Epoch 89/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.0851 - acc: 0.6148 - val_loss: 0.9945 - val_acc: 0.6468\n",
      "Epoch 90/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0951 - acc: 0.6121 - val_loss: 1.0012 - val_acc: 0.6447\n",
      "Epoch 91/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0770 - acc: 0.6181 - val_loss: 1.0250 - val_acc: 0.6390\n",
      "Epoch 92/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 1.0615 - acc: 0.6218 - val_loss: 1.0115 - val_acc: 0.6356\n",
      "Epoch 93/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.0825 - acc: 0.6187 - val_loss: 1.0351 - val_acc: 0.6352\n",
      "Epoch 94/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0605 - acc: 0.6249 - val_loss: 1.0279 - val_acc: 0.6333\n",
      "Epoch 95/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0590 - acc: 0.6249 - val_loss: 1.0216 - val_acc: 0.6366\n",
      "Epoch 96/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 1.0557 - acc: 0.6253 - val_loss: 0.9926 - val_acc: 0.6440\n",
      "Epoch 97/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.0617 - acc: 0.6238 - val_loss: 1.0370 - val_acc: 0.6359\n",
      "Epoch 98/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0379 - acc: 0.6300 - val_loss: 1.0491 - val_acc: 0.6266\n",
      "Epoch 99/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0430 - acc: 0.6270 - val_loss: 0.9768 - val_acc: 0.6538\n",
      "Epoch 100/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0385 - acc: 0.6309 - val_loss: 1.1013 - val_acc: 0.6185\n",
      "Epoch 101/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0381 - acc: 0.6334 - val_loss: 0.9919 - val_acc: 0.6482\n",
      "Epoch 102/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0434 - acc: 0.6288 - val_loss: 1.0009 - val_acc: 0.6471\n",
      "Epoch 103/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 1.0204 - acc: 0.6341 - val_loss: 1.0327 - val_acc: 0.6314\n",
      "Epoch 104/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0378 - acc: 0.6318 - val_loss: 1.0010 - val_acc: 0.6500\n",
      "Epoch 105/600\n",
      "40000/40000 [==============================] - 459s 11ms/step - loss: 1.0098 - acc: 0.6423 - val_loss: 1.0123 - val_acc: 0.6446\n",
      "Epoch 106/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.0225 - acc: 0.6364 - val_loss: 1.0062 - val_acc: 0.6471\n",
      "Epoch 107/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 1.0166 - acc: 0.6368 - val_loss: 0.9716 - val_acc: 0.6562\n",
      "Epoch 108/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0075 - acc: 0.6432 - val_loss: 1.0181 - val_acc: 0.6446\n",
      "Epoch 109/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0048 - acc: 0.6451 - val_loss: 0.9772 - val_acc: 0.6534\n",
      "Epoch 110/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.0008 - acc: 0.6448 - val_loss: 0.9659 - val_acc: 0.6560\n",
      "Epoch 111/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 1.0045 - acc: 0.6409 - val_loss: 1.0081 - val_acc: 0.6452\n",
      "Epoch 112/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.9959 - acc: 0.6456 - val_loss: 0.9309 - val_acc: 0.6671\n",
      "Epoch 113/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.9798 - acc: 0.6500 - val_loss: 0.9812 - val_acc: 0.6495\n",
      "Epoch 114/600\n",
      "40000/40000 [==============================] - 4536s 113ms/step - loss: 0.9967 - acc: 0.6465 - val_loss: 0.9481 - val_acc: 0.6672\n",
      "Epoch 115/600\n",
      "40000/40000 [==============================] - 245s 6ms/step - loss: 0.9860 - acc: 0.6516 - val_loss: 0.9411 - val_acc: 0.6705\n",
      "Epoch 116/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.9730 - acc: 0.6538 - val_loss: 0.9488 - val_acc: 0.6648\n",
      "Epoch 117/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.9889 - acc: 0.6496 - val_loss: 0.9521 - val_acc: 0.6638\n",
      "Epoch 118/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.9729 - acc: 0.6563 - val_loss: 0.9564 - val_acc: 0.6649\n",
      "Epoch 119/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.9718 - acc: 0.6549 - val_loss: 0.9423 - val_acc: 0.6647\n",
      "Epoch 120/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.9794 - acc: 0.6523 - val_loss: 0.9660 - val_acc: 0.6602\n",
      "Epoch 121/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.9639 - acc: 0.6576 - val_loss: 1.0172 - val_acc: 0.6463\n",
      "Epoch 122/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.9660 - acc: 0.6554 - val_loss: 0.9547 - val_acc: 0.6633\n",
      "Epoch 123/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.9588 - acc: 0.6603 - val_loss: 0.9815 - val_acc: 0.6590\n",
      "Epoch 124/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.9618 - acc: 0.6580 - val_loss: 1.0450 - val_acc: 0.6377\n",
      "Epoch 125/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.9453 - acc: 0.6657 - val_loss: 1.0847 - val_acc: 0.6217\n",
      "Epoch 126/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.9450 - acc: 0.6655 - val_loss: 1.0055 - val_acc: 0.6446\n",
      "Epoch 127/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.9554 - acc: 0.6605 - val_loss: 0.9361 - val_acc: 0.6659\n",
      "Epoch 128/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.9300 - acc: 0.6720 - val_loss: 0.9235 - val_acc: 0.6764\n",
      "Epoch 129/600\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.9429 - acc: 0.6631 - val_loss: 0.9434 - val_acc: 0.6731\n",
      "Epoch 130/600\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.9244 - acc: 0.6743 - val_loss: 1.0288 - val_acc: 0.6429\n",
      "Epoch 131/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.9436 - acc: 0.6650 - val_loss: 0.9697 - val_acc: 0.6630\n",
      "Epoch 132/600\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.9211 - acc: 0.6718 - val_loss: 0.9783 - val_acc: 0.6636\n",
      "Epoch 133/600\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.9352 - acc: 0.6703 - val_loss: 0.9629 - val_acc: 0.6647\n",
      "Epoch 134/600\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.9277 - acc: 0.6715 - val_loss: 0.9489 - val_acc: 0.6693\n",
      "Epoch 135/600\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.9208 - acc: 0.6737 - val_loss: 0.9249 - val_acc: 0.6781\n",
      "Epoch 136/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.9187 - acc: 0.6745 - val_loss: 0.8782 - val_acc: 0.6931\n",
      "Epoch 137/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.9240 - acc: 0.6750 - val_loss: 0.9330 - val_acc: 0.6700\n",
      "Epoch 138/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.9024 - acc: 0.6790 - val_loss: 0.9276 - val_acc: 0.6765\n",
      "Epoch 139/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.9099 - acc: 0.6778 - val_loss: 0.9823 - val_acc: 0.6601\n",
      "Epoch 140/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.9173 - acc: 0.6751 - val_loss: 0.8947 - val_acc: 0.6865\n",
      "Epoch 141/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8956 - acc: 0.6848 - val_loss: 0.9018 - val_acc: 0.6866\n",
      "Epoch 142/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.9069 - acc: 0.6801 - val_loss: 0.9180 - val_acc: 0.6766\n",
      "Epoch 143/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.9006 - acc: 0.6798 - val_loss: 0.8827 - val_acc: 0.6964\n",
      "Epoch 144/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.9009 - acc: 0.6828 - val_loss: 0.8632 - val_acc: 0.6971\n",
      "Epoch 145/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8858 - acc: 0.6855 - val_loss: 0.8975 - val_acc: 0.6853\n",
      "Epoch 146/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8963 - acc: 0.6833 - val_loss: 0.9517 - val_acc: 0.6656\n",
      "Epoch 147/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8887 - acc: 0.6863 - val_loss: 0.8531 - val_acc: 0.7008\n",
      "Epoch 148/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8804 - acc: 0.6878 - val_loss: 0.9427 - val_acc: 0.6688\n",
      "Epoch 149/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8982 - acc: 0.6816 - val_loss: 0.8736 - val_acc: 0.6924\n",
      "Epoch 150/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8727 - acc: 0.6918 - val_loss: 0.8788 - val_acc: 0.6939\n",
      "Epoch 151/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8553 - acc: 0.6950 - val_loss: 0.9167 - val_acc: 0.6785\n",
      "Epoch 152/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8762 - acc: 0.6890 - val_loss: 0.9303 - val_acc: 0.6732\n",
      "Epoch 153/600\n",
      "40000/40000 [==============================] - 111s 3ms/step - loss: 0.8610 - acc: 0.6959 - val_loss: 0.8910 - val_acc: 0.6842\n",
      "Epoch 154/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8709 - acc: 0.6932 - val_loss: 0.8499 - val_acc: 0.7047\n",
      "Epoch 155/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8768 - acc: 0.6878 - val_loss: 0.8546 - val_acc: 0.6997\n",
      "Epoch 156/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.8592 - acc: 0.6936 - val_loss: 0.8970 - val_acc: 0.6907\n",
      "Epoch 157/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8534 - acc: 0.6981 - val_loss: 0.9235 - val_acc: 0.6816\n",
      "Epoch 158/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.8559 - acc: 0.6961 - val_loss: 0.8690 - val_acc: 0.6973\n",
      "Epoch 159/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8475 - acc: 0.6990 - val_loss: 0.9524 - val_acc: 0.6733\n",
      "Epoch 160/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.8622 - acc: 0.6929 - val_loss: 0.8963 - val_acc: 0.6878\n",
      "Epoch 161/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8559 - acc: 0.6965 - val_loss: 0.8423 - val_acc: 0.7037\n",
      "Epoch 162/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.8510 - acc: 0.7003 - val_loss: 0.8258 - val_acc: 0.7141\n",
      "Epoch 163/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8338 - acc: 0.7069 - val_loss: 0.8420 - val_acc: 0.7026\n",
      "Epoch 164/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8447 - acc: 0.7003 - val_loss: 0.8476 - val_acc: 0.7058\n",
      "Epoch 165/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.8364 - acc: 0.7010 - val_loss: 0.8762 - val_acc: 0.6956\n",
      "Epoch 166/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.8307 - acc: 0.7048 - val_loss: 0.8542 - val_acc: 0.7015\n",
      "Epoch 167/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.8455 - acc: 0.6996 - val_loss: 0.8172 - val_acc: 0.7129\n",
      "Epoch 168/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.8322 - acc: 0.7037 - val_loss: 0.8587 - val_acc: 0.7016\n",
      "Epoch 169/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.8213 - acc: 0.7080 - val_loss: 0.8434 - val_acc: 0.7060\n",
      "Epoch 170/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.8331 - acc: 0.7037 - val_loss: 0.8417 - val_acc: 0.7049\n",
      "Epoch 171/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.8267 - acc: 0.7066 - val_loss: 0.8361 - val_acc: 0.7082\n",
      "Epoch 172/600\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.8318 - acc: 0.7060 - val_loss: 0.8385 - val_acc: 0.7064\n",
      "Epoch 173/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.8086 - acc: 0.7145 - val_loss: 0.8955 - val_acc: 0.6917\n",
      "Epoch 174/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.8386 - acc: 0.7039 - val_loss: 0.8936 - val_acc: 0.6902\n",
      "Epoch 175/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.8090 - acc: 0.7141 - val_loss: 0.8362 - val_acc: 0.7093\n",
      "Epoch 176/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.8106 - acc: 0.7099 - val_loss: 0.9388 - val_acc: 0.6820\n",
      "Epoch 177/600\n",
      "40000/40000 [==============================] - 2149s 54ms/step - loss: 0.8107 - acc: 0.7089 - val_loss: 0.7990 - val_acc: 0.7185\n",
      "Epoch 178/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.8169 - acc: 0.7112 - val_loss: 0.8498 - val_acc: 0.7027\n",
      "Epoch 179/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7948 - acc: 0.7159 - val_loss: 0.8779 - val_acc: 0.6962\n",
      "Epoch 180/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.8149 - acc: 0.7086 - val_loss: 0.8342 - val_acc: 0.7105\n",
      "Epoch 181/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.7912 - acc: 0.7186 - val_loss: 0.8759 - val_acc: 0.6978\n",
      "Epoch 182/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.7988 - acc: 0.7157 - val_loss: 0.8265 - val_acc: 0.7098\n",
      "Epoch 183/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.7936 - acc: 0.7192 - val_loss: 0.8289 - val_acc: 0.7113\n",
      "Epoch 184/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7930 - acc: 0.7175 - val_loss: 0.8133 - val_acc: 0.7181\n",
      "Epoch 185/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7918 - acc: 0.7160 - val_loss: 0.8884 - val_acc: 0.6935\n",
      "Epoch 186/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.7902 - acc: 0.7208 - val_loss: 0.8248 - val_acc: 0.7117\n",
      "Epoch 187/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.7833 - acc: 0.7216 - val_loss: 0.8439 - val_acc: 0.7056\n",
      "Epoch 188/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7905 - acc: 0.7200 - val_loss: 0.8839 - val_acc: 0.6915\n",
      "Epoch 189/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.7903 - acc: 0.7217 - val_loss: 0.7937 - val_acc: 0.7241\n",
      "Epoch 190/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.7849 - acc: 0.7240 - val_loss: 0.7881 - val_acc: 0.7220\n",
      "Epoch 191/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.7829 - acc: 0.7231 - val_loss: 0.8987 - val_acc: 0.6925\n",
      "Epoch 192/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7768 - acc: 0.7223 - val_loss: 0.8220 - val_acc: 0.7181\n",
      "Epoch 193/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7630 - acc: 0.7295 - val_loss: 0.8378 - val_acc: 0.7073\n",
      "Epoch 194/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.7700 - acc: 0.7284 - val_loss: 0.8740 - val_acc: 0.6977\n",
      "Epoch 195/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7753 - acc: 0.7244 - val_loss: 0.8072 - val_acc: 0.7192\n",
      "Epoch 196/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.7766 - acc: 0.7246 - val_loss: 0.8173 - val_acc: 0.7157\n",
      "Epoch 197/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7691 - acc: 0.7250 - val_loss: 0.9201 - val_acc: 0.6851\n",
      "Epoch 198/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.7806 - acc: 0.7255 - val_loss: 0.8188 - val_acc: 0.7157\n",
      "Epoch 199/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.7575 - acc: 0.7320 - val_loss: 0.8081 - val_acc: 0.7184\n",
      "Epoch 200/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7606 - acc: 0.7294 - val_loss: 0.7841 - val_acc: 0.7269\n",
      "Epoch 201/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7580 - acc: 0.7302 - val_loss: 0.7920 - val_acc: 0.7251\n",
      "Epoch 202/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.7563 - acc: 0.7330 - val_loss: 0.7953 - val_acc: 0.7250\n",
      "Epoch 203/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7528 - acc: 0.7320 - val_loss: 0.7962 - val_acc: 0.7232\n",
      "Epoch 204/600\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.7550 - acc: 0.7328 - val_loss: 0.7785 - val_acc: 0.7273\n",
      "Epoch 205/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.7548 - acc: 0.7309 - val_loss: 0.8015 - val_acc: 0.7227\n",
      "Epoch 206/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7441 - acc: 0.7361 - val_loss: 0.8906 - val_acc: 0.6918\n",
      "Epoch 207/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7484 - acc: 0.7343 - val_loss: 0.8234 - val_acc: 0.7087\n",
      "Epoch 208/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7446 - acc: 0.7348 - val_loss: 0.8060 - val_acc: 0.7177\n",
      "Epoch 209/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7573 - acc: 0.7307 - val_loss: 0.8211 - val_acc: 0.7181\n",
      "Epoch 210/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.7338 - acc: 0.7385 - val_loss: 0.7741 - val_acc: 0.7292\n",
      "Epoch 211/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.7307 - acc: 0.7415 - val_loss: 0.8699 - val_acc: 0.6947\n",
      "Epoch 212/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7371 - acc: 0.7392 - val_loss: 0.7958 - val_acc: 0.7269\n",
      "Epoch 213/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.7395 - acc: 0.7383 - val_loss: 0.8060 - val_acc: 0.7244\n",
      "Epoch 214/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7354 - acc: 0.7393 - val_loss: 0.8124 - val_acc: 0.7171\n",
      "Epoch 215/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.7261 - acc: 0.7423 - val_loss: 0.8037 - val_acc: 0.7233\n",
      "Epoch 216/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.7230 - acc: 0.7421 - val_loss: 0.8190 - val_acc: 0.7160\n",
      "Epoch 217/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.7315 - acc: 0.7390 - val_loss: 0.8029 - val_acc: 0.7230\n",
      "Epoch 218/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.7124 - acc: 0.7454 - val_loss: 0.7749 - val_acc: 0.7345\n",
      "Epoch 219/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.7306 - acc: 0.7387 - val_loss: 0.8259 - val_acc: 0.7099\n",
      "Epoch 220/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7220 - acc: 0.7442 - val_loss: 0.8094 - val_acc: 0.7205\n",
      "Epoch 221/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.7198 - acc: 0.7433 - val_loss: 0.8385 - val_acc: 0.7135\n",
      "Epoch 222/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.7123 - acc: 0.7470 - val_loss: 0.7964 - val_acc: 0.7279\n",
      "Epoch 223/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.7144 - acc: 0.7473 - val_loss: 0.8160 - val_acc: 0.7227\n",
      "Epoch 224/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.7141 - acc: 0.7439 - val_loss: 0.8035 - val_acc: 0.7254\n",
      "Epoch 225/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.7111 - acc: 0.7467 - val_loss: 0.7733 - val_acc: 0.7346\n",
      "Epoch 226/600\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.7013 - acc: 0.7504 - val_loss: 0.8177 - val_acc: 0.7202\n",
      "Epoch 227/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.7172 - acc: 0.7429 - val_loss: 0.8349 - val_acc: 0.7187\n",
      "Epoch 228/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.7050 - acc: 0.7485 - val_loss: 0.7669 - val_acc: 0.7372\n",
      "Epoch 229/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.7039 - acc: 0.7519 - val_loss: 0.9113 - val_acc: 0.6970\n",
      "Epoch 230/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.6982 - acc: 0.7493 - val_loss: 0.8469 - val_acc: 0.7078\n",
      "Epoch 231/600\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.7088 - acc: 0.7475 - val_loss: 0.7881 - val_acc: 0.7308\n",
      "Epoch 232/600\n",
      "40000/40000 [==============================] - 5717s 143ms/step - loss: 0.6918 - acc: 0.7526 - val_loss: 0.8631 - val_acc: 0.7085\n",
      "Epoch 233/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.7076 - acc: 0.7458 - val_loss: 0.8394 - val_acc: 0.7094\n",
      "Epoch 234/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.6826 - acc: 0.7579 - val_loss: 0.7976 - val_acc: 0.7255\n",
      "Epoch 235/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.7005 - acc: 0.7493 - val_loss: 0.7860 - val_acc: 0.7280\n",
      "Epoch 236/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6771 - acc: 0.7586 - val_loss: 0.7832 - val_acc: 0.7306\n",
      "Epoch 237/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.6945 - acc: 0.7540 - val_loss: 0.8625 - val_acc: 0.7040\n",
      "Epoch 238/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6871 - acc: 0.7527 - val_loss: 0.8265 - val_acc: 0.7154\n",
      "Epoch 239/600\n",
      "40000/40000 [==============================] - 3724s 93ms/step - loss: 0.6947 - acc: 0.7542 - val_loss: 0.7852 - val_acc: 0.7267\n",
      "Epoch 240/600\n",
      "40000/40000 [==============================] - 3718s 93ms/step - loss: 0.6733 - acc: 0.7631 - val_loss: 0.8067 - val_acc: 0.7258\n",
      "Epoch 241/600\n",
      "40000/40000 [==============================] - 7310s 183ms/step - loss: 0.6775 - acc: 0.7586 - val_loss: 0.7663 - val_acc: 0.7358\n",
      "Epoch 242/600\n",
      "40000/40000 [==============================] - 10916s 273ms/step - loss: 0.6853 - acc: 0.7570 - val_loss: 0.8217 - val_acc: 0.7190\n",
      "Epoch 243/600\n",
      "40000/40000 [==============================] - 7308s 183ms/step - loss: 0.6791 - acc: 0.7608 - val_loss: 0.7896 - val_acc: 0.7326\n",
      "Epoch 244/600\n",
      "40000/40000 [==============================] - 7308s 183ms/step - loss: 0.6819 - acc: 0.7576 - val_loss: 0.7925 - val_acc: 0.7278\n",
      "Epoch 245/600\n",
      "40000/40000 [==============================] - 4443s 111ms/step - loss: 0.6732 - acc: 0.7590 - val_loss: 0.7564 - val_acc: 0.7394\n",
      "Epoch 246/600\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.6703 - acc: 0.7619 - val_loss: 0.7594 - val_acc: 0.7414\n",
      "Epoch 247/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6659 - acc: 0.7646 - val_loss: 0.8733 - val_acc: 0.7052\n",
      "Epoch 248/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6727 - acc: 0.7593 - val_loss: 0.7743 - val_acc: 0.7368\n",
      "Epoch 249/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6695 - acc: 0.7623 - val_loss: 0.8269 - val_acc: 0.7184\n",
      "Epoch 250/600\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6783 - acc: 0.7598 - val_loss: 0.8025 - val_acc: 0.7259\n",
      "Epoch 251/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.6612 - acc: 0.7630 - val_loss: 0.8040 - val_acc: 0.7279\n",
      "Epoch 252/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.6708 - acc: 0.7605 - val_loss: 0.8102 - val_acc: 0.7228\n",
      "Epoch 253/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.6577 - acc: 0.7669 - val_loss: 0.8237 - val_acc: 0.7233\n",
      "Epoch 254/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.6577 - acc: 0.7653 - val_loss: 0.7851 - val_acc: 0.7336\n",
      "Epoch 255/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.6628 - acc: 0.7642 - val_loss: 0.7709 - val_acc: 0.7408\n",
      "Epoch 256/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.6532 - acc: 0.7689 - val_loss: 0.8396 - val_acc: 0.7178\n",
      "Epoch 257/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6601 - acc: 0.7648 - val_loss: 0.7979 - val_acc: 0.7299\n",
      "Epoch 258/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.6505 - acc: 0.7693 - val_loss: 0.7655 - val_acc: 0.7389\n",
      "Epoch 259/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.6666 - acc: 0.7611 - val_loss: 0.7837 - val_acc: 0.7323\n",
      "Epoch 260/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.6449 - acc: 0.7690 - val_loss: 0.8122 - val_acc: 0.7263\n",
      "Epoch 261/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6527 - acc: 0.7699 - val_loss: 0.8064 - val_acc: 0.7253\n",
      "Epoch 262/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6497 - acc: 0.7677 - val_loss: 0.7578 - val_acc: 0.7400\n",
      "Epoch 263/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6388 - acc: 0.7728 - val_loss: 0.7388 - val_acc: 0.7472\n",
      "Epoch 264/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6326 - acc: 0.7744 - val_loss: 0.7628 - val_acc: 0.7408\n",
      "Epoch 265/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.6533 - acc: 0.7682 - val_loss: 0.7540 - val_acc: 0.7421\n",
      "Epoch 266/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.6367 - acc: 0.7735 - val_loss: 0.8314 - val_acc: 0.7207\n",
      "Epoch 267/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6419 - acc: 0.7701 - val_loss: 0.8678 - val_acc: 0.7084\n",
      "Epoch 268/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.6369 - acc: 0.7722 - val_loss: 0.7687 - val_acc: 0.7404\n",
      "Epoch 269/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.6404 - acc: 0.7723 - val_loss: 0.8501 - val_acc: 0.7238\n",
      "Epoch 270/600\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.6385 - acc: 0.7720 - val_loss: 0.7861 - val_acc: 0.7343\n",
      "Epoch 271/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.6286 - acc: 0.7754 - val_loss: 0.8236 - val_acc: 0.7264\n",
      "Epoch 272/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.6333 - acc: 0.7744 - val_loss: 0.8134 - val_acc: 0.7283\n",
      "Epoch 273/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6299 - acc: 0.7742 - val_loss: 0.8855 - val_acc: 0.7076\n",
      "Epoch 274/600\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.6351 - acc: 0.7722 - val_loss: 0.8568 - val_acc: 0.7149\n",
      "Epoch 275/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.6148 - acc: 0.7824 - val_loss: 0.8254 - val_acc: 0.7222\n",
      "Epoch 276/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.6318 - acc: 0.7718 - val_loss: 0.7981 - val_acc: 0.7352\n",
      "Epoch 277/600\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.6208 - acc: 0.7767 - val_loss: 0.8102 - val_acc: 0.7284\n",
      "Epoch 278/600\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.6205 - acc: 0.7810 - val_loss: 0.7848 - val_acc: 0.7373\n",
      "Epoch 279/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.6121 - acc: 0.7808 - val_loss: 0.7992 - val_acc: 0.7335\n",
      "Epoch 280/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6164 - acc: 0.7795 - val_loss: 0.7765 - val_acc: 0.7395\n",
      "Epoch 281/600\n",
      "40000/40000 [==============================] - 119s 3ms/step - loss: 0.6268 - acc: 0.7777 - val_loss: 0.7698 - val_acc: 0.7380\n",
      "Epoch 282/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6092 - acc: 0.7839 - val_loss: 0.7667 - val_acc: 0.7433\n",
      "Epoch 283/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6166 - acc: 0.7798 - val_loss: 0.7746 - val_acc: 0.7375\n",
      "Epoch 284/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.6118 - acc: 0.7811 - val_loss: 0.8393 - val_acc: 0.7145\n",
      "Epoch 285/600\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.6028 - acc: 0.7837 - val_loss: 0.7642 - val_acc: 0.7385\n",
      "Epoch 286/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6123 - acc: 0.7812 - val_loss: 0.7876 - val_acc: 0.7354\n",
      "Epoch 287/600\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6089 - acc: 0.7853 - val_loss: 0.7577 - val_acc: 0.7423\n",
      "Epoch 288/600\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6054 - acc: 0.7820 - val_loss: 0.7552 - val_acc: 0.7435\n",
      "Epoch 289/600\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6066 - acc: 0.7836 - val_loss: 0.8281 - val_acc: 0.7283\n",
      "Epoch 290/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6024 - acc: 0.7861 - val_loss: 0.7461 - val_acc: 0.7461\n",
      "Epoch 291/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.6043 - acc: 0.7837 - val_loss: 0.7521 - val_acc: 0.7430\n",
      "Epoch 292/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.6095 - acc: 0.7818 - val_loss: 0.7717 - val_acc: 0.7380\n",
      "Epoch 293/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.5828 - acc: 0.7916 - val_loss: 0.7938 - val_acc: 0.7290\n",
      "Epoch 294/600\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.5993 - acc: 0.7841 - val_loss: 0.7529 - val_acc: 0.7485\n",
      "Epoch 295/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.6082 - acc: 0.7837 - val_loss: 0.7303 - val_acc: 0.7503\n",
      "Epoch 296/600\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.5983 - acc: 0.7869 - val_loss: 0.7492 - val_acc: 0.7449\n",
      "Epoch 297/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.5943 - acc: 0.7866 - val_loss: 0.7428 - val_acc: 0.7452\n",
      "Epoch 298/600\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 0.5879 - acc: 0.7903 - val_loss: 0.7805 - val_acc: 0.7370\n",
      "Epoch 299/600\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.5879 - acc: 0.7886 - val_loss: 0.8380 - val_acc: 0.7264\n",
      "Epoch 300/600\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 0.5955 - acc: 0.7865 - val_loss: 0.7472 - val_acc: 0.7463\n",
      "Epoch 301/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.5850 - acc: 0.7928 - val_loss: 0.7500 - val_acc: 0.7486\n",
      "Epoch 302/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.5729 - acc: 0.7945 - val_loss: 0.7634 - val_acc: 0.7444\n",
      "Epoch 303/600\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.5941 - acc: 0.7860 - val_loss: 0.7835 - val_acc: 0.7375\n",
      "Epoch 304/600\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.5807 - acc: 0.7930 - val_loss: 0.7290 - val_acc: 0.7557\n",
      "Epoch 305/600\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.5803 - acc: 0.7913 - val_loss: 0.7607 - val_acc: 0.7446\n",
      "Epoch 306/600\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.5866 - acc: 0.7885 - val_loss: 0.7439 - val_acc: 0.7480\n",
      "Epoch 307/600\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.5635 - acc: 0.8006 - val_loss: 0.7815 - val_acc: 0.7404\n",
      "Epoch 308/600\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.5760 - acc: 0.7973 - val_loss: 0.7562 - val_acc: 0.7448\n",
      "Epoch 309/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.5834 - acc: 0.7907 - val_loss: 0.7756 - val_acc: 0.7448\n",
      "Epoch 310/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.5774 - acc: 0.7912 - val_loss: 0.7603 - val_acc: 0.7441\n",
      "Epoch 311/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.5643 - acc: 0.7991 - val_loss: 0.8950 - val_acc: 0.7138\n",
      "Epoch 312/600\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.5763 - acc: 0.7946 - val_loss: 0.7505 - val_acc: 0.7486\n",
      "Epoch 313/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.5663 - acc: 0.7978 - val_loss: 0.7886 - val_acc: 0.7406\n",
      "Epoch 314/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.5621 - acc: 0.7981 - val_loss: 0.7764 - val_acc: 0.7375\n",
      "Epoch 315/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.5677 - acc: 0.7953 - val_loss: 0.7614 - val_acc: 0.7473\n",
      "Epoch 316/600\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.5659 - acc: 0.7968 - val_loss: 0.8106 - val_acc: 0.7360\n",
      "Epoch 317/600\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.5640 - acc: 0.7963 - val_loss: 0.7829 - val_acc: 0.7429\n",
      "Epoch 318/600\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.5515 - acc: 0.8027 - val_loss: 0.8115 - val_acc: 0.7310\n",
      "Epoch 319/600\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.5560 - acc: 0.8013 - val_loss: 0.7856 - val_acc: 0.7400\n",
      "Epoch 320/600\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.5619 - acc: 0.7980 - val_loss: 0.7352 - val_acc: 0.7543\n",
      "Epoch 321/600\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.5578 - acc: 0.8004 - val_loss: 0.7709 - val_acc: 0.7448\n",
      "Epoch 322/600\n",
      "20000/40000 [==============>...............] - ETA: 58s - loss: 0.5705 - acc: 0.7928 "
     ]
    }
   ],
   "source": [
    "history_4 = model_conv_1.fit(X_train/255, y_train,\n",
    "          batch_size=2000,\n",
    "          epochs=600,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_4.history['acc'])\n",
    "plt.plot(history_4.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_4.history['loss'])\n",
    "plt.plot(history_4.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.007)\n",
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 14.4952 - acc: 0.0995 - val_loss: 14.4821 - val_acc: 0.1015\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 14.5123 - acc: 0.0996 - val_loss: 14.4821 - val_acc: 0.1015\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 4.8432 - acc: 0.1145 - val_loss: 2.1935 - val_acc: 0.1698\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 2.1600 - acc: 0.1879 - val_loss: 2.2174 - val_acc: 0.1865\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 2.0495 - acc: 0.2266 - val_loss: 1.9710 - val_acc: 0.2463\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.9864 - acc: 0.2541 - val_loss: 1.8968 - val_acc: 0.2746\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.9513 - acc: 0.2663 - val_loss: 1.8637 - val_acc: 0.2831\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.9553 - acc: 0.2624 - val_loss: 1.9047 - val_acc: 0.2535\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.9330 - acc: 0.2695 - val_loss: 1.9568 - val_acc: 0.2573\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.9344 - acc: 0.2716 - val_loss: 1.8753 - val_acc: 0.2788\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.9372 - acc: 0.2712 - val_loss: 1.9178 - val_acc: 0.2605\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.9520 - acc: 0.2680 - val_loss: 2.1385 - val_acc: 0.2088\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.9590 - acc: 0.2591 - val_loss: 1.9588 - val_acc: 0.2581\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.9852 - acc: 0.2508 - val_loss: 2.1190 - val_acc: 0.2234\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.9854 - acc: 0.2386 - val_loss: 1.9435 - val_acc: 0.2676\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.9626 - acc: 0.2597 - val_loss: 1.8979 - val_acc: 0.2387\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 1.9545 - acc: 0.2637 - val_loss: 1.9369 - val_acc: 0.2576\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 267s 7ms/step - loss: 1.9604 - acc: 0.2580 - val_loss: 1.8081 - val_acc: 0.3082\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 338s 8ms/step - loss: 1.9575 - acc: 0.2591 - val_loss: 1.8677 - val_acc: 0.2667\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 308s 8ms/step - loss: 1.9433 - acc: 0.2552 - val_loss: 1.8609 - val_acc: 0.2704\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 292s 7ms/step - loss: 1.9449 - acc: 0.2559 - val_loss: 1.8251 - val_acc: 0.2952\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 339s 8ms/step - loss: 1.9903 - acc: 0.2606 - val_loss: 1.8034 - val_acc: 0.2933\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 287s 7ms/step - loss: 1.9656 - acc: 0.2537 - val_loss: 1.8410 - val_acc: 0.2721\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 281s 7ms/step - loss: 2.0008 - acc: 0.2525 - val_loss: 1.8930 - val_acc: 0.2789\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 291s 7ms/step - loss: 2.0122 - acc: 0.2404 - val_loss: 2.0194 - val_acc: 0.2031\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 246s 6ms/step - loss: 2.1504 - acc: 0.1806 - val_loss: 2.0100 - val_acc: 0.2058\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 246s 6ms/step - loss: 2.1690 - acc: 0.1731 - val_loss: 2.0835 - val_acc: 0.1922\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 321s 8ms/step - loss: 2.1281 - acc: 0.1730 - val_loss: 2.0732 - val_acc: 0.1925\n",
      "Epoch 29/200\n",
      " 9180/40000 [=====>........................] - ETA: 3:53 - loss: 2.3355 - acc: 0.1520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0fc953a3c7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second update Wednesday morning\n",
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 250s 6ms/step - loss: 1.8238 - acc: 0.3275 - val_loss: 1.5485 - val_acc: 0.4318\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 155s 4ms/step - loss: 1.4591 - acc: 0.4724 - val_loss: 1.4112 - val_acc: 0.4916\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.3075 - acc: 0.5308 - val_loss: 1.2044 - val_acc: 0.5688\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 1.1995 - acc: 0.5709 - val_loss: 1.2178 - val_acc: 0.5738\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 1.1099 - acc: 0.6070 - val_loss: 1.0817 - val_acc: 0.6162\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 1.0372 - acc: 0.6337 - val_loss: 1.1651 - val_acc: 0.5930\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 160s 4ms/step - loss: 0.9762 - acc: 0.6583 - val_loss: 1.0667 - val_acc: 0.6371\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9178 - acc: 0.6760 - val_loss: 1.0766 - val_acc: 0.6338\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.8757 - acc: 0.6944 - val_loss: 1.0611 - val_acc: 0.6464\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 0.8332 - acc: 0.7067 - val_loss: 0.9762 - val_acc: 0.6618\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.7977 - acc: 0.7203 - val_loss: 0.9556 - val_acc: 0.6780\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.7638 - acc: 0.7362 - val_loss: 0.9523 - val_acc: 0.6813\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.7340 - acc: 0.7430 - val_loss: 0.9320 - val_acc: 0.6880\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 0.7121 - acc: 0.7517 - val_loss: 0.9603 - val_acc: 0.6792\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.6916 - acc: 0.7610 - val_loss: 1.1205 - val_acc: 0.6623\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.6707 - acc: 0.7693 - val_loss: 1.0262 - val_acc: 0.6775\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.6556 - acc: 0.7730 - val_loss: 1.2017 - val_acc: 0.6503\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 5856s 146ms/step - loss: 0.6403 - acc: 0.7789 - val_loss: 1.0210 - val_acc: 0.6852\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 475s 12ms/step - loss: 0.6196 - acc: 0.7883 - val_loss: 1.0837 - val_acc: 0.6927\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.6185 - acc: 0.7890 - val_loss: 1.1091 - val_acc: 0.6967\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.6093 - acc: 0.7907 - val_loss: 1.1440 - val_acc: 0.6912\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 0.6102 - acc: 0.7925 - val_loss: 1.0847 - val_acc: 0.6854\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.5910 - acc: 0.8002 - val_loss: 1.0992 - val_acc: 0.6897\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.5986 - acc: 0.8030 - val_loss: 1.2083 - val_acc: 0.6736\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.5941 - acc: 0.8036 - val_loss: 1.2195 - val_acc: 0.6896\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.5898 - acc: 0.8012 - val_loss: 1.2004 - val_acc: 0.6971\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.5837 - acc: 0.8047 - val_loss: 1.2009 - val_acc: 0.6943\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.5813 - acc: 0.8071 - val_loss: 1.0653 - val_acc: 0.6981\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.5913 - acc: 0.8071 - val_loss: 1.1822 - val_acc: 0.6961\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.6015 - acc: 0.8003 - val_loss: 1.2213 - val_acc: 0.6424\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.6127 - acc: 0.8028 - val_loss: 1.2692 - val_acc: 0.7043\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.6153 - acc: 0.7978 - val_loss: 1.4916 - val_acc: 0.6597\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 158s 4ms/step - loss: 0.6156 - acc: 0.7987 - val_loss: 1.4619 - val_acc: 0.6584\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.6267 - acc: 0.7988 - val_loss: 1.1963 - val_acc: 0.6978\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.6222 - acc: 0.7975 - val_loss: 1.2607 - val_acc: 0.6630\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.6278 - acc: 0.7980 - val_loss: 1.2544 - val_acc: 0.6872\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.6362 - acc: 0.7941 - val_loss: 1.1919 - val_acc: 0.6737\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.6362 - acc: 0.7946 - val_loss: 2.0186 - val_acc: 0.5398\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.6476 - acc: 0.7938 - val_loss: 1.4567 - val_acc: 0.6905\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.6338 - acc: 0.7968 - val_loss: 1.1410 - val_acc: 0.6937\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.6536 - acc: 0.7887 - val_loss: 1.4750 - val_acc: 0.6719\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6578 - acc: 0.7899 - val_loss: 1.1655 - val_acc: 0.6880\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6537 - acc: 0.7901 - val_loss: 1.5648 - val_acc: 0.6830\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6698 - acc: 0.7883 - val_loss: 1.1764 - val_acc: 0.7094\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6749 - acc: 0.7870 - val_loss: 1.3104 - val_acc: 0.6865\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.6787 - acc: 0.7847 - val_loss: 1.2001 - val_acc: 0.6960\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.6802 - acc: 0.7851 - val_loss: 1.1171 - val_acc: 0.6920\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.6873 - acc: 0.7843 - val_loss: 1.1290 - val_acc: 0.7001\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.7070 - acc: 0.7775 - val_loss: 0.9936 - val_acc: 0.7057\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.7123 - acc: 0.7762 - val_loss: 1.0767 - val_acc: 0.6885\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.7255 - acc: 0.7745 - val_loss: 1.2881 - val_acc: 0.6382\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.7242 - acc: 0.7748 - val_loss: 1.0580 - val_acc: 0.6924\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.7452 - acc: 0.7703 - val_loss: 1.5175 - val_acc: 0.6792\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.7533 - acc: 0.7689 - val_loss: 1.0246 - val_acc: 0.6918\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.7451 - acc: 0.7653 - val_loss: 1.3472 - val_acc: 0.6313\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.7617 - acc: 0.7642 - val_loss: 1.1613 - val_acc: 0.6376\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.7857 - acc: 0.7566 - val_loss: 1.3047 - val_acc: 0.6908\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.7862 - acc: 0.7550 - val_loss: 1.1592 - val_acc: 0.6650\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8104 - acc: 0.7508 - val_loss: 1.2170 - val_acc: 0.5919\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.7969 - acc: 0.7521 - val_loss: 1.0348 - val_acc: 0.6700\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8057 - acc: 0.7519 - val_loss: 1.0688 - val_acc: 0.6524\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8282 - acc: 0.7427 - val_loss: 1.8857 - val_acc: 0.6700\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.8372 - acc: 0.7408 - val_loss: 1.3668 - val_acc: 0.6851\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.8420 - acc: 0.7421 - val_loss: 1.1672 - val_acc: 0.6833\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8528 - acc: 0.7399 - val_loss: 1.0444 - val_acc: 0.6659\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 672s 17ms/step - loss: 0.8564 - acc: 0.7378 - val_loss: 1.3187 - val_acc: 0.6325\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.8777 - acc: 0.7342 - val_loss: 1.1851 - val_acc: 0.6572\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.8883 - acc: 0.7273 - val_loss: 1.1682 - val_acc: 0.6771\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.9162 - acc: 0.7229 - val_loss: 1.1075 - val_acc: 0.6806\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.9291 - acc: 0.7175 - val_loss: 1.1822 - val_acc: 0.6261\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.9310 - acc: 0.7178 - val_loss: 1.1397 - val_acc: 0.6316\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.9270 - acc: 0.7234 - val_loss: 1.1243 - val_acc: 0.6790\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.9443 - acc: 0.7148 - val_loss: 1.6007 - val_acc: 0.6097\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.9575 - acc: 0.7146 - val_loss: 1.6907 - val_acc: 0.5297\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.9307 - acc: 0.7189 - val_loss: 1.0807 - val_acc: 0.6901\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.9922 - acc: 0.7102 - val_loss: 1.1602 - val_acc: 0.6631\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 0.9942 - acc: 0.7080 - val_loss: 1.1729 - val_acc: 0.6610\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.9748 - acc: 0.7062 - val_loss: 1.1788 - val_acc: 0.6702\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9897 - acc: 0.7030 - val_loss: 1.1699 - val_acc: 0.6454\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.9886 - acc: 0.7049 - val_loss: 1.3742 - val_acc: 0.6555\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.9922 - acc: 0.7008 - val_loss: 1.0632 - val_acc: 0.6622\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.0374 - acc: 0.6911 - val_loss: 1.3781 - val_acc: 0.6460\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.0275 - acc: 0.6922 - val_loss: 1.4495 - val_acc: 0.5986\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.0511 - acc: 0.6879 - val_loss: 1.7541 - val_acc: 0.4196\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.0660 - acc: 0.6855 - val_loss: 1.1857 - val_acc: 0.6081\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.0447 - acc: 0.6861 - val_loss: 1.2270 - val_acc: 0.6027\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.1092 - acc: 0.6768 - val_loss: 1.2322 - val_acc: 0.6234\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.0959 - acc: 0.6706 - val_loss: 1.0951 - val_acc: 0.6362\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.1240 - acc: 0.6760 - val_loss: 1.2349 - val_acc: 0.5882\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.1547 - acc: 0.6565 - val_loss: 1.4126 - val_acc: 0.6533\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 1.2161 - acc: 0.6418 - val_loss: 2.1359 - val_acc: 0.5491\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.1747 - acc: 0.6445 - val_loss: 1.4702 - val_acc: 0.5828\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 1.2358 - acc: 0.6359 - val_loss: 1.5421 - val_acc: 0.5590\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.1783 - acc: 0.6462 - val_loss: 1.2715 - val_acc: 0.6261\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.2459 - acc: 0.6277 - val_loss: 1.4204 - val_acc: 0.4989\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.2479 - acc: 0.6232 - val_loss: 1.4036 - val_acc: 0.6466\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 161s 4ms/step - loss: 1.2699 - acc: 0.6168 - val_loss: 1.2629 - val_acc: 0.6232\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 1.2865 - acc: 0.6217 - val_loss: 1.1754 - val_acc: 0.6204\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 1.2989 - acc: 0.6208 - val_loss: 1.3703 - val_acc: 0.5649\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 1.2946 - acc: 0.6224 - val_loss: 1.3524 - val_acc: 0.5431\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.2904 - acc: 0.6117 - val_loss: 1.4632 - val_acc: 0.4777\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.2742 - acc: 0.6139 - val_loss: 1.3384 - val_acc: 0.6349\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 1.3110 - acc: 0.6054 - val_loss: 1.2978 - val_acc: 0.5882\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.3266 - acc: 0.5940 - val_loss: 1.6077 - val_acc: 0.4233\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.3377 - acc: 0.5931 - val_loss: 1.2546 - val_acc: 0.5845\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.3449 - acc: 0.6013 - val_loss: 1.5601 - val_acc: 0.4512\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3621 - acc: 0.5980 - val_loss: 1.3200 - val_acc: 0.5500\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3054 - acc: 0.6038 - val_loss: 1.2006 - val_acc: 0.6507\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.3756 - acc: 0.5895 - val_loss: 1.9453 - val_acc: 0.4600\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.3829 - acc: 0.5870 - val_loss: 1.6768 - val_acc: 0.4080\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3744 - acc: 0.5881 - val_loss: 1.2292 - val_acc: 0.5917\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.3850 - acc: 0.5835 - val_loss: 1.2382 - val_acc: 0.5919\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.4076 - acc: 0.5873 - val_loss: 1.4926 - val_acc: 0.4906\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.3271 - acc: 0.5853 - val_loss: 1.4754 - val_acc: 0.5498\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.4205 - acc: 0.5763 - val_loss: 1.3065 - val_acc: 0.5679\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.3642 - acc: 0.5845 - val_loss: 1.2002 - val_acc: 0.6308\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.3957 - acc: 0.5743 - val_loss: 1.4106 - val_acc: 0.5354\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.4229 - acc: 0.5733 - val_loss: 1.2989 - val_acc: 0.6080\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 1.8392 - acc: 0.5652 - val_loss: 1.5075 - val_acc: 0.6007\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 235s 6ms/step - loss: 1.5071 - acc: 0.5713 - val_loss: 1.3346 - val_acc: 0.5150\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 272s 7ms/step - loss: 1.4576 - acc: 0.5670 - val_loss: 1.4032 - val_acc: 0.5028\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 261s 7ms/step - loss: 1.4214 - acc: 0.5683 - val_loss: 1.4151 - val_acc: 0.5038\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 253s 6ms/step - loss: 1.4176 - acc: 0.5601 - val_loss: 1.5455 - val_acc: 0.4469\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 248s 6ms/step - loss: 1.4656 - acc: 0.5633 - val_loss: 1.8816 - val_acc: 0.4306\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 235s 6ms/step - loss: 1.4549 - acc: 0.5695 - val_loss: 1.5860 - val_acc: 0.5314\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 219s 5ms/step - loss: 1.4493 - acc: 0.5512 - val_loss: 1.7158 - val_acc: 0.4777\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 217s 5ms/step - loss: 1.4677 - acc: 0.5581 - val_loss: 2.0624 - val_acc: 0.5322\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 217s 5ms/step - loss: 1.4628 - acc: 0.5593 - val_loss: 4.4438 - val_acc: 0.4573\n",
      "Epoch 129/200\n",
      "40000/40000 [==============================] - 227s 6ms/step - loss: 1.4969 - acc: 0.5403 - val_loss: 1.6452 - val_acc: 0.4491\n",
      "Epoch 130/200\n",
      "40000/40000 [==============================] - 236s 6ms/step - loss: 8.1592 - acc: 0.3191 - val_loss: 1.7063 - val_acc: 0.3726\n",
      "Epoch 131/200\n",
      "40000/40000 [==============================] - 226s 6ms/step - loss: 1.5229 - acc: 0.5373 - val_loss: 1.5611 - val_acc: 0.4859\n",
      "Epoch 132/200\n",
      "40000/40000 [==============================] - 214s 5ms/step - loss: 1.5168 - acc: 0.5349 - val_loss: 1.8596 - val_acc: 0.3594\n",
      "Epoch 133/200\n",
      "40000/40000 [==============================] - 194s 5ms/step - loss: 1.4825 - acc: 0.5349 - val_loss: 1.5399 - val_acc: 0.4148\n",
      "Epoch 134/200\n",
      "40000/40000 [==============================] - 193s 5ms/step - loss: 1.5244 - acc: 0.5297 - val_loss: 1.6276 - val_acc: 0.4530\n",
      "Epoch 135/200\n",
      "40000/40000 [==============================] - 192s 5ms/step - loss: 1.5542 - acc: 0.5190 - val_loss: 1.4649 - val_acc: 0.4962\n",
      "Epoch 136/200\n",
      "40000/40000 [==============================] - 212s 5ms/step - loss: 1.5757 - acc: 0.5207 - val_loss: 2.0674 - val_acc: 0.5538\n",
      "Epoch 137/200\n",
      "40000/40000 [==============================] - 239s 6ms/step - loss: 1.6410 - acc: 0.5197 - val_loss: 1.6173 - val_acc: 0.3929\n",
      "Epoch 138/200\n",
      "40000/40000 [==============================] - 212s 5ms/step - loss: 1.6588 - acc: 0.5038 - val_loss: 1.4270 - val_acc: 0.4991\n",
      "Epoch 139/200\n",
      "40000/40000 [==============================] - 186s 5ms/step - loss: 1.6389 - acc: 0.5110 - val_loss: 3.6493 - val_acc: 0.4541\n",
      "Epoch 140/200\n",
      "40000/40000 [==============================] - 187s 5ms/step - loss: 1.5955 - acc: 0.5091 - val_loss: 2.0512 - val_acc: 0.4047\n",
      "Epoch 141/200\n",
      "40000/40000 [==============================] - 183s 5ms/step - loss: 1.6138 - acc: 0.5105 - val_loss: 1.3525 - val_acc: 0.5577\n",
      "Epoch 142/200\n",
      "40000/40000 [==============================] - 30066s 752ms/step - loss: 1.6029 - acc: 0.5075 - val_loss: 1.5340 - val_acc: 0.4789\n",
      "Epoch 143/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.7026 - acc: 0.4787 - val_loss: 1.6536 - val_acc: 0.3936\n",
      "Epoch 144/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.7337 - acc: 0.4928 - val_loss: 1.7835 - val_acc: 0.5520\n",
      "Epoch 145/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.6963 - acc: 0.4916 - val_loss: 1.4889 - val_acc: 0.5691\n",
      "Epoch 146/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.8118 - acc: 0.4652 - val_loss: 1.4853 - val_acc: 0.4986\n",
      "Epoch 147/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.7054 - acc: 0.4753 - val_loss: 1.5385 - val_acc: 0.4454\n",
      "Epoch 148/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.5857 - acc: 0.4913 - val_loss: 1.5979 - val_acc: 0.3798\n",
      "Epoch 149/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.6683 - acc: 0.4897 - val_loss: 2.9992 - val_acc: 0.4282\n",
      "Epoch 150/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.8244 - acc: 0.4734 - val_loss: 1.4540 - val_acc: 0.5627\n",
      "Epoch 151/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.7956 - acc: 0.4517 - val_loss: 1.8411 - val_acc: 0.3851\n",
      "Epoch 152/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.7110 - acc: 0.4739 - val_loss: 1.6336 - val_acc: 0.5111\n",
      "Epoch 153/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.7328 - acc: 0.4673 - val_loss: 1.5511 - val_acc: 0.4655\n",
      "Epoch 154/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.7952 - acc: 0.4575 - val_loss: 1.3744 - val_acc: 0.5420\n",
      "Epoch 155/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.7947 - acc: 0.4562 - val_loss: 1.4210 - val_acc: 0.4826\n",
      "Epoch 156/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.7960 - acc: 0.4499 - val_loss: 1.5875 - val_acc: 0.4012\n",
      "Epoch 157/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.7208 - acc: 0.4443 - val_loss: 1.8756 - val_acc: 0.5015\n",
      "Epoch 158/200\n",
      "40000/40000 [==============================] - 3407s 85ms/step - loss: 2.0184 - acc: 0.4340 - val_loss: 1.4503 - val_acc: 0.4876\n",
      "Epoch 159/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7120 - acc: 0.4614 - val_loss: 1.7272 - val_acc: 0.5438\n",
      "Epoch 160/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.7395 - acc: 0.4661 - val_loss: 1.5958 - val_acc: 0.4377\n",
      "Epoch 161/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.8394 - acc: 0.4535 - val_loss: 1.4266 - val_acc: 0.4908\n",
      "Epoch 162/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.7234 - acc: 0.4630 - val_loss: 1.5029 - val_acc: 0.5269\n",
      "Epoch 163/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.7479 - acc: 0.4685 - val_loss: 1.7298 - val_acc: 0.3919\n",
      "Epoch 164/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.7504 - acc: 0.4476 - val_loss: 1.5122 - val_acc: 0.5018\n",
      "Epoch 165/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.8192 - acc: 0.4610 - val_loss: 1.5730 - val_acc: 0.5074\n",
      "Epoch 166/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7151 - acc: 0.4547 - val_loss: 1.7155 - val_acc: 0.4986\n",
      "Epoch 167/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.8154 - acc: 0.4657 - val_loss: 1.6586 - val_acc: 0.4329\n",
      "Epoch 168/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.7172 - acc: 0.4732 - val_loss: 1.7101 - val_acc: 0.4355\n",
      "Epoch 169/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.7633 - acc: 0.4679 - val_loss: 1.5857 - val_acc: 0.4514\n",
      "Epoch 170/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.7844 - acc: 0.4641 - val_loss: 1.5483 - val_acc: 0.4744\n",
      "Epoch 171/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.7548 - acc: 0.4550 - val_loss: 1.6111 - val_acc: 0.4786\n",
      "Epoch 172/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.9340 - acc: 0.4490 - val_loss: 1.4564 - val_acc: 0.5101\n",
      "Epoch 173/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.9740 - acc: 0.4594 - val_loss: 1.8086 - val_acc: 0.4686\n",
      "Epoch 174/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.7035 - acc: 0.4658 - val_loss: 1.5316 - val_acc: 0.4522\n",
      "Epoch 175/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.8709 - acc: 0.4484 - val_loss: 1.3184 - val_acc: 0.5528\n",
      "Epoch 176/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.7118 - acc: 0.4701 - val_loss: 1.8058 - val_acc: 0.5221\n",
      "Epoch 177/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7123 - acc: 0.4741 - val_loss: 1.6827 - val_acc: 0.4787\n",
      "Epoch 178/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7581 - acc: 0.4638 - val_loss: 1.5107 - val_acc: 0.4576\n",
      "Epoch 179/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7179 - acc: 0.4651 - val_loss: 1.6084 - val_acc: 0.4421\n",
      "Epoch 180/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 2.2466 - acc: 0.4355 - val_loss: 1.8357 - val_acc: 0.3727\n",
      "Epoch 181/200\n",
      "40000/40000 [==============================] - 266s 7ms/step - loss: 1.9919 - acc: 0.4556 - val_loss: 1.8881 - val_acc: 0.3848\n",
      "Epoch 182/200\n",
      "40000/40000 [==============================] - 3725s 93ms/step - loss: 3.3913 - acc: 0.4252 - val_loss: 13.0734 - val_acc: 0.1889\n",
      "Epoch 183/200\n",
      "40000/40000 [==============================] - 184s 5ms/step - loss: 13.9160 - acc: 0.1366 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 184/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 14.4559 - acc: 0.1031 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 185/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 14.4563 - acc: 0.1031 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 186/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 14.4575 - acc: 0.1030 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 187/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 14.4499 - acc: 0.1035 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 188/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 14.4406 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 189/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 14.4398 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 190/200\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 14.4398 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 191/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 14.4467 - acc: 0.1037 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 192/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 14.4378 - acc: 0.1043 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 193/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 14.4551 - acc: 0.1032 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 194/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 14.4523 - acc: 0.1034 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 195/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 14.4599 - acc: 0.1029 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 196/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 14.4499 - acc: 0.1035 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 197/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 14.4410 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 198/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 14.4563 - acc: 0.1031 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 199/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 14.4487 - acc: 0.1036 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 200/200\n",
      "40000/40000 [==============================] - 161s 4ms/step - loss: 14.4479 - acc: 0.1036 - val_loss: 14.5353 - val_acc: 0.0982\n"
     ]
    }
   ],
   "source": [
    "history = model_conv.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4HNW5h9+zRdpd9WZJttx7N9iYagzYBtMh9JIASS4h\nIQlJSEJuQgqpJEAahEuHBAjgQAgkmIABm2aMO+69qve+2nruH2dGM7taSStba1nSvM+jR9LUMyv7\n/OarR0gpsbCwsLCwALD19QAsLCwsLI4fLFGwsLCwsGjHEgULCwsLi3YsUbCwsLCwaMcSBQsLCwuL\ndixRsLCwsLBoxxIFi0GFEOIZIcQv4zz2gBBiYaLHZGFxPGGJgoWFhYVFO5YoWFj0Q4QQjr4eg8XA\nxBIFi+MOzW3zPSHEJiFEixDiSSFEvhDiTSFEkxDiHSFElun4S4QQW4UQ9UKIFUKIyaZ9Jwgh1mvn\nvQS4ou51kRBio3buSiHEjDjHeKEQYoMQolEIcVgI8bOo/Wdo16vX9t+sbXcLIR4QQhwUQjQIIT7S\ntp0lhCiO8Tks1H7+mRDiZSHEc0KIRuBmIcRcIcQn2j3KhBAPCSGSTOdPFUIsE0LUCiEqhBA/FEIU\nCCFahRA5puNOFEJUCSGc8Ty7xcDGEgWL45UrgEXABOBi4E3gh0Ae6t/tNwGEEBOAF4BvafuWAv8W\nQiRpE+S/gGeBbOAf2nXRzj0BeAr4CpADPAq8LoRIjmN8LcAXgEzgQuCrQojLtOuO1Mb7oDamWcBG\n7bz7gdnAadqYvg+E4/xMLgVe1u75PBACvg3kAqcCC4CvaWNIA94B/gsMBcYB70opy4EVwNWm634e\neFFKGYhzHBYDGEsULI5XHpRSVkgpS4APgU+llBuklG3Aq8AJ2nHXAG9IKZdpk9r9gBs16Z4COIE/\nSikDUsqXgTWme9wKPCql/FRKGZJS/hXwaed1iZRyhZRys5QyLKXchBKm+dru64F3pJQvaPetkVJu\nFELYgC8Cd0gpS7R7rpRS+uL8TD6RUv5Lu6dXSrlOSrlKShmUUh5AiZo+houAcinlA1LKNillk5Ty\nU23fX4EbAYQQduA6lHBaWFiiYHHcUmH62Rvj91Tt56HAQX2HlDIMHAaGaftKZGTXx4Omn0cCd2ru\nl3ohRD0wXDuvS4QQJwshlmtulwbgNtQbO9o19sY4LRflvoq1Lx4OR41hghDiP0KIcs2l9Os4xgDw\nGjBFCDEaZY01SClXH+GYLAYYlihY9HdKUZM7AEIIgZoQS4AyYJi2TWeE6efDwK+klJmmL4+U8oU4\n7vt34HVguJQyA3gE0O9zGBgb45xqoK2TfS2Ax/QcdpTryUx0S+P/A3YA46WU6Sj3mnkMY2INXLO2\nlqCshc9jWQkWJixRsOjvLAEuFEIs0AKld6JcQCuBT4Ag8E0hhFMI8Tlgruncx4HbtLd+IYRI0QLI\naXHcNw2olVK2CSHmolxGOs8DC4UQVwshHEKIHCHELM2KeQr4vRBiqBDCLoQ4VYth7AJc2v2dwN1A\nd7GNNKARaBZCTAK+atr3H6BQCPEtIUSyECJNCHGyaf/fgJuBS7BEwcKEJQoW/Rop5U7UG++DqDfx\ni4GLpZR+KaUf+Bxq8qtFxR/+aTp3LfA/wENAHbBHOzYevgb8XAjRBPwEJU76dQ8BF6AEqhYVZJ6p\n7f4usBkV26gFfgvYpJQN2jWfQFk5LUBENlIMvosSoyaUwL1kGkMTyjV0MVAO7AbONu3/GBXgXi+l\nNLvULAY5wlpkx8JicCKEeA/4u5Tyib4ei8XxgyUKFhaDECHEScAyVEykqa/HY3H8YLmPLCwGGUKI\nv6JqGL5lCYJFNJalYGFhYWHRjmUpWFhYWFi00++aauXm5spRo0b19TAsLCws+hXr1q2rllJG1750\noN+JwqhRo1i7dm1fD8PCwsKiXyGEiCv12HIfWVhYWFi0Y4mChYWFhUU7lihYWFhYWLTT72IKsQgE\nAhQXF9PW1tbXQ0k4LpeLoqIinE5rPRQLC4veZ0CIQnFxMWlpaYwaNYrIhpgDCyklNTU1FBcXM3r0\n6L4ejoWFxQBkQLiP2trayMnJGdCCACCEICcnZ1BYRBYWFn3DgBAFYMALgs5geU4LC4u+IaGiIIRY\nLITYKYTYI4T4QYz9GUKIfwshPtMWXr8lkeM53gmGwlQ1+ShvaKOu1Y/VgsTCwuJYkzBR0FaO+gtw\nPjAFuE4IMSXqsNuBbVLKmcBZwAPaYuv9ivr6eh5++OG4jw9LSYM3wMJzF7OvuIJAKEx5g5cd5U2U\nNXipbGrjcG0rtS1+guEwXn8wgaO3sLCwMEikpTAX2COl3KctdvIicGnUMRJI05ZLTEUtOtLvZsDO\nRCEY7PgojW0BdpU3cbCmhd8/9SLNJLO9rJHKJh/pbicT8tOYPiyDNJeT0vo2dpQ1sbuymcrGNsJh\nSVsgRCAUpr7VfywezcLCYpCRyOyjYUQuNF4MnBx1zEOodW5LUUsLXqMtWRiBEOJW4FaAESNGRO/u\nc37wgx+wd+9eZs2ahdPpxOVykZWVxY4dO9i1axeXXXYZhw8fxutt46qbbuWGm77IqJwUZkwezwcr\nV1FeXc/nr76MM+fNY+XKlQwbNoxXXn2VsLSTZLchpaS8sY3yRhVgrmj0ccnPlzE8202rL8QJIzL5\n9eXTyUtTqzcGQpKX1hwi2WHn6pOG9+VHY2Fh0c/o65TU81BLFZ6DWsx8mRDiQyllo/kgKeVjwGMA\nc+bM6dLRfs+/t7KttLGrQ3rMlKHp/PTiqZ3uv/fee9myZQsbN25kxYoVXHjhhWzZsqU9bfSpp54i\nJS2DXaU1fO68s7jt5utJd6tlgD1JDoaku9i7Zw8vvfgijz/+OFdffTWvvfoqN954I6BSUT0tfkJh\nSbLDRrAmiTsXTWBnRRMup53/bCrlzPuWEw6D3SZwJ9mpbfEjBIzKTWHu6Oxe/TwsLCwGLokUhRLA\n/JpapG0zcwtwr1QR1T1CiP3AJGB1AseVcObOncvwESNpaPXjDYT51a/v4+2l/wagsqyEg/v3Upgf\n2axw9OjRzJo1C4DZs2dz4MCB9n1CCHJTjTXc3Ul2vrFgfPvvt80fyzMr95OS7CAQlFQ1+7hoRiG/\nfGMb3/3HZ8yfkMeh2lZCYckJIzJZPK2AKYXpViaThYVFBxIpCmuA8UKI0SgxuBa1yLiZQ8AC4EMh\nRD4wEdh3NDft6o3+WOHxpLCnshl/KMzaTz7ikw9XsGzFhxRkZ3DuwnNi1hkkJxuTvt1ux+v1xn2/\ncUNS+eVl0ztsz3A7ueGJT3l1Qwlj8lKQEv6yfA8PvreHkTkeFk8r4KSR2bT4g2R6kphSmN7ugrKw\nsBicJEwUpJRBIcTXgbcAO/CUlHKrEOI2bf8jwC+AZ4QQmwEB3CWlrE7UmBJFWloaTU3Gqob+UBh/\nKMzInBT2OoIUDsllxBAVY1i1atUxG9cpY3JY86OFZLid2G3KKqhp9vH2tgre3FLOkx/u59H3DQ1O\ndth48dZTOGFE1jEbo4WFxfFFQmMKUsqlwNKobY+Yfi4Fzk3kGI4FOTk5nH766UybNo1kl5vUzBwy\nPUlkuJ1ccMH5PPbYo0yePJmJEydyyimnHNOxZadEZvjmpCZz3dwRXDd3BA2tAfZUNZPhdlDV5Of7\nr3zGV55dx8u3ncaIHM8xHaeFhcXxQb9bo3nOnDkyepGd7du3M3ny5D4akUEgGGZvVTNhCeOHpOJ0\nJCbjN1HPu72skSv+byWt/hCTCtKYPyGPIekuHDbBNScNx+W09/o9LSwsjg1CiHVSyjndHdfX2UcD\nhhZfkMN1rQTDkjF5KQkThEQyuTCdN++Yx5tbynl/ZxVPfbyfQEi9NLyyvpg/X3sCo3JT+niUFhYW\nicQShV6gwRvgUE0LTruN0bkpeJL678c6MieF2+aP5bb5Y2n1BwmEJKv21fDtlzZy1v0rGJnj4fvn\nTeKC6QVW9pKFxQCk/85exwnBcJiSOi8up50xeSnYbf3PQugMXdzOm1rAW986k3e2V/DK+mJu//t6\nZg7P5NQxOdx+9ljSXNbaDhYWA4WBM4P1ERUNbYTCYYqy3ANKEKIZnu3hltNH86+vnc7dF07GYRM8\n9sFefvDKZqtxn4XFAGLgzmLHgBZfkJoWPzmpybj7scuoJzjsNr48bwyvfPU0vnfeJN7YXMbDK/bS\n7Ot3LassLCxiYInCERKWkpJ6L067jfx0V18Pp0/4ypljmD8hj/ve2smJv1jG/63YSzgsOVjTwoHq\nFtoCob4eooWFRQ+xROEIqW720RYIMSzTTVNjQ49aZ5v54x//SGtray+P7thgswmevGkOL956CmdP\nzOO3/93B9J+9xfz7VnDW/StY8MD7NHgDfT1MCwuLHmCJwhHgC4aobPSR4XaS7nb2eD0FM/1ZFEC5\nk04Zk8MjN87mN5+bzoUzCvn15dP5xaVTKWvw8odlu/p6iBYWFj1gcDjCexEpJSV1XgQwNMMNRLbO\nXrRoEUOGDGHJkiX4fD4uv/xy7rnnHlpaWrj66qspLi4mFArx4x//mIqKCkpLSzn77LPJzc1l+fLl\nfftwR4EQor1SWmdXRTN/++QAJ4zIZPbILLaWNjI2L5VxQ1L7bqAWFhZdMvBE4c0fQPnm3r1mwXQ4\n/14AKpt8NPuCDMt0txeomVtnv/3227z88susXr0aKSWXXHIJH3zwAVVVVQwdOpQ33ngDgIaGBjIy\nMvj973/P8uXLyc3N7d0xHwfcee4EPthdxR0vbozYPndUNtfOHc4F0wutKmkLi+OMgScKCaSxLUBF\nYxtZnqQOPYV03n77bd5++21OOOEEAJqbm9m9ezfz5s3jzjvv5K677uKiiy5i3rx5x3LofUKmJ4l3\nvzOflXtr2F/dwtSh6aw9WMeLqw/xnSWf8eB7e7j3c9M5eUxOXw/VwsJCY+CJgvZGnwiqm3wk2W0M\ny3R3Ws0rpeR///d/+cpXvtJh3/r161m6dCl33303CxYs4Cc/+UnCxnq84LDbOHNCHmdOUOtHzBmV\nza3zxvD+rip+8voWrnt8Fc/cMrd9v4WFRd9iBZrjJBAK0+ILkelxYrNFCoK5dfZ5553HU089RXNz\nMwAlJSVUVlZSWlqKx+Phxhtv5Hvf+x7r16/vcO5gwWYTnD1pCG/ecSYT8tP4xgsbOFzbf4PtFhYD\nCUsU4qTRG0AiyfB0dBuZW2cvW7aM66+/nlNPPZXp06dz5ZVX0tTUxObNm5k7dy6zZs3innvu4e67\n7wbg1ltvZfHixZx99tnH+pH6nNRkB49+fjZSSq5+9BM2FddH7PcFQ3j9Vq2DhcWxxGqdHSd7q5oJ\nhSTj81P7vBHc8dIqvLfYUtLAV55dR1Wzj0dvnM3Zk4ZQ3tDGNY99gifJwWu3n05SP+w6a2FxPBFv\n62zrf1oc+IMhWnxBMjzOPheEgci0YRm8/vXTmZCfyleeXcfPXt/KdY+voryhje1ljTz2wV4APthV\nxQV/+pBP9tb08YgtLAYulijEQW2LHwFkxXAdWfQOOanJPPelk5k1PJMXVh8iFJY8+6WTuXBGIX9+\nbw/XPPoJNz29mm1ljXz/lc8st5KFRYIYMNlHUsqEvMWHpaS2JUCay3lcuDCO2t3nbwFvPWQM650B\n9SKZniSW3HZqxLbRuSlIKalq8nHjySM5e1IeX3xmLb9auo0fXjC5X69dYWFxPJLQ/1FCiMXAnwA7\n8ISU8t6o/d8DbjCNZTKQJ6Ws7cl9XC4XNTU15OTk9LowNHoDBMNhslPcvXrdI0FKSU1NDS7XUTTg\ne+9XsOlFuHMX2GP8+VtrISkFHMlHfo9eJC8tmYdvmB2x7YaTR/DcqkO8tqGURVPyuWTWUOZPyLNc\nexYWvUDCAs1CCDuwC1gEFANrgOuklNs6Of5i4NtSynO6um6sQHMgEKC4uJi2trZeGbuZqiYfobAk\nP91Fn8w5QR/4msCTA0LgcrkoKirC6TzChW0enQ9lG+HW92HorMh9Bz6G56+EWTfAhfcf/dgThJSS\ntQfreGnNYd7ZXkF9a4BTx+TQGgjR6A3wy8umcfq4gVchbmFxNBwPazTPBfZIKfdpA3oRuBSIKQrA\ndcALR3Ijp9PJ6NGjj2iQXbGpuJ4v/PVj7r5wMmdPGdPr149JKAA2B+0K9N6v4IPfwR2fQdaoo7t2\n0AcVW9XPB1dGikLJenj+Kgi0wtZ/wuJ7Y1sSxwFCCE4alc1Jo7IJhMI8v+ogD763h8JMJdw3Pvkp\n9105kytnF/X1UC0s+h2JdJIPAw6bfi/WtnVACOEBFgOvdLL/ViHEWiHE2qqqql4faGc8+dF+UpMd\nXHPS8CO7QMk6ePWrUHew82NaauC5K2DFb+GTv8BviuBPM9XPAC3a89abPsq/XwMrjqByu3wLhLVW\n1odWQt0BWPdXaGuEV74Mnmw4/3fQWgOHPun59fsAp93GzaePZt2PF/Gfb8zjP984gzkjs/jVG9us\ntt0WFkdA30dOFRcDH3cWS5BSPialnCOlnJOXd2zaIVQ3+3hjUxlXzxl+ZGsQr3kCnjwXPvs7/PVi\naCjpeIyU8NrXYO97sOLX8NYPYeTp4MpQPwfaTKJwSH0Ph9Txe9/r/N51B9Uk/9aPIreXqipqRs2D\ng5+oY/79TSVCtXvh0r8o15HDBTv+o471NcPapyEUVOM9uFJ9j36O4wRPkoOfXTKVem+AX/5nG89/\nepBdFYOrYtzC4mhIpCiUAOZX7CJtWyyu5QhdR4nivR2VBMOSK2bHMG5q9qoJuzP2fwBvfBfGngOf\nfxW8dYYwfHAffPwnddzaJ2HXf+G83ygf/w2vwI2vwNxb1f7mcmiuVD83aJZCYymE/FC1U03G+pdO\nUzk8fCps/gesflyNs61RWRol6yElD6ZfBa3VULwGZt2ohObUr8OY+ZCcCmMXwPZ/q+vuXAr/+RZs\n+xdsfB6ePl9ZQDpv3qXiEMcCXzM8cxFUbu/ysKlDM7hqdhH/WFfMj17dwsUPfsSStYettaQtLOIg\nkU7jNcB4IcRolBhcC1wffZAQIgOYD9yYwLH0mHe2VTA0w8WUwvTIHfs/gL9dBhPPh2uf73hiay38\n81bIGQdXPq0m2RtehmcvhwdPhGAb2Jxw4k3w6aNQNBdO/goRUey0QvW9qbyjpVC3X31vq4eWamVp\nuLPhc4+q7TV7IdACs2+BdU9DyVolDjvfVFlFRSfByNPUsUNPgEsehIv/qOIYOuMXwc431L0aS9W2\nNU8ocQElUEVavKpkPZRuUOLj7CQrqqEE9ixTYpN5hK44UO6uAx8qwRrSdUX3Ty+eyvnTCinMdHHP\n69v4/subePyDfUwoSCMQDPPbK2aQ1UmnWwuLwUzCLAUpZRD4OvAWsB1YIqXcKoS4TQhxm+nQy4G3\npZQtiRpLT2kLhPhwdzULJudHpjlW7YIlN6l0zR3/gR1LO568cyk0lcFlDytBABhxMtywBDJHqDfy\ncADe/x1U74KZ19IhrSldF4WyjqJQu884rvwz2LtcWQW6eyroVd8nXQjCpsa48021zVsLw05UgnXO\nj+GyR8BmA7szcgx6QLuxVAkTqBhDpRakbjbFdZrK1PN0tYbFOz+Df98Bf5wGG56LfUw4HNvFZibo\nU99LN3R9HJCS7ODsSUOYVJDOs1+ay++vnokn2cG20kbe3VHJ/W/v7PYaFhaDkYTGFKSUS6WUE6SU\nY6WUv9K2PSKlfMR0zDNSymsTOY6esnJvNd5AiIVT8o2NH/4eHjkDkHDrChgyBZZ+D4J+NVkdXKmO\nq9oJ9mQYFplbz6gz4Otr4NxfQuZIWPUwCDtMubTjAHRLoXYf+FW31ZiisOkfakKWIdjwrNoW0EQh\nNR8KZsCaxyHkg+tfggU/hTlfVAJw5ndhyKTYH0C65jJrLFWTflqhijN4cpTQtGgurXDYEI2StbGv\n5WtWAjrlUhgyFdY82fGYqp3w1HlKNOoOGNvXPq0sFB1d8OIQBTMOu43PnVjEa7efzvLvnsVNp47i\n76sPsaWkoUfXsbAYDBwvgebjimXbKklJsnPKmGy1oWIbvHsPjFsIX/0E8ibCOXdDY7FyJ33ykPK1\n1+5Tb/8548DWyYpiQsDUywGpfPgpMfLp3VlgT4KyTer3lCHQWKJ8/7X7IXssOD2w7TW1f+iJKoso\nFDRiHU43jJ6n4g8Zw2H0fJj3HUgd0v0HoFsquqWQOx4ufAAu+iN4cqG5Qu1vrTGymcxxBjPb/63S\nXE/+Kky/UgW7zRaBlCoVtmwjyLByf+msfVI9l47+bE1l0FjW/XN0wrcWjScnJZlvv7SRisY2bn9+\nPT97fesRX8/CYiBhiUIU4bDk3e0VzJ+YR7JDm9i3vKze6i/+kzFhjl0ASamw/TXYqMXIS9YrUcgd\n3/VNpl8FCJjRiYEkBKQVGC6ZojkQDqrJsHa/Ep2ccerNOXssnPZ1aCpVE6v+Nu1wqSwjgKmXKTdR\nvCSnQXKGYSmkFsAJN8KUS5So6O6jJm1idrih2GQpVO9RAgaqejpzJIw4BSZfrLbteMM4tmon1B+E\nU76mftcFB6ChWMVodIKm4H6ZtsSnt77bwHM06S4nf752FgdqWph/33Le2FzGi2sO0Raw+ilZWFii\nEMWW0gYqm3wsnKy5jqSELa+ot/pUUzqs0wUTzoPPXoKa3WrboVXK/ZE7oeubFEyDb22CGVd3fkxa\noUoTBRUHAOVCqt0H2aOVtQIw/GTD3eNrNNxHTo+yDk76MsztuApct6QPVdZJU7kSKJ2UPMN9pLuO\nxi1QQenWWvX18MkquN1cCfvehxnXKKHLHa8+Gz3dFWDvu+r7LK3biS40/haVtdVq6ohqFoXSDepv\ns+QLKvU3FOzR4502Lpf7r5qJ027jqtlFtAXCrDnQo+4qFhYDEksUonhnWwU2AWdP1NwsJevURD8t\nRtrl5EuUv96eBLkTlTtHhrsXBVBB5676ZuhxBTDiEyXrVGZR9hjjHsPnKlcRgL/VJAou9XXhA0eW\n8ZNeqKqfQ77IsaTmmywFLTNp0kXa+NarN/1wUH0WO98EZGTcZNKFcOAjFWsAVW+RMx7yJqj6DF1o\nzIFzv7Yqm/5syRlw+FPY+irsf1+JYaXJ/aPXVHTDpbOG8dlPzuWeS6eS5LDxwa5jVxhpYXG8YolC\nFMu2VzJnVLaRrrj5ZTXpT7qw48HjF4EzRaWnjj7TeIPOi0MUusM8EQ89QX3Xs52yRiuhsDmUi8iZ\norYHvMbbtOMoG/ilDzXSX82WQqpmKUhpTOAjtc6mjcXq7R5UttL6vynxy59qnD9sjgqMV+9UMYID\nHytLA5SbSrcUGkwV3K3V6rv+bBMXw74V8PItyjUFcHi1+l6zF/4wxagI7wabTeBJcjB3VDbvW6Jg\nYWGJgpnSei/byxpZOFmzEsIh1Qdo/Lngzux4QlIK3LIULnjAmLhB+fuPFn0iTs5Qgeec8ao1BUDO\nWDWR3rkLcscZlkKgRQV17ck9iyHEIt1UtGcWqJQhanL2NaoJPCVPWQ+gXEe6KCBVRtKkiyItojwt\n46lqpxKOoFfFZ/Rn1oWm0RSM1l1IuigsvheueFIF7K95Vt2/eA20NcAL1ylrZe1TPaq0nj8hj10V\nzZTWe+M+x8JiIHJ8djzrI3Sf8mljtYygAx+pCWbaFZ2fpDeV00UhY7gSi6NFn4j17KRblyuXibde\niQJASo76nuRR3wPerovIekL6UNNYzJaCJpjNVSoDKK1AiZLDrSZvXRScHiVQ0RZW1ihleVXtUEFz\nYTOK6dIKjdTehmLjHF0U9Oyj5DSVyTRdc+kVnaREYdlPoGYPzLxetRcp3wSFM+N63LPHZ9Fk/yfL\nPhvNTfOnxHWOhcVAxLIUTGw4VI/baWdSQZrasOVllWE0YXH3J+dOUBNjPPGEeNAnYn0STk5TKbHT\nY8Q2nJoo+FvUm/fRuo4gylKIIQotlVoNgyYenhwlCLoozLpBCeTwUyKva3coq6dqJxSvVq4lvcgv\nTXMfSanFFDQLo0W3FLwqC8we1Yuq6CQVgF//N5j7P3Der5Rrbcs/437ccb5tfMf5MgdXv9H9wb3B\np48qEbOwOM6wRMHE+kN1zCjKwGG3qaK0ba/DxAuMN/GusDtgwY+NvkVHi/6mnhJHA0B7knrjbrcU\nekMUtPu7MiOvl6JbCpWRmUmeLMN9JOzKxXP76tjtt/MmqtqP4nWqzYdOWoGqe2itVTEF3Q1nthRi\nPdtw7RrOFDjze6rb65izlOsvXheSVjleV1fDttJGqppU9XQgFOb+t3ZyqKZVXevNu3pcPBeTXW8Z\nleYWFscRlvtIoy0QYltpI/9zprZuwtZXVX+hmT0otj719t4bkD7ZxiMKQqgJMdCqvnpTFMzxBDAs\nhcYSNZHq+93Zqo2Gt04rvnN0vh5D3iQ1YYMxoYPxzE1lyn1UME1ZAOaYgiOGa6xwlrJUzvi24W4b\nfy7seUcJV3phx3Oi0UQh3d7GzU+vprLJx08vnkKyw85Dy/fgDYT48cIi+PQR9Rlf8mD31+wKX5PR\ntsPC4jjCshQ0NhU3EAxLThyeod4IVz2sXEFju1wILnEkp8H481R9RDw43Wqy6mzi7CmuTOWWMruO\nwGh1Ub4FkMaE68k2LAV3VtfX1mssQLl+dMyNABtLlPvJk929KCR54M6dcNo3jG16VlJjN/2UdFpU\nhtNJhU4CoTCTCtK4762d/H7ZLkB1zW0PdB9aFd81u8LXpKrNLSyOMyxR0Fh/qI486lj46onwzIWq\nYvbk27quJUg0NyyJ3RspFkkerU6hl9xHQqjgeeGMyO02uxKG3W+p3/XJ151tBJq7FQUtA8mTo2ou\ndHQBqtiiJuCM4eoYPSU14O08iB4dZ8jQYiLmgHU0tftUS/PmqvZ7XDAhjbV3L+KJm1QX2OpmHxfP\nHMr+6hYOVWjjqN5lxDlicfATqN7d+X7QLIXeXz7WwuJosURBY8OhOs7MrEIEW1UGjDurZ66jvkbP\n9ukt9xHAzW/Awns6bk/NVwIw5TJVNQ1q8m6rV9u7E4XsMSoQXHRSpOimaqJw+FP1PWOY6rWkt7oI\n+uIPomf9Z6UCAAAgAElEQVRoS3F2JQorH1S9q4rXtLuPbP4m7DZBUZaH3189i+8smsD3z1OWzepd\nJqtDH2MsXrsd3vtF1+PzNam4VbwEvEYvLAuLBGKJgsam4gbmZGhVtl9aBl9+t3dSS48Vuij0lvsI\n1IQdy1IqmqOK9S5/xKiH8GSrau66g92LgiMJFv0cTvtm1DO4lNtq51L1DIUzo9xHXVgK0bgyVeZY\nZ+4jbx189qL6ubHEePPXK62BxdMK+OaC8QzP9jAhP5V1e0qN87tartTfbHS1jYWUqs4j1IOYwvpn\n4fFzlJhYWCQQK9AMVDa1UdbQxqTCevUGO/SE43bR+k5xurXsI2/vWQqdcfGf1MRmFgy31lHW19i9\nKEDnQfn0YUrYrntRVUN7ctr9/QR6IHhCqGuZK6PNbHhOiShCWRP6uhW+xsjj/C3w8Cl8bcRdPL+2\nGpJB2hyIruIKgbauLRR/CyBVO5BwqPOOumYaDqnMLG+dijdZWCSIfjbzJYZNh1Vf/eG2KpV1098E\nAZRV01h6bEQBOloQnmzj53hEoTMu+r1KsdWbAHpyVFZTOKwsBU+MVuOdkVHU+eS84XlVQ9FcrnVj\n1YTH3xx5XP1hqD/EJXPrCc3Kge2wI3kGk4tXs+G+i3g66xsE3bms3l/H508ZyR0Lx6tx+ho6/1uY\n3/aDvvhSnvV+U5alYJFgLPcRcHj3Z9iEJMtfbgRO+xu6pdBbxWs9xd1LojDiFEMQQKWYyrCKVwR9\nPavWzhgWezW3xjKo2g6TLoD0IuXq0eMW0ZOuJha2gJcrZihB+lH9JTwUvpLpzR8zr+olPjvcQIbb\nwcMr9lBZ32JkFTWWEhOzNRJvsFm3ZNoauz7OwuIosUShbBO3bLiK67J2Y284rFwW/ZH2OoVeanPR\nUzwmITgaUehwXa2VR2utEr2exEsyhqvK6+h6gP3vq+9jzlLCUbEV0IrcfFGWQnvhXEt7l9aFsyfx\nTNJ1hNKLuGq84OMfnMNTN59EMCx54E1TMLgz15VZeOJNS+3MvWVh0csMelGQ2gIt57s2q6KpfisK\nbjWhhXxG24tjiT55Qy+LgmaBtFb3PIiuZyDpwWa9unnfCmXZ5E9XcYeAtjx4UmpHS0GPZ/hb2xcw\n+trCaaz+4QKSM4e2Lwo0MieFq+cU8fZnB9pPbSjfH3tcEZZCnMHmFst9ZHFsSKgoCCEWCyF2CiH2\nCCF+0MkxZwkhNgohtgoh3k/keGLRWL4HgNnNywHZf0UhyaP82NB72Uc9ITldBekhQZZCTc/jJelR\ntQp/v0YtyrNvhSoKtNkM4QDVkjw6phDDUsDpwWYTkV1dgbsWT+Luc0e3//7+mg3IWG02omMK3SGl\nyX3UoBo1/mlm97UQFhZHQMJEQQhhB/4CnA9MAa4TQkyJOiYTeBi4REo5FbgqUePpjJbyfQC4/ZpP\nub+Kgtk6OBaB5miEMMQgVpvxI8WlXcurxRSOxFLQ4woVW9XiP01lynVkPgYge5SasM0TudlSMC9g\nBB1EIdOT1B53APBWHWTlXqPIzR8MEwiFkea4QDxpqd46lakEysoo3agWflpyk7EAkYVFL5FIS2Eu\nsEdKuU9K6QdeBKLLc68H/imlPAQgpaxM4HhiU3eQgDSlBGYcwSplxwN9LQpgBJt701LQr+Wt04Lo\nPRCFaEsh5FOtu3PGqRYi5mNAWQpILWVUo72a2iQKeiA/NR/8TZHHB431GEY5a3noPWWJBkNhrnt8\nFaff+x6vrtrRfkx1QxwxAl2YQAWavdoLTOVW+PD+7s+3sOgBiRSFYYA50lasbTMzAcgSQqwQQqwT\nQnwh1oWEELcKIdYKIdZWVfXu6ljulsN8ImYCQnX3TI8eYj/BLAR9kX0Emv9fqGU1e4vkdNVrqVl7\nI+9JEN3pUkKlr+YW9MGE8+Eb64yeTXo7DGEzrESze6fdUtDakpsXMDL3atLR13xISmOSq55P9tWw\n7mAdj36wj3UH6/Ak2dlfYhz/wbY4ejO1mP7N+5qUSyslD/KngRYTs7DoLfo6Id8BzAYWAG7gEyHE\nKinlLvNBUsrHgMcA5syZE/9yWt0RCpARqKQy5SxIDSh/bX+sUYDI6uu+yD4C5f93ZcRXjBUvNlvk\n2s09FTynx/DbB32qmtqMK1NlbjndhqvKHFfQYwr+lo4xjTRtxbmmcmPhI91SyBlLeuV2Mt0Orn3s\nE8ISLphewIPXnUjNq0thszrs450lXC4lQgjY8opyC+VOhMkXGfdpMRnQvkY1Dne2Gm9bQ88+DwuL\nbkjkDFgCmH0xRdo2M8VAjZSyBWgRQnwAzAR2cSxoKMZGmHDGCJh3sTEB9EfMk1VfuY+Gz+3REphx\n484yRKGngudIVllLUir3UbT7SQgVVxA2Y7Efc3ZQS5T7yPzZ6r2ammNYCrnjEWUbeeHGcby+J0BN\ns4+7Fk/CbhMMSTLSUOsam7n3vztYNNLOnJe/qDYmpVExbA9pLgeeJIcxhtR85T7yNyurzJWhRMTC\nohdJpCisAcYLIUajxOBaVAzBzGvAQ0IIB5AEnAz8IYFjiqC1ch8eIDlvbOSbWX/EabIU+sp9dPod\ncHoCruvKNFxAPc2sciQrMdDrAexJHY8ZeZoK5OrtI/RaBSlNlkIMUWhf/6HC2KZbClkqC2lyqpfJ\n506BXf+FFK3jrMk9dcrwFH79/j7eEJV8lAylYghD/ZWc9ptluJxOrpoznJ+mVgKCRncRGb5GFV/J\nHqNca5alYNHLJCymIKUMAl8H3gK2A0uklFuFELcJIW7TjtkO/BfYBKwGnpBSbknUmKKpLVYGSebQ\nccfqlokjwlLoI/dRonBnqSpkODJRCPoMF1Ks8y/+I1z6kKpTAFXh/JeTYd9y1W8IVEpqsC1ScN1Z\nSmTMloJ+H72+ItgGBz+GF6+DknVqm69JTejAracX8e6d83ngMtWJtQYVj7l+ZjYLp+TzzMoD7Duw\nnyZbBqvLJc2NdaqQz52lsrwsUbDoZRLqQJdSLgWWRm17JOr3+4D7EjmOzmip2EdQ2hg6Ymxf3L53\nMffP6YvitUTizjQKzHrqGnO41MTcLgrJnR+rWwr7P4CqHbD2afV7yhA1kUe3JRdCuZAiAs2apaDH\nJ4JthjvKW6+++5pU+w5fIwR9jM1LZWxQCdKE0aNg325+cf5IwmnDKK7zsvfAAYKk0io8NDeUkCLq\nEJ5s9Xf2N0Eo2H9jYRbHHYO6olnWHaCUHEbkpff1UI4esxD0RfFaIjGnuB6ppRDqgSiUrFXf97yr\nvmcOV24hf4y1KtLyI0VB72WkjznQZghFQKsp8DUZjf304zXRSk7Pbz/GZhP84tJpZNNIwJXD1NFF\nZIVqESE/h9pMgXGr9YVFLzJ4RWHzy4yvfpc9jgkkO3oxW6avOB7qFBKFy1QM12NRiLIU7HGIQq0q\naGy3TvRUVW9tx882Nb+91YU6RxMAt8lS0Cf+dlFoNNbe1mMd+jEpWgW3FneYMjSd6Zk+Jo4dw5ii\noSQL5c56cFUtm7Vwx0+XfMyuitjtL1p8QWqa+2gtaH9LYhIPLBLK4BSFiq3wypfY6pjMksLv9fVo\neoeBLApmS+GIso/MMYUYgeb2Y12qVgWMlh1giEJLdQxLodAIgoMxuZvdR7EsBX3y14/XLRldLPS3\nfylJbqvBmZ6PzW1YtKmZQ3h6nSpiW7/zAJf95WO+8NRqzr5/BdtK1bk1zT4u+POHXPf4qtjtNhJJ\nUzncP0FVkFv0KwanKFSrAPNPAzeTn5fXx4PpJcwxhb7KPkoU5rYZPX02e3Lk23pXloYQhrUwYbGx\nvd1SqOt4/7QCFezVq5oDXhV81v8eZivF36qtutZk9HTSl+QMRouC9uZfsVXFDYZMjlhc56aFJ1Lm\nV89y7wUjOGlUNmX1Xupb/dz5j88oa/Dypb+u5WBNK7sqmtlXbaq6PhZs/7dKna3ppD9TwKvadVgc\ndwxOUfDWAVDsczMipx8tudkV+mQlbB0Xse/vRMQUunD/xMKRrCberlJSzegT76h5Kq3U6TF1gJUd\nLYUCLc20WItD6BlKuvjoa1y0/9ym0l9dGWosIVNhHRixBj0tdteb6vuExZBsVIqPKhrO/15+MgBT\ns8P89YtzWfad+fzuyplsL2vkjN8uZ2tpAz+9WLUbe3d7BVJKdpY38drGEpp9wa4/h6Nl+7/Vd32d\nimg2Pg9PLOjYqtyizxmcKQvaP9RGUhiZPUAydWw2NRnZ7LHXVe7PmGMKR5x9FIelAEZaat5EGLcQ\nDq+KrAGJvv+IU5QQH/hIdV4NaOtI6/cJ+oyCtkCLsUhOcppmxUSJQoouCpqlsPO/MPREZZG4TAkR\nnmxmjNPGZUpLXTQln6+cOYbDda1899yJjMlLZcnaYt7ZVslnhxt4Y7Nydd25aALfWDC+688iHpqr\n4JkL4boXjKru1lr1eUDnBaEtNUocA61G0aDFccGgtRSCdjc+khiZM0BEAdSENdAyj6B3so90N013\nloZuKeRNgvN+Bbe8GZXuGyUKrnQonAUHPlS/62s+tIuC1xAkf6sx2Senq/hGuyjogWaTKDRVqEyo\niRdEjg2hhFLvMRVVq/C/F0zm4RtmMyZPTbYLJg1h9YFa3thcxlfPGsvkwnSW7+xB78m3fgTrn429\nr2Y3VO+E0g3Gtp1vggwpge1MFPT4Srwrzx0rQkH48IFBbcEMUlGox2tXb13DB4qlAKr/0UALMkNk\nTOGoLYXuRCFVuWnSCtSxyWmRfaVixTRGz1PuI73q2eFSLjxh0wTJlH3kM1kKDpfhPtLdW06P+vI1\nwu631baJ52vnaJaCK0PVJSSlqMC4Xv/QCYumqDTXS2cN5fvnTeTcKflsOFxPbUucq75t/DvseSf2\nPj2IrrlkATi0UrnBRpzShSho58W7yNCxonQ9vPtz2P56X4+kzxikolBHk0ilIN2FyzkA0lF1nO4B\nKgq6pSC6jwlE43Cpt1b9zbSrlFSA0fNh+hWRLriu3Eeg4g/hABz+VAmA06XOd7jU5Bcwi4JuKaSp\nZ4m2FHQh8jWpDqhOD+RPVft095FeLS1EXFXNM4dn8vcvn8zvrpyBEIKzJw1BSvhwdxwdh4N+lYrb\n2Ru9vt0sTP4W9Tfz5HYhCi2R5x8v6O69iq19O44+ZJCKQi014VRGDCTXEQxc95HTrSZzh6vn8RI9\nBVX/z96dpXDGt+CiqPZbEe6jGJ/viFPUG/vBjzVLQRMOh0uzFEyBZrMoOGLEFOwmUWiuUHUQ+jPr\nloJ56VNXRlytLk4bl9tejzNjWAY5KUks3xGHC0lv262/2UcTy1LQ1wn35HQeaO7MUvA1wcoHIRyK\nfd6bP4Dnr+5+3EeKzxKFQSoKdVQG3QMnyKyTnG4ESgca7qwj6+mki2T7UqU9zF6CqBqQGP9mktNU\np9W6g4aloN876DWlpLZ0FIWQKSVV2JVbKEIUhkTeB4zFjEBrn921+ygam01wxvjc9lXh2gIh9lQ2\nx65l0Ps6dWcpmMcQ1ITRk63SUgMxzvV3ElPY9Ra8fTeUdZKuWrpBtSBJFPrfp3Jb4u5xnDMos49k\nax0VgcKBFWQGWPBTYIBWkLqzjqz5my4C8VoKsYiIKXQiTCl5apW2QJuRVurULIVALEsh3aihACNA\nDWry9zert+9cU4aQza5E32MWhfgshWjy0100tqnq6OdWHeSXb2xnRlEGkwvSGZKezO1nj1Ou1eZK\nY3yx6MxScCQbFo23FpxDo87rRBR0y6K5EyumuTxyEaTeRr92c4XKrEodIHVMPWDwWQpSIr11NJA6\ncGoUdIafpNY0GIi4M49sQtcnWn3i7C6mEAt7klHp3FmzwZRc5WoJeiMthYC3i0BzcmTxmu7qSk6P\ndB+ZOe2bMP1K43dXRteB5gMfGVXFm5aoTCLAaRf4g2EAalr82AQEQ5L3d1Xx4Ht7+PJf19LqDxot\nPGK97UNsUQhqLcZ18YrlQmoXhSj3kbcLUZBSZWT5E5gZZBacysHpQhp8ohBoxRb2Uy9TGZ41AIOy\nA5W0gsg35HjRhcR3FJaCEIa10JkLKyVX5d4H2jrGFPSJU3cf2ZPVOPS1HiByASA9ldNb11EUzrpL\n1U/odBdoXvkgLPuJ+nnrv1TRGJBktxOWEApL/MEwniQHS++Yx6ofLuCBq2aycm81P3ltq7FWRLCT\nmEJ7oNksCtqz6JZCrGBze0whSmz068QSBV+jGkfIb4gpwOHVncc8eoqvSWWNAVQMThfS4BMF7a2l\njlTy0wdgUHagsvheuOLJnp9nthRsjiNfKlS3EDqzFDydWArmNhe6+0iPDUQXr+mClZxm9FNK6cZ9\n0Z37yNek+hBJqa7prYdwCKdDBa8DoTCBUBin3QjgXzG7iC+ePpp/ri+mobpYG3t3lkJ95DanuxtR\n6MxS0EWhgg6YFzPSrYXGUnjyXNj8cuzx9RRfoxLilLxBG2yOSxSEEP8UQlwohOj/IqL9o2uQqeSk\n9jC90aLvSCuA7NE9P89uyj46EteRjp6B1FVMIRxQLx26peDURcHUEM8sChHZR23G+Ew9jjpYCtG4\nMpSV0dmbsq9RXbutXptoJbQ1kGRX/5X9oTD+YJgkR+R/7dvOGovLaWf33r0AtLW18u72GBO12VLQ\nA9V6fKQrUfB3Igq6q6klhqVgFgrdzVO1Qz2Tt5Msp56i/33yp1ruo254GLWU5m4hxL1CiIkJHFNi\n0UQhmJwxMFpmW3RNe/ZR45G5jnT0WoWuYgqA6o9kjim0daxTiCkK/khLQadbUdAK+zqzFvTJs7HU\nWPfBW9cuAv5gGH8ojNMeORXkpiZz02mjsLWoidgWbONLf13Lqn1RE7z+xm8WpkCbshT0+pKYMYVO\nUlK7ch/FEoXqPZHXO1r0v0/2WKjd3zvX7GfEJQpSyneklDcAJwIHgHeEECuFELcIIfpX9zXtH53t\nSPzTFv2PdvfRUYqCbil0FlPQM44gKqZgshTCQfXWrNcbRDTEi8o+0jGnpMZCn3gbS2Lv1yfP8i2q\niA+gtbZdBAKdWAoA31o4nilpStCSRJDUJMFrG0sjDzK7lfQJPWiq6nZldOI+6qR4rT3QHMt9ZFrM\nSHcf6V1Y/b3UBVYXhcwRyroahMudxu0OEkLkADcDXwY2AH9CicSyhIwsUWj/cJPScro50GJA0J6S\n2nCUlkI3MYUUkyjEiino6zM0V5gsBVcnMQVT47vuYgqj5ysRWvdM7P26KJh7E3lNohCUBELhdneS\nmWS7DZevuj3wev6kTN7cUsbKvdXM+917fOeljWw9ZEzeodZaVXQW8huV356cjqIQCiiB1J/78Br4\nv9PVWNsthRjV1ua1sPXeRNWaKByppbDxBdj+H+N3f7P6+2SNVL/XHz6y6/Zj4o0pvAp8CHiAi6WU\nl0gpX5JSfgPotFpKCLFYCLFTCLFHCPGDGPvPEkI0CCE2al8/OdIHiRvtTcSdntvNgRYDAnNjuqOK\nKaREXi8asyjoxzj1Nhde440+QhRMbS5Cvo7uI1dG9wV7KTkw6zr47MWOLpegzyiOM4tCa63hPgqF\nOrUUaGtQopZRBMBFk7Opbw1wy9NraAuEeWd7BZW1RoD5z2+s6dhjKpYomN/qg22qUK1iC1TuUAFr\nh0utIRH99h8RaNbEriaG++ijP8CT58VXz/DeL2Hln43ffU1KlPU1NOoPdn+NAUa8lsKfpZRTpJS/\nkVKWmXdIKefEOkEIYQf+ApwPTAGuE0JMiXHoh1LKWdrXz3sy+CNBttbhlUlkpg+AdZktusdsHfSG\npdCZKJjdR06T+8jXBEijCjnYFpl9ZF5Pod19pL1ndRdP0DnldvX2/cRC+PMJ0KC5ksyTYvkm42dv\nHUlatpE/KAmEZIeYAmCITKZ6az51ZArpLmXx/O2Lc1n340XMH5OG1OItew8eJuTXRaELS8E8gQd9\nxuRfsQWQRsGefv/mKiUIzeWQPkx7tmYVrG7Q3uR1d9RHf4B3fqZani//dWefmPY51ENjMdQdMLb5\nGjX3kW4pHOr6GgOQeEVhihCivVWlECJLCPG1bs6ZC+yRUu6TUvqBF4FLj3CcvUawpZZ6UslLO4oJ\nwqL/YJ7Ejzam4HCpdSti4XRBksktpH/XJ31zvyKz+0iGVbvmYJuRKaXvT+kmnqCTOw5OvV2dV7vP\nEAC9NgOMgDACvIaloMcUzCmp7ejuGs2VkiT9PD+vllfPDzK5MB2n3YYt2IZIV9XKnnAThyqrjc9D\nf+7oQHP7WFDPrYuCPu5cLY9F77v0+tfh2cuUMOhrNviaoHav6Zpa48F3fwGTLoLZN8Onj3S9upve\nLqO5InJVvOQ0NW5nimpdMsiIVxT+R0rZbidKKeuA/+nmnGGA2SFXrG2L5jQhxCYhxJtCiKmxLiSE\nuFUIsVYIsbaqKo7Ojl3gb66hXqZYojBYMK/JfDTNAkeeDuPP7foY3YVkthR0zIkNZvcRaHEHv8lS\n0KzY7oLMZs77FVy/RP2sB2Sj3SeeHOXGMgWa/SGVfZQUKxOv3VIY1T7O6bseYsrOh41jAl5IL1SH\n0czeUk0UzJZCS5WRrgpRouAzfi/TRCFvknb/CuN5KrepdRtyxqlt/mYjnpCcrhUGNqpg+pizYOE9\nYHPC5n/E/LiAyDqE+kNqHDKs/j5CKBeSZSl0il0Ioz2l5hrqjST/9cAIKeUM4EHgX7EOklI+JqWc\nI6Wck3eUayqHmmtosCyFwYN5Yu5p220zM66GazpZaEZHFwVzTEHHvFCQPunrx4X8WvZRVEwhXvdR\n+/21/xv6ZKqLQrqKCZBWqMQpItAc1gLNMSwF3TWj14cE2tTE22B61wt6wZOLtDnIsbdyoFy5in69\nbD93/2szTUm5ylry1lHV5OPVDcVU1Zqb55ksBX2SHhIlCmZxSx9muOX0eEL+VCVOekZSUqqq9E7N\nh5bqzj+vyu3Gz3UHIpsVgiUK3fBf4CUhxAIhxALgBW1bV5QAw02/F2nb2pFSNkopm7WflwJOIURC\nI8DCW0udtERh0GAWgqNxH8WDPinHaynoYwv6IgPNrgw1semuknhxJKk38/bJVJskdR99WoESJ3Od\nQhcpqdQfUsfrzxX0qms2liqXF7TXJAh3FiPc/nZRqGgVPLfqEB+VaxnrTeXc/9ZOvv3SZ3zruY+N\ne4RMloKeupszHhBGBpJZFNIK1Gfjb1ausvRh6pkDrcbz6jEZvR9VZ1Rug+wx6ucIUdBEO2ukCjTH\n6h47gIlXFO4ClgNf1b7eBb7fzTlrgPFCiNFCiCTgWiBiOSMhRIFugQgh5mrj6WRVjt7B2VZNlcwk\nL9UShUGBvtgNJF4U9LiBOaYQvQ8ii9fASFvVf3ckw+2fwok39XwMqQVGlo4+yeVpPvq0AhXwbq01\nKpqDYU7yreKbJd/vOPnVHVQBV13kAm1qMpYhaNLqFfSaBHcWw1xt7NHcR58/cxJj8lL4rEEF6AP1\nJby5pYwFk4YwMUdLz9VXpovOMkrJixK3JhhztnJJ5U9Vk76vSYlT+jCVBBBojbQUQIlCayeWgpTK\nMhl9phY7OBDZrBCUpeBr7HFr8v5OXK2zpZRh4P+0r7iQUgaFEF8H3gLswFNSyq1CiNu0/Y8AVwJf\nFUIEAS9wrYzZ1L2XCPpxBRqoIYMsj9XiYtDgSI5sI5Eo9DfqWKLgjhVT0Maju4/M49PSQHtM6hAj\nQKxPcrqlkFqg6ggqt5mK1yRTgtuY5FurxmEWzvpDMGRy5JoUeopr/WE1aZqql/PCrbhQ+2eNLmBm\nRTKf7FaWwu49u2lsG8mNp4zEvjUFNkMoORN7sM2oWWj/rDK156hUWVVBL4w8DW542bTeRLOKNeRN\nVEkAAa9hKeii4MntvKldU7ma7IdMhaw1yiLo4D4yZSCZ3X8DnLhEQQgxHvgNKrW0/V+6lHJMV+dp\nLqGlUdseMf38EPBQD8Z7dGimZFtyDjZbD1fwsui/OFzAURavxUN7oDlGTCFiDQS9olnv4KpNZr0x\nvrQCw9euT3K5E4x9AW9EnUIgFCY5bOriqo8hHFaT4cTFhqXQYjLiGw6rt+1Aa7soZLQcJhm1RoMj\nyc3MojSWbvCAC/bv302mZxynj8ulrSYJNkODSCM76ANMHU9dGappoTtbFbJFLF+qTVdJ2noTTWUw\n9mxlcfhbjdqFaPeRlJEr9kkJm15SPw+ZDFmjoG6/cS9dVPRahdr9UDgz/r9BPyde99HTKCshCJwN\n/A14LlGDShiaORr0DL6FMwY1dpNbJpGMOQsmLDZy6Tu1FPRAs15tXd/x+CMlNV/9O9fTK4Udhp6o\n2m2POQs8WRBowSnVROwPhnFJU8M+nZZK5e/PHGmM0+yfrz+kWQ1SjduViTvcxFUzjQysGcMz8ZGE\nz5lJU9Vhzp9WQJLDRrpDWQZlfjcE2wi0NdPq1N7E9Tdyt7aiXPTbO6hJv7lCWUJpBYb7KNpSSNGC\n3P5m1h2spaRee86374Z3fgpjz1FLqWaNUu6jtkj3UVnySIIiieY9K4/kL9FviVcU3FLKdwEhpTwo\npfwZcGHihpUgtH/UYU8PUv0s+j+OYyQK+VPh+pdM9zNN8q4Mo09/tPvoaNZ6iCY1X03W+lt2cpqa\nRG98RbmRtEnXFVD39IfCuHVR8JtEQc/PzxxppJdGi4JehOZ0gysD0dbAwvFGZtWUwnQcNsHBQDo5\n4Vq+cOoo7T4qhlDud+P3eWlpamCTrwAAqYunO7OjpaCTlKqCzKAyqpxukCHVZgNMloJ6+du8ey9X\nP7qKSx/6iF0l1bD6cZj6Oc0d5VSiEGg1rpmczsq91Zz/lzV8GhxP/db+1cnnaIlXFHxa2+zdQoiv\nCyEup4v2FsctlqUwONEn50THFDq7LyhXkt5p1VzRDEe3VGg0aVoaa3NFZEdWHW3STQoo6yQQMlsK\npoCvnoqZOcJwg5lFoeGwqaWFS3uzbzSExenG5bQzsSCNslAmE1OamVyoCUbAi0TQhBt/WyvS30Ix\n+d1Sw9EAAB82SURBVPilnRKfdi9Xpqo4jiEK4aQ0Iw6RVkjQrs755wfr1TZzTAF48PVPKMxwYbcJ\nfv3E35X1MO1zxtoaWsqt/+Cn2r1Sue+tnaS5HDQPO40i/z627NpLTbMv9jrWA4x4ReEOVN+jbwKz\ngRuBI0iN6GO0Yhx/stX3aFAR6839WGCOKThc6o3W5uiYDdVuKfSG+0i9cdNUbrRsMKPFNpx+JQpt\ngTApaJO7OQuo/oD6njnCGJee859epALNUZYCSGMdBO2c2SOzqBTZDLObuo0GWpXLx+Em6GvDEfIy\nvCCP8qRRfFztobKpTYlM0GtkD2kut2ZfkLf3GMtxhlILeH69inU426oJ25PV2z+ovlCAbKnmoetP\n5KVbT+VMl4q3PH7AVAMy/GSkPRn7oY/x46Q5ZGdTcQOXzBzKGYuuAOCxvz3D7F++w782dtKNdgDR\nrShohWrXSCmbpZTFUspbpJRXSClXHYPx9S7NlTTjwemyluEcVLRPwsc448wRJQpJHqNaFkwxBU0U\njqa4Tic1PkvB4VOTdIsviKddFEzuo/pDyv2S5FGTrLAblsKQycpS0GMQWkwBMNJhteD0nYsmcs5J\nM7G3VppqG1oRSR6y0tMg6MWNj9zsHOTnX+XXgev5w7LdRmxB71KqPcc9r29lpylDdMnOIOvL1Phz\nacBnc7NkzWHOeWAFm+uVOJw70sas4ZmMyk3h5mGllCeN4LcfVrOrQlkhjbj5WMzCTphG6ebltYcJ\nhSWnjsklZdRJBBwpXJO7n5QkO2sOmJYdHaB0KwpSyhBwxjEYS+JpqaRKZuB2WovrDCr6ylKIcB+5\n1dtxRMBUc6foKaS9Mb7u3EdavYSjXi0g0+IPkiL0RYBMloJeo2Aev24p5E9RcQvdxeT0aJYCKiPI\n5mx3zWR4nOQUjFDtI3RR8auMpbzsdFJowyHCDM3LZuSIkXzu1Em8uOYQ93+giYt+D+05Vu2voShf\nuX+bpYsfLT3A0Dz1TEOdzTSFXTy4fDf7qlq49nnVG2nRSC1rKRzCdng1mZPmk+py8ON/bUFKySMr\n9rLEe5J2TTd/fHc3Trtg9sgssDtwjpnH6Wxi6tB0tpcpAV+6uUxZNAOQeN1HG4QQrwshPi+E+Jz+\nldCRJQDZXEGVzMBlicLgQheF3ngT79F9zS02kjVRMHXnTc1XwWd9ha/eiCkkp6nYRVMnopA+FEac\nivjwAYbb62n1hUzuoyhLQU/J1J9Fsy4YorUoq9qpvjtdhig0VxgprDppqmFee8FboBWcKRRmZ+IU\nauEfd4r6XL577kS+cc546qSKv5Qe3NX+XPWtfg7XesnJVu7fWnsOErj6VFWcl29roCaQxOFaLzed\nOpKAcOGzuclEs8Qqt4GvAdfYM7hr8SQ+3V/Lj1/bwtMfH8A15QKwJxN0plLfGmDW8EzcSdo8Meki\nqD/IoowSdpY3caimla89v56nPz7Q7Z+jPxKvKLhQlcbnABdrXxclalCJQjZXUSXTLVEYbByr7KNo\nnKYAt82mGseZi9LsDtUJVc/06a3xpQ7p3FIQAi79C4T83ON4ima/yX3UvrRmQLmHskYZ55kFLk+r\ne9C7lDrcKgYAKpYR/RxppjiHfh+nm8x009i01uQpyQ6+s2gCP7nqdADqSvciEeBMYUuJmtwL8pQo\nZOWP4P4rZzK6UP3uCTbQgovc1CR+dOEU3v/+WSSl5xkWyt7l6vvI07j2pOFcN3cEz606RCAU5mvn\nzYKTvkRT/lwATh1jqkCffDHYk5jvW06rP8Rzn6q/144yUxfaAUS8Fc23JHogxwLRXEGVHG25jwYb\nx6rNRWf31cXh0r90bCWRXghln2nH99L40grVm76+YEw0OWNh9k2c8ekTPN/mJVlovn490Fx/SGX3\nmHsvmVeT091KNfuMfbql0FoNGSYLA5R1AqotBagAtdODMAuNvoiRRnKqin0Mt9fQHHax81A9m0pU\nMKGoQLmP0vJGcMXsIihTgWaBxCvcXDd3BEkOG4UZbpWBpLu9tv4Thp4AmSMQwC8vm4bTLhia6WZU\nbgos/g3ZNS3kP/oJ504tMAbjzoQJ5zH6wNvYWczfP1UurR3lcSzi0w+Jt6L5aaBDLpaU8ou9PqJE\nEWhD+BqplhkMtURhcNHuPjrGomBPAoSR569PnGbShhqrovXW+EadAR/eb7SBjkVaIUkEcXpNvYF0\nS0HP1882iYL+DEmpKgjscBuV0w63EWiGjqvFpeShGtxVGvdJyYuKuUQtc6oFmtNlE5W2HO54cSOT\nC9MYnu0mVW9mqVsgpnNnjx/OKQvGR967uRxq9qrP+dxfte+y2wQ/v3RaxG1H5qTw6Q8X0oHpV+Hc\n/m9Ot2/jA990kuw2yhraqG/1kznAWubE6z76D/CG9vUukA40d3nG8YaWKldFJu6kuJemthgIxOpF\ndCzQm/F1ZQFoaxEAvTe+qZcrQYDORUGbdFPaTOse65ZCjeYWMlsK5rbeQkDGMCNA7nQrsdCL86Kf\nw2ZXqbB6eqkWaI74XKIsBbOApqZnU1Lv5Z3tlUwflmE8U1qhcX8NT2pm5CpyKZqlsOWf6vepl8f8\nOLpl9HwATk9Tz3zpLGX9bC8beNZCXLOjlPIV09fzwNVAzGU4j1u0NrzVMt1yHw022mMKffBG53R1\nDLyaSTOLQi9ZCkMmG6uXdSYKWr1Chs+0um67pbBX9RdKMRV56s+gVwvrLiF9n81mTOSxntdjamMd\n8KpgeBfuI2x2SFbX86RlcsY4FTeYNixDBcAzR0LRXO1+JisjOaqmNiVXWShrn4QRpykxOxJcGWBz\nMtbThhDw5Xmq7duO8oEXVzjSV+bxQP/qFaFVM1fJTCvQPNiw91FKqn7Pru5rnlx7SxSEUBW70K2l\nkB2sMLbplkLtPsgZE9lETn8GfcnR9KKO+3RRiPW8KblGQ71AS0dLIdp9BODOaH+GH5w/iTSXg3nj\n8pSP/1uboGh2x3OjxSW9CMIBtX3RPR3vES9CgCeHE/Mkv7h0GhPyU8lJSWLHALQU4o0paKuPt1OO\nWmOh/5CSS/XICynbmWOJwmCjr1JSwahk7oxEWAoAM66Bz16AIVNi79dEITdYabwamt1HQ2dFHq/H\nCTqzFMCIK8R63pRcY6WzdveR2VKIIQquTOAQJKcxbVgGm356LkLE6G7sSFauKxk2WlzonPh5KJwB\nRScZbS2OlJRcsmnixlNUoH1SYdrgtRSklGlSynTT1wQp5SuJHlyvMnwum079g4opWKIwuOirmIJ+\nzy5jCkMjj+0tskfDHZ8ZC+xEo1U2F0itSaQ9SbmPQgGVfZQdtepbu6UQJQqmQrUuLQXdfeRrVu0r\nUnKjLIWUjufoVc2atRNTENQOw1qItoycbtUJ9WgFAbS4iNE+fFJBOjsrmgZcP6S4REEIcbkQIsP0\ne6YQ4rLEDSsxeP0q+NZelGIxOOjLmMKw2VA4q/P9ZkvhWFoyWl3BUKGCv0F3nnqDrzuoVlaLXgpU\nn+j1SVevtzBbBXqtQkz3UZ7qeqpXKKcXdW8p/H97dx8sV13fcfz92d1799483ASSK8RAIIEo0hEt\njalTwYdBLeBDQG1FqQ9VJ0NHqIw6ilKVqTOdoZ3aTh1qTCsjbWmxPqAZjUVlHBxGqQk0ICCRGGVI\nhCRCBELuzX369o9zdu+5l937lD279+5+XjN3snv27N7vPbs53/3+fr/z+1Ver9aw2sm6MqOj8rJo\nxYSV3Jb1djE4PMboWHslhRk1HwGfiYhbK3ci4neSPgN8M5+w8jEwnFw92VNyUugo/WclJ9/FLZgd\n95Ibpn68py85kY0OT2zDz1tXL8dUZnWaFMYWrUza+msNR033B8aTQqVSyCaFakdznT4FgMfvS/5d\ntjpTKWSG7WZVmqPq9YtMiK9SKeSdFMYrhVIxeb9GxoJ2OqXMtKO51n4zTSjzxmAlKXhIamdZez58\n5KGZnVxaYemqljRtPVvoY7GOAekaI0OZNQUqC9pXVE7g1eajGgsJVU7itU7wlaRQuVCvL5MUuhYl\no5cm651DUpjc0dxIi1Ym03mnE/t1FcZXr2snMz077pT0OUlnpD+fA+7OM7A8VJKC+xRsXulb1ZKm\nraPF5GQ7FiIWr0z6FJ75TdKMtXjS9PKlSUNSKxewzbRSSNc24De7ACWVRrWfokbTUeV3wMySQuU1\nunNM/ItWAFFdKa9SKbRb89FMk8JVJAupfgW4BRgEPjjdkyRdKGm3pD2Srpliv5dJGpH0thnGMycD\nQ2ml4KRg88mKM8dPmk10tJScxI9SRt1LkkrhmQPJmgyTm7K6JvUpVC5gm7Dk6FSVQtp09/h9yUSA\nxa6JlUIt8675KF0VLp02o1SsVArtlRRmOvfRs0Ddk3ot6ToMNwCvA/YBOyRti4gHa+x3PfC92bz+\nXAyOjFIqaOIVj2at9trrJi5w0yQDxaQD91l6WFZenPQpPPPY+PTbWaUaHbnPexGMZZpOqkNSp+hT\nGDoyPiKqWinUafKZjx3NUO1X6CpU+hQ6sPlI0vclLc/cP0HSbdM8bSOwJyL2RsQQSYWxqcZ+VwFf\nBw7OMOY5Gxgac9ORzT89yyYOTW2SY11pUogeCuVFySR4Tz06PqdQVnaai4pLtsBbto7fn6pPofcE\nIK0+qv0R01QKJ56RPGf5mtqPZzWjT6GS2NKkUKkURtqsUpjpV+aVEVFd7ygiDjP9Fc2rgUcz9/el\n26okrQYuBb4w1QtJ2ixpp6Sdhw4dmmrXKQ0Mj1J2UjAD4FhXpfmoh2Kl2eXwI+NLembV+iZeXjKx\nuWaqPoVCcfybdmU463SVwqpz4GN7619rMSG+OtcpNNLkSiHtU+jUjuYxSdV0Lel0asyaOgf/CHw8\nIqY8qhGxNSI2RMSG/v65DyscHB71ZHhmqUpSeJYeCpUTc4zWbj6qJIPe5c99rGLpyYDq949UvmlX\nKoVCKbkSeapv95V2/Ol0L0qmM6msz5yH9IK/yrUKxcL4kNR2MtNhpdcCd0q6g6QGPB/YPM1z9gOn\nZu6fkm7L2gDckl6puBK4WNJIRORy/cPg8Kibj8xSw93JCX6AXpT9xp+9oK5i/evh0q31p80AWH4q\n/MWPk+tCalncD4ceGp+UrjKLbL3mo9l40ZtqT03eSF09SXI8+iQApTYdkjrTjub/kbSBJBH8H8lF\nawPTPG0HsF7SWpJkcBnwzkmvu7ZyW9KXgW/nlRAgaT7yyCOzRDUpqHfiiblm81EPvOTt07/oSVMk\njUrzy4TJ9Mr1h6TOxrpXJz95W7QimXX1Ox/hxOUXAe3XpzDTCfE+AHyI5Nv+LuDlwE9IluesKSJG\nJF0J3AYUgRsj4gFJV6SPbznO2GdtYMhJwaxipJwkhWOF3okn5lrNR41QGZaanb76gk/DSS/O5/fl\nYdEK2L0dho/S/+IlwMaObT76EPAy4K6IeI2ks4C/me5JEbEd2D5pW81kEBHvnWEsczY4Msby3hzb\nHM0WkNF0vYJB9UyckK5W81Ej9L8wWZN6SSbpbFg4izcCSVJI150oxRAAI23WfDTTXtfBiBgEkFSO\niIeAGQwJmF8Gh0bp6XJHsxnAWE9yxfBgYdF4pVAojXeoNtqG9ycztzZixtJWqTSBAaUYBjq3o3lf\nep3CN4HvSzoMPJJfWPkYcEezWdVY74k8GUs4WDp5vE9hyUm15yFqhEKhMf0HrdT/gmT01MDvKKZJ\nod06mme6nsKlEfG7iLgO+BTwJWDBTZ2dDEl1UjADKHaVecWxf+KH5deODwutdeGajTvvw3DV3dC9\nmOJYpfmoMyuFqoi4I49AmmFgeJRyO81xa3YcyqUCA/TQ3VXKVApOClOSqkuJVpNCJ05z0S5cKZiN\nq8wB1lUsjCeFvEYetZtidzUptNuEeB2TFEZGxxgeDfcpmKUqSaG7VEja+899N5z1hhZHtUCUyhTa\ntFJYcAvlzNXgSPLGefSRWaK7lKkUAN78+RZGs8CUyhRG27NPoWPOkJW1FFwpmCUqE7p5Kvk5KGYr\nBSeFBam6FKeTghkA3WkyKJc65jTQOKXu8aTQiUNS24GTgtlE481HmmZPe45iGY26o3lBG/D6zGYT\nTOhottkpldHoMaD9Opo75tNQ7VPwkFQzoEZHs81cyZXCgufRR2YTdbtSmLtiplJwUliYKpWC+xTM\nEtXmI1cKs1fqRiNDSG4+WrCWL+rivDNXsnxRd6tDMZsX3Hx0HIplGD1GV6HQdkNSO+bitZevW8HL\n162YfkezDlEZdeTmozkodcPIEKWiPCTVzNpDb1eRk/t6OG3FAp/OuhWKZRgZpFhov47mjqkUzGyi\nUrHAXZ+8oNVhLEylHiDoLYT7FGZD0oWSdkvaI+maGo9vknSfpF2Sdko6L894zMwaopT0TfYWRtpu\n9FFulYKkInAD8DpgH7BD0raIeDCz2+3AtogISecA/w2clVdMZmYNUSwDsKg42nbNR3lWChuBPRGx\nNyKGgFuATdkdIuJIRFSO6GKgvY6umbWnaqUw6uajWVgNPJq5vy/dNoGkSyU9BHwHeF+O8ZiZNUZa\nKfQWRtpuSGrLRx9FxK0RcRbJms+frbWPpM1pn8POQ4cONTdAM7PJSpmk4CGpM7YfODVz/5R0W00R\n8SNgnaSVNR7bGhEbImJDf39/4yM1M5uNCUnBlcJM7QDWS1orqRu4DNiW3UHSmZKU3j4XKANP5BiT\nmdnxS5uPejTCcJs1H+U2+igiRiRdCdwGFIEbI+IBSVekj28B3gq8W9IwMAC8PdPxbGY2P6UdzT1q\nv+ajXC9ei4jtwPZJ27Zkbl8PXJ9nDGZmDZepFNx8ZGbW6SqVQmGEYQ9JNTPrcKUeAMq4UjAzs2rz\n0bCvUzAz63hp81G5DTuanRTMzGYrrRS68RXNZmZWrRSGGXalYGbW4dKO5m6G3dFsZtbxJjQfuVIw\nM+tshQIUSnQz5PUUzMwMKJbpwqOPzMwMoNSd9Cl49JGZmVEsUwonBTMzAyiV6YphRseCdprc2UnB\nzGwuSmW6GAZoq85mJwUzs7lIm4+AthqW6qRgZjYXpW5KY0OAKwUzMyuWKUWSFNppWKqTgpnZXJS6\nM81HrhTMzDpbqYdi2nzkpDBDki6UtFvSHknX1Hj8ckn3SfqZpB9Lekme8ZiZNUyxm2KlUnDz0fQk\nFYEbgIuAs4F3SDp70m6/Al4VES8GPgtszSseM7OGKpWrlYI7mmdmI7AnIvZGxBBwC7Apu0NE/Dgi\nDqd37wJOyTEeM7PGKZYzzUeuFGZiNfBo5v6+dFs97we+m2M8ZmaNU+oeTwptVCmUWh0AgKTXkCSF\n8+o8vhnYDLBmzZomRmZmVkeph0K1+ciVwkzsB07N3D8l3TaBpHOAfwU2RcQTtV4oIrZGxIaI2NDf\n359LsGZms1LspjDq0UezsQNYL2mtpG7gMmBbdgdJa4BvAO+KiF/kGIuZWWOllYIYa6tKIbfmo4gY\nkXQlcBtQBG6MiAckXZE+vgX4NLAC+GdJACMRsSGvmMzMGqa8BIDFDDLaRpVCrn0KEbEd2D5p25bM\n7Q8AH8gzBjOzXPQsA2ApA23V0ewrms3M5qLcB0Cfnm2r5iMnBTOzuahWCkfd0Wxm1vF6KpXCUVcK\nZmYdr5ypFNynYGbW4dLmoz4d9TQXZmYdL20+WspRT4hnZtbxSmWi1EOfjrbVdQpOCmZmcxTlPvpw\nR7OZmQGU+9I+BVcKZmbWsywdfeRKwcys46l3WXqdgisFM7OOp3IfSz0k1czMAOhZxjId5fDR4VZH\n0jBOCmZmc9XTRx8D7Dl4pNWRNIyTgpnZXJWXUeYYjxw43OpIGsZJwcxsrtKpLoaPPsUTR461OJjG\ncFIwM5urnvE1FdqlCclJwcxsrjKrrz3spGBm1uHS1dee19U+nc25JgVJF0raLWmPpGtqPH6WpJ9I\nOibpo3nGYmbWcGmlsH558PDBZ1ocTGOU8nphSUXgBuB1wD5gh6RtEfFgZrcngb8ELskrDjOz3KR9\nCuuWjPC1x49w//6nkGBsDJ4eHOapgeTn6YFhhkbGKBREX0+JsYDDR4dYu3Ixq5b1Mjw6xrLeLiQ4\n8PQgjz91jJGxMc7oX8LoWPD04DBjAWf2L+Hs5/fl+ifllhSAjcCeiNgLIOkWYBNQTQoRcRA4KOkN\nOcZhZpaPtFI4bfEovz1yjDd+/s5cf90VrzpjQSeF1cCjmfv7gD+cywtJ2gxsBlizZs3xR2Zm1gjd\nSwGxcf9NPHDSt4jMFEgFiWJBFATFghAiCCoTqhYEQ6PB6FggYCyCCCgVRakgQAyPjiElrwUw0ns5\ncFauf1KeSaFhImIrsBVgw4YN7TPzlJktbIUCvOaT6MD9LJ7B7mJiR27PNPt3TbpfPvH5swpvLvJM\nCvuBUzP3T0m3mZm1j1d9rNURNFSeo492AOslrZXUDVwGbMvx95mZ2XHKrVKIiBFJVwK3AUXgxoh4\nQNIV6eNbJJ0M7AT6gDFJVwNnR8TTecVlZmb15dqnEBHbge2Ttm3J3H6cpFnJzMzmAV/RbGZmVU4K\nZmZW5aRgZmZVTgpmZlblpGBmZlWKWFgXCEs6BDwyx6evBH7bwHAaab7G5rhmZ77GBfM3Nsc1O3ON\n67SI6J9upwWXFI6HpJ0RsaHVcdQyX2NzXLMzX+OC+Rub45qdvONy85GZmVU5KZiZWVWnJYWtrQ5g\nCvM1Nsc1O/M1Lpi/sTmu2ck1ro7qUzAzs6l1WqVgZmZTcFIwM7OqjkkKki6UtFvSHknXtDCOUyX9\nUNKDkh6Q9KF0+3WS9kvalf5c3ILYfi3pZ+nv35luO1HS9yU9nP57QgviemHmuOyS9LSkq1txzCTd\nKOmgpPsz2+oeI0mfSD9zuyX9cZPj+jtJD0m6T9Ktkpan20+XNJA5blvqv3IucdV935p1vKaI7SuZ\nuH4taVe6vSnHbIrzQ/M+YxHR9j8k6zn8ElgHdAP3kqzb0IpYVgHnpreXAr8AzgauAz7a4uP0a2Dl\npG1/C1yT3r4GuH4evJePA6e14pgBrwTOBe6f7hil7+u9QBlYm34Gi02M6/VAKb19fSau07P7teB4\n1Xzfmnm86sU26fG/Bz7dzGM2xfmhaZ+xTqkUNgJ7ImJvRAwBtwCbWhFIRDwWEfekt58Bfg6sbkUs\nM7QJuCm9fRNwSQtjAbgA+GVEzPWq9uMSET8Cnpy0ud4x2gTcEhHHIuJXwB6Sz2JT4oqI70XESHr3\nLlqwdkmd41VP047XdLFJEvCnwH/l9fvrxFTv/NC0z1inJIXVwKOZ+/uYBydiSacDvw/8b7rpqrTU\nv7EVzTRAAD+QdLekzem2kyLisfT248BJLYgr6zIm/kdt9TGD+sdoPn3u3gd8N3N/bdoMcoek81sQ\nT633bT4dr/OBAxHxcGZbU4/ZpPND0z5jnZIU5h1JS4CvA1dHsvzoF0iat14KPEZSujbbeRHxUuAi\n4IOSXpl9MJJ6tWVjmJWs9f1m4KvppvlwzCZo9TGqRdK1wAhwc7rpMWBN+l5/GPhPSX1NDGnevW81\nvIOJXz6aesxqnB+q8v6MdUpS2A+cmrl/SrqtJSR1kbzhN0fENwAi4kBEjEbEGPAv5Fg21xMR+9N/\nDwK3pjEckLQqjXsVcLDZcWVcBNwTEQdgfhyzVL1j1PLPnaT3Am8ELk9PJqRNDU+kt+8maYd+QbNi\nmuJ9a/nxApBUAt4CfKWyrZnHrNb5gSZ+xjolKewA1ktam37bvAzY1opA0rbKLwE/j4jPZbavyux2\nKXD/5OfmHNdiSUsrt0k6Ke8nOU7vSXd7D/CtZsY1yYRvb60+Zhn1jtE24DJJZUlrgfXAT5sVlKQL\ngY8Bb46Io5nt/ZKK6e11aVx7mxhXvfetpccr47XAQxGxr7KhWces3vmBZn7G8u5Nny8/wMUkPfm/\nBK5tYRznkZR+9wG70p+LgX8HfpZu3wasanJc60hGMdwLPFA5RsAK4HbgYeAHwIktOm6LgSeAZZlt\nTT9mJEnpMWCYpP32/VMdI+Da9DO3G7ioyXHtIWlvrnzOtqT7vjV9j3cB9wBvanJcdd+3Zh2verGl\n278MXDFp36YcsynOD037jHmaCzMzq+qU5iMzM5sBJwUzM6tyUjAzsyonBTMzq3JSMDOzKicFsyaS\n9GpJ3251HGb1OCmYmVmVk4JZDZL+TNJP0wnQviipKOmIpH9I57m/XVJ/uu9LJd2l8XULTki3nynp\nB5LulXSPpDPSl18i6WtK1jq4Ob2K1WxecFIwm0TSi4C3A6+IZAK0UeBykquqd0bE7wF3AJ9Jn/Jv\nwMcj4hySK3Ur228GboiIlwB/RHL1LCQzX15NMhf+OuAVuf9RZjNUanUAZvPQBcAfADvSL/G9JBOQ\njTE+Sdp/AN+QtAxYHhF3pNtvAr6aziO1OiJuBYiIQYD09X4a6bw66cpepwN35v9nmU3PScHsuQTc\nFBGfmLBR+tSk/eY6R8yxzO1R/P/Q5hE3H5k91+3A2yQ9D6rr455G8v/lbek+7wTujIingMOZRVfe\nBdwRyapZ+yRdkr5GWdKipv4VZnPgbyhmk0TEg5L+CviepALJLJofBJ4FNqaPHSTpd4BkKuMt6Ul/\nL/Dn6fZ3AV+U9Nfpa/xJE/8MsznxLKlmMyTpSEQsaXUcZnly85GZmVW5UjAzsypXCmZmVuWkYGZm\nVU4KZmZW5aRgZmZVTgpmZlb1/32MSsongxD/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fa75278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fbdd7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
