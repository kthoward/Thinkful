{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_1 = unpickle(\"./cifar-10-batches-py/data_batch_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2 = unpickle(\"./cifar-10-batches-py/data_batch_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_3 = unpickle(\"./cifar-10-batches-py/data_batch_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_4 = unpickle(\"./cifar-10-batches-py/data_batch_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_5 = unpickle(\"./cifar-10-batches-py/data_batch_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'batch_label': b'training batch 1 of 5',\n",
       " b'data': array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "        [154, 126, 105, ..., 139, 142, 144],\n",
       "        [255, 253, 253, ...,  83,  83,  84],\n",
       "        ..., \n",
       "        [ 71,  60,  74, ...,  68,  69,  68],\n",
       "        [250, 254, 211, ..., 215, 255, 254],\n",
       "        [ 62,  61,  60, ..., 130, 130, 131]], dtype=uint8),\n",
       " b'filenames': [b'leptodactylus_pentadactylus_s_000004.png',\n",
       "  b'camion_s_000148.png',\n",
       "  b'tipper_truck_s_001250.png',\n",
       "  b'american_elk_s_001521.png',\n",
       "  b'station_wagon_s_000293.png',\n",
       "  b'coupe_s_001735.png',\n",
       "  b'cassowary_s_001300.png',\n",
       "  b'cow_pony_s_001168.png',\n",
       "  b'sea_boat_s_001584.png',\n",
       "  b'tabby_s_001355.png',\n",
       "  b'muntjac_s_001000.png',\n",
       "  b'arabian_s_001354.png',\n",
       "  b'quarter_horse_s_000672.png',\n",
       "  b'passerine_s_000343.png',\n",
       "  b'camion_s_001895.png',\n",
       "  b'trailer_truck_s_000335.png',\n",
       "  b'dumper_s_000821.png',\n",
       "  b'alley_cat_s_000200.png',\n",
       "  b'accentor_s_000677.png',\n",
       "  b'frog_s_001671.png',\n",
       "  b'capreolus_capreolus_s_000051.png',\n",
       "  b'tomcat_s_000772.png',\n",
       "  b'pickerel_frog_s_000446.png',\n",
       "  b'bufo_s_001242.png',\n",
       "  b'cassowary_s_001246.png',\n",
       "  b'toad_s_001748.png',\n",
       "  b'cat_s_000081.png',\n",
       "  b'chihuahua_s_000825.png',\n",
       "  b'alces_alces_s_000959.png',\n",
       "  b'stealth_bomber_s_000554.png',\n",
       "  b'twinjet_s_000663.png',\n",
       "  b'trucking_rig_s_001402.png',\n",
       "  b'auto_s_000609.png',\n",
       "  b'tabby_cat_s_000983.png',\n",
       "  b'wapiti_s_000416.png',\n",
       "  b'monoplane_s_000895.png',\n",
       "  b'true_cat_s_000247.png',\n",
       "  b'tennessee_walker_s_000486.png',\n",
       "  b'house_cat_s_000243.png',\n",
       "  b'house_cat_s_001196.png',\n",
       "  b'pekinese_s_001337.png',\n",
       "  b'ostrich_s_001368.png',\n",
       "  b'ostrich_s_001150.png',\n",
       "  b'stallion_s_000046.png',\n",
       "  b'station_waggon_s_000041.png',\n",
       "  b'coupe_s_001944.png',\n",
       "  b'estate_car_s_000580.png',\n",
       "  b'accentor_s_000759.png',\n",
       "  b'emu_novaehollandiae_s_000795.png',\n",
       "  b'dive_bomber_s_001390.png',\n",
       "  b'articulated_lorry_s_000131.png',\n",
       "  b'pekinese_s_001093.png',\n",
       "  b'broodmare_s_001463.png',\n",
       "  b'delivery_truck_s_000834.png',\n",
       "  b'songbird_s_001052.png',\n",
       "  b'emu_s_000692.png',\n",
       "  b'puppy_s_000115.png',\n",
       "  b'wagtail_s_001821.png',\n",
       "  b'dama_dama_s_000658.png',\n",
       "  b'domestic_cat_s_001970.png',\n",
       "  b'ambulance_s_003039.png',\n",
       "  b'convertible_s_001763.png',\n",
       "  b'tank_ship_s_001229.png',\n",
       "  b'cassowary_s_001055.png',\n",
       "  b'wagon_s_001142.png',\n",
       "  b'police_cruiser_s_000620.png',\n",
       "  b'moose_s_002308.png',\n",
       "  b'aerial_ladder_truck_s_000584.png',\n",
       "  b'saddle_horse_s_000717.png',\n",
       "  b'tanker_s_001350.png',\n",
       "  b'mongrel_s_001571.png',\n",
       "  b'truck_s_000835.png',\n",
       "  b'pickerel_frog_s_001195.png',\n",
       "  b'lipizzan_s_000399.png',\n",
       "  b'tabby_s_000074.png',\n",
       "  b'automobile_s_001887.png',\n",
       "  b'moving_van_s_001665.png',\n",
       "  b'attack_aircraft_s_000153.png',\n",
       "  b'domestic_cat_s_001596.png',\n",
       "  b'compact_car_s_000048.png',\n",
       "  b'domestic_cat_s_000009.png',\n",
       "  b'pekingese_s_002089.png',\n",
       "  b'capreolus_capreolus_s_001095.png',\n",
       "  b'blenheim_spaniel_s_001103.png',\n",
       "  b'stallion_s_000040.png',\n",
       "  b'stud_mare_s_001672.png',\n",
       "  b'elk_s_000920.png',\n",
       "  b'lipizzan_s_001123.png',\n",
       "  b'fire_truck_s_002721.png',\n",
       "  b'elk_s_001888.png',\n",
       "  b'finch_s_000750.png',\n",
       "  b'tabby_s_000880.png',\n",
       "  b'banana_boat_s_001324.png',\n",
       "  b'twinjet_s_000591.png',\n",
       "  b'shooting_brake_s_000029.png',\n",
       "  b'rana_pipiens_s_000101.png',\n",
       "  b'station_wagon_s_001387.png',\n",
       "  b'station_wagon_s_002712.png',\n",
       "  b'odocoileus_hemionus_s_000221.png',\n",
       "  b'convertible_s_001715.png',\n",
       "  b'abandoned_ship_s_000574.png',\n",
       "  b'true_cat_s_000114.png',\n",
       "  b'dustcart_s_000063.png',\n",
       "  b'frog_s_000797.png',\n",
       "  b'green_frog_s_001320.png',\n",
       "  b'compact_car_s_001038.png',\n",
       "  b'freighter_s_001351.png',\n",
       "  b'mutt_s_000251.png',\n",
       "  b'accentor_s_000303.png',\n",
       "  b'lorry_s_001154.png',\n",
       "  b'fire_truck_s_000894.png',\n",
       "  b'cargo_vessel_s_000731.png',\n",
       "  b'cruiser_s_000774.png',\n",
       "  b'lippizan_s_000359.png',\n",
       "  b'broodmare_s_001741.png',\n",
       "  b'fighter_aircraft_s_000876.png',\n",
       "  b'amphibious_aircraft_s_000216.png',\n",
       "  b'leopard_frog_s_000339.png',\n",
       "  b'trucking_rig_s_001315.png',\n",
       "  b'shooting_brake_s_000886.png',\n",
       "  b'pipit_s_000549.png',\n",
       "  b'ostrich_s_002148.png',\n",
       "  b'trucking_rig_s_001600.png',\n",
       "  b'alauda_arvensis_s_000755.png',\n",
       "  b'rana_temporaria_s_001087.png',\n",
       "  b'bufo_s_000136.png',\n",
       "  b'estate_car_s_000529.png',\n",
       "  b'aerial_ladder_truck_s_000997.png',\n",
       "  b'toy_spaniel_s_000384.png',\n",
       "  b'amphibious_aircraft_s_001195.png',\n",
       "  b'fallow_deer_s_001598.png',\n",
       "  b'quarter_horse_s_000414.png',\n",
       "  b'anuran_s_000712.png',\n",
       "  b'arabian_s_001366.png',\n",
       "  b'auto_s_000040.png',\n",
       "  b'cargo_ship_s_001063.png',\n",
       "  b'motorcar_s_000121.png',\n",
       "  b'motorcar_s_000305.png',\n",
       "  b'anthus_pratensis_s_001071.png',\n",
       "  b'cruiser_s_000294.png',\n",
       "  b'wagon_s_000763.png',\n",
       "  b'alley_cat_s_002579.png',\n",
       "  b'tabby_cat_s_000825.png',\n",
       "  b'spadefoot_s_000051.png',\n",
       "  b'wagtail_s_002075.png',\n",
       "  b'fallow_deer_s_001697.png',\n",
       "  b'garbage_truck_s_000777.png',\n",
       "  b'moving_van_s_000067.png',\n",
       "  b'mongrel_s_000696.png',\n",
       "  b'red_deer_s_000741.png',\n",
       "  b'true_cat_s_001768.png',\n",
       "  b'spadefoot_s_000377.png',\n",
       "  b'lipizzan_s_001233.png',\n",
       "  b'japanese_deer_s_000163.png',\n",
       "  b'toad_s_002436.png',\n",
       "  b'police_boat_s_001421.png',\n",
       "  b'puppy_s_002062.png',\n",
       "  b'lapdog_s_001869.png',\n",
       "  b'elk_s_001196.png',\n",
       "  b'domestic_cat_s_001235.png',\n",
       "  b'car_s_000057.png',\n",
       "  b'lightship_s_000064.png',\n",
       "  b'fallow_deer_s_001997.png',\n",
       "  b'quarter_horse_s_000022.png',\n",
       "  b'bufo_calamita_s_000566.png',\n",
       "  b'biplane_s_000624.png',\n",
       "  b'fire_truck_s_001616.png',\n",
       "  b'japanese_spaniel_s_000037.png',\n",
       "  b'ambulance_s_001394.png',\n",
       "  b'domestic_cat_s_001056.png',\n",
       "  b'tugboat_s_001384.png',\n",
       "  b'emu_s_000072.png',\n",
       "  b'gelding_s_000206.png',\n",
       "  b'puppy_s_000637.png',\n",
       "  b'tabby_cat_s_002607.png',\n",
       "  b'elk_s_001198.png',\n",
       "  b'estate_car_s_001496.png',\n",
       "  b'peke_s_001338.png',\n",
       "  b'arabian_s_001018.png',\n",
       "  b'fighter_aircraft_s_000538.png',\n",
       "  b'fallow_deer_s_001164.png',\n",
       "  b'lippizan_s_000390.png',\n",
       "  b'toy_dog_s_000932.png',\n",
       "  b'maltese_s_002151.png',\n",
       "  b'car_s_000002.png',\n",
       "  b'jumbo_jet_s_000697.png',\n",
       "  b'dump_truck_s_000008.png',\n",
       "  b'spadefoot_s_000084.png',\n",
       "  b'motortruck_s_000014.png',\n",
       "  b'jetliner_s_001502.png',\n",
       "  b'tanker_s_000157.png',\n",
       "  b'riding_horse_s_001870.png',\n",
       "  b'cabin_cruiser_s_000096.png',\n",
       "  b'dredger_s_001623.png',\n",
       "  b'pipit_s_001102.png',\n",
       "  b'mongrel_s_002430.png',\n",
       "  b'skylark_s_000150.png',\n",
       "  b'cat_s_001604.png',\n",
       "  b'puppy_s_000211.png',\n",
       "  b'airbus_s_001124.png',\n",
       "  b'leopard_frog_s_001021.png',\n",
       "  b'wagon_s_001824.png',\n",
       "  b'garbage_truck_s_000147.png',\n",
       "  b'tabby_s_001354.png',\n",
       "  b'rana_temporaria_s_000295.png',\n",
       "  b'lorry_s_000364.png',\n",
       "  b'estate_car_s_000075.png',\n",
       "  b'tabby_s_000970.png',\n",
       "  b'fire_engine_s_000985.png',\n",
       "  b'toad_s_000022.png',\n",
       "  b'texas_toad_s_000215.png',\n",
       "  b'broodmare_s_000707.png',\n",
       "  b'car_s_000286.png',\n",
       "  b'twinjet_s_000565.png',\n",
       "  b'truck_s_001385.png',\n",
       "  b'toy_dog_s_000752.png',\n",
       "  b'police_boat_s_000092.png',\n",
       "  b'toy_dog_s_001197.png',\n",
       "  b'cassowary_s_002446.png',\n",
       "  b'articulated_lorry_s_000050.png',\n",
       "  b'monoplane_s_000781.png',\n",
       "  b'lightship_s_000343.png',\n",
       "  b'powerboat_s_001486.png',\n",
       "  b'jumbo_jet_s_000071.png',\n",
       "  b'bufo_debilis_s_000024.png',\n",
       "  b'transporter_s_000272.png',\n",
       "  b'estate_car_s_000743.png',\n",
       "  b'motorcar_s_000697.png',\n",
       "  b'rana_catesbeiana_s_001655.png',\n",
       "  b'tabby_s_000675.png',\n",
       "  b'remount_s_000285.png',\n",
       "  b'rana_catesbeiana_s_001521.png',\n",
       "  b'true_frog_s_000223.png',\n",
       "  b'airliner_s_002304.png',\n",
       "  b'natterjack_s_000987.png',\n",
       "  b'leptodactylid_s_000082.png',\n",
       "  b'coupe_s_001892.png',\n",
       "  b'quarter_horse_s_000419.png',\n",
       "  b'car_s_001323.png',\n",
       "  b'maltese_dog_s_001043.png',\n",
       "  b'boat_s_000543.png',\n",
       "  b'tabby_cat_s_001537.png',\n",
       "  b'rana_catesbeiana_s_001096.png',\n",
       "  b'bufo_bufo_s_000326.png',\n",
       "  b'pilot_boat_s_000646.png',\n",
       "  b'toad_frog_s_001672.png',\n",
       "  b'boat_s_000988.png',\n",
       "  b'deer_s_001383.png',\n",
       "  b'bufo_bufo_s_000261.png',\n",
       "  b'leopard_frog_s_000763.png',\n",
       "  b'coupe_s_002178.png',\n",
       "  b'cat_s_000663.png',\n",
       "  b'ship_s_000175.png',\n",
       "  b'house_cat_s_001471.png',\n",
       "  b'stag_s_001458.png',\n",
       "  b'station_wagon_s_001337.png',\n",
       "  b'quarter_horse_s_002201.png',\n",
       "  b'coupe_s_000698.png',\n",
       "  b'tomcat_s_002553.png',\n",
       "  b'scow_s_001266.png',\n",
       "  b'chihuahua_s_000385.png',\n",
       "  b'convertible_s_001500.png',\n",
       "  b'convertible_s_000408.png',\n",
       "  b'odocoileus_hemionus_s_000512.png',\n",
       "  b'jumbo_jet_s_000971.png',\n",
       "  b'truck_s_000650.png',\n",
       "  b'domestic_cat_s_001288.png',\n",
       "  b'walking_horse_s_000194.png',\n",
       "  b'elk_s_001657.png',\n",
       "  b'tipper_truck_s_000287.png',\n",
       "  b'dumper_s_000648.png',\n",
       "  b'struthio_camelus_s_000225.png',\n",
       "  b'woodland_caribou_s_000803.png',\n",
       "  b'ladder_truck_s_000795.png',\n",
       "  b'pantechnicon_s_000230.png',\n",
       "  b'shooting_brake_s_000229.png',\n",
       "  b'jetliner_s_001349.png',\n",
       "  b'toy_dog_s_001280.png',\n",
       "  b'dump_truck_s_002104.png',\n",
       "  b'twinjet_s_001360.png',\n",
       "  b'tugboat_s_000715.png',\n",
       "  b'passerine_s_000360.png',\n",
       "  b'automobile_s_002337.png',\n",
       "  b'honey_eater_s_001422.png',\n",
       "  b'attack_aircraft_s_001304.png',\n",
       "  b'toy_dog_s_000114.png',\n",
       "  b'anuran_s_000550.png',\n",
       "  b'mouser_s_000382.png',\n",
       "  b'sparrow_s_000037.png',\n",
       "  b'stud_mare_s_001038.png',\n",
       "  b'speedboat_s_002588.png',\n",
       "  b'passenger_ship_s_002153.png',\n",
       "  b'bufo_bufo_s_001876.png',\n",
       "  b'biplane_s_000651.png',\n",
       "  b'gelding_s_001785.png',\n",
       "  b'trucking_rig_s_001598.png',\n",
       "  b'deer_s_000309.png',\n",
       "  b'pekingese_s_001946.png',\n",
       "  b'frog_s_002802.png',\n",
       "  b'cervus_unicolor_s_000290.png',\n",
       "  b'honey_eater_s_000478.png',\n",
       "  b'automobile_s_001152.png',\n",
       "  b'coupe_s_000846.png',\n",
       "  b'flying_bird_s_000252.png',\n",
       "  b'automobile_s_002343.png',\n",
       "  b'japanese_spaniel_s_001162.png',\n",
       "  b'fire_truck_s_000043.png',\n",
       "  b'semi_s_001207.png',\n",
       "  b'fighter_aircraft_s_001840.png',\n",
       "  b'ship_s_001976.png',\n",
       "  b'caribou_s_001222.png',\n",
       "  b'police_cruiser_s_001347.png',\n",
       "  b'coupe_s_000042.png',\n",
       "  b'bufo_bufo_s_000995.png',\n",
       "  b'tabby_s_001148.png',\n",
       "  b'true_cat_s_000476.png',\n",
       "  b'garbage_truck_s_001442.png',\n",
       "  b'fighter_aircraft_s_000385.png',\n",
       "  b'quarter_horse_s_001602.png',\n",
       "  b'camion_s_000116.png',\n",
       "  b'tennessee_walker_s_000539.png',\n",
       "  b'lipizzan_s_000452.png',\n",
       "  b'trucking_rig_s_001594.png',\n",
       "  b'shooting_brake_s_001611.png',\n",
       "  b'puppy_s_002096.png',\n",
       "  b'auto_s_000855.png',\n",
       "  b'bufo_marinus_s_001364.png',\n",
       "  b'natterjack_s_000036.png',\n",
       "  b'pontoon_s_000643.png',\n",
       "  b'stallion_s_001256.png',\n",
       "  b'coupe_s_001863.png',\n",
       "  b'house_cat_s_000079.png',\n",
       "  b'jetliner_s_000981.png',\n",
       "  b'felis_catus_s_000853.png',\n",
       "  b'tabby_cat_s_000827.png',\n",
       "  b'struthio_camelus_s_001250.png',\n",
       "  b'fawn_s_001915.png',\n",
       "  b'canis_familiaris_s_001298.png',\n",
       "  b'lipizzan_s_002122.png',\n",
       "  b'toy_spaniel_s_001133.png',\n",
       "  b'dump_truck_s_000029.png',\n",
       "  b'fighter_s_001042.png',\n",
       "  b'domestic_cat_s_000493.png',\n",
       "  b'cervus_elaphus_s_001764.png',\n",
       "  b'airbus_s_000627.png',\n",
       "  b'fawn_s_001452.png',\n",
       "  b'fallow_deer_s_000959.png',\n",
       "  b'spadefoot_s_000777.png',\n",
       "  b'seaplane_s_001639.png',\n",
       "  b'plane_s_001066.png',\n",
       "  b'bufo_viridis_s_000348.png',\n",
       "  b'crapaud_s_000051.png',\n",
       "  b'seaplane_s_001447.png',\n",
       "  b'boat_s_000404.png',\n",
       "  b'convertible_s_001242.png',\n",
       "  b'toad_s_000201.png',\n",
       "  b'finch_s_000494.png',\n",
       "  b'fire_engine_s_001437.png',\n",
       "  b'pipit_s_000226.png',\n",
       "  b'toy_dog_s_000960.png',\n",
       "  b'delivery_truck_s_001121.png',\n",
       "  b'bufo_marinus_s_001506.png',\n",
       "  b'studhorse_s_000010.png',\n",
       "  b'woodland_caribou_s_001003.png',\n",
       "  b'station_wagon_s_000260.png',\n",
       "  b'pilot_boat_s_000356.png',\n",
       "  b'stud_mare_s_001497.png',\n",
       "  b'tabby_cat_s_001966.png',\n",
       "  b'american_toad_s_000006.png',\n",
       "  b'tipper_s_000010.png',\n",
       "  b'tomcat_s_002230.png',\n",
       "  b'dive_bomber_s_001120.png',\n",
       "  b'wapiti_s_000802.png',\n",
       "  b'biplane_s_000862.png',\n",
       "  b'blenheim_spaniel_s_000589.png',\n",
       "  b'shooting_brake_s_001161.png',\n",
       "  b'jetliner_s_000400.png',\n",
       "  b'tabby_s_000923.png',\n",
       "  b'alces_alces_s_000361.png',\n",
       "  b'police_boat_s_000002.png',\n",
       "  b'mutt_s_001696.png',\n",
       "  b'cervus_elaphus_s_001695.png',\n",
       "  b'quarter_horse_s_000752.png',\n",
       "  b'accentor_s_000449.png',\n",
       "  b'tabby_cat_s_000144.png',\n",
       "  b'fire_engine_s_001513.png',\n",
       "  b'tennessee_walking_horse_s_001187.png',\n",
       "  b'bufo_s_001247.png',\n",
       "  b'arabian_s_002147.png',\n",
       "  b'compact_s_001258.png',\n",
       "  b'elk_s_002339.png',\n",
       "  b'stud_mare_s_001384.png',\n",
       "  b'fighter_aircraft_s_000745.png',\n",
       "  b'police_cruiser_s_001047.png',\n",
       "  b'lipizzan_s_001764.png',\n",
       "  b'cat_s_000331.png',\n",
       "  b'taxi_s_000701.png',\n",
       "  b'boat_s_000432.png',\n",
       "  b'japanese_deer_s_000942.png',\n",
       "  b'sambar_s_000076.png',\n",
       "  b'ostrich_s_001559.png',\n",
       "  b'dive_bomber_s_000185.png',\n",
       "  b'lark_s_001532.png',\n",
       "  b'flightless_bird_s_000480.png',\n",
       "  b'stealth_fighter_s_001312.png',\n",
       "  b'jumbojet_s_000789.png',\n",
       "  b'delivery_truck_s_000510.png',\n",
       "  b'biplane_s_001901.png',\n",
       "  b'lorry_s_001185.png',\n",
       "  b'crapaud_s_000511.png',\n",
       "  b'boat_s_001280.png',\n",
       "  b'bird_s_001357.png',\n",
       "  b'stud_mare_s_001477.png',\n",
       "  b'arabian_s_000751.png',\n",
       "  b'rangifer_caribou_s_000586.png',\n",
       "  b'monoplane_s_000036.png',\n",
       "  b'house_cat_s_001321.png',\n",
       "  b'stealth_fighter_s_000240.png',\n",
       "  b'oil_tanker_s_000129.png',\n",
       "  b'tipper_truck_s_000416.png',\n",
       "  b'roe_deer_s_000358.png',\n",
       "  b'rhea_americana_s_000475.png',\n",
       "  b'lippizaner_s_000517.png',\n",
       "  b'cassowary_s_000297.png',\n",
       "  b'pekingese_s_001675.png',\n",
       "  b'accentor_s_001058.png',\n",
       "  b'lapdog_s_000227.png',\n",
       "  b'auto_s_001153.png',\n",
       "  b'fire_engine_s_000443.png',\n",
       "  b'muntjac_s_000815.png',\n",
       "  b'lightship_s_000090.png',\n",
       "  b'dog_s_002441.png',\n",
       "  b'auto_s_001951.png',\n",
       "  b'arabian_s_000328.png',\n",
       "  b'odocoileus_hemionus_s_000188.png',\n",
       "  b'barking_deer_s_000019.png',\n",
       "  b'dive_bomber_s_001486.png',\n",
       "  b'bufo_viridis_s_000503.png',\n",
       "  b'wrecker_s_002294.png',\n",
       "  b'monoplane_s_000206.png',\n",
       "  b'arabian_s_002282.png',\n",
       "  b'speedboat_s_002160.png',\n",
       "  b'cargo_vessel_s_002136.png',\n",
       "  b'garbage_truck_s_000726.png',\n",
       "  b'delivery_truck_s_001468.png',\n",
       "  b'house_cat_s_002240.png',\n",
       "  b'house_cat_s_000045.png',\n",
       "  b'caribou_s_002007.png',\n",
       "  b'propeller_plane_s_000451.png',\n",
       "  b'fallow_deer_s_001082.png',\n",
       "  b'dog_s_001897.png',\n",
       "  b'bufo_bufo_s_001631.png',\n",
       "  b'spadefoot_s_000191.png',\n",
       "  b'multiengine_airplane_s_000186.png',\n",
       "  b'auto_s_002359.png',\n",
       "  b'stealth_bomber_s_001607.png',\n",
       "  b'passenger_ship_s_000582.png',\n",
       "  b'airliner_s_001297.png',\n",
       "  b'fallow_deer_s_000973.png',\n",
       "  b'speedboat_s_000178.png',\n",
       "  b'boat_s_001444.png',\n",
       "  b'estate_car_s_000712.png',\n",
       "  b'english_toy_spaniel_s_000888.png',\n",
       "  b'passerine_s_000699.png',\n",
       "  b'rana_pipiens_s_000286.png',\n",
       "  b'tanker_s_000025.png',\n",
       "  b'car_s_001956.png',\n",
       "  b'stealth_bomber_s_000424.png',\n",
       "  b'airliner_s_001070.png',\n",
       "  b'stud_mare_s_001348.png',\n",
       "  b'quarter_horse_s_000282.png',\n",
       "  b'maltese_s_001818.png',\n",
       "  b'lorry_s_001786.png',\n",
       "  b'true_frog_s_000064.png',\n",
       "  b'cassowary_s_001372.png',\n",
       "  b'houseboat_s_000626.png',\n",
       "  b'domestic_cat_s_000035.png',\n",
       "  b'deer_s_001596.png',\n",
       "  b'cow_pony_s_000803.png',\n",
       "  b'tabby_cat_s_001466.png',\n",
       "  b'tipper_truck_s_001525.png',\n",
       "  b'stealth_fighter_s_000533.png',\n",
       "  b'station_wagon_s_000750.png',\n",
       "  b'wagtail_s_001549.png',\n",
       "  b'rangifer_caribou_s_000985.png',\n",
       "  b'cargo_ship_s_000408.png',\n",
       "  b'cab_s_000788.png',\n",
       "  b'flatboat_s_000044.png',\n",
       "  b'bufo_viridis_s_000457.png',\n",
       "  b'fallow_deer_s_000040.png',\n",
       "  b'barren_ground_caribou_s_000077.png',\n",
       "  b'pekinese_s_000453.png',\n",
       "  b'female_horse_s_000358.png',\n",
       "  b'compact_car_s_000869.png',\n",
       "  b'tabby_cat_s_002454.png',\n",
       "  b'delivery_truck_s_000864.png',\n",
       "  b'cargo_ship_s_000802.png',\n",
       "  b'attack_aircraft_s_000037.png',\n",
       "  b'convertible_s_000037.png',\n",
       "  b'broodmare_s_001448.png',\n",
       "  b'toy_spaniel_s_001286.png',\n",
       "  b'packet_boat_s_001206.png',\n",
       "  b'ostrich_s_001564.png',\n",
       "  b'hospital_ship_s_001658.png',\n",
       "  b'stealth_bomber_s_001035.png',\n",
       "  b'stag_s_002583.png',\n",
       "  b'police_cruiser_s_000629.png',\n",
       "  b'merchant_ship_s_001242.png',\n",
       "  b'dump_truck_s_001584.png',\n",
       "  b'cargo_ship_s_001830.png',\n",
       "  b'nandu_s_000721.png',\n",
       "  b'garbage_truck_s_001119.png',\n",
       "  b'lorry_s_002317.png',\n",
       "  b'wagtail_s_001445.png',\n",
       "  b'lipizzan_s_002049.png',\n",
       "  b'maltese_s_000948.png',\n",
       "  b'quarter_horse_s_000798.png',\n",
       "  b'house_cat_s_001472.png',\n",
       "  b'tank_ship_s_001155.png',\n",
       "  b'hospital_ship_s_001838.png',\n",
       "  b'mule_deer_s_000581.png',\n",
       "  b'deer_s_001329.png',\n",
       "  b'rhea_americana_s_000268.png',\n",
       "  b'lippizan_s_000440.png',\n",
       "  b'convertible_s_002117.png',\n",
       "  b'bufo_marinus_s_001567.png',\n",
       "  b'roe_deer_s_000690.png',\n",
       "  b'twinjet_s_000481.png',\n",
       "  b'american_elk_s_001743.png',\n",
       "  b'bufo_viridis_s_000254.png',\n",
       "  b'dumper_s_000179.png',\n",
       "  b'lippizan_s_000323.png',\n",
       "  b'leopard_frog_s_000058.png',\n",
       "  b'prunella_modularis_s_000578.png',\n",
       "  b'maltese_s_001867.png',\n",
       "  b'toy_dog_s_001358.png',\n",
       "  b'police_cruiser_s_000500.png',\n",
       "  b'broodmare_s_001589.png',\n",
       "  b'passerine_s_000381.png',\n",
       "  b'accentor_s_000424.png',\n",
       "  b'finch_s_000617.png',\n",
       "  b'trucking_rig_s_001111.png',\n",
       "  b'canis_familiaris_s_001059.png',\n",
       "  b'cervus_elaphus_s_000813.png',\n",
       "  b'cassowary_s_000339.png',\n",
       "  b'buckskin_s_000325.png',\n",
       "  b'lightship_s_001019.png',\n",
       "  b'shooting_brake_s_001330.png',\n",
       "  b'tomcat_s_002649.png',\n",
       "  b'barren_ground_caribou_s_000174.png',\n",
       "  b'cat_s_000572.png',\n",
       "  b'walking_horse_s_002079.png',\n",
       "  b'rana_temporaria_s_001139.png',\n",
       "  b'tipper_s_000007.png',\n",
       "  b'freighter_s_000661.png',\n",
       "  b'stealth_fighter_s_000020.png',\n",
       "  b'goliath_frog_s_000205.png',\n",
       "  b'stealth_bomber_s_000703.png',\n",
       "  b'prunella_modularis_s_000795.png',\n",
       "  b'struthio_camelus_s_000331.png',\n",
       "  b'dunnock_s_000956.png',\n",
       "  b'auto_s_001284.png',\n",
       "  b'scow_s_000831.png',\n",
       "  b'fallow_deer_s_001256.png',\n",
       "  b'stealth_fighter_s_001297.png',\n",
       "  b'estate_car_s_000845.png',\n",
       "  b'ship_s_001691.png',\n",
       "  b'abandoned_ship_s_000977.png',\n",
       "  b'wagon_s_001298.png',\n",
       "  b'feist_s_000354.png',\n",
       "  b'lipizzan_s_001600.png',\n",
       "  b'leopard_frog_s_001700.png',\n",
       "  b'cervus_elaphus_s_000066.png',\n",
       "  b'toy_s_000517.png',\n",
       "  b'cargo_ship_s_000556.png',\n",
       "  b'stallion_s_001717.png',\n",
       "  b'coupe_s_001004.png',\n",
       "  b'camion_s_001068.png',\n",
       "  b'coupe_s_000001.png',\n",
       "  b'garbage_truck_s_001253.png',\n",
       "  b'sea_boat_s_001960.png',\n",
       "  b'capreolus_capreolus_s_001169.png',\n",
       "  b'lippizan_s_000681.png',\n",
       "  b'tabby_s_000512.png',\n",
       "  b'pontoon_s_000271.png',\n",
       "  b'sea_boat_s_001502.png',\n",
       "  b'lark_s_000700.png',\n",
       "  b'leopard_frog_s_001074.png',\n",
       "  b'rana_palustris_s_000022.png',\n",
       "  b'lippizaner_s_001181.png',\n",
       "  b'wagon_s_000155.png',\n",
       "  b'american_toad_s_001410.png',\n",
       "  b'cargo_vessel_s_000659.png',\n",
       "  b'station_wagon_s_001705.png',\n",
       "  b'transporter_s_000076.png',\n",
       "  b'broodmare_s_001098.png',\n",
       "  b'passenger_ship_s_000050.png',\n",
       "  b'tabby_cat_s_000484.png',\n",
       "  b'fighter_s_000276.png',\n",
       "  b'car_s_000506.png',\n",
       "  b'stealth_fighter_s_000139.png',\n",
       "  b'pleasure_craft_s_000076.png',\n",
       "  b'cruiser_s_000744.png',\n",
       "  b'tabby_cat_s_002478.png',\n",
       "  b'attack_aircraft_s_000423.png',\n",
       "  b'stealth_fighter_s_000460.png',\n",
       "  b'police_cruiser_s_000787.png',\n",
       "  b'mongrel_s_000381.png',\n",
       "  b'attack_aircraft_s_001166.png',\n",
       "  b'supertanker_s_000557.png',\n",
       "  b'tugboat_s_000807.png',\n",
       "  b'stud_mare_s_000413.png',\n",
       "  b'lorry_s_001039.png',\n",
       "  b'trailer_truck_s_001496.png',\n",
       "  b'airliner_s_001477.png',\n",
       "  b'camion_s_001521.png',\n",
       "  b'alces_alces_s_000404.png',\n",
       "  b'automobile_s_000315.png',\n",
       "  b'tabby_cat_s_001542.png',\n",
       "  b'pickerel_frog_s_000003.png',\n",
       "  b'spring_frog_s_000318.png',\n",
       "  b'roebuck_s_000153.png',\n",
       "  b'reindeer_s_000121.png',\n",
       "  b'male_horse_s_000008.png',\n",
       "  b'dog_s_000052.png',\n",
       "  b'bufo_bufo_s_001959.png',\n",
       "  b'stealth_bomber_s_000492.png',\n",
       "  b'pilot_boat_s_001819.png',\n",
       "  b'attack_aircraft_s_001187.png',\n",
       "  b'house_cat_s_000812.png',\n",
       "  b'cassowary_s_000138.png',\n",
       "  b'boat_s_000490.png',\n",
       "  b'wapiti_s_000884.png',\n",
       "  b'bufo_bufo_s_001492.png',\n",
       "  b'delivery_truck_s_000157.png',\n",
       "  b'truck_s_000060.png',\n",
       "  b'stud_mare_s_000506.png',\n",
       "  b'fighter_aircraft_s_000655.png',\n",
       "  b'true_cat_s_000127.png',\n",
       "  b'tabby_s_000228.png',\n",
       "  b'american_green_toad_s_000007.png',\n",
       "  b'quarter_horse_s_001833.png',\n",
       "  b'odocoileus_hemionus_s_001090.png',\n",
       "  b'tipper_lorry_s_000527.png',\n",
       "  b'compact_car_s_001200.png',\n",
       "  b'pickerel_frog_s_000242.png',\n",
       "  b'finch_s_000308.png',\n",
       "  b'arabian_s_002456.png',\n",
       "  b'dunnock_s_001476.png',\n",
       "  b'emu_s_002196.png',\n",
       "  b'plane_s_000065.png',\n",
       "  b'european_toad_s_000360.png',\n",
       "  b'arabian_s_002383.png',\n",
       "  b'dog_s_002098.png',\n",
       "  b'quarter_horse_s_001757.png',\n",
       "  b'rana_temporaria_s_001922.png',\n",
       "  b'cargo_ship_s_001318.png',\n",
       "  b'tip_truck_s_001017.png',\n",
       "  b'biplane_s_001055.png',\n",
       "  b'camion_s_001255.png',\n",
       "  b'red_deer_s_000756.png',\n",
       "  b'capreolus_capreolus_s_000822.png',\n",
       "  b'lippizaner_s_000840.png',\n",
       "  b'propeller_plane_s_001438.png',\n",
       "  b'dumper_s_000646.png',\n",
       "  b'capreolus_capreolus_s_000284.png',\n",
       "  b'truck_s_000526.png',\n",
       "  b'bufo_s_001473.png',\n",
       "  b'fire_engine_s_001449.png',\n",
       "  b'elk_s_001164.png',\n",
       "  b'canis_familiaris_s_001319.png',\n",
       "  b'tennessee_walking_horse_s_000005.png',\n",
       "  b'truck_s_000838.png',\n",
       "  b'rhea_s_001649.png',\n",
       "  b'mule_deer_s_002514.png',\n",
       "  b'canis_familiaris_s_000143.png',\n",
       "  b'police_cruiser_s_001073.png',\n",
       "  b'cervus_sika_s_000273.png',\n",
       "  b'cat_s_001584.png',\n",
       "  b'tipper_lorry_s_000313.png',\n",
       "  b'bufo_americanus_s_001202.png',\n",
       "  b'english_toy_spaniel_s_000735.png',\n",
       "  b'bufo_calamita_s_000043.png',\n",
       "  b'tipper_s_000180.png',\n",
       "  b'tabby_cat_s_000881.png',\n",
       "  b'tabby_cat_s_001644.png',\n",
       "  b'blenheim_spaniel_s_000330.png',\n",
       "  b'attack_aircraft_s_001138.png',\n",
       "  b'arabian_s_002270.png',\n",
       "  b'cassowary_s_000845.png',\n",
       "  b'wagon_s_000692.png',\n",
       "  b'tabby_cat_s_000319.png',\n",
       "  b'american_toad_s_000077.png',\n",
       "  b'moose_s_001500.png',\n",
       "  b'airliner_s_001406.png',\n",
       "  b'fighter_aircraft_s_000966.png',\n",
       "  b'dunnock_s_000055.png',\n",
       "  b'pekingese_s_002399.png',\n",
       "  b'twinjet_s_000739.png',\n",
       "  b'automobile_s_002249.png',\n",
       "  b'airliner_s_000669.png',\n",
       "  b'wagtail_s_001656.png',\n",
       "  b'tabby_s_001949.png',\n",
       "  b'dump_truck_s_001009.png',\n",
       "  b'freighter_s_000436.png',\n",
       "  b'fallow_deer_s_001171.png',\n",
       "  b'trucking_rig_s_000684.png',\n",
       "  b'ship_s_000249.png',\n",
       "  b'monoplane_s_000416.png',\n",
       "  b'songbird_s_002066.png',\n",
       "  b'rana_catesbeiana_s_000602.png',\n",
       "  b'red_deer_s_000536.png',\n",
       "  b'elk_s_001583.png',\n",
       "  b'stealth_fighter_s_001313.png',\n",
       "  b'station_wagon_s_002498.png',\n",
       "  b'pleasure_boat_s_001949.png',\n",
       "  b'cargo_ship_s_000060.png',\n",
       "  b'tabby_cat_s_001767.png',\n",
       "  b'crapaud_s_000055.png',\n",
       "  b'dump_truck_s_002008.png',\n",
       "  b'bufo_viridis_s_000506.png',\n",
       "  b'bullfrog_s_001295.png',\n",
       "  b'riding_horse_s_001884.png',\n",
       "  b'boat_s_002344.png',\n",
       "  b'ostrich_s_000950.png',\n",
       "  b'muntjac_s_000681.png',\n",
       "  b'pekingese_s_000279.png',\n",
       "  b'lipizzan_s_002052.png',\n",
       "  b'rana_pipiens_s_000570.png',\n",
       "  b'toy_s_002250.png',\n",
       "  b'tabby_s_001940.png',\n",
       "  b'dive_bomber_s_001294.png',\n",
       "  b'mutt_s_000086.png',\n",
       "  b'biplane_s_000701.png',\n",
       "  b'toy_s_000964.png',\n",
       "  b'jetliner_s_000807.png',\n",
       "  b'barge_s_000426.png',\n",
       "  b'cassowary_s_000255.png',\n",
       "  b'rana_clamitans_s_000059.png',\n",
       "  b'quarter_horse_s_000222.png',\n",
       "  b'tomcat_s_000377.png',\n",
       "  b'scow_s_000687.png',\n",
       "  b'emu_s_001571.png',\n",
       "  b'coupe_s_001625.png',\n",
       "  b'roan_s_000199.png',\n",
       "  b'bufo_bufo_s_000626.png',\n",
       "  b'tennessee_walking_horse_s_001470.png',\n",
       "  b'station_wagon_s_001497.png',\n",
       "  b'jumbo_jet_s_001056.png',\n",
       "  b'trailer_truck_s_000099.png',\n",
       "  b'puppy_s_000502.png',\n",
       "  b'pekingese_s_002101.png',\n",
       "  b'propeller_plane_s_001193.png',\n",
       "  b'car_s_001009.png',\n",
       "  b'lipizzan_s_002011.png',\n",
       "  b'green_frog_s_001375.png',\n",
       "  b'tipper_lorry_s_000586.png',\n",
       "  b'jetliner_s_000509.png',\n",
       "  b'musk_deer_s_000211.png',\n",
       "  b'appaloosa_s_002251.png',\n",
       "  b'arabian_s_001642.png',\n",
       "  b'compact_car_s_000435.png',\n",
       "  b'chihuahua_s_000063.png',\n",
       "  b'semi_s_000462.png',\n",
       "  b'wapiti_s_000455.png',\n",
       "  b'twinjet_s_001120.png',\n",
       "  b'cargo_vessel_s_001235.png',\n",
       "  b'peke_s_000274.png',\n",
       "  b'dump_truck_s_001422.png',\n",
       "  b'dump_truck_s_001863.png',\n",
       "  b'tailed_frog_s_000638.png',\n",
       "  b'gelding_s_000965.png',\n",
       "  b'coupe_s_000056.png',\n",
       "  b'pleasure_craft_s_000625.png',\n",
       "  b'house_cat_s_001328.png',\n",
       "  b'struthio_camelus_s_000081.png',\n",
       "  b'domestic_cat_s_000051.png',\n",
       "  b'passenger_ship_s_000948.png',\n",
       "  b'sparrow_s_002697.png',\n",
       "  b'prunella_modularis_s_000302.png',\n",
       "  b'fallow_deer_s_000192.png',\n",
       "  b'rana_palustris_s_000107.png',\n",
       "  b'attack_aircraft_s_001075.png',\n",
       "  b'twinjet_s_000046.png',\n",
       "  b'chihuahua_s_001038.png',\n",
       "  b'tabby_cat_s_001004.png',\n",
       "  b'cargo_vessel_s_001812.png',\n",
       "  b'dunnock_s_000037.png',\n",
       "  b'tabby_s_001740.png',\n",
       "  b'gelding_s_001638.png',\n",
       "  b'anthus_pratensis_s_001166.png',\n",
       "  b'camion_s_001988.png',\n",
       "  b'alley_cat_s_001645.png',\n",
       "  b'oil_tanker_s_001626.png',\n",
       "  b'tennessee_walking_horse_s_000355.png',\n",
       "  b'containership_s_000812.png',\n",
       "  b'passerine_s_000801.png',\n",
       "  b'lipizzan_s_000635.png',\n",
       "  b'truck_s_000635.png',\n",
       "  b'twinjet_s_000450.png',\n",
       "  b'honey_eater_s_000702.png',\n",
       "  b'tabby_s_000039.png',\n",
       "  b'pipit_s_000827.png',\n",
       "  b'elephant_bird_s_001004.png',\n",
       "  b'night_bird_s_000241.png',\n",
       "  b'alley_cat_s_001301.png',\n",
       "  b'house_cat_s_000332.png',\n",
       "  b'bufo_americanus_s_000492.png',\n",
       "  b'wagtail_s_001443.png',\n",
       "  b'tabby_cat_s_000587.png',\n",
       "  b'gamecock_s_000098.png',\n",
       "  b'cargo_ship_s_001867.png',\n",
       "  b'seaplane_s_000416.png',\n",
       "  b'english_toy_spaniel_s_001058.png',\n",
       "  b'japanese_spaniel_s_001324.png',\n",
       "  b'coupe_s_000179.png',\n",
       "  b'american_elk_s_000201.png',\n",
       "  b'chihuahua_s_000666.png',\n",
       "  b'spadefoot_s_000219.png',\n",
       "  b'bufo_s_001733.png',\n",
       "  b'struthio_camelus_s_001368.png',\n",
       "  b'arab_s_000456.png',\n",
       "  b'monoplane_s_001154.png',\n",
       "  b'coupe_s_001690.png',\n",
       "  b'dawn_horse_s_000752.png',\n",
       "  b'tennessee_walking_horse_s_000013.png',\n",
       "  b'cabin_cruiser_s_001500.png',\n",
       "  b'sparrow_s_001750.png',\n",
       "  b'aerial_ladder_truck_s_001306.png',\n",
       "  b'dromaius_novaehollandiae_s_000433.png',\n",
       "  b'passerine_s_000376.png',\n",
       "  b'caribou_s_001447.png',\n",
       "  b'bird_s_000356.png',\n",
       "  b'motorcar_s_001389.png',\n",
       "  b'auto_s_000675.png',\n",
       "  b'automobile_s_001833.png',\n",
       "  b'rana_temporaria_s_000008.png',\n",
       "  b'spring_frog_s_001263.png',\n",
       "  b'barking_frog_s_000397.png',\n",
       "  b'puppy_s_000785.png',\n",
       "  b'convertible_s_001861.png',\n",
       "  b'convertible_s_000859.png',\n",
       "  b'american_saddle_horse_s_000039.png',\n",
       "  b'biplane_s_000244.png',\n",
       "  b'red_deer_s_000704.png',\n",
       "  b'tabby_s_001071.png',\n",
       "  b'tabby_cat_s_002548.png',\n",
       "  b'tennessee_walker_s_001791.png',\n",
       "  b'auto_s_002048.png',\n",
       "  b'dromaius_novaehollandiae_s_000296.png',\n",
       "  b'felis_catus_s_001460.png',\n",
       "  b'peke_s_000440.png',\n",
       "  b'chihuahua_s_000349.png',\n",
       "  b'lapdog_s_002195.png',\n",
       "  b'bufo_marinus_s_001243.png',\n",
       "  b'convertible_s_001433.png',\n",
       "  b'mule_deer_s_000628.png',\n",
       "  b'tabby_s_000410.png',\n",
       "  b'stud_s_000099.png',\n",
       "  b'passenger_ship_s_001530.png',\n",
       "  b'scow_s_000131.png',\n",
       "  b'tabby_s_001868.png',\n",
       "  b'bufo_bufo_s_002334.png',\n",
       "  b'bufo_viridis_s_000152.png',\n",
       "  b'passerine_s_001306.png',\n",
       "  b'house_cat_s_002116.png',\n",
       "  b'jetliner_s_001188.png',\n",
       "  b'moving_van_s_001359.png',\n",
       "  b'wapiti_s_000663.png',\n",
       "  b'felis_catus_s_000998.png',\n",
       "  b'sea_boat_s_001979.png',\n",
       "  b'fighter_aircraft_s_001776.png',\n",
       "  b'fighter_aircraft_s_001096.png',\n",
       "  b'shooting_brake_s_000774.png',\n",
       "  b'automobile_s_001310.png',\n",
       "  b'pekingese_s_001023.png',\n",
       "  b'american_elk_s_000455.png',\n",
       "  b'trucking_rig_s_001243.png',\n",
       "  b'house_cat_s_000877.png',\n",
       "  b'taxi_s_000040.png',\n",
       "  b'pilot_boat_s_001100.png',\n",
       "  b'articulated_lorry_s_000223.png',\n",
       "  b'felis_domesticus_s_000164.png',\n",
       "  b'trucking_rig_s_001300.png',\n",
       "  b'tractor_trailer_s_000254.png',\n",
       "  b'flightless_bird_s_000142.png',\n",
       "  b'dump_truck_s_000939.png',\n",
       "  b'deer_s_000967.png',\n",
       "  b'ship_s_001904.png',\n",
       "  b'wagtail_s_000309.png',\n",
       "  b'trailer_truck_s_000815.png',\n",
       "  b'container_ship_s_002477.png',\n",
       "  b'guard_boat_s_000285.png',\n",
       "  b'estate_car_s_000446.png',\n",
       "  b'puppy_s_001621.png',\n",
       "  b'tabby_cat_s_000689.png',\n",
       "  b'american_toad_s_000650.png',\n",
       "  b'boat_s_002381.png',\n",
       "  b'female_horse_s_000089.png',\n",
       "  b'bufo_viridis_s_001370.png',\n",
       "  b'ladder_truck_s_000044.png',\n",
       "  b'cargo_ship_s_001321.png',\n",
       "  b'twinjet_s_000547.png',\n",
       "  b'cascades_frog_s_000061.png',\n",
       "  b'odocoileus_hemionus_s_001017.png',\n",
       "  b'attack_aircraft_s_001254.png',\n",
       "  b'airplane_s_000006.png',\n",
       "  b'songbird_s_001232.png',\n",
       "  b'peke_s_000388.png',\n",
       "  b'boat_s_002143.png',\n",
       "  b'accentor_s_001182.png',\n",
       "  b'dive_bomber_s_001066.png',\n",
       "  b'dunnock_s_001650.png',\n",
       "  b'dawn_horse_s_001127.png',\n",
       "  b'european_toad_s_000359.png',\n",
       "  b'wrecker_s_002325.png',\n",
       "  b'lipizzan_s_000118.png',\n",
       "  b'automobile_s_000327.png',\n",
       "  b'pekinese_s_002027.png',\n",
       "  b'maltese_s_001913.png',\n",
       "  b'toad_frog_s_000836.png',\n",
       "  b'leopard_frog_s_001231.png',\n",
       "  b'domestic_cat_s_001066.png',\n",
       "  b'barking_frog_s_000840.png',\n",
       "  b'finch_s_000038.png',\n",
       "  b'capreolus_capreolus_s_000257.png',\n",
       "  b'lippizan_s_000126.png',\n",
       "  b'attack_aircraft_s_000089.png',\n",
       "  b'mutt_s_002278.png',\n",
       "  b'true_toad_s_000021.png',\n",
       "  b'capreolus_capreolus_s_000178.png',\n",
       "  b'bufo_viridis_s_000600.png',\n",
       "  b'chihuahua_s_000097.png',\n",
       "  b'meadow_pipit_s_001282.png',\n",
       "  b'red_deer_s_000008.png',\n",
       "  b'leopard_frog_s_001528.png',\n",
       "  b'estate_car_s_001220.png',\n",
       "  b'bullfrog_s_000408.png',\n",
       "  b'stealth_fighter_s_001637.png',\n",
       "  b'capreolus_capreolus_s_001669.png',\n",
       "  b'airbus_s_000736.png',\n",
       "  b'tabby_s_000292.png',\n",
       "  b'station_wagon_s_002793.png',\n",
       "  b'boat_s_000765.png',\n",
       "  b'english_toy_spaniel_s_000814.png',\n",
       "  b'european_elk_s_000265.png',\n",
       "  b'roe_deer_s_000371.png',\n",
       "  b'convertible_s_000414.png',\n",
       "  b'cow_pony_s_000145.png',\n",
       "  b'tabby_s_000126.png',\n",
       "  b'trucking_rig_s_001628.png',\n",
       "  b'muntjac_s_001738.png',\n",
       "  b'broodmare_s_000582.png',\n",
       "  b'fire_engine_s_001540.png',\n",
       "  b'gelding_s_000640.png',\n",
       "  b'tabby_cat_s_002533.png',\n",
       "  b'appaloosa_s_000342.png',\n",
       "  b'pipit_s_001066.png',\n",
       "  b'speedboat_s_000410.png',\n",
       "  b'roe_deer_s_001014.png',\n",
       "  b'american_toad_s_001815.png',\n",
       "  b'bufo_calamita_s_000773.png',\n",
       "  b'estate_car_s_000044.png',\n",
       "  b'cassowary_s_000021.png',\n",
       "  b'lorry_s_001709.png',\n",
       "  b'amphibious_aircraft_s_000141.png',\n",
       "  b'capreolus_capreolus_s_000073.png',\n",
       "  b'motorboat_s_000773.png',\n",
       "  b'lipizzan_s_001658.png',\n",
       "  b'tabby_cat_s_000531.png',\n",
       "  b'truck_s_000289.png',\n",
       "  b'speedboat_s_001711.png',\n",
       "  b'quarter_horse_s_000557.png',\n",
       "  b'quarter_horse_s_000942.png',\n",
       "  b'jumbo_jet_s_001476.png',\n",
       "  b'struthio_camelus_s_001197.png',\n",
       "  b'fallow_deer_s_001623.png',\n",
       "  b'compact_car_s_000850.png',\n",
       "  b'coupe_s_001912.png',\n",
       "  b'roe_deer_s_000678.png',\n",
       "  b'car_s_000674.png',\n",
       "  b'chihuahua_s_001937.png',\n",
       "  b'red_deer_s_002698.png',\n",
       "  b'jumbojet_s_001587.png',\n",
       "  b'toy_spaniel_s_001692.png',\n",
       "  b'rana_clamitans_s_000766.png',\n",
       "  b'pipit_s_000025.png',\n",
       "  b'cargo_vessel_s_001809.png',\n",
       "  b'pekingese_s_001613.png',\n",
       "  b'dive_bomber_s_000707.png',\n",
       "  b'lark_s_000550.png',\n",
       "  b'estate_car_s_000292.png',\n",
       "  b'tabby_cat_s_001853.png',\n",
       "  b'lapdog_s_001963.png',\n",
       "  b'palfrey_s_000300.png',\n",
       "  b'cat_s_002298.png',\n",
       "  b'mutt_s_000188.png',\n",
       "  b'police_cruiser_s_000471.png',\n",
       "  b'true_cat_s_001598.png',\n",
       "  b'peke_s_000638.png',\n",
       "  ...],\n",
       " b'labels': [6,\n",
       "  9,\n",
       "  9,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  9,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  7,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  9,\n",
       "  2,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  1,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  9,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  6,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  8,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  9,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  9,\n",
       "  5,\n",
       "  0,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  8,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  7,\n",
       "  6,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  0,\n",
       "  4,\n",
       "  7,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  6,\n",
       "  1,\n",
       "  9,\n",
       "  3,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  8,\n",
       "  5,\n",
       "  2,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  6,\n",
       "  9,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  7,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  8,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  6,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  8,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  0,\n",
       "  9,\n",
       "  3,\n",
       "  7,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  4,\n",
       "  9,\n",
       "  9,\n",
       "  1,\n",
       "  0,\n",
       "  5,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  6,\n",
       "  0,\n",
       "  7,\n",
       "  9,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  9,\n",
       "  9,\n",
       "  0,\n",
       "  8,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  3,\n",
       "  3,\n",
       "  9,\n",
       "  0,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  7,\n",
       "  9,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  5,\n",
       "  9,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  4,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  8,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  9,\n",
       "  2,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  6,\n",
       "  9,\n",
       "  3,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  5,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  4,\n",
       "  8,\n",
       "  5,\n",
       "  4,\n",
       "  7,\n",
       "  2,\n",
       "  3,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  9,\n",
       "  0,\n",
       "  9,\n",
       "  6,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  7,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  8,\n",
       "  9,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  5,\n",
       "  1,\n",
       "  9,\n",
       "  4,\n",
       "  8,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  9,\n",
       "  9,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  4,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  6,\n",
       "  8,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  7,\n",
       "  5,\n",
       "  9,\n",
       "  6,\n",
       "  2,\n",
       "  8,\n",
       "  3,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  8,\n",
       "  1,\n",
       "  8,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  1,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  8,\n",
       "  0,\n",
       "  4,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  8,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  4,\n",
       "  0,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  6,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  9,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  7,\n",
       "  8,\n",
       "  1,\n",
       "  3,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  6,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  8,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  4,\n",
       "  5,\n",
       "  8,\n",
       "  7,\n",
       "  1,\n",
       "  9,\n",
       "  1,\n",
       "  9,\n",
       "  8,\n",
       "  4,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  8,\n",
       "  2,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  6,\n",
       "  8,\n",
       "  1,\n",
       "  9,\n",
       "  7,\n",
       "  8,\n",
       "  3,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  5,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  7,\n",
       "  9,\n",
       "  9,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  5,\n",
       "  6,\n",
       "  0,\n",
       "  8,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  9,\n",
       "  9,\n",
       "  7,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  7,\n",
       "  4,\n",
       "  9,\n",
       "  1,\n",
       "  6,\n",
       "  2,\n",
       "  7,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  6,\n",
       "  7,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  9,\n",
       "  6,\n",
       "  9,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  9,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  9,\n",
       "  6,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  0,\n",
       "  7,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  4,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  2,\n",
       "  6,\n",
       "  4,\n",
       "  4,\n",
       "  0,\n",
       "  1,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  6,\n",
       "  9,\n",
       "  6,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  7,\n",
       "  6,\n",
       "  5,\n",
       "  3,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  8,\n",
       "  2,\n",
       "  6,\n",
       "  7,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  1,\n",
       "  7,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  0,\n",
       "  9,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  0,\n",
       "  4,\n",
       "  7,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  9,\n",
       "  4,\n",
       "  0,\n",
       "  8,\n",
       "  5,\n",
       "  9,\n",
       "  9,\n",
       "  6,\n",
       "  7,\n",
       "  1,\n",
       "  8,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  3,\n",
       "  8,\n",
       "  2,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  9,\n",
       "  3,\n",
       "  8,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  7,\n",
       "  9,\n",
       "  0,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  8,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  1,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  7,\n",
       "  0,\n",
       "  1,\n",
       "  7,\n",
       "  7,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  2,\n",
       "  2,\n",
       "  4,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  6,\n",
       "  6,\n",
       "  6,\n",
       "  5,\n",
       "  1,\n",
       "  1,\n",
       "  7,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  7,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  1,\n",
       "  4,\n",
       "  3,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  3,\n",
       "  6,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  0,\n",
       "  9,\n",
       "  4,\n",
       "  3,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  9,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  9,\n",
       "  3,\n",
       "  9,\n",
       "  9,\n",
       "  2,\n",
       "  9,\n",
       "  4,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  8,\n",
       "  8,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  6,\n",
       "  8,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  8,\n",
       "  0,\n",
       "  6,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  5,\n",
       "  8,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  7,\n",
       "  6,\n",
       "  9,\n",
       "  7,\n",
       "  1,\n",
       "  5,\n",
       "  5,\n",
       "  6,\n",
       "  6,\n",
       "  3,\n",
       "  6,\n",
       "  2,\n",
       "  4,\n",
       "  7,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  4,\n",
       "  6,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  6,\n",
       "  1,\n",
       "  6,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  3,\n",
       "  1,\n",
       "  8,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  4,\n",
       "  7,\n",
       "  9,\n",
       "  7,\n",
       "  3,\n",
       "  7,\n",
       "  2,\n",
       "  8,\n",
       "  4,\n",
       "  6,\n",
       "  6,\n",
       "  1,\n",
       "  2,\n",
       "  9,\n",
       "  0,\n",
       "  4,\n",
       "  8,\n",
       "  7,\n",
       "  3,\n",
       "  9,\n",
       "  8,\n",
       "  7,\n",
       "  7,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  4,\n",
       "  0,\n",
       "  5,\n",
       "  6,\n",
       "  2,\n",
       "  8,\n",
       "  5,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  3,\n",
       "  5,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  ...]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'data'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,  43,  50, ..., 140,  84,  72], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[b'data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "shaped = np.reshape(data_1[b'labels'], (-1,1))\n",
    "onehot.fit(shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_2 = OneHotEncoder(sparse=False)\n",
    "shaped_2 = np.reshape(data_2[b'labels'], (-1,1))\n",
    "onehot_2.fit(shaped_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_3 = OneHotEncoder(sparse=False)\n",
    "shaped_3 = np.reshape(data_3[b'labels'], (-1,1))\n",
    "onehot_3.fit(shaped_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_4 = OneHotEncoder(sparse=False)\n",
    "shaped_4 = np.reshape(data_4[b'labels'], (-1,1))\n",
    "onehot_4.fit(shaped_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_5 = OneHotEncoder(sparse=False)\n",
    "shaped_5 = np.reshape(data_5[b'labels'], (-1,1))\n",
    "onehot_5.fit(shaped_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_var = onehot.transform(shaped)\n",
    "y_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_var_2 = onehot.transform(shaped_2)\n",
    "y_var_3 = onehot.transform(shaped_3)\n",
    "y_var_4 = onehot.transform(shaped_4)\n",
    "y_var_5 = onehot.transform(shaped_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_0 = data_1[b'data'][1000].reshape([3,32,32])\n",
    "np.rollaxis(image_0,0,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1321a0e80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUeT8QiSKnF+IRJHzC5Eo/weNQcytcD1B0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12eaffda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.rollaxis(image_0,0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for data in data_1[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll = []\n",
    "for image in images:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_2 = []\n",
    "for data in data_2[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_2.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_2 = []\n",
    "for image in images_2:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_2.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_3 = []\n",
    "for data in data_3[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_3.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_3 = []\n",
    "for image in images_3:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_3.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_4 = []\n",
    "for data in data_4[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_4.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_4 = []\n",
    "for image in images_4:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_4.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_5 = []\n",
    "for data in data_5[b'data']:\n",
    "    data = data.reshape([3,32,32])\n",
    "    images_5.append(data)\n",
    "#data_1[b'data'].map(lambda x: x.reshape([3,32,32]))\n",
    "#np.rollaxis(image_0,0,3).shape\n",
    "images_roll_5 = []\n",
    "for image in images_5:\n",
    "    image = np.rollaxis(image,0,3)\n",
    "    images_roll_5.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[222, 221, 229],\n",
       "       [239, 239, 249],\n",
       "       [233, 234, 246],\n",
       "       [232, 233, 246],\n",
       "       [234, 235, 246],\n",
       "       [240, 240, 248],\n",
       "       [244, 244, 250],\n",
       "       [244, 243, 249],\n",
       "       [240, 239, 247],\n",
       "       [233, 235, 244],\n",
       "       [230, 231, 242],\n",
       "       [229, 231, 240],\n",
       "       [230, 231, 239],\n",
       "       [228, 228, 238],\n",
       "       [222, 223, 234],\n",
       "       [216, 218, 230],\n",
       "       [212, 215, 227],\n",
       "       [210, 214, 226],\n",
       "       [209, 213, 225],\n",
       "       [210, 213, 226],\n",
       "       [211, 214, 229],\n",
       "       [213, 216, 231],\n",
       "       [215, 220, 235],\n",
       "       [216, 221, 237],\n",
       "       [217, 219, 234],\n",
       "       [214, 216, 228],\n",
       "       [221, 222, 234],\n",
       "       [220, 222, 234],\n",
       "       [220, 221, 234],\n",
       "       [223, 223, 236],\n",
       "       [227, 228, 238],\n",
       "       [210, 211, 220]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_roll = np.asarray(images_roll)\n",
    "image_one = images_roll[1]\n",
    "image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_roll_2 = np.asarray(images_roll_2)\n",
    "images_roll_3 = np.asarray(images_roll_3)\n",
    "images_roll_4 = np.asarray(images_roll_4)\n",
    "images_roll_5 = np.asarray(images_roll_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.concatenate((images_roll,images_roll_3),axis=0)\n",
    "images = np.concatenate((images,images_roll_4),axis=0)\n",
    "images = np.concatenate((images,images_roll_5),axis=0)\n",
    "images = np.concatenate((images,images_roll_2),axis=0)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate((y_var,y_var_3),axis=0)\n",
    "y = np.concatenate((y,y_var_4),axis=0)\n",
    "y = np.concatenate((y,y_var_5),axis=0)\n",
    "y = np.concatenate((y,y_var_2),axis=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, y, test_size= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20ff7b128>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d0f99470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_1 = images_roll[1]\n",
    "plt.imshow(image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(3072,)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2000\n",
      "8000/8000 [==============================] - 3s 335us/step - loss: 3.4326 - acc: 0.1791 - val_loss: 2.2253 - val_acc: 0.2290\n",
      "Epoch 2/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 2.2689 - acc: 0.2146 - val_loss: 2.0181 - val_acc: 0.2565\n",
      "Epoch 3/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 2.2057 - acc: 0.2281 - val_loss: 2.0382 - val_acc: 0.2845\n",
      "Epoch 4/2000\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 2.1817 - acc: 0.2469 - val_loss: 1.9699 - val_acc: 0.2800\n",
      "Epoch 5/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 2.1106 - acc: 0.2590 - val_loss: 2.4490 - val_acc: 0.2065\n",
      "Epoch 6/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 2.0775 - acc: 0.2684 - val_loss: 1.8798 - val_acc: 0.3100\n",
      "Epoch 7/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 2.0227 - acc: 0.2801 - val_loss: 1.9939 - val_acc: 0.2850\n",
      "Epoch 8/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.9991 - acc: 0.2914 - val_loss: 2.0019 - val_acc: 0.3105\n",
      "Epoch 9/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.9807 - acc: 0.2979 - val_loss: 1.9090 - val_acc: 0.3320\n",
      "Epoch 10/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.9531 - acc: 0.3066 - val_loss: 2.1551 - val_acc: 0.2405\n",
      "Epoch 11/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.9350 - acc: 0.3123 - val_loss: 1.9382 - val_acc: 0.3180\n",
      "Epoch 12/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.9060 - acc: 0.3181 - val_loss: 2.0751 - val_acc: 0.2710\n",
      "Epoch 13/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.8928 - acc: 0.3289 - val_loss: 1.8188 - val_acc: 0.3580\n",
      "Epoch 14/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.8794 - acc: 0.3341 - val_loss: 1.8754 - val_acc: 0.3455\n",
      "Epoch 15/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.8635 - acc: 0.3421 - val_loss: 1.9018 - val_acc: 0.3215\n",
      "Epoch 16/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.8534 - acc: 0.3441 - val_loss: 1.8137 - val_acc: 0.3635\n",
      "Epoch 17/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.8399 - acc: 0.3488 - val_loss: 1.9362 - val_acc: 0.3270\n",
      "Epoch 18/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.8311 - acc: 0.3470 - val_loss: 1.8849 - val_acc: 0.3460\n",
      "Epoch 19/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.8221 - acc: 0.3580 - val_loss: 1.8883 - val_acc: 0.3470\n",
      "Epoch 20/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.8228 - acc: 0.3605 - val_loss: 1.9102 - val_acc: 0.3240\n",
      "Epoch 21/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.8187 - acc: 0.3579 - val_loss: 1.8704 - val_acc: 0.3340\n",
      "Epoch 22/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.8112 - acc: 0.3640 - val_loss: 1.8532 - val_acc: 0.3395\n",
      "Epoch 23/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.8123 - acc: 0.3614 - val_loss: 1.8350 - val_acc: 0.3575\n",
      "Epoch 24/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.7991 - acc: 0.3653 - val_loss: 1.8610 - val_acc: 0.3585\n",
      "Epoch 25/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.7866 - acc: 0.3654 - val_loss: 1.8099 - val_acc: 0.3850\n",
      "Epoch 26/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.7850 - acc: 0.3709 - val_loss: 1.8558 - val_acc: 0.3460\n",
      "Epoch 27/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.7769 - acc: 0.3725 - val_loss: 1.8468 - val_acc: 0.3555\n",
      "Epoch 28/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.7790 - acc: 0.3739 - val_loss: 1.7919 - val_acc: 0.3645\n",
      "Epoch 29/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.7701 - acc: 0.3762 - val_loss: 1.8467 - val_acc: 0.3495\n",
      "Epoch 30/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.7606 - acc: 0.3817 - val_loss: 1.9286 - val_acc: 0.3035\n",
      "Epoch 31/2000\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 1.7576 - acc: 0.3824 - val_loss: 1.8955 - val_acc: 0.3400\n",
      "Epoch 32/2000\n",
      "8000/8000 [==============================] - 2s 247us/step - loss: 1.7526 - acc: 0.3795 - val_loss: 1.8298 - val_acc: 0.3535\n",
      "Epoch 33/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.7539 - acc: 0.3806 - val_loss: 1.8963 - val_acc: 0.3295\n",
      "Epoch 34/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.7523 - acc: 0.3825 - val_loss: 1.8808 - val_acc: 0.3405\n",
      "Epoch 35/2000\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 1.7584 - acc: 0.3799 - val_loss: 1.9423 - val_acc: 0.3150\n",
      "Epoch 36/2000\n",
      "8000/8000 [==============================] - 2s 300us/step - loss: 1.7587 - acc: 0.3816 - val_loss: 1.9021 - val_acc: 0.3400\n",
      "Epoch 37/2000\n",
      "8000/8000 [==============================] - 3s 313us/step - loss: 1.7554 - acc: 0.3849 - val_loss: 1.7903 - val_acc: 0.3740\n",
      "Epoch 38/2000\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 1.7489 - acc: 0.3869 - val_loss: 1.8727 - val_acc: 0.3565\n",
      "Epoch 39/2000\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 1.7488 - acc: 0.3810 - val_loss: 1.8259 - val_acc: 0.3705\n",
      "Epoch 40/2000\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 1.7421 - acc: 0.3891 - val_loss: 1.8762 - val_acc: 0.3545\n",
      "Epoch 41/2000\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 1.7471 - acc: 0.3831 - val_loss: 1.8014 - val_acc: 0.3805\n",
      "Epoch 42/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.7345 - acc: 0.3920 - val_loss: 1.8355 - val_acc: 0.3540\n",
      "Epoch 43/2000\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 1.7237 - acc: 0.3946 - val_loss: 1.7785 - val_acc: 0.3775\n",
      "Epoch 44/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.7215 - acc: 0.3924 - val_loss: 1.8234 - val_acc: 0.3520\n",
      "Epoch 45/2000\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 1.7201 - acc: 0.3935 - val_loss: 1.8326 - val_acc: 0.3470\n",
      "Epoch 46/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.7162 - acc: 0.3934 - val_loss: 1.8042 - val_acc: 0.3745\n",
      "Epoch 47/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.7120 - acc: 0.3958 - val_loss: 1.8949 - val_acc: 0.3250\n",
      "Epoch 48/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.7081 - acc: 0.4032 - val_loss: 1.8967 - val_acc: 0.3380\n",
      "Epoch 49/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.7142 - acc: 0.4005 - val_loss: 1.8525 - val_acc: 0.3445\n",
      "Epoch 50/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.7033 - acc: 0.3996 - val_loss: 1.8275 - val_acc: 0.3710\n",
      "Epoch 51/2000\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 1.7075 - acc: 0.4027 - val_loss: 1.8557 - val_acc: 0.3685\n",
      "Epoch 52/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.6966 - acc: 0.4126 - val_loss: 1.9085 - val_acc: 0.3370\n",
      "Epoch 53/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.7010 - acc: 0.4031 - val_loss: 1.8615 - val_acc: 0.3495\n",
      "Epoch 54/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.6904 - acc: 0.4035 - val_loss: 1.8493 - val_acc: 0.3540\n",
      "Epoch 55/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.6899 - acc: 0.4088 - val_loss: 1.8119 - val_acc: 0.3775\n",
      "Epoch 56/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.6985 - acc: 0.4055 - val_loss: 1.8363 - val_acc: 0.3670\n",
      "Epoch 57/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.6893 - acc: 0.4042 - val_loss: 1.9105 - val_acc: 0.3440\n",
      "Epoch 58/2000\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 1.6907 - acc: 0.4076 - val_loss: 1.8330 - val_acc: 0.3745\n",
      "Epoch 59/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.6863 - acc: 0.4216 - val_loss: 1.8073 - val_acc: 0.3795\n",
      "Epoch 60/2000\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 1.6713 - acc: 0.4135 - val_loss: 1.8948 - val_acc: 0.3550\n",
      "Epoch 61/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.6743 - acc: 0.4153 - val_loss: 1.8088 - val_acc: 0.3750\n",
      "Epoch 62/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.6743 - acc: 0.4186 - val_loss: 1.9456 - val_acc: 0.3160\n",
      "Epoch 63/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.6725 - acc: 0.4183 - val_loss: 1.8117 - val_acc: 0.3680\n",
      "Epoch 64/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 1.6780 - acc: 0.4121 - val_loss: 1.9242 - val_acc: 0.3380\n",
      "Epoch 65/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.6705 - acc: 0.4201 - val_loss: 1.8365 - val_acc: 0.3670\n",
      "Epoch 66/2000\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 1.6614 - acc: 0.4200 - val_loss: 1.9150 - val_acc: 0.3225\n",
      "Epoch 67/2000\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 1.6621 - acc: 0.4260 - val_loss: 1.8524 - val_acc: 0.3670\n",
      "Epoch 68/2000\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 1.6537 - acc: 0.4229 - val_loss: 1.8755 - val_acc: 0.3470\n",
      "Epoch 69/2000\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 1.6678 - acc: 0.4206 - val_loss: 1.8313 - val_acc: 0.3635\n",
      "Epoch 70/2000\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 1.6652 - acc: 0.4218 - val_loss: 1.9448 - val_acc: 0.3315\n",
      "Epoch 71/2000\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 1.6546 - acc: 0.4200 - val_loss: 1.8159 - val_acc: 0.3720\n",
      "Epoch 72/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.6514 - acc: 0.4234 - val_loss: 1.8229 - val_acc: 0.3790\n",
      "Epoch 73/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.6557 - acc: 0.4224 - val_loss: 1.8594 - val_acc: 0.3675\n",
      "Epoch 74/2000\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 1.6581 - acc: 0.4219 - val_loss: 1.8450 - val_acc: 0.3715\n",
      "Epoch 75/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.6527 - acc: 0.4253 - val_loss: 1.8690 - val_acc: 0.3570\n",
      "Epoch 76/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.6513 - acc: 0.4269 - val_loss: 1.9631 - val_acc: 0.3270\n",
      "Epoch 77/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.6423 - acc: 0.4273 - val_loss: 1.8333 - val_acc: 0.3775\n",
      "Epoch 78/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.6392 - acc: 0.4321 - val_loss: 1.8668 - val_acc: 0.3540\n",
      "Epoch 79/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.6517 - acc: 0.4276 - val_loss: 1.8399 - val_acc: 0.3635\n",
      "Epoch 80/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.6515 - acc: 0.4255 - val_loss: 1.8363 - val_acc: 0.3595\n",
      "Epoch 81/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.6449 - acc: 0.4280 - val_loss: 1.8315 - val_acc: 0.3675\n",
      "Epoch 82/2000\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.6413 - acc: 0.4246 - val_loss: 1.8909 - val_acc: 0.3425\n",
      "Epoch 83/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.6510 - acc: 0.4271 - val_loss: 1.8252 - val_acc: 0.3500\n",
      "Epoch 84/2000\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 1.6434 - acc: 0.4313 - val_loss: 1.9259 - val_acc: 0.3465\n",
      "Epoch 85/2000\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 1.6483 - acc: 0.4259 - val_loss: 1.8774 - val_acc: 0.3625\n",
      "Epoch 86/2000\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 1.6437 - acc: 0.4268 - val_loss: 1.8720 - val_acc: 0.3535\n",
      "Epoch 87/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.6395 - acc: 0.4289 - val_loss: 1.8930 - val_acc: 0.3345\n",
      "Epoch 88/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.6223 - acc: 0.4331 - val_loss: 1.9077 - val_acc: 0.3465\n",
      "Epoch 89/2000\n",
      "8000/8000 [==============================] - 2s 251us/step - loss: 1.6244 - acc: 0.4360 - val_loss: 1.8795 - val_acc: 0.3420\n",
      "Epoch 90/2000\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 1.6256 - acc: 0.4365 - val_loss: 1.8913 - val_acc: 0.3560\n",
      "Epoch 91/2000\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 1.6255 - acc: 0.4298 - val_loss: 1.9340 - val_acc: 0.3285\n",
      "Epoch 92/2000\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 1.6310 - acc: 0.4314 - val_loss: 1.8481 - val_acc: 0.3685\n",
      "Epoch 93/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.6241 - acc: 0.4366 - val_loss: 1.8839 - val_acc: 0.3465\n",
      "Epoch 94/2000\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 1.6213 - acc: 0.4338 - val_loss: 1.8311 - val_acc: 0.3630\n",
      "Epoch 95/2000\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 1.6223 - acc: 0.4387 - val_loss: 1.9095 - val_acc: 0.3605\n",
      "Epoch 96/2000\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 1.6243 - acc: 0.4345 - val_loss: 1.8445 - val_acc: 0.3670\n",
      "Epoch 97/2000\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 1.6240 - acc: 0.4301 - val_loss: 1.8656 - val_acc: 0.3620\n",
      "Epoch 98/2000\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 1.6172 - acc: 0.4400 - val_loss: 1.8451 - val_acc: 0.3770\n",
      "Epoch 99/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.6180 - acc: 0.4373 - val_loss: 1.8967 - val_acc: 0.3560\n",
      "Epoch 100/2000\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 1.6221 - acc: 0.4390 - val_loss: 1.8484 - val_acc: 0.3635\n",
      "Epoch 101/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.6101 - acc: 0.4373 - val_loss: 1.9543 - val_acc: 0.3390\n",
      "Epoch 102/2000\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.6087 - acc: 0.4417 - val_loss: 2.0214 - val_acc: 0.3415\n",
      "Epoch 103/2000\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 1.6075 - acc: 0.4449 - val_loss: 1.8982 - val_acc: 0.3525\n",
      "Epoch 104/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.6121 - acc: 0.4385 - val_loss: 1.8543 - val_acc: 0.3570\n",
      "Epoch 105/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5986 - acc: 0.4373 - val_loss: 2.3413 - val_acc: 0.2925\n",
      "Epoch 106/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.6017 - acc: 0.4379 - val_loss: 1.9472 - val_acc: 0.3475\n",
      "Epoch 107/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.6059 - acc: 0.4415 - val_loss: 1.8917 - val_acc: 0.3535\n",
      "Epoch 108/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.5965 - acc: 0.4429 - val_loss: 1.8666 - val_acc: 0.3650\n",
      "Epoch 109/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.5958 - acc: 0.4458 - val_loss: 1.8645 - val_acc: 0.3740\n",
      "Epoch 110/2000\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 1.5969 - acc: 0.4442 - val_loss: 1.8892 - val_acc: 0.3680\n",
      "Epoch 111/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.5948 - acc: 0.4501 - val_loss: 1.8998 - val_acc: 0.3630\n",
      "Epoch 112/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.5985 - acc: 0.4466 - val_loss: 1.8667 - val_acc: 0.3630\n",
      "Epoch 113/2000\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 1.5953 - acc: 0.4447 - val_loss: 1.8847 - val_acc: 0.3475\n",
      "Epoch 114/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.5956 - acc: 0.4459 - val_loss: 1.9326 - val_acc: 0.3510\n",
      "Epoch 115/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.5937 - acc: 0.4467 - val_loss: 1.9576 - val_acc: 0.3485\n",
      "Epoch 116/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5916 - acc: 0.4452 - val_loss: 1.8785 - val_acc: 0.3730\n",
      "Epoch 117/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.5861 - acc: 0.4493 - val_loss: 1.8977 - val_acc: 0.3555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5840 - acc: 0.4550 - val_loss: 1.9895 - val_acc: 0.3365\n",
      "Epoch 119/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5727 - acc: 0.4507 - val_loss: 2.1343 - val_acc: 0.3035\n",
      "Epoch 120/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5873 - acc: 0.4465 - val_loss: 1.8912 - val_acc: 0.3490\n",
      "Epoch 121/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5769 - acc: 0.4527 - val_loss: 1.9333 - val_acc: 0.3670\n",
      "Epoch 122/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5869 - acc: 0.4441 - val_loss: 1.8616 - val_acc: 0.3650\n",
      "Epoch 123/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5804 - acc: 0.4504 - val_loss: 1.9397 - val_acc: 0.3505\n",
      "Epoch 124/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.5806 - acc: 0.4549 - val_loss: 1.9403 - val_acc: 0.3540\n",
      "Epoch 125/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.5722 - acc: 0.4506 - val_loss: 1.8922 - val_acc: 0.3715\n",
      "Epoch 126/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5730 - acc: 0.4609 - val_loss: 1.9964 - val_acc: 0.3280\n",
      "Epoch 127/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5798 - acc: 0.4502 - val_loss: 1.8899 - val_acc: 0.3660\n",
      "Epoch 128/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.5726 - acc: 0.4550 - val_loss: 1.9653 - val_acc: 0.3485\n",
      "Epoch 129/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.5738 - acc: 0.4521 - val_loss: 1.8982 - val_acc: 0.3575\n",
      "Epoch 130/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.5771 - acc: 0.4502 - val_loss: 1.9437 - val_acc: 0.3355\n",
      "Epoch 131/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.5681 - acc: 0.4583 - val_loss: 1.8487 - val_acc: 0.3825\n",
      "Epoch 132/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5683 - acc: 0.4534 - val_loss: 1.8507 - val_acc: 0.3755\n",
      "Epoch 133/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5706 - acc: 0.4475 - val_loss: 1.9539 - val_acc: 0.3455\n",
      "Epoch 134/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5640 - acc: 0.4524 - val_loss: 1.9061 - val_acc: 0.3485\n",
      "Epoch 135/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5702 - acc: 0.4549 - val_loss: 1.9195 - val_acc: 0.3740\n",
      "Epoch 136/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5626 - acc: 0.4562 - val_loss: 1.9013 - val_acc: 0.3635\n",
      "Epoch 137/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5603 - acc: 0.4606 - val_loss: 1.9157 - val_acc: 0.3605\n",
      "Epoch 138/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5700 - acc: 0.4542 - val_loss: 1.8791 - val_acc: 0.3625\n",
      "Epoch 139/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.5648 - acc: 0.4489 - val_loss: 1.9444 - val_acc: 0.3590\n",
      "Epoch 140/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.5618 - acc: 0.4544 - val_loss: 1.9030 - val_acc: 0.3535\n",
      "Epoch 141/2000\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 1.5616 - acc: 0.4600 - val_loss: 2.0377 - val_acc: 0.3315\n",
      "Epoch 142/2000\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 1.5651 - acc: 0.4631 - val_loss: 2.0389 - val_acc: 0.3250\n",
      "Epoch 143/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5567 - acc: 0.4616 - val_loss: 1.8547 - val_acc: 0.3600\n",
      "Epoch 144/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.5563 - acc: 0.4597 - val_loss: 1.9774 - val_acc: 0.3385\n",
      "Epoch 145/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5556 - acc: 0.4619 - val_loss: 1.9048 - val_acc: 0.3815\n",
      "Epoch 146/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5590 - acc: 0.4650 - val_loss: 1.9035 - val_acc: 0.3580\n",
      "Epoch 147/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5625 - acc: 0.4595 - val_loss: 1.8817 - val_acc: 0.3620\n",
      "Epoch 148/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5509 - acc: 0.4575 - val_loss: 1.8964 - val_acc: 0.3595\n",
      "Epoch 149/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5525 - acc: 0.4639 - val_loss: 1.9110 - val_acc: 0.3605\n",
      "Epoch 150/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.5549 - acc: 0.4636 - val_loss: 1.8969 - val_acc: 0.3545\n",
      "Epoch 151/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.5517 - acc: 0.4625 - val_loss: 2.0094 - val_acc: 0.3375\n",
      "Epoch 152/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5493 - acc: 0.4611 - val_loss: 1.9564 - val_acc: 0.3605\n",
      "Epoch 153/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.5460 - acc: 0.4654 - val_loss: 1.9831 - val_acc: 0.3330\n",
      "Epoch 154/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.5465 - acc: 0.4604 - val_loss: 1.9531 - val_acc: 0.3435\n",
      "Epoch 155/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.5489 - acc: 0.4676 - val_loss: 1.9710 - val_acc: 0.3495\n",
      "Epoch 156/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.5481 - acc: 0.4641 - val_loss: 1.9331 - val_acc: 0.3490\n",
      "Epoch 157/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5439 - acc: 0.4660 - val_loss: 1.9047 - val_acc: 0.3540\n",
      "Epoch 158/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5526 - acc: 0.4576 - val_loss: 1.9299 - val_acc: 0.3520\n",
      "Epoch 159/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5465 - acc: 0.4549 - val_loss: 1.9331 - val_acc: 0.3520\n",
      "Epoch 160/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.5456 - acc: 0.4571 - val_loss: 1.9958 - val_acc: 0.3315\n",
      "Epoch 161/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.5520 - acc: 0.4646 - val_loss: 1.9069 - val_acc: 0.3610\n",
      "Epoch 162/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.5426 - acc: 0.4602 - val_loss: 1.9621 - val_acc: 0.3320\n",
      "Epoch 163/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.5463 - acc: 0.4656 - val_loss: 1.8840 - val_acc: 0.3540\n",
      "Epoch 164/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5394 - acc: 0.4606 - val_loss: 1.8901 - val_acc: 0.3570\n",
      "Epoch 165/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5363 - acc: 0.4641 - val_loss: 1.8755 - val_acc: 0.3645\n",
      "Epoch 166/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5393 - acc: 0.4627 - val_loss: 1.8991 - val_acc: 0.3695\n",
      "Epoch 167/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5425 - acc: 0.4723 - val_loss: 1.9482 - val_acc: 0.3540\n",
      "Epoch 168/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5393 - acc: 0.4705 - val_loss: 1.8987 - val_acc: 0.3515\n",
      "Epoch 169/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.5421 - acc: 0.4650 - val_loss: 1.9039 - val_acc: 0.3565\n",
      "Epoch 170/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5351 - acc: 0.4688 - val_loss: 2.0122 - val_acc: 0.3425\n",
      "Epoch 171/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5350 - acc: 0.4664 - val_loss: 1.9677 - val_acc: 0.3365\n",
      "Epoch 172/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5328 - acc: 0.4644 - val_loss: 1.8859 - val_acc: 0.3740\n",
      "Epoch 173/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.5299 - acc: 0.4704 - val_loss: 1.9800 - val_acc: 0.3440\n",
      "Epoch 174/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5377 - acc: 0.4676 - val_loss: 1.8876 - val_acc: 0.3600\n",
      "Epoch 175/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5252 - acc: 0.4702 - val_loss: 1.9100 - val_acc: 0.3545\n",
      "Epoch 176/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5305 - acc: 0.4713 - val_loss: 1.8855 - val_acc: 0.3635\n",
      "Epoch 177/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5241 - acc: 0.4710 - val_loss: 1.9822 - val_acc: 0.3440\n",
      "Epoch 178/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5274 - acc: 0.4718 - val_loss: 1.9250 - val_acc: 0.3640\n",
      "Epoch 179/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5214 - acc: 0.4688 - val_loss: 1.9744 - val_acc: 0.3485\n",
      "Epoch 180/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5278 - acc: 0.4744 - val_loss: 1.9119 - val_acc: 0.3480\n",
      "Epoch 181/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5369 - acc: 0.4654 - val_loss: 1.9401 - val_acc: 0.3500\n",
      "Epoch 182/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.5175 - acc: 0.4756 - val_loss: 1.9467 - val_acc: 0.3520\n",
      "Epoch 183/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.5099 - acc: 0.4786 - val_loss: 1.9531 - val_acc: 0.3575\n",
      "Epoch 184/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5283 - acc: 0.4695 - val_loss: 1.9564 - val_acc: 0.3495\n",
      "Epoch 185/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5161 - acc: 0.4795 - val_loss: 1.9798 - val_acc: 0.3445\n",
      "Epoch 186/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5152 - acc: 0.4789 - val_loss: 1.9636 - val_acc: 0.3395\n",
      "Epoch 187/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5193 - acc: 0.4754 - val_loss: 1.9203 - val_acc: 0.3720\n",
      "Epoch 188/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5103 - acc: 0.4769 - val_loss: 1.8850 - val_acc: 0.3695\n",
      "Epoch 189/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5182 - acc: 0.4734 - val_loss: 1.9105 - val_acc: 0.3520\n",
      "Epoch 190/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5200 - acc: 0.4765 - val_loss: 1.9185 - val_acc: 0.3655\n",
      "Epoch 191/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.5139 - acc: 0.4761 - val_loss: 1.9720 - val_acc: 0.3560\n",
      "Epoch 192/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5181 - acc: 0.4695 - val_loss: 1.9818 - val_acc: 0.3485\n",
      "Epoch 193/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.5194 - acc: 0.4711 - val_loss: 1.9439 - val_acc: 0.3485\n",
      "Epoch 194/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5180 - acc: 0.4700 - val_loss: 1.9732 - val_acc: 0.3510\n",
      "Epoch 195/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5174 - acc: 0.4760 - val_loss: 1.9492 - val_acc: 0.3395\n",
      "Epoch 196/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.5114 - acc: 0.4800 - val_loss: 1.9705 - val_acc: 0.3355\n",
      "Epoch 197/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5028 - acc: 0.4824 - val_loss: 2.0000 - val_acc: 0.3510\n",
      "Epoch 198/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.5178 - acc: 0.4718 - val_loss: 1.9104 - val_acc: 0.3635\n",
      "Epoch 199/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.5149 - acc: 0.4746 - val_loss: 1.9319 - val_acc: 0.3540\n",
      "Epoch 200/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.5133 - acc: 0.4729 - val_loss: 1.9248 - val_acc: 0.3580\n",
      "Epoch 201/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.5152 - acc: 0.4743 - val_loss: 1.9024 - val_acc: 0.3615\n",
      "Epoch 202/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5065 - acc: 0.4725 - val_loss: 1.8962 - val_acc: 0.3740\n",
      "Epoch 203/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5111 - acc: 0.4720 - val_loss: 2.0854 - val_acc: 0.3260\n",
      "Epoch 204/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.5027 - acc: 0.4799 - val_loss: 1.9300 - val_acc: 0.3680\n",
      "Epoch 205/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.4957 - acc: 0.4805 - val_loss: 1.9258 - val_acc: 0.3600\n",
      "Epoch 206/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4975 - acc: 0.4843 - val_loss: 1.9791 - val_acc: 0.3440\n",
      "Epoch 207/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.5132 - acc: 0.4773 - val_loss: 1.9361 - val_acc: 0.3655\n",
      "Epoch 208/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.5123 - acc: 0.4764 - val_loss: 1.8998 - val_acc: 0.3535\n",
      "Epoch 209/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4998 - acc: 0.4839 - val_loss: 2.0332 - val_acc: 0.3285\n",
      "Epoch 210/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4977 - acc: 0.4830 - val_loss: 1.9871 - val_acc: 0.3455\n",
      "Epoch 211/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4932 - acc: 0.4840 - val_loss: 1.9934 - val_acc: 0.3390\n",
      "Epoch 212/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4944 - acc: 0.4739 - val_loss: 1.9392 - val_acc: 0.3610\n",
      "Epoch 213/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4988 - acc: 0.4818 - val_loss: 2.0704 - val_acc: 0.3175\n",
      "Epoch 214/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.5034 - acc: 0.4803 - val_loss: 1.9708 - val_acc: 0.3450\n",
      "Epoch 215/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4990 - acc: 0.4835 - val_loss: 1.9262 - val_acc: 0.3600\n",
      "Epoch 216/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5006 - acc: 0.4848 - val_loss: 1.9890 - val_acc: 0.3435\n",
      "Epoch 217/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4953 - acc: 0.4854 - val_loss: 1.9348 - val_acc: 0.3585\n",
      "Epoch 218/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.5015 - acc: 0.4785 - val_loss: 1.9219 - val_acc: 0.3655\n",
      "Epoch 219/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4953 - acc: 0.4819 - val_loss: 1.9492 - val_acc: 0.3525\n",
      "Epoch 220/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4966 - acc: 0.4864 - val_loss: 2.0604 - val_acc: 0.3180\n",
      "Epoch 221/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.4993 - acc: 0.4808 - val_loss: 1.9833 - val_acc: 0.3425\n",
      "Epoch 222/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4993 - acc: 0.4785 - val_loss: 2.1078 - val_acc: 0.3210\n",
      "Epoch 223/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4912 - acc: 0.4773 - val_loss: 1.9635 - val_acc: 0.3380\n",
      "Epoch 224/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4861 - acc: 0.4860 - val_loss: 1.9825 - val_acc: 0.3525\n",
      "Epoch 225/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4940 - acc: 0.4863 - val_loss: 1.9187 - val_acc: 0.3635\n",
      "Epoch 226/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4910 - acc: 0.4851 - val_loss: 1.9410 - val_acc: 0.3555\n",
      "Epoch 227/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4875 - acc: 0.4844 - val_loss: 1.9186 - val_acc: 0.3535\n",
      "Epoch 228/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4837 - acc: 0.4895 - val_loss: 1.9883 - val_acc: 0.3415\n",
      "Epoch 229/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4837 - acc: 0.4895 - val_loss: 1.8964 - val_acc: 0.3590\n",
      "Epoch 230/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4953 - acc: 0.4863 - val_loss: 1.9996 - val_acc: 0.3420\n",
      "Epoch 231/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4899 - acc: 0.4822 - val_loss: 1.9843 - val_acc: 0.3445\n",
      "Epoch 232/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4960 - acc: 0.4783 - val_loss: 2.0036 - val_acc: 0.3365\n",
      "Epoch 233/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4781 - acc: 0.4854 - val_loss: 1.9728 - val_acc: 0.3575\n",
      "Epoch 234/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4848 - acc: 0.4870 - val_loss: 1.9796 - val_acc: 0.3550\n",
      "Epoch 235/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4757 - acc: 0.4880 - val_loss: 2.0934 - val_acc: 0.3120\n",
      "Epoch 236/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4862 - acc: 0.4806 - val_loss: 1.9754 - val_acc: 0.3500\n",
      "Epoch 237/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4816 - acc: 0.4834 - val_loss: 2.0079 - val_acc: 0.3415\n",
      "Epoch 238/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4775 - acc: 0.4890 - val_loss: 1.9388 - val_acc: 0.3645\n",
      "Epoch 239/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4884 - acc: 0.4775 - val_loss: 2.0760 - val_acc: 0.3240\n",
      "Epoch 240/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4818 - acc: 0.4873 - val_loss: 2.0012 - val_acc: 0.3360\n",
      "Epoch 241/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4860 - acc: 0.4827 - val_loss: 1.9593 - val_acc: 0.3600\n",
      "Epoch 242/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4751 - acc: 0.4875 - val_loss: 2.0525 - val_acc: 0.3370\n",
      "Epoch 243/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4858 - acc: 0.4864 - val_loss: 1.9840 - val_acc: 0.3465\n",
      "Epoch 244/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4772 - acc: 0.4939 - val_loss: 1.9761 - val_acc: 0.3530\n",
      "Epoch 245/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4844 - acc: 0.4933 - val_loss: 1.9226 - val_acc: 0.3670\n",
      "Epoch 246/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4793 - acc: 0.4854 - val_loss: 1.9865 - val_acc: 0.3530\n",
      "Epoch 247/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4667 - acc: 0.4954 - val_loss: 1.9858 - val_acc: 0.3520\n",
      "Epoch 248/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4688 - acc: 0.4885 - val_loss: 1.9988 - val_acc: 0.3420\n",
      "Epoch 249/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4767 - acc: 0.4886 - val_loss: 1.9919 - val_acc: 0.3465\n",
      "Epoch 250/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4722 - acc: 0.4898 - val_loss: 1.9756 - val_acc: 0.3490\n",
      "Epoch 251/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4680 - acc: 0.4912 - val_loss: 1.9723 - val_acc: 0.3490\n",
      "Epoch 252/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4754 - acc: 0.4974 - val_loss: 1.9990 - val_acc: 0.3510\n",
      "Epoch 253/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4702 - acc: 0.4901 - val_loss: 1.9797 - val_acc: 0.3585\n",
      "Epoch 254/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4653 - acc: 0.4911 - val_loss: 2.1604 - val_acc: 0.3250\n",
      "Epoch 255/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4665 - acc: 0.4970 - val_loss: 1.9970 - val_acc: 0.3600\n",
      "Epoch 256/2000\n",
      "8000/8000 [==============================] - 2s 253us/step - loss: 1.4696 - acc: 0.4904 - val_loss: 2.0547 - val_acc: 0.3525\n",
      "Epoch 257/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.4683 - acc: 0.4904 - val_loss: 2.0248 - val_acc: 0.3505\n",
      "Epoch 258/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4652 - acc: 0.4947 - val_loss: 1.9639 - val_acc: 0.3605\n",
      "Epoch 259/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.4720 - acc: 0.4931 - val_loss: 1.9868 - val_acc: 0.3540\n",
      "Epoch 260/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4692 - acc: 0.4861 - val_loss: 2.0471 - val_acc: 0.3380\n",
      "Epoch 261/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4675 - acc: 0.4915 - val_loss: 2.0409 - val_acc: 0.3450\n",
      "Epoch 262/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4613 - acc: 0.4909 - val_loss: 2.2580 - val_acc: 0.3125\n",
      "Epoch 263/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4792 - acc: 0.4885 - val_loss: 1.9838 - val_acc: 0.3510\n",
      "Epoch 264/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4624 - acc: 0.4926 - val_loss: 1.9772 - val_acc: 0.3625\n",
      "Epoch 265/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.4671 - acc: 0.4841 - val_loss: 1.9253 - val_acc: 0.3575\n",
      "Epoch 266/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 1.4605 - acc: 0.4975 - val_loss: 1.9988 - val_acc: 0.3510\n",
      "Epoch 267/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.4604 - acc: 0.4863 - val_loss: 1.9674 - val_acc: 0.3440\n",
      "Epoch 268/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.4621 - acc: 0.4901 - val_loss: 1.9687 - val_acc: 0.3645\n",
      "Epoch 269/2000\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 1.4681 - acc: 0.4910 - val_loss: 1.9933 - val_acc: 0.3465\n",
      "Epoch 270/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4712 - acc: 0.4794 - val_loss: 1.9984 - val_acc: 0.3485\n",
      "Epoch 271/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4739 - acc: 0.4845 - val_loss: 1.9670 - val_acc: 0.3515\n",
      "Epoch 272/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4721 - acc: 0.4966 - val_loss: 1.9870 - val_acc: 0.3420\n",
      "Epoch 273/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4564 - acc: 0.4970 - val_loss: 2.0417 - val_acc: 0.3380\n",
      "Epoch 274/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4655 - acc: 0.4956 - val_loss: 1.9849 - val_acc: 0.3505\n",
      "Epoch 275/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.4669 - acc: 0.4930 - val_loss: 1.9779 - val_acc: 0.3500\n",
      "Epoch 276/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4554 - acc: 0.4991 - val_loss: 1.9821 - val_acc: 0.3445\n",
      "Epoch 277/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4569 - acc: 0.4928 - val_loss: 1.9813 - val_acc: 0.3555\n",
      "Epoch 278/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4543 - acc: 0.4977 - val_loss: 1.9920 - val_acc: 0.3375\n",
      "Epoch 279/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4576 - acc: 0.4905 - val_loss: 2.0634 - val_acc: 0.3575\n",
      "Epoch 280/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4575 - acc: 0.4949 - val_loss: 1.9634 - val_acc: 0.3570\n",
      "Epoch 281/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4491 - acc: 0.4972 - val_loss: 2.0493 - val_acc: 0.3295\n",
      "Epoch 282/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4567 - acc: 0.4926 - val_loss: 2.0671 - val_acc: 0.3405\n",
      "Epoch 283/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4434 - acc: 0.5076 - val_loss: 2.0217 - val_acc: 0.3385\n",
      "Epoch 284/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4560 - acc: 0.5050 - val_loss: 2.0481 - val_acc: 0.3480\n",
      "Epoch 285/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4524 - acc: 0.4985 - val_loss: 2.0253 - val_acc: 0.3530\n",
      "Epoch 286/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4522 - acc: 0.4926 - val_loss: 2.0811 - val_acc: 0.3390\n",
      "Epoch 287/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4453 - acc: 0.5021 - val_loss: 2.0150 - val_acc: 0.3400\n",
      "Epoch 288/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4483 - acc: 0.5001 - val_loss: 2.0586 - val_acc: 0.3305\n",
      "Epoch 289/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4509 - acc: 0.5008 - val_loss: 2.0361 - val_acc: 0.3465\n",
      "Epoch 290/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4596 - acc: 0.4989 - val_loss: 2.1533 - val_acc: 0.3195\n",
      "Epoch 291/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4479 - acc: 0.4976 - val_loss: 2.0816 - val_acc: 0.3315\n",
      "Epoch 292/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4519 - acc: 0.4954 - val_loss: 2.0436 - val_acc: 0.3425\n",
      "Epoch 293/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4532 - acc: 0.4929 - val_loss: 1.9744 - val_acc: 0.3600\n",
      "Epoch 294/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4559 - acc: 0.4986 - val_loss: 2.0707 - val_acc: 0.3310\n",
      "Epoch 295/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4424 - acc: 0.4989 - val_loss: 2.0461 - val_acc: 0.3270\n",
      "Epoch 296/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4405 - acc: 0.5015 - val_loss: 2.0526 - val_acc: 0.3330\n",
      "Epoch 297/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4431 - acc: 0.4976 - val_loss: 2.0983 - val_acc: 0.3425\n",
      "Epoch 298/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4497 - acc: 0.5052 - val_loss: 1.9793 - val_acc: 0.3515\n",
      "Epoch 299/2000\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 1.4410 - acc: 0.5042 - val_loss: 2.0630 - val_acc: 0.3380\n",
      "Epoch 300/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4449 - acc: 0.5004 - val_loss: 1.9646 - val_acc: 0.3570\n",
      "Epoch 301/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4530 - acc: 0.5014 - val_loss: 2.0553 - val_acc: 0.3250\n",
      "Epoch 302/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4407 - acc: 0.5076 - val_loss: 2.1221 - val_acc: 0.3310\n",
      "Epoch 303/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.4445 - acc: 0.4961 - val_loss: 2.0376 - val_acc: 0.3460\n",
      "Epoch 304/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4360 - acc: 0.4994 - val_loss: 2.2332 - val_acc: 0.3155\n",
      "Epoch 305/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4421 - acc: 0.5092 - val_loss: 2.1078 - val_acc: 0.3275\n",
      "Epoch 306/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4400 - acc: 0.5012 - val_loss: 1.9868 - val_acc: 0.3510\n",
      "Epoch 307/2000\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 1.4465 - acc: 0.5041 - val_loss: 2.0739 - val_acc: 0.3375\n",
      "Epoch 308/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.4305 - acc: 0.5048 - val_loss: 2.0452 - val_acc: 0.3365\n",
      "Epoch 309/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.4457 - acc: 0.4989 - val_loss: 2.1060 - val_acc: 0.3370\n",
      "Epoch 310/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4496 - acc: 0.4909 - val_loss: 2.0249 - val_acc: 0.3470\n",
      "Epoch 311/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.4333 - acc: 0.5039 - val_loss: 2.0971 - val_acc: 0.3440\n",
      "Epoch 312/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.4337 - acc: 0.5014 - val_loss: 2.1174 - val_acc: 0.3480\n",
      "Epoch 313/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.4352 - acc: 0.5002 - val_loss: 2.0003 - val_acc: 0.3375\n",
      "Epoch 314/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.4401 - acc: 0.5015 - val_loss: 2.0188 - val_acc: 0.3335\n",
      "Epoch 315/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.4414 - acc: 0.5039 - val_loss: 1.9904 - val_acc: 0.3515\n",
      "Epoch 316/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.4399 - acc: 0.5068 - val_loss: 2.0410 - val_acc: 0.3370\n",
      "Epoch 317/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4385 - acc: 0.5008 - val_loss: 2.1204 - val_acc: 0.3310\n",
      "Epoch 318/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4321 - acc: 0.5065 - val_loss: 2.1270 - val_acc: 0.3310\n",
      "Epoch 319/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4357 - acc: 0.5080 - val_loss: 2.0549 - val_acc: 0.3410\n",
      "Epoch 320/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4371 - acc: 0.5002 - val_loss: 2.0712 - val_acc: 0.3340\n",
      "Epoch 321/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4393 - acc: 0.5022 - val_loss: 2.0256 - val_acc: 0.3380\n",
      "Epoch 322/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.4282 - acc: 0.5080 - val_loss: 2.0097 - val_acc: 0.3580\n",
      "Epoch 323/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4315 - acc: 0.5072 - val_loss: 2.0493 - val_acc: 0.3420\n",
      "Epoch 324/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4352 - acc: 0.5054 - val_loss: 1.9925 - val_acc: 0.3440\n",
      "Epoch 325/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4345 - acc: 0.4976 - val_loss: 2.0416 - val_acc: 0.3435\n",
      "Epoch 326/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4314 - acc: 0.5054 - val_loss: 2.0709 - val_acc: 0.3365\n",
      "Epoch 327/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4264 - acc: 0.5076 - val_loss: 2.1118 - val_acc: 0.3365\n",
      "Epoch 328/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4271 - acc: 0.5098 - val_loss: 2.1323 - val_acc: 0.3395\n",
      "Epoch 329/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4293 - acc: 0.5075 - val_loss: 2.0335 - val_acc: 0.3475\n",
      "Epoch 330/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4376 - acc: 0.5046 - val_loss: 2.0720 - val_acc: 0.3445\n",
      "Epoch 331/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4452 - acc: 0.5052 - val_loss: 2.1488 - val_acc: 0.3170\n",
      "Epoch 332/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4219 - acc: 0.5054 - val_loss: 2.0217 - val_acc: 0.3500\n",
      "Epoch 333/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4294 - acc: 0.5059 - val_loss: 2.0463 - val_acc: 0.3310\n",
      "Epoch 334/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4373 - acc: 0.5019 - val_loss: 2.0241 - val_acc: 0.3435\n",
      "Epoch 335/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4261 - acc: 0.5103 - val_loss: 2.1583 - val_acc: 0.3385\n",
      "Epoch 336/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4293 - acc: 0.5031 - val_loss: 2.0066 - val_acc: 0.3555\n",
      "Epoch 337/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4231 - acc: 0.5076 - val_loss: 2.1577 - val_acc: 0.3200\n",
      "Epoch 338/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4232 - acc: 0.5086 - val_loss: 1.9997 - val_acc: 0.3590\n",
      "Epoch 339/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4252 - acc: 0.5076 - val_loss: 2.0453 - val_acc: 0.3410\n",
      "Epoch 340/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4183 - acc: 0.5119 - val_loss: 2.2437 - val_acc: 0.3040\n",
      "Epoch 341/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4229 - acc: 0.5119 - val_loss: 2.0354 - val_acc: 0.3460\n",
      "Epoch 342/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4204 - acc: 0.5099 - val_loss: 2.0732 - val_acc: 0.3395\n",
      "Epoch 343/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4266 - acc: 0.5011 - val_loss: 2.1107 - val_acc: 0.3390\n",
      "Epoch 344/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4173 - acc: 0.5094 - val_loss: 2.1305 - val_acc: 0.3265\n",
      "Epoch 345/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4181 - acc: 0.5075 - val_loss: 2.0025 - val_acc: 0.3435\n",
      "Epoch 346/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4174 - acc: 0.5090 - val_loss: 2.2816 - val_acc: 0.3050\n",
      "Epoch 347/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.4231 - acc: 0.5051 - val_loss: 2.0365 - val_acc: 0.3385\n",
      "Epoch 348/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.4333 - acc: 0.5052 - val_loss: 2.0531 - val_acc: 0.3510\n",
      "Epoch 349/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.4210 - acc: 0.5081 - val_loss: 2.0038 - val_acc: 0.3520\n",
      "Epoch 350/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 230us/step - loss: 1.4234 - acc: 0.5109 - val_loss: 2.2539 - val_acc: 0.3095\n",
      "Epoch 351/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4263 - acc: 0.5028 - val_loss: 2.0033 - val_acc: 0.3530\n",
      "Epoch 352/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4296 - acc: 0.5074 - val_loss: 2.0785 - val_acc: 0.3405\n",
      "Epoch 353/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4281 - acc: 0.5142 - val_loss: 1.9978 - val_acc: 0.3475\n",
      "Epoch 354/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4192 - acc: 0.5154 - val_loss: 2.1611 - val_acc: 0.3380\n",
      "Epoch 355/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4195 - acc: 0.5108 - val_loss: 2.0355 - val_acc: 0.3515\n",
      "Epoch 356/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4162 - acc: 0.5121 - val_loss: 2.1395 - val_acc: 0.3375\n",
      "Epoch 357/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4151 - acc: 0.5118 - val_loss: 2.0530 - val_acc: 0.3450\n",
      "Epoch 358/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4107 - acc: 0.5131 - val_loss: 2.0674 - val_acc: 0.3395\n",
      "Epoch 359/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4208 - acc: 0.5084 - val_loss: 2.0261 - val_acc: 0.3465\n",
      "Epoch 360/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.4160 - acc: 0.5108 - val_loss: 2.0466 - val_acc: 0.3540\n",
      "Epoch 361/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4176 - acc: 0.5065 - val_loss: 2.1032 - val_acc: 0.3270\n",
      "Epoch 362/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4173 - acc: 0.5074 - val_loss: 2.1049 - val_acc: 0.3390\n",
      "Epoch 363/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4219 - acc: 0.5145 - val_loss: 2.0129 - val_acc: 0.3525\n",
      "Epoch 364/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4213 - acc: 0.5109 - val_loss: 2.1514 - val_acc: 0.3095\n",
      "Epoch 365/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4120 - acc: 0.5049 - val_loss: 2.1350 - val_acc: 0.3190\n",
      "Epoch 366/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4024 - acc: 0.5130 - val_loss: 2.0590 - val_acc: 0.3300\n",
      "Epoch 367/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.4146 - acc: 0.5152 - val_loss: 2.0299 - val_acc: 0.3435\n",
      "Epoch 368/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.4034 - acc: 0.5204 - val_loss: 2.0022 - val_acc: 0.3525\n",
      "Epoch 369/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4099 - acc: 0.5114 - val_loss: 2.0172 - val_acc: 0.3515\n",
      "Epoch 370/2000\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 1.4090 - acc: 0.5147 - val_loss: 2.0433 - val_acc: 0.3435\n",
      "Epoch 371/2000\n",
      "8000/8000 [==============================] - 2s 243us/step - loss: 1.4167 - acc: 0.5103 - val_loss: 2.0701 - val_acc: 0.3420\n",
      "Epoch 372/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.4132 - acc: 0.5124 - val_loss: 2.0256 - val_acc: 0.3565\n",
      "Epoch 373/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4128 - acc: 0.5140 - val_loss: 2.0950 - val_acc: 0.3320\n",
      "Epoch 374/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.4148 - acc: 0.5112 - val_loss: 2.0518 - val_acc: 0.3400\n",
      "Epoch 375/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4114 - acc: 0.5160 - val_loss: 2.1524 - val_acc: 0.3250\n",
      "Epoch 376/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.4023 - acc: 0.5134 - val_loss: 2.0433 - val_acc: 0.3500\n",
      "Epoch 377/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.4091 - acc: 0.5111 - val_loss: 2.0471 - val_acc: 0.3460\n",
      "Epoch 378/2000\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 1.4116 - acc: 0.5125 - val_loss: 2.1642 - val_acc: 0.3355\n",
      "Epoch 379/2000\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: 1.4163 - acc: 0.5166 - val_loss: 2.0420 - val_acc: 0.3410\n",
      "Epoch 380/2000\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 1.4077 - acc: 0.5119 - val_loss: 2.2041 - val_acc: 0.3160\n",
      "Epoch 381/2000\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 1.4164 - acc: 0.5080 - val_loss: 2.0913 - val_acc: 0.3450\n",
      "Epoch 382/2000\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 1.4164 - acc: 0.5125 - val_loss: 2.0611 - val_acc: 0.3430\n",
      "Epoch 383/2000\n",
      "8000/8000 [==============================] - 2s 261us/step - loss: 1.4224 - acc: 0.5101 - val_loss: 2.0024 - val_acc: 0.3620\n",
      "Epoch 384/2000\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 1.4097 - acc: 0.5095 - val_loss: 2.1386 - val_acc: 0.3310\n",
      "Epoch 385/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.4052 - acc: 0.5130 - val_loss: 2.1784 - val_acc: 0.3170\n",
      "Epoch 386/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3929 - acc: 0.5191 - val_loss: 1.9951 - val_acc: 0.3615\n",
      "Epoch 387/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.4012 - acc: 0.5159 - val_loss: 2.0078 - val_acc: 0.3665\n",
      "Epoch 388/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.4105 - acc: 0.5118 - val_loss: 2.1350 - val_acc: 0.3395\n",
      "Epoch 389/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.4073 - acc: 0.5163 - val_loss: 2.0349 - val_acc: 0.3440\n",
      "Epoch 390/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.4078 - acc: 0.5069 - val_loss: 2.2477 - val_acc: 0.3060\n",
      "Epoch 391/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4045 - acc: 0.5108 - val_loss: 2.0776 - val_acc: 0.3360\n",
      "Epoch 392/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.4057 - acc: 0.5166 - val_loss: 2.0959 - val_acc: 0.3345\n",
      "Epoch 393/2000\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 1.4036 - acc: 0.5129 - val_loss: 2.1529 - val_acc: 0.3205\n",
      "Epoch 394/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.4024 - acc: 0.5120 - val_loss: 2.1185 - val_acc: 0.3325\n",
      "Epoch 395/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.4042 - acc: 0.5137 - val_loss: 2.0406 - val_acc: 0.3535\n",
      "Epoch 396/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4063 - acc: 0.5144 - val_loss: 2.1054 - val_acc: 0.3345\n",
      "Epoch 397/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4051 - acc: 0.5119 - val_loss: 2.2202 - val_acc: 0.3085\n",
      "Epoch 398/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.4002 - acc: 0.5158 - val_loss: 2.1316 - val_acc: 0.3285\n",
      "Epoch 399/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3991 - acc: 0.5109 - val_loss: 2.0585 - val_acc: 0.3380\n",
      "Epoch 400/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4026 - acc: 0.5191 - val_loss: 2.1321 - val_acc: 0.3380\n",
      "Epoch 401/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4036 - acc: 0.5159 - val_loss: 2.1064 - val_acc: 0.3375\n",
      "Epoch 402/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.4031 - acc: 0.5152 - val_loss: 2.0783 - val_acc: 0.3440\n",
      "Epoch 403/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3914 - acc: 0.5179 - val_loss: 2.1032 - val_acc: 0.3395\n",
      "Epoch 404/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3920 - acc: 0.5159 - val_loss: 2.0844 - val_acc: 0.3410\n",
      "Epoch 405/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.4044 - acc: 0.5114 - val_loss: 2.0627 - val_acc: 0.3500\n",
      "Epoch 406/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3959 - acc: 0.5147 - val_loss: 2.0704 - val_acc: 0.3570\n",
      "Epoch 407/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3878 - acc: 0.5212 - val_loss: 2.1126 - val_acc: 0.3375\n",
      "Epoch 408/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3939 - acc: 0.5186 - val_loss: 2.0844 - val_acc: 0.3260\n",
      "Epoch 409/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3958 - acc: 0.5166 - val_loss: 2.1866 - val_acc: 0.3205\n",
      "Epoch 410/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3983 - acc: 0.5197 - val_loss: 2.1633 - val_acc: 0.3180\n",
      "Epoch 411/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3996 - acc: 0.5119 - val_loss: 2.0783 - val_acc: 0.3480\n",
      "Epoch 412/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.4036 - acc: 0.5186 - val_loss: 2.0305 - val_acc: 0.3575\n",
      "Epoch 413/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.4045 - acc: 0.5121 - val_loss: 2.0772 - val_acc: 0.3430\n",
      "Epoch 414/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3863 - acc: 0.5242 - val_loss: 2.0881 - val_acc: 0.3350\n",
      "Epoch 415/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3858 - acc: 0.5181 - val_loss: 2.1681 - val_acc: 0.3290\n",
      "Epoch 416/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3911 - acc: 0.5180 - val_loss: 2.1383 - val_acc: 0.3335\n",
      "Epoch 417/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3908 - acc: 0.5224 - val_loss: 2.0818 - val_acc: 0.3400\n",
      "Epoch 418/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3945 - acc: 0.5221 - val_loss: 2.2899 - val_acc: 0.3095\n",
      "Epoch 419/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3894 - acc: 0.5163 - val_loss: 2.0850 - val_acc: 0.3365\n",
      "Epoch 420/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3886 - acc: 0.5185 - val_loss: 2.0826 - val_acc: 0.3390\n",
      "Epoch 421/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3995 - acc: 0.5150 - val_loss: 2.1627 - val_acc: 0.3365\n",
      "Epoch 422/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3980 - acc: 0.5202 - val_loss: 2.0421 - val_acc: 0.3475\n",
      "Epoch 423/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3912 - acc: 0.5266 - val_loss: 2.0920 - val_acc: 0.3305\n",
      "Epoch 424/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3958 - acc: 0.5131 - val_loss: 2.1164 - val_acc: 0.3355\n",
      "Epoch 425/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3834 - acc: 0.5240 - val_loss: 2.3462 - val_acc: 0.3100\n",
      "Epoch 426/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3877 - acc: 0.5195 - val_loss: 2.1727 - val_acc: 0.3360\n",
      "Epoch 427/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3784 - acc: 0.5161 - val_loss: 2.1427 - val_acc: 0.3385\n",
      "Epoch 428/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3907 - acc: 0.5226 - val_loss: 2.1629 - val_acc: 0.3220\n",
      "Epoch 429/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3871 - acc: 0.5220 - val_loss: 2.1030 - val_acc: 0.3335\n",
      "Epoch 430/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3858 - acc: 0.5181 - val_loss: 2.1193 - val_acc: 0.3270\n",
      "Epoch 431/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3748 - acc: 0.5276 - val_loss: 2.1272 - val_acc: 0.3370\n",
      "Epoch 432/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3798 - acc: 0.5223 - val_loss: 2.1397 - val_acc: 0.3375\n",
      "Epoch 433/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3917 - acc: 0.5235 - val_loss: 2.0617 - val_acc: 0.3385\n",
      "Epoch 434/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3815 - acc: 0.5207 - val_loss: 2.0875 - val_acc: 0.3445\n",
      "Epoch 435/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3844 - acc: 0.5214 - val_loss: 2.2730 - val_acc: 0.3225\n",
      "Epoch 436/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3891 - acc: 0.5274 - val_loss: 2.1316 - val_acc: 0.3360\n",
      "Epoch 437/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3824 - acc: 0.5165 - val_loss: 2.0999 - val_acc: 0.3380\n",
      "Epoch 438/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3714 - acc: 0.5214 - val_loss: 2.3600 - val_acc: 0.3030\n",
      "Epoch 439/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.3876 - acc: 0.5204 - val_loss: 2.1276 - val_acc: 0.3350\n",
      "Epoch 440/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3792 - acc: 0.5266 - val_loss: 2.3752 - val_acc: 0.3055\n",
      "Epoch 441/2000\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 1.3858 - acc: 0.5179 - val_loss: 2.1304 - val_acc: 0.3265\n",
      "Epoch 442/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.3842 - acc: 0.5274 - val_loss: 2.0803 - val_acc: 0.3520\n",
      "Epoch 443/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3867 - acc: 0.5254 - val_loss: 2.1876 - val_acc: 0.3270\n",
      "Epoch 444/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3938 - acc: 0.5200 - val_loss: 2.1595 - val_acc: 0.3250\n",
      "Epoch 445/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3735 - acc: 0.5267 - val_loss: 2.1120 - val_acc: 0.3410\n",
      "Epoch 446/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3794 - acc: 0.5234 - val_loss: 2.1234 - val_acc: 0.3200\n",
      "Epoch 447/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3755 - acc: 0.5241 - val_loss: 2.1213 - val_acc: 0.3305\n",
      "Epoch 448/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3703 - acc: 0.5288 - val_loss: 2.0605 - val_acc: 0.3425\n",
      "Epoch 449/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3783 - acc: 0.5216 - val_loss: 2.0890 - val_acc: 0.3315\n",
      "Epoch 450/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3743 - acc: 0.5232 - val_loss: 2.0850 - val_acc: 0.3460\n",
      "Epoch 451/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3862 - acc: 0.5151 - val_loss: 2.1493 - val_acc: 0.3390\n",
      "Epoch 452/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3723 - acc: 0.5217 - val_loss: 2.1695 - val_acc: 0.3320\n",
      "Epoch 453/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3705 - acc: 0.5272 - val_loss: 2.1369 - val_acc: 0.3290\n",
      "Epoch 454/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3811 - acc: 0.5159 - val_loss: 2.1200 - val_acc: 0.3320\n",
      "Epoch 455/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3731 - acc: 0.5315 - val_loss: 2.0812 - val_acc: 0.3370\n",
      "Epoch 456/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3641 - acc: 0.5336 - val_loss: 2.0990 - val_acc: 0.3345\n",
      "Epoch 457/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3680 - acc: 0.5236 - val_loss: 2.2567 - val_acc: 0.3195\n",
      "Epoch 458/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3680 - acc: 0.5240 - val_loss: 2.1005 - val_acc: 0.3355\n",
      "Epoch 459/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3673 - acc: 0.5235 - val_loss: 2.2341 - val_acc: 0.3080\n",
      "Epoch 460/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3752 - acc: 0.5226 - val_loss: 2.1419 - val_acc: 0.3240\n",
      "Epoch 461/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3725 - acc: 0.5211 - val_loss: 2.0848 - val_acc: 0.3355\n",
      "Epoch 462/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3660 - acc: 0.5248 - val_loss: 2.1662 - val_acc: 0.3325\n",
      "Epoch 463/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3856 - acc: 0.5194 - val_loss: 2.0853 - val_acc: 0.3455\n",
      "Epoch 464/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3694 - acc: 0.5284 - val_loss: 2.1677 - val_acc: 0.3295\n",
      "Epoch 465/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3707 - acc: 0.5269 - val_loss: 2.1551 - val_acc: 0.3340\n",
      "Epoch 466/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3735 - acc: 0.5186 - val_loss: 2.2642 - val_acc: 0.3030\n",
      "Epoch 467/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3744 - acc: 0.5280 - val_loss: 2.1274 - val_acc: 0.3430\n",
      "Epoch 468/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3841 - acc: 0.5160 - val_loss: 2.0516 - val_acc: 0.3445\n",
      "Epoch 469/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3564 - acc: 0.5276 - val_loss: 2.0809 - val_acc: 0.3435\n",
      "Epoch 470/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3716 - acc: 0.5285 - val_loss: 2.1116 - val_acc: 0.3340\n",
      "Epoch 471/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3646 - acc: 0.5336 - val_loss: 2.2252 - val_acc: 0.3190\n",
      "Epoch 472/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3615 - acc: 0.5272 - val_loss: 2.1335 - val_acc: 0.3420\n",
      "Epoch 473/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3643 - acc: 0.5351 - val_loss: 2.1906 - val_acc: 0.3335\n",
      "Epoch 474/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3684 - acc: 0.5224 - val_loss: 2.1667 - val_acc: 0.3305\n",
      "Epoch 475/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3661 - acc: 0.5283 - val_loss: 2.0673 - val_acc: 0.3515\n",
      "Epoch 476/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3580 - acc: 0.5272 - val_loss: 2.1956 - val_acc: 0.3265\n",
      "Epoch 477/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3642 - acc: 0.5341 - val_loss: 2.1292 - val_acc: 0.3315\n",
      "Epoch 478/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3635 - acc: 0.5269 - val_loss: 2.1970 - val_acc: 0.3345\n",
      "Epoch 479/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3723 - acc: 0.5261 - val_loss: 2.0668 - val_acc: 0.3480\n",
      "Epoch 480/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3623 - acc: 0.5358 - val_loss: 2.2111 - val_acc: 0.3140\n",
      "Epoch 481/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3765 - acc: 0.5271 - val_loss: 2.1332 - val_acc: 0.3430\n",
      "Epoch 482/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3674 - acc: 0.5314 - val_loss: 2.0600 - val_acc: 0.3480\n",
      "Epoch 483/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3530 - acc: 0.5280 - val_loss: 2.1473 - val_acc: 0.3320\n",
      "Epoch 484/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3619 - acc: 0.5264 - val_loss: 2.0949 - val_acc: 0.3400\n",
      "Epoch 485/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3632 - acc: 0.5244 - val_loss: 2.2035 - val_acc: 0.3360\n",
      "Epoch 486/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3728 - acc: 0.5246 - val_loss: 2.0720 - val_acc: 0.3405\n",
      "Epoch 487/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3572 - acc: 0.5283 - val_loss: 2.1258 - val_acc: 0.3410\n",
      "Epoch 488/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3649 - acc: 0.5271 - val_loss: 2.1337 - val_acc: 0.3320\n",
      "Epoch 489/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3666 - acc: 0.5274 - val_loss: 2.1730 - val_acc: 0.3240\n",
      "Epoch 490/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3642 - acc: 0.5317 - val_loss: 2.0963 - val_acc: 0.3415\n",
      "Epoch 491/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3639 - acc: 0.5320 - val_loss: 2.2348 - val_acc: 0.3270\n",
      "Epoch 492/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3669 - acc: 0.5279 - val_loss: 2.2067 - val_acc: 0.3255\n",
      "Epoch 493/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3613 - acc: 0.5271 - val_loss: 2.1510 - val_acc: 0.3390\n",
      "Epoch 494/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3660 - acc: 0.5302 - val_loss: 2.1335 - val_acc: 0.3325\n",
      "Epoch 495/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3630 - acc: 0.5276 - val_loss: 2.1531 - val_acc: 0.3390\n",
      "Epoch 496/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3661 - acc: 0.5251 - val_loss: 2.1520 - val_acc: 0.3295\n",
      "Epoch 497/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3618 - acc: 0.5250 - val_loss: 2.0859 - val_acc: 0.3375\n",
      "Epoch 498/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3644 - acc: 0.5295 - val_loss: 2.1583 - val_acc: 0.3435\n",
      "Epoch 499/2000\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 1.3589 - acc: 0.5335 - val_loss: 2.1036 - val_acc: 0.3295\n",
      "Epoch 500/2000\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 1.3606 - acc: 0.5295 - val_loss: 2.1973 - val_acc: 0.3385\n",
      "Epoch 501/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3565 - acc: 0.5306 - val_loss: 2.1524 - val_acc: 0.3390\n",
      "Epoch 502/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3536 - acc: 0.5356 - val_loss: 2.0728 - val_acc: 0.3575\n",
      "Epoch 503/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3574 - acc: 0.5342 - val_loss: 2.1883 - val_acc: 0.3285\n",
      "Epoch 504/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3598 - acc: 0.5291 - val_loss: 2.1412 - val_acc: 0.3490\n",
      "Epoch 505/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.3686 - acc: 0.5274 - val_loss: 2.2070 - val_acc: 0.3240\n",
      "Epoch 506/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3620 - acc: 0.5344 - val_loss: 2.1344 - val_acc: 0.3360\n",
      "Epoch 507/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3571 - acc: 0.5277 - val_loss: 2.1745 - val_acc: 0.3340\n",
      "Epoch 508/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3560 - acc: 0.5276 - val_loss: 2.1274 - val_acc: 0.3545\n",
      "Epoch 509/2000\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 1.3528 - acc: 0.5250 - val_loss: 2.3359 - val_acc: 0.3090\n",
      "Epoch 510/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.3538 - acc: 0.5331 - val_loss: 2.2682 - val_acc: 0.3230\n",
      "Epoch 511/2000\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 1.3583 - acc: 0.5304 - val_loss: 2.1284 - val_acc: 0.3295\n",
      "Epoch 512/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3533 - acc: 0.5325 - val_loss: 2.1663 - val_acc: 0.3305\n",
      "Epoch 513/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3522 - acc: 0.5358 - val_loss: 2.1163 - val_acc: 0.3335\n",
      "Epoch 514/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3523 - acc: 0.5391 - val_loss: 2.1399 - val_acc: 0.3355\n",
      "Epoch 515/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3486 - acc: 0.5391 - val_loss: 2.1327 - val_acc: 0.3285\n",
      "Epoch 516/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3605 - acc: 0.5245 - val_loss: 2.1656 - val_acc: 0.3160\n",
      "Epoch 517/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3527 - acc: 0.5302 - val_loss: 2.1398 - val_acc: 0.3285\n",
      "Epoch 518/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3555 - acc: 0.5301 - val_loss: 2.2067 - val_acc: 0.3150\n",
      "Epoch 519/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3509 - acc: 0.5389 - val_loss: 2.2778 - val_acc: 0.3185\n",
      "Epoch 520/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3543 - acc: 0.5302 - val_loss: 2.2334 - val_acc: 0.3345\n",
      "Epoch 521/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3541 - acc: 0.5330 - val_loss: 2.1544 - val_acc: 0.3450\n",
      "Epoch 522/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3469 - acc: 0.5309 - val_loss: 2.1280 - val_acc: 0.3405\n",
      "Epoch 523/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3557 - acc: 0.5308 - val_loss: 2.1808 - val_acc: 0.3285\n",
      "Epoch 524/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3482 - acc: 0.5340 - val_loss: 2.1998 - val_acc: 0.3115\n",
      "Epoch 525/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3482 - acc: 0.5309 - val_loss: 2.1236 - val_acc: 0.3475\n",
      "Epoch 526/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3508 - acc: 0.5261 - val_loss: 2.1664 - val_acc: 0.3245\n",
      "Epoch 527/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3555 - acc: 0.5286 - val_loss: 2.1013 - val_acc: 0.3380\n",
      "Epoch 528/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3499 - acc: 0.5389 - val_loss: 2.1403 - val_acc: 0.3480\n",
      "Epoch 529/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3479 - acc: 0.5337 - val_loss: 2.1680 - val_acc: 0.3295\n",
      "Epoch 530/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3489 - acc: 0.5353 - val_loss: 2.2404 - val_acc: 0.3120\n",
      "Epoch 531/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3508 - acc: 0.5335 - val_loss: 2.2023 - val_acc: 0.3320\n",
      "Epoch 532/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3383 - acc: 0.5349 - val_loss: 2.1403 - val_acc: 0.3330\n",
      "Epoch 533/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3503 - acc: 0.5345 - val_loss: 2.1956 - val_acc: 0.3355\n",
      "Epoch 534/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3522 - acc: 0.5379 - val_loss: 2.1983 - val_acc: 0.3340\n",
      "Epoch 535/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3411 - acc: 0.5385 - val_loss: 2.1741 - val_acc: 0.3340\n",
      "Epoch 536/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3462 - acc: 0.5405 - val_loss: 2.0914 - val_acc: 0.3445\n",
      "Epoch 537/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3459 - acc: 0.5364 - val_loss: 2.0894 - val_acc: 0.3385\n",
      "Epoch 538/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3441 - acc: 0.5405 - val_loss: 2.2251 - val_acc: 0.3235\n",
      "Epoch 539/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3400 - acc: 0.5413 - val_loss: 2.2094 - val_acc: 0.3295\n",
      "Epoch 540/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3544 - acc: 0.5304 - val_loss: 2.1827 - val_acc: 0.3315\n",
      "Epoch 541/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3484 - acc: 0.5337 - val_loss: 2.1865 - val_acc: 0.3355\n",
      "Epoch 542/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3452 - acc: 0.5306 - val_loss: 2.1238 - val_acc: 0.3435\n",
      "Epoch 543/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3528 - acc: 0.5298 - val_loss: 2.1050 - val_acc: 0.3365\n",
      "Epoch 544/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3435 - acc: 0.5377 - val_loss: 2.0642 - val_acc: 0.3645\n",
      "Epoch 545/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3392 - acc: 0.5364 - val_loss: 2.2103 - val_acc: 0.3290\n",
      "Epoch 546/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3365 - acc: 0.5373 - val_loss: 2.3193 - val_acc: 0.3245\n",
      "Epoch 547/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3491 - acc: 0.5377 - val_loss: 2.2006 - val_acc: 0.3405\n",
      "Epoch 548/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3425 - acc: 0.5390 - val_loss: 2.1403 - val_acc: 0.3230\n",
      "Epoch 549/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3457 - acc: 0.5365 - val_loss: 2.1851 - val_acc: 0.3290\n",
      "Epoch 550/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3502 - acc: 0.5327 - val_loss: 2.1966 - val_acc: 0.3245\n",
      "Epoch 551/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3361 - acc: 0.5373 - val_loss: 2.1734 - val_acc: 0.3230\n",
      "Epoch 552/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3426 - acc: 0.5435 - val_loss: 2.0943 - val_acc: 0.3485\n",
      "Epoch 553/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3393 - acc: 0.5428 - val_loss: 2.2499 - val_acc: 0.3120\n",
      "Epoch 554/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3356 - acc: 0.5413 - val_loss: 2.1626 - val_acc: 0.3270\n",
      "Epoch 555/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.3359 - acc: 0.5363 - val_loss: 2.1479 - val_acc: 0.3395\n",
      "Epoch 556/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.3402 - acc: 0.5383 - val_loss: 2.1046 - val_acc: 0.3455\n",
      "Epoch 557/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.3378 - acc: 0.5371 - val_loss: 2.1180 - val_acc: 0.3475\n",
      "Epoch 558/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3434 - acc: 0.5294 - val_loss: 2.3587 - val_acc: 0.3065\n",
      "Epoch 559/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3411 - acc: 0.5393 - val_loss: 2.1086 - val_acc: 0.3485\n",
      "Epoch 560/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.3366 - acc: 0.5393 - val_loss: 2.1641 - val_acc: 0.3325\n",
      "Epoch 561/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.3393 - acc: 0.5381 - val_loss: 2.1499 - val_acc: 0.3325\n",
      "Epoch 562/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3322 - acc: 0.5423 - val_loss: 2.1743 - val_acc: 0.3390\n",
      "Epoch 563/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3420 - acc: 0.5330 - val_loss: 2.1360 - val_acc: 0.3450\n",
      "Epoch 564/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3359 - acc: 0.5426 - val_loss: 2.1644 - val_acc: 0.3370\n",
      "Epoch 565/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3348 - acc: 0.5394 - val_loss: 2.2180 - val_acc: 0.3285\n",
      "Epoch 566/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.3350 - acc: 0.5387 - val_loss: 2.2316 - val_acc: 0.3300\n",
      "Epoch 567/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3347 - acc: 0.5399 - val_loss: 2.2885 - val_acc: 0.3265\n",
      "Epoch 568/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3346 - acc: 0.5360 - val_loss: 2.3316 - val_acc: 0.3130\n",
      "Epoch 569/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3319 - acc: 0.5364 - val_loss: 2.3152 - val_acc: 0.3275\n",
      "Epoch 570/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3277 - acc: 0.5358 - val_loss: 2.3127 - val_acc: 0.2980\n",
      "Epoch 571/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3388 - acc: 0.5396 - val_loss: 2.1639 - val_acc: 0.3450\n",
      "Epoch 572/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3349 - acc: 0.5429 - val_loss: 2.2067 - val_acc: 0.3350\n",
      "Epoch 573/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3365 - acc: 0.5373 - val_loss: 2.1585 - val_acc: 0.3385\n",
      "Epoch 574/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3322 - acc: 0.5426 - val_loss: 2.2837 - val_acc: 0.3265\n",
      "Epoch 575/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3347 - acc: 0.5406 - val_loss: 2.2215 - val_acc: 0.3315\n",
      "Epoch 576/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3323 - acc: 0.5446 - val_loss: 2.1307 - val_acc: 0.3340\n",
      "Epoch 577/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3326 - acc: 0.5426 - val_loss: 2.1668 - val_acc: 0.3185\n",
      "Epoch 578/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3301 - acc: 0.5400 - val_loss: 2.2339 - val_acc: 0.3380\n",
      "Epoch 579/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3437 - acc: 0.5390 - val_loss: 2.2216 - val_acc: 0.3300\n",
      "Epoch 580/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3293 - acc: 0.5333 - val_loss: 2.2046 - val_acc: 0.3265\n",
      "Epoch 581/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3395 - acc: 0.5379 - val_loss: 2.1792 - val_acc: 0.3355\n",
      "Epoch 582/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3334 - acc: 0.5404 - val_loss: 2.1812 - val_acc: 0.3335\n",
      "Epoch 583/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3210 - acc: 0.5402 - val_loss: 2.1685 - val_acc: 0.3355\n",
      "Epoch 584/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3294 - acc: 0.5410 - val_loss: 2.1574 - val_acc: 0.3350\n",
      "Epoch 585/2000\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 1.3260 - acc: 0.5435 - val_loss: 2.3266 - val_acc: 0.2990\n",
      "Epoch 586/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3230 - acc: 0.5478 - val_loss: 2.1580 - val_acc: 0.3265\n",
      "Epoch 587/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3282 - acc: 0.5455 - val_loss: 2.1789 - val_acc: 0.3350\n",
      "Epoch 588/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3301 - acc: 0.5387 - val_loss: 2.1193 - val_acc: 0.3385\n",
      "Epoch 589/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3273 - acc: 0.5461 - val_loss: 2.3477 - val_acc: 0.3130\n",
      "Epoch 590/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3325 - acc: 0.5425 - val_loss: 2.2343 - val_acc: 0.3175\n",
      "Epoch 591/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3217 - acc: 0.5389 - val_loss: 2.1116 - val_acc: 0.3380\n",
      "Epoch 592/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3218 - acc: 0.5443 - val_loss: 2.1794 - val_acc: 0.3260\n",
      "Epoch 593/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3301 - acc: 0.5383 - val_loss: 2.1784 - val_acc: 0.3295\n",
      "Epoch 594/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3289 - acc: 0.5421 - val_loss: 2.2201 - val_acc: 0.3435\n",
      "Epoch 595/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3383 - acc: 0.5364 - val_loss: 2.1125 - val_acc: 0.3385\n",
      "Epoch 596/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3204 - acc: 0.5449 - val_loss: 2.1823 - val_acc: 0.3255\n",
      "Epoch 597/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3307 - acc: 0.5400 - val_loss: 2.1981 - val_acc: 0.3380\n",
      "Epoch 598/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3350 - acc: 0.5423 - val_loss: 2.3636 - val_acc: 0.3165\n",
      "Epoch 599/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3330 - acc: 0.5356 - val_loss: 2.1655 - val_acc: 0.3330\n",
      "Epoch 600/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3275 - acc: 0.5396 - val_loss: 2.1056 - val_acc: 0.3500\n",
      "Epoch 601/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3243 - acc: 0.5389 - val_loss: 2.2585 - val_acc: 0.3245\n",
      "Epoch 602/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3303 - acc: 0.5377 - val_loss: 2.2539 - val_acc: 0.3190\n",
      "Epoch 603/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3267 - acc: 0.5381 - val_loss: 2.2101 - val_acc: 0.3240\n",
      "Epoch 604/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3343 - acc: 0.5326 - val_loss: 2.1730 - val_acc: 0.3330\n",
      "Epoch 605/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.3239 - acc: 0.5419 - val_loss: 2.2728 - val_acc: 0.3280\n",
      "Epoch 606/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3241 - acc: 0.5451 - val_loss: 2.1640 - val_acc: 0.3250\n",
      "Epoch 607/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3260 - acc: 0.5413 - val_loss: 2.1870 - val_acc: 0.3385\n",
      "Epoch 608/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3334 - acc: 0.5394 - val_loss: 2.1588 - val_acc: 0.3360\n",
      "Epoch 609/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3227 - acc: 0.5481 - val_loss: 2.1742 - val_acc: 0.3375\n",
      "Epoch 610/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3251 - acc: 0.5400 - val_loss: 2.2173 - val_acc: 0.3300\n",
      "Epoch 611/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3344 - acc: 0.5364 - val_loss: 2.2672 - val_acc: 0.3210\n",
      "Epoch 612/2000\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 1.3378 - acc: 0.5421 - val_loss: 2.2969 - val_acc: 0.3150\n",
      "Epoch 613/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3204 - acc: 0.5443 - val_loss: 2.1605 - val_acc: 0.3455\n",
      "Epoch 614/2000\n",
      "8000/8000 [==============================] - 2s 263us/step - loss: 1.3286 - acc: 0.5389 - val_loss: 2.1600 - val_acc: 0.3355\n",
      "Epoch 615/2000\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 1.3407 - acc: 0.5359 - val_loss: 2.2142 - val_acc: 0.3300\n",
      "Epoch 616/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3283 - acc: 0.5397 - val_loss: 2.1324 - val_acc: 0.3385\n",
      "Epoch 617/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3116 - acc: 0.5509 - val_loss: 2.1808 - val_acc: 0.3350\n",
      "Epoch 618/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3237 - acc: 0.5462 - val_loss: 2.1792 - val_acc: 0.3375\n",
      "Epoch 619/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3255 - acc: 0.5459 - val_loss: 2.1291 - val_acc: 0.3485\n",
      "Epoch 620/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3257 - acc: 0.5402 - val_loss: 2.2565 - val_acc: 0.3145\n",
      "Epoch 621/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.3260 - acc: 0.5369 - val_loss: 2.2003 - val_acc: 0.3320\n",
      "Epoch 622/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.3244 - acc: 0.5430 - val_loss: 2.2546 - val_acc: 0.3205\n",
      "Epoch 623/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.3213 - acc: 0.5445 - val_loss: 2.4099 - val_acc: 0.3140\n",
      "Epoch 624/2000\n",
      "8000/8000 [==============================] - 1s 145us/step - loss: 1.3317 - acc: 0.5431 - val_loss: 2.1966 - val_acc: 0.3300\n",
      "Epoch 625/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.3155 - acc: 0.5435 - val_loss: 2.1863 - val_acc: 0.3215\n",
      "Epoch 626/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3210 - acc: 0.5401 - val_loss: 2.1531 - val_acc: 0.3400\n",
      "Epoch 627/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3208 - acc: 0.5465 - val_loss: 2.2452 - val_acc: 0.3190\n",
      "Epoch 628/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3309 - acc: 0.5385 - val_loss: 2.1917 - val_acc: 0.3395\n",
      "Epoch 629/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3112 - acc: 0.5459 - val_loss: 2.2806 - val_acc: 0.3250\n",
      "Epoch 630/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3148 - acc: 0.5479 - val_loss: 2.2376 - val_acc: 0.3355\n",
      "Epoch 631/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3168 - acc: 0.5383 - val_loss: 2.2049 - val_acc: 0.3305\n",
      "Epoch 632/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3176 - acc: 0.5474 - val_loss: 2.2363 - val_acc: 0.3295\n",
      "Epoch 633/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3153 - acc: 0.5437 - val_loss: 2.1891 - val_acc: 0.3300\n",
      "Epoch 634/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3140 - acc: 0.5499 - val_loss: 2.2107 - val_acc: 0.3250\n",
      "Epoch 635/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3268 - acc: 0.5445 - val_loss: 2.1579 - val_acc: 0.3260\n",
      "Epoch 636/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3116 - acc: 0.5500 - val_loss: 2.4185 - val_acc: 0.2975\n",
      "Epoch 637/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3159 - acc: 0.5405 - val_loss: 2.4027 - val_acc: 0.2965\n",
      "Epoch 638/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3166 - acc: 0.5455 - val_loss: 2.2030 - val_acc: 0.3245\n",
      "Epoch 639/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3179 - acc: 0.5415 - val_loss: 2.2831 - val_acc: 0.3280\n",
      "Epoch 640/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3167 - acc: 0.5503 - val_loss: 2.2548 - val_acc: 0.3165\n",
      "Epoch 641/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3198 - acc: 0.5491 - val_loss: 2.1956 - val_acc: 0.3280\n",
      "Epoch 642/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3108 - acc: 0.5439 - val_loss: 2.3205 - val_acc: 0.3115\n",
      "Epoch 643/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.3205 - acc: 0.5419 - val_loss: 2.2600 - val_acc: 0.3215\n",
      "Epoch 644/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3131 - acc: 0.5462 - val_loss: 2.1858 - val_acc: 0.3305\n",
      "Epoch 645/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3155 - acc: 0.5464 - val_loss: 2.2075 - val_acc: 0.3360\n",
      "Epoch 646/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3235 - acc: 0.5408 - val_loss: 2.1896 - val_acc: 0.3380\n",
      "Epoch 647/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3140 - acc: 0.5431 - val_loss: 2.2997 - val_acc: 0.3150\n",
      "Epoch 648/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3177 - acc: 0.5486 - val_loss: 2.2194 - val_acc: 0.3440\n",
      "Epoch 649/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3168 - acc: 0.5381 - val_loss: 2.2389 - val_acc: 0.3315\n",
      "Epoch 650/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3155 - acc: 0.5471 - val_loss: 2.2755 - val_acc: 0.3135\n",
      "Epoch 651/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3149 - acc: 0.5499 - val_loss: 2.1682 - val_acc: 0.3420\n",
      "Epoch 652/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3160 - acc: 0.5479 - val_loss: 2.2185 - val_acc: 0.3250\n",
      "Epoch 653/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3058 - acc: 0.5473 - val_loss: 2.2939 - val_acc: 0.3180\n",
      "Epoch 654/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3037 - acc: 0.5467 - val_loss: 2.2263 - val_acc: 0.3305\n",
      "Epoch 655/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3145 - acc: 0.5462 - val_loss: 2.1668 - val_acc: 0.3390\n",
      "Epoch 656/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.3151 - acc: 0.5485 - val_loss: 2.1645 - val_acc: 0.3410\n",
      "Epoch 657/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3166 - acc: 0.5425 - val_loss: 2.2438 - val_acc: 0.3215\n",
      "Epoch 658/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3071 - acc: 0.5489 - val_loss: 2.1551 - val_acc: 0.3415\n",
      "Epoch 659/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3063 - acc: 0.5450 - val_loss: 2.2223 - val_acc: 0.3265\n",
      "Epoch 660/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3144 - acc: 0.5460 - val_loss: 2.2647 - val_acc: 0.3225\n",
      "Epoch 661/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3125 - acc: 0.5418 - val_loss: 2.2985 - val_acc: 0.3155\n",
      "Epoch 662/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.3090 - acc: 0.5461 - val_loss: 2.2687 - val_acc: 0.3210\n",
      "Epoch 663/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3008 - acc: 0.5533 - val_loss: 2.2342 - val_acc: 0.3245\n",
      "Epoch 664/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3110 - acc: 0.5381 - val_loss: 2.2635 - val_acc: 0.3145\n",
      "Epoch 665/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3128 - acc: 0.5431 - val_loss: 2.2049 - val_acc: 0.3385\n",
      "Epoch 666/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3046 - acc: 0.5489 - val_loss: 2.2723 - val_acc: 0.3270\n",
      "Epoch 667/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3149 - acc: 0.5445 - val_loss: 2.1836 - val_acc: 0.3300\n",
      "Epoch 668/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3099 - acc: 0.5501 - val_loss: 2.2055 - val_acc: 0.3290\n",
      "Epoch 669/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2965 - acc: 0.5505 - val_loss: 2.1664 - val_acc: 0.3315\n",
      "Epoch 670/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3020 - acc: 0.5474 - val_loss: 2.2992 - val_acc: 0.3220\n",
      "Epoch 671/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.3041 - acc: 0.5513 - val_loss: 2.2050 - val_acc: 0.3405\n",
      "Epoch 672/2000\n",
      "8000/8000 [==============================] - 2s 252us/step - loss: 1.3134 - acc: 0.5504 - val_loss: 2.2232 - val_acc: 0.3350\n",
      "Epoch 673/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3096 - acc: 0.5510 - val_loss: 2.2341 - val_acc: 0.3205\n",
      "Epoch 674/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3041 - acc: 0.5550 - val_loss: 2.2086 - val_acc: 0.3400\n",
      "Epoch 675/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3075 - acc: 0.5479 - val_loss: 2.1624 - val_acc: 0.3400\n",
      "Epoch 676/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3126 - acc: 0.5473 - val_loss: 2.4207 - val_acc: 0.2955\n",
      "Epoch 677/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3048 - acc: 0.5533 - val_loss: 2.1866 - val_acc: 0.3355\n",
      "Epoch 678/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3021 - acc: 0.5514 - val_loss: 2.2711 - val_acc: 0.3155\n",
      "Epoch 679/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.3046 - acc: 0.5499 - val_loss: 2.1974 - val_acc: 0.3355\n",
      "Epoch 680/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3088 - acc: 0.5513 - val_loss: 2.2842 - val_acc: 0.3310\n",
      "Epoch 681/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3104 - acc: 0.5488 - val_loss: 2.2701 - val_acc: 0.3270\n",
      "Epoch 682/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2998 - acc: 0.5500 - val_loss: 2.2258 - val_acc: 0.3290\n",
      "Epoch 683/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2992 - acc: 0.5524 - val_loss: 2.2131 - val_acc: 0.3265\n",
      "Epoch 684/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3004 - acc: 0.5496 - val_loss: 2.2502 - val_acc: 0.3140\n",
      "Epoch 685/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3025 - acc: 0.5481 - val_loss: 2.1752 - val_acc: 0.3370\n",
      "Epoch 686/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2981 - acc: 0.5521 - val_loss: 2.1919 - val_acc: 0.3250\n",
      "Epoch 687/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2938 - acc: 0.5527 - val_loss: 2.2620 - val_acc: 0.3275\n",
      "Epoch 688/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3011 - acc: 0.5513 - val_loss: 2.2058 - val_acc: 0.3305\n",
      "Epoch 689/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3015 - acc: 0.5553 - val_loss: 2.2299 - val_acc: 0.3270\n",
      "Epoch 690/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2942 - acc: 0.5520 - val_loss: 2.2104 - val_acc: 0.3295\n",
      "Epoch 691/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3064 - acc: 0.5464 - val_loss: 2.1975 - val_acc: 0.3325\n",
      "Epoch 692/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3106 - acc: 0.5449 - val_loss: 2.2173 - val_acc: 0.3060\n",
      "Epoch 693/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2963 - acc: 0.5545 - val_loss: 2.3002 - val_acc: 0.3170\n",
      "Epoch 694/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3026 - acc: 0.5497 - val_loss: 2.4520 - val_acc: 0.2940\n",
      "Epoch 695/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.3074 - acc: 0.5516 - val_loss: 2.3430 - val_acc: 0.3180\n",
      "Epoch 696/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2991 - acc: 0.5534 - val_loss: 2.3403 - val_acc: 0.3290\n",
      "Epoch 697/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.3059 - acc: 0.5488 - val_loss: 2.3039 - val_acc: 0.3240\n",
      "Epoch 698/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3024 - acc: 0.5465 - val_loss: 2.3188 - val_acc: 0.3130\n",
      "Epoch 699/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2998 - acc: 0.5520 - val_loss: 2.2637 - val_acc: 0.3165\n",
      "Epoch 700/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3144 - acc: 0.5389 - val_loss: 2.4367 - val_acc: 0.3110\n",
      "Epoch 701/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2975 - acc: 0.5514 - val_loss: 2.2607 - val_acc: 0.3175\n",
      "Epoch 702/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2893 - acc: 0.5514 - val_loss: 2.2457 - val_acc: 0.3335\n",
      "Epoch 703/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2944 - acc: 0.5497 - val_loss: 2.3741 - val_acc: 0.3305\n",
      "Epoch 704/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2843 - acc: 0.5614 - val_loss: 2.4092 - val_acc: 0.3215\n",
      "Epoch 705/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2995 - acc: 0.5467 - val_loss: 2.2872 - val_acc: 0.3255\n",
      "Epoch 706/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2954 - acc: 0.5525 - val_loss: 2.2152 - val_acc: 0.3480\n",
      "Epoch 707/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.3004 - acc: 0.5490 - val_loss: 2.1808 - val_acc: 0.3390\n",
      "Epoch 708/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2968 - acc: 0.5571 - val_loss: 2.2223 - val_acc: 0.3245\n",
      "Epoch 709/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.3007 - acc: 0.5464 - val_loss: 2.2897 - val_acc: 0.3250\n",
      "Epoch 710/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.3005 - acc: 0.5496 - val_loss: 2.2093 - val_acc: 0.3330\n",
      "Epoch 711/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2876 - acc: 0.5548 - val_loss: 2.3634 - val_acc: 0.3225\n",
      "Epoch 712/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2978 - acc: 0.5522 - val_loss: 2.2610 - val_acc: 0.3210\n",
      "Epoch 713/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2952 - acc: 0.5506 - val_loss: 2.3451 - val_acc: 0.3175\n",
      "Epoch 714/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3071 - acc: 0.5496 - val_loss: 2.1792 - val_acc: 0.3375\n",
      "Epoch 715/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2982 - acc: 0.5505 - val_loss: 2.2194 - val_acc: 0.3325\n",
      "Epoch 716/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2970 - acc: 0.5501 - val_loss: 2.2508 - val_acc: 0.3310\n",
      "Epoch 717/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2899 - acc: 0.5460 - val_loss: 2.2882 - val_acc: 0.3345\n",
      "Epoch 718/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2969 - acc: 0.5504 - val_loss: 2.2723 - val_acc: 0.3360\n",
      "Epoch 719/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2831 - acc: 0.5586 - val_loss: 2.2674 - val_acc: 0.3215\n",
      "Epoch 720/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.3011 - acc: 0.5514 - val_loss: 2.4208 - val_acc: 0.2960\n",
      "Epoch 721/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3024 - acc: 0.5524 - val_loss: 2.2722 - val_acc: 0.3250\n",
      "Epoch 722/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2986 - acc: 0.5514 - val_loss: 2.3675 - val_acc: 0.3140\n",
      "Epoch 723/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2929 - acc: 0.5554 - val_loss: 2.1560 - val_acc: 0.3390\n",
      "Epoch 724/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2942 - acc: 0.5598 - val_loss: 2.2363 - val_acc: 0.3355\n",
      "Epoch 725/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.3063 - acc: 0.5446 - val_loss: 2.5256 - val_acc: 0.2765\n",
      "Epoch 726/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2923 - acc: 0.5540 - val_loss: 2.4055 - val_acc: 0.2890\n",
      "Epoch 727/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2949 - acc: 0.5506 - val_loss: 2.2048 - val_acc: 0.3305\n",
      "Epoch 728/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2877 - acc: 0.5526 - val_loss: 2.2447 - val_acc: 0.3240\n",
      "Epoch 729/2000\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 1.2974 - acc: 0.5524 - val_loss: 2.1589 - val_acc: 0.3385\n",
      "Epoch 730/2000\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 1.3070 - acc: 0.5481 - val_loss: 2.1763 - val_acc: 0.3245\n",
      "Epoch 731/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2997 - acc: 0.5522 - val_loss: 2.3769 - val_acc: 0.3080\n",
      "Epoch 732/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2982 - acc: 0.5481 - val_loss: 2.5141 - val_acc: 0.2955\n",
      "Epoch 733/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2990 - acc: 0.5457 - val_loss: 2.2209 - val_acc: 0.3205\n",
      "Epoch 734/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2865 - acc: 0.5529 - val_loss: 2.2000 - val_acc: 0.3325\n",
      "Epoch 735/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2911 - acc: 0.5594 - val_loss: 2.2390 - val_acc: 0.3370\n",
      "Epoch 736/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.2905 - acc: 0.5577 - val_loss: 2.3069 - val_acc: 0.3150\n",
      "Epoch 737/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.2895 - acc: 0.5560 - val_loss: 2.2288 - val_acc: 0.3365\n",
      "Epoch 738/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.3014 - acc: 0.5495 - val_loss: 2.2270 - val_acc: 0.3320\n",
      "Epoch 739/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2897 - acc: 0.5548 - val_loss: 2.4125 - val_acc: 0.2975\n",
      "Epoch 740/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2906 - acc: 0.5579 - val_loss: 2.3064 - val_acc: 0.3300\n",
      "Epoch 741/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2929 - acc: 0.5527 - val_loss: 2.2258 - val_acc: 0.3325\n",
      "Epoch 742/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2890 - acc: 0.5604 - val_loss: 2.2515 - val_acc: 0.3260\n",
      "Epoch 743/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2938 - acc: 0.5515 - val_loss: 2.2753 - val_acc: 0.3230\n",
      "Epoch 744/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2811 - acc: 0.5555 - val_loss: 2.4124 - val_acc: 0.3190\n",
      "Epoch 745/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2973 - acc: 0.5540 - val_loss: 2.2130 - val_acc: 0.3265\n",
      "Epoch 746/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2832 - acc: 0.5589 - val_loss: 2.2806 - val_acc: 0.3225\n",
      "Epoch 747/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2959 - acc: 0.5530 - val_loss: 2.2438 - val_acc: 0.3300\n",
      "Epoch 748/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2851 - acc: 0.5554 - val_loss: 2.2750 - val_acc: 0.3225\n",
      "Epoch 749/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2899 - acc: 0.5569 - val_loss: 2.3862 - val_acc: 0.3075\n",
      "Epoch 750/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2857 - acc: 0.5575 - val_loss: 2.3277 - val_acc: 0.3175\n",
      "Epoch 751/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2885 - acc: 0.5554 - val_loss: 2.2650 - val_acc: 0.3105\n",
      "Epoch 752/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2986 - acc: 0.5503 - val_loss: 2.2113 - val_acc: 0.3395\n",
      "Epoch 753/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2819 - acc: 0.5570 - val_loss: 2.2174 - val_acc: 0.3330\n",
      "Epoch 754/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2951 - acc: 0.5511 - val_loss: 2.4676 - val_acc: 0.3030\n",
      "Epoch 755/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2856 - acc: 0.5529 - val_loss: 2.3230 - val_acc: 0.3290\n",
      "Epoch 756/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2888 - acc: 0.5580 - val_loss: 2.2631 - val_acc: 0.3255\n",
      "Epoch 757/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2806 - acc: 0.5550 - val_loss: 2.2274 - val_acc: 0.3335\n",
      "Epoch 758/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2907 - acc: 0.5519 - val_loss: 2.2066 - val_acc: 0.3405\n",
      "Epoch 759/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2845 - acc: 0.5499 - val_loss: 2.2459 - val_acc: 0.3390\n",
      "Epoch 760/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2740 - acc: 0.5541 - val_loss: 2.5545 - val_acc: 0.3125\n",
      "Epoch 761/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2920 - acc: 0.5534 - val_loss: 2.2154 - val_acc: 0.3350\n",
      "Epoch 762/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2874 - acc: 0.5546 - val_loss: 2.2727 - val_acc: 0.3165\n",
      "Epoch 763/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2922 - acc: 0.5507 - val_loss: 2.2604 - val_acc: 0.3165\n",
      "Epoch 764/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2899 - acc: 0.5517 - val_loss: 2.1853 - val_acc: 0.3270\n",
      "Epoch 765/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2872 - acc: 0.5550 - val_loss: 2.3813 - val_acc: 0.3055\n",
      "Epoch 766/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2944 - acc: 0.5507 - val_loss: 2.2284 - val_acc: 0.3290\n",
      "Epoch 767/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2804 - acc: 0.5580 - val_loss: 2.3577 - val_acc: 0.3120\n",
      "Epoch 768/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2832 - acc: 0.5534 - val_loss: 2.2120 - val_acc: 0.3305\n",
      "Epoch 769/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2803 - acc: 0.5611 - val_loss: 2.2137 - val_acc: 0.3275\n",
      "Epoch 770/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2800 - acc: 0.5582 - val_loss: 2.3530 - val_acc: 0.3255\n",
      "Epoch 771/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2826 - acc: 0.5586 - val_loss: 2.2313 - val_acc: 0.3270\n",
      "Epoch 772/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2748 - acc: 0.5634 - val_loss: 2.2927 - val_acc: 0.3145\n",
      "Epoch 773/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2854 - acc: 0.5561 - val_loss: 2.2557 - val_acc: 0.3240\n",
      "Epoch 774/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2808 - acc: 0.5623 - val_loss: 2.3129 - val_acc: 0.3120\n",
      "Epoch 775/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2836 - acc: 0.5615 - val_loss: 2.1945 - val_acc: 0.3430\n",
      "Epoch 776/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2768 - acc: 0.5577 - val_loss: 2.3141 - val_acc: 0.3265\n",
      "Epoch 777/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2800 - acc: 0.5538 - val_loss: 2.2446 - val_acc: 0.3350\n",
      "Epoch 778/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2865 - acc: 0.5522 - val_loss: 2.3237 - val_acc: 0.3225\n",
      "Epoch 779/2000\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 1.2906 - acc: 0.5516 - val_loss: 2.2776 - val_acc: 0.3140\n",
      "Epoch 780/2000\n",
      "8000/8000 [==============================] - 2s 258us/step - loss: 1.2836 - acc: 0.5560 - val_loss: 2.4144 - val_acc: 0.3005\n",
      "Epoch 781/2000\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.2815 - acc: 0.5541 - val_loss: 2.2488 - val_acc: 0.3240\n",
      "Epoch 782/2000\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 1.2965 - acc: 0.5469 - val_loss: 2.2509 - val_acc: 0.3415\n",
      "Epoch 783/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.2932 - acc: 0.5501 - val_loss: 2.2771 - val_acc: 0.3265\n",
      "Epoch 784/2000\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 1.2857 - acc: 0.5530 - val_loss: 2.2814 - val_acc: 0.3160\n",
      "Epoch 785/2000\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: 1.2946 - acc: 0.5493 - val_loss: 2.2620 - val_acc: 0.3185\n",
      "Epoch 786/2000\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 1.2912 - acc: 0.5519 - val_loss: 2.2954 - val_acc: 0.3100\n",
      "Epoch 787/2000\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 1.2890 - acc: 0.5493 - val_loss: 2.2913 - val_acc: 0.3290\n",
      "Epoch 788/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.2822 - acc: 0.5550 - val_loss: 2.2577 - val_acc: 0.3260\n",
      "Epoch 789/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2837 - acc: 0.5507 - val_loss: 2.3004 - val_acc: 0.3210\n",
      "Epoch 790/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2813 - acc: 0.5560 - val_loss: 2.2559 - val_acc: 0.3175\n",
      "Epoch 791/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2734 - acc: 0.5654 - val_loss: 2.3333 - val_acc: 0.3265\n",
      "Epoch 792/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2818 - acc: 0.5616 - val_loss: 2.2819 - val_acc: 0.3205\n",
      "Epoch 793/2000\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 1.2860 - acc: 0.5561 - val_loss: 2.2436 - val_acc: 0.3330\n",
      "Epoch 794/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2762 - acc: 0.5587 - val_loss: 2.3467 - val_acc: 0.3240\n",
      "Epoch 795/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2826 - acc: 0.5543 - val_loss: 2.2102 - val_acc: 0.3395\n",
      "Epoch 796/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2734 - acc: 0.5565 - val_loss: 2.3509 - val_acc: 0.3170\n",
      "Epoch 797/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2773 - acc: 0.5540 - val_loss: 2.2662 - val_acc: 0.3175\n",
      "Epoch 798/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2747 - acc: 0.5586 - val_loss: 2.2890 - val_acc: 0.3045\n",
      "Epoch 799/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2837 - acc: 0.5543 - val_loss: 2.2229 - val_acc: 0.3345\n",
      "Epoch 800/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2707 - acc: 0.5606 - val_loss: 2.3133 - val_acc: 0.3245\n",
      "Epoch 801/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2783 - acc: 0.5551 - val_loss: 2.2403 - val_acc: 0.3365\n",
      "Epoch 802/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2834 - acc: 0.5557 - val_loss: 2.3271 - val_acc: 0.3230\n",
      "Epoch 803/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2765 - acc: 0.5604 - val_loss: 2.3751 - val_acc: 0.3085\n",
      "Epoch 804/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2704 - acc: 0.5575 - val_loss: 2.2908 - val_acc: 0.3330\n",
      "Epoch 805/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2797 - acc: 0.5544 - val_loss: 2.3374 - val_acc: 0.3040\n",
      "Epoch 806/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2591 - acc: 0.5650 - val_loss: 2.2642 - val_acc: 0.3290\n",
      "Epoch 807/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2737 - acc: 0.5584 - val_loss: 2.3820 - val_acc: 0.3210\n",
      "Epoch 808/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2748 - acc: 0.5623 - val_loss: 2.3512 - val_acc: 0.3140\n",
      "Epoch 809/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2820 - acc: 0.5609 - val_loss: 2.3109 - val_acc: 0.3265\n",
      "Epoch 810/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2688 - acc: 0.5656 - val_loss: 2.2780 - val_acc: 0.3305\n",
      "Epoch 811/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2629 - acc: 0.5616 - val_loss: 2.2653 - val_acc: 0.3290\n",
      "Epoch 812/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2847 - acc: 0.5550 - val_loss: 2.3452 - val_acc: 0.3245\n",
      "Epoch 813/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2761 - acc: 0.5605 - val_loss: 2.5039 - val_acc: 0.2930\n",
      "Epoch 814/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2738 - acc: 0.5634 - val_loss: 2.2684 - val_acc: 0.3205\n",
      "Epoch 815/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2761 - acc: 0.5534 - val_loss: 2.2461 - val_acc: 0.3220\n",
      "Epoch 816/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2760 - acc: 0.5574 - val_loss: 2.3608 - val_acc: 0.3135\n",
      "Epoch 817/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2684 - acc: 0.5592 - val_loss: 2.5304 - val_acc: 0.2910\n",
      "Epoch 818/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2675 - acc: 0.5623 - val_loss: 2.2027 - val_acc: 0.3430\n",
      "Epoch 819/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2787 - acc: 0.5584 - val_loss: 2.2857 - val_acc: 0.3110\n",
      "Epoch 820/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2689 - acc: 0.5660 - val_loss: 2.1974 - val_acc: 0.3325\n",
      "Epoch 821/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2689 - acc: 0.5594 - val_loss: 2.2460 - val_acc: 0.3295\n",
      "Epoch 822/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2630 - acc: 0.5576 - val_loss: 2.3220 - val_acc: 0.3150\n",
      "Epoch 823/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2743 - acc: 0.5641 - val_loss: 2.3274 - val_acc: 0.3210\n",
      "Epoch 824/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2598 - acc: 0.5614 - val_loss: 2.2715 - val_acc: 0.3205\n",
      "Epoch 825/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2717 - acc: 0.5594 - val_loss: 2.2527 - val_acc: 0.3305\n",
      "Epoch 826/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2582 - acc: 0.5635 - val_loss: 2.2687 - val_acc: 0.3385\n",
      "Epoch 827/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2690 - acc: 0.5590 - val_loss: 2.4138 - val_acc: 0.3210\n",
      "Epoch 828/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2664 - acc: 0.5611 - val_loss: 2.2755 - val_acc: 0.3220\n",
      "Epoch 829/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.2684 - acc: 0.5625 - val_loss: 2.3046 - val_acc: 0.3180\n",
      "Epoch 830/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2591 - acc: 0.5654 - val_loss: 2.3380 - val_acc: 0.3285\n",
      "Epoch 831/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2595 - acc: 0.5652 - val_loss: 2.4782 - val_acc: 0.3030\n",
      "Epoch 832/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2755 - acc: 0.5557 - val_loss: 2.2519 - val_acc: 0.3465\n",
      "Epoch 833/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2584 - acc: 0.5624 - val_loss: 2.2882 - val_acc: 0.3195\n",
      "Epoch 834/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2688 - acc: 0.5576 - val_loss: 2.2867 - val_acc: 0.3270\n",
      "Epoch 835/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2560 - acc: 0.5694 - val_loss: 2.2999 - val_acc: 0.3340\n",
      "Epoch 836/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2661 - acc: 0.5563 - val_loss: 2.2733 - val_acc: 0.3245\n",
      "Epoch 837/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2688 - acc: 0.5616 - val_loss: 2.4364 - val_acc: 0.2965\n",
      "Epoch 838/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2724 - acc: 0.5651 - val_loss: 2.3361 - val_acc: 0.3210\n",
      "Epoch 839/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2723 - acc: 0.5640 - val_loss: 2.3432 - val_acc: 0.3045\n",
      "Epoch 840/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2624 - acc: 0.5609 - val_loss: 2.2213 - val_acc: 0.3455\n",
      "Epoch 841/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2658 - acc: 0.5615 - val_loss: 2.3199 - val_acc: 0.3205\n",
      "Epoch 842/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2752 - acc: 0.5619 - val_loss: 2.3154 - val_acc: 0.3110\n",
      "Epoch 843/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2664 - acc: 0.5611 - val_loss: 2.5404 - val_acc: 0.2815\n",
      "Epoch 844/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2623 - acc: 0.5669 - val_loss: 2.3608 - val_acc: 0.3150\n",
      "Epoch 845/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2724 - acc: 0.5527 - val_loss: 2.3552 - val_acc: 0.3225\n",
      "Epoch 846/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2646 - acc: 0.5661 - val_loss: 2.3738 - val_acc: 0.3050\n",
      "Epoch 847/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2667 - acc: 0.5551 - val_loss: 2.2951 - val_acc: 0.3195\n",
      "Epoch 848/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2713 - acc: 0.5598 - val_loss: 2.5365 - val_acc: 0.2855\n",
      "Epoch 849/2000\n",
      "8000/8000 [==============================] - 10804s 1s/step - loss: 1.2605 - acc: 0.5661 - val_loss: 2.4475 - val_acc: 0.3050\n",
      "Epoch 850/2000\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 1.2673 - acc: 0.5645 - val_loss: 2.5276 - val_acc: 0.3060\n",
      "Epoch 851/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2608 - acc: 0.5574 - val_loss: 2.2852 - val_acc: 0.3265\n",
      "Epoch 852/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2615 - acc: 0.5579 - val_loss: 2.3101 - val_acc: 0.3175\n",
      "Epoch 853/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.2639 - acc: 0.5663 - val_loss: 2.2686 - val_acc: 0.3230\n",
      "Epoch 854/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2549 - acc: 0.5671 - val_loss: 2.2905 - val_acc: 0.3255\n",
      "Epoch 855/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2670 - acc: 0.5701 - val_loss: 2.2990 - val_acc: 0.3145\n",
      "Epoch 856/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.2554 - acc: 0.5702 - val_loss: 2.2428 - val_acc: 0.3310\n",
      "Epoch 857/2000\n",
      "8000/8000 [==============================] - 17994s 2s/step - loss: 1.2669 - acc: 0.5673 - val_loss: 2.2791 - val_acc: 0.3360\n",
      "Epoch 858/2000\n",
      "8000/8000 [==============================] - 2s 311us/step - loss: 1.2787 - acc: 0.5616 - val_loss: 2.2274 - val_acc: 0.3260\n",
      "Epoch 859/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.2666 - acc: 0.5587 - val_loss: 2.4735 - val_acc: 0.3070\n",
      "Epoch 860/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.2681 - acc: 0.5621 - val_loss: 2.2752 - val_acc: 0.3155\n",
      "Epoch 861/2000\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 1.2643 - acc: 0.5630 - val_loss: 2.3404 - val_acc: 0.3075\n",
      "Epoch 862/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2618 - acc: 0.5626 - val_loss: 2.2770 - val_acc: 0.3120\n",
      "Epoch 863/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2626 - acc: 0.5625 - val_loss: 2.4430 - val_acc: 0.3025\n",
      "Epoch 864/2000\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 1.2574 - acc: 0.5642 - val_loss: 2.4007 - val_acc: 0.3225\n",
      "Epoch 865/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2643 - acc: 0.5637 - val_loss: 2.2408 - val_acc: 0.3325\n",
      "Epoch 866/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2655 - acc: 0.5594 - val_loss: 2.3057 - val_acc: 0.3230\n",
      "Epoch 867/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2558 - acc: 0.5631 - val_loss: 2.3621 - val_acc: 0.3115\n",
      "Epoch 868/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2624 - acc: 0.5644 - val_loss: 2.3137 - val_acc: 0.3220\n",
      "Epoch 869/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2653 - acc: 0.5645 - val_loss: 2.3441 - val_acc: 0.3245\n",
      "Epoch 870/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2601 - acc: 0.5681 - val_loss: 2.2522 - val_acc: 0.3355\n",
      "Epoch 871/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2612 - acc: 0.5617 - val_loss: 2.2577 - val_acc: 0.3225\n",
      "Epoch 872/2000\n",
      "8000/8000 [==============================] - 36890s 5s/step - loss: 1.2581 - acc: 0.5584 - val_loss: 2.3566 - val_acc: 0.3285\n",
      "Epoch 873/2000\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 1.2608 - acc: 0.5695 - val_loss: 2.2652 - val_acc: 0.3255\n",
      "Epoch 874/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.2660 - acc: 0.5656 - val_loss: 2.4031 - val_acc: 0.3095\n",
      "Epoch 875/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.2576 - acc: 0.5665 - val_loss: 2.3750 - val_acc: 0.3210\n",
      "Epoch 876/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2563 - acc: 0.5661 - val_loss: 2.2805 - val_acc: 0.3315\n",
      "Epoch 877/2000\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 1.2667 - acc: 0.5635 - val_loss: 2.2296 - val_acc: 0.3390\n",
      "Epoch 878/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2593 - acc: 0.5665 - val_loss: 2.3413 - val_acc: 0.3195\n",
      "Epoch 879/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2547 - acc: 0.5666 - val_loss: 2.2802 - val_acc: 0.3295\n",
      "Epoch 880/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2566 - acc: 0.5660 - val_loss: 2.2800 - val_acc: 0.3145\n",
      "Epoch 881/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2651 - acc: 0.5685 - val_loss: 2.3360 - val_acc: 0.3160\n",
      "Epoch 882/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2508 - acc: 0.5723 - val_loss: 2.3169 - val_acc: 0.3305\n",
      "Epoch 883/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2517 - acc: 0.5684 - val_loss: 2.2918 - val_acc: 0.3075\n",
      "Epoch 884/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2482 - acc: 0.5717 - val_loss: 2.3345 - val_acc: 0.3160\n",
      "Epoch 885/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2571 - acc: 0.5644 - val_loss: 2.3555 - val_acc: 0.3040\n",
      "Epoch 886/2000\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 1.2579 - acc: 0.5637 - val_loss: 2.2463 - val_acc: 0.3270\n",
      "Epoch 887/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2538 - acc: 0.5636 - val_loss: 2.2430 - val_acc: 0.3275\n",
      "Epoch 888/2000\n",
      "8000/8000 [==============================] - 7190s 899ms/step - loss: 1.2576 - acc: 0.5650 - val_loss: 2.3101 - val_acc: 0.3245\n",
      "Epoch 889/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.2544 - acc: 0.5647 - val_loss: 2.3203 - val_acc: 0.3270\n",
      "Epoch 890/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2479 - acc: 0.5728 - val_loss: 2.4289 - val_acc: 0.3050\n",
      "Epoch 891/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2628 - acc: 0.5631 - val_loss: 2.2961 - val_acc: 0.3205\n",
      "Epoch 892/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.2544 - acc: 0.5677 - val_loss: 2.2922 - val_acc: 0.3250\n",
      "Epoch 893/2000\n",
      "8000/8000 [==============================] - 2s 216us/step - loss: 1.2607 - acc: 0.5673 - val_loss: 2.2868 - val_acc: 0.3310\n",
      "Epoch 894/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2605 - acc: 0.5614 - val_loss: 2.2489 - val_acc: 0.3390\n",
      "Epoch 895/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2551 - acc: 0.5650 - val_loss: 2.3523 - val_acc: 0.3140\n",
      "Epoch 896/2000\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 1.2534 - acc: 0.5680 - val_loss: 2.2798 - val_acc: 0.3400\n",
      "Epoch 897/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2540 - acc: 0.5661 - val_loss: 2.3161 - val_acc: 0.3230\n",
      "Epoch 898/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2537 - acc: 0.5659 - val_loss: 2.4087 - val_acc: 0.3110\n",
      "Epoch 899/2000\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 1.2577 - acc: 0.5660 - val_loss: 2.3301 - val_acc: 0.3175\n",
      "Epoch 900/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2613 - acc: 0.5635 - val_loss: 2.3506 - val_acc: 0.3175\n",
      "Epoch 901/2000\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 1.2541 - acc: 0.5658 - val_loss: 2.3395 - val_acc: 0.3250\n",
      "Epoch 902/2000\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 1.2607 - acc: 0.5640 - val_loss: 2.4164 - val_acc: 0.3085\n",
      "Epoch 903/2000\n",
      "8000/8000 [==============================] - 7818s 977ms/step - loss: 1.2497 - acc: 0.5640 - val_loss: 2.3321 - val_acc: 0.3210\n",
      "Epoch 904/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2478 - acc: 0.5709 - val_loss: 2.3262 - val_acc: 0.3195\n",
      "Epoch 905/2000\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 1.2623 - acc: 0.5656 - val_loss: 2.4954 - val_acc: 0.3130\n",
      "Epoch 906/2000\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 1.2601 - acc: 0.5658 - val_loss: 2.3366 - val_acc: 0.3220\n",
      "Epoch 907/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.2482 - acc: 0.5747 - val_loss: 2.3760 - val_acc: 0.3060\n",
      "Epoch 908/2000\n",
      "8000/8000 [==============================] - 3s 375us/step - loss: 1.2613 - acc: 0.5665 - val_loss: 2.2740 - val_acc: 0.3360\n",
      "Epoch 909/2000\n",
      "8000/8000 [==============================] - 3s 330us/step - loss: 1.2622 - acc: 0.5645 - val_loss: 2.3284 - val_acc: 0.3230\n",
      "Epoch 910/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2609 - acc: 0.5658 - val_loss: 2.3448 - val_acc: 0.3240\n",
      "Epoch 911/2000\n",
      "8000/8000 [==============================] - 3s 321us/step - loss: 1.2569 - acc: 0.5629 - val_loss: 2.3154 - val_acc: 0.3280\n",
      "Epoch 912/2000\n",
      "8000/8000 [==============================] - 2s 301us/step - loss: 1.2566 - acc: 0.5677 - val_loss: 2.3059 - val_acc: 0.3225\n",
      "Epoch 913/2000\n",
      "8000/8000 [==============================] - 3s 341us/step - loss: 1.2621 - acc: 0.5641 - val_loss: 2.5209 - val_acc: 0.2890\n",
      "Epoch 914/2000\n",
      "8000/8000 [==============================] - 2s 311us/step - loss: 1.2716 - acc: 0.5581 - val_loss: 2.4197 - val_acc: 0.2995\n",
      "Epoch 915/2000\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 1.2630 - acc: 0.5671 - val_loss: 2.3253 - val_acc: 0.3295\n",
      "Epoch 916/2000\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 1.2609 - acc: 0.5658 - val_loss: 2.2703 - val_acc: 0.3315\n",
      "Epoch 917/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2463 - acc: 0.5728 - val_loss: 2.3160 - val_acc: 0.3225\n",
      "Epoch 918/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2581 - acc: 0.5651 - val_loss: 2.3493 - val_acc: 0.3285\n",
      "Epoch 919/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2556 - acc: 0.5611 - val_loss: 2.4504 - val_acc: 0.3130\n",
      "Epoch 920/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2442 - acc: 0.5707 - val_loss: 2.3799 - val_acc: 0.3165\n",
      "Epoch 921/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2491 - acc: 0.5728 - val_loss: 2.4372 - val_acc: 0.3085\n",
      "Epoch 922/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2532 - acc: 0.5702 - val_loss: 2.2914 - val_acc: 0.3275\n",
      "Epoch 923/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.2291 - acc: 0.5691 - val_loss: 2.4192 - val_acc: 0.3205\n",
      "Epoch 924/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2525 - acc: 0.5614 - val_loss: 2.3657 - val_acc: 0.3175\n",
      "Epoch 925/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2459 - acc: 0.5659 - val_loss: 2.3472 - val_acc: 0.3135\n",
      "Epoch 926/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2566 - acc: 0.5623 - val_loss: 2.3007 - val_acc: 0.3280\n",
      "Epoch 927/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2479 - acc: 0.5660 - val_loss: 2.2944 - val_acc: 0.3160\n",
      "Epoch 928/2000\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 1.2609 - acc: 0.5659 - val_loss: 2.4793 - val_acc: 0.3145\n",
      "Epoch 929/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2492 - acc: 0.5649 - val_loss: 2.4277 - val_acc: 0.3230\n",
      "Epoch 930/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.2560 - acc: 0.5660 - val_loss: 2.2662 - val_acc: 0.3345\n",
      "Epoch 931/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2335 - acc: 0.5735 - val_loss: 2.2599 - val_acc: 0.3350\n",
      "Epoch 932/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2607 - acc: 0.5640 - val_loss: 2.2559 - val_acc: 0.3325\n",
      "Epoch 933/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2512 - acc: 0.5689 - val_loss: 2.3381 - val_acc: 0.3170\n",
      "Epoch 934/2000\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 1.2458 - acc: 0.5737 - val_loss: 2.4757 - val_acc: 0.3070\n",
      "Epoch 935/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2462 - acc: 0.5717 - val_loss: 2.3551 - val_acc: 0.3145\n",
      "Epoch 936/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2441 - acc: 0.5737 - val_loss: 2.2590 - val_acc: 0.3400\n",
      "Epoch 937/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.2445 - acc: 0.5683 - val_loss: 2.3450 - val_acc: 0.3240\n",
      "Epoch 938/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2461 - acc: 0.5729 - val_loss: 2.3933 - val_acc: 0.3000\n",
      "Epoch 939/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2453 - acc: 0.5712 - val_loss: 2.3765 - val_acc: 0.3060\n",
      "Epoch 940/2000\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 1.2307 - acc: 0.5707 - val_loss: 2.3482 - val_acc: 0.3200\n",
      "Epoch 941/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2452 - acc: 0.5740 - val_loss: 2.3170 - val_acc: 0.3145\n",
      "Epoch 942/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.2405 - acc: 0.5753 - val_loss: 2.3262 - val_acc: 0.3320\n",
      "Epoch 943/2000\n",
      "8000/8000 [==============================] - 2s 260us/step - loss: 1.2593 - acc: 0.5608 - val_loss: 2.3549 - val_acc: 0.3065\n",
      "Epoch 944/2000\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 1.2532 - acc: 0.5699 - val_loss: 2.2545 - val_acc: 0.3420\n",
      "Epoch 945/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2376 - acc: 0.5717 - val_loss: 2.4215 - val_acc: 0.3155\n",
      "Epoch 946/2000\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 1.2484 - acc: 0.5711 - val_loss: 2.3771 - val_acc: 0.3190\n",
      "Epoch 947/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2553 - acc: 0.5647 - val_loss: 2.2597 - val_acc: 0.3200\n",
      "Epoch 948/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2516 - acc: 0.5671 - val_loss: 2.3333 - val_acc: 0.3215\n",
      "Epoch 949/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2493 - acc: 0.5674 - val_loss: 2.3888 - val_acc: 0.3095\n",
      "Epoch 950/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2407 - acc: 0.5677 - val_loss: 2.3985 - val_acc: 0.3165\n",
      "Epoch 951/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.2411 - acc: 0.5729 - val_loss: 2.4028 - val_acc: 0.3175\n",
      "Epoch 952/2000\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 1.2460 - acc: 0.5656 - val_loss: 2.3313 - val_acc: 0.3235\n",
      "Epoch 953/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2486 - acc: 0.5646 - val_loss: 2.3501 - val_acc: 0.3260\n",
      "Epoch 954/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2484 - acc: 0.5598 - val_loss: 2.3373 - val_acc: 0.3125\n",
      "Epoch 955/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2410 - acc: 0.5724 - val_loss: 2.3512 - val_acc: 0.3160\n",
      "Epoch 956/2000\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 1.2461 - acc: 0.5679 - val_loss: 2.3601 - val_acc: 0.3190\n",
      "Epoch 957/2000\n",
      "8000/8000 [==============================] - 2s 224us/step - loss: 1.2294 - acc: 0.5726 - val_loss: 2.2796 - val_acc: 0.3310\n",
      "Epoch 958/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.2483 - acc: 0.5660 - val_loss: 2.2710 - val_acc: 0.3340\n",
      "Epoch 959/2000\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 1.2363 - acc: 0.5716 - val_loss: 2.4853 - val_acc: 0.3005\n",
      "Epoch 960/2000\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 1.2349 - acc: 0.5793 - val_loss: 2.3060 - val_acc: 0.3215\n",
      "Epoch 961/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2318 - acc: 0.5774 - val_loss: 2.5232 - val_acc: 0.2935\n",
      "Epoch 962/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2371 - acc: 0.5668 - val_loss: 2.4556 - val_acc: 0.3095\n",
      "Epoch 963/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2536 - acc: 0.5652 - val_loss: 2.3740 - val_acc: 0.3295\n",
      "Epoch 964/2000\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 1.2326 - acc: 0.5710 - val_loss: 2.3321 - val_acc: 0.3225\n",
      "Epoch 965/2000\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 1.2507 - acc: 0.5677 - val_loss: 2.3349 - val_acc: 0.3210\n",
      "Epoch 966/2000\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 1.2439 - acc: 0.5741 - val_loss: 2.5235 - val_acc: 0.3015\n",
      "Epoch 967/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.2429 - acc: 0.5745 - val_loss: 2.3470 - val_acc: 0.3180\n",
      "Epoch 968/2000\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 1.2284 - acc: 0.5734 - val_loss: 2.2610 - val_acc: 0.3395\n",
      "Epoch 969/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.2341 - acc: 0.5754 - val_loss: 2.3788 - val_acc: 0.3100\n",
      "Epoch 970/2000\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 1.2421 - acc: 0.5696 - val_loss: 2.3516 - val_acc: 0.3165\n",
      "Epoch 971/2000\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 1.2331 - acc: 0.5734 - val_loss: 2.5869 - val_acc: 0.2970\n",
      "Epoch 972/2000\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 1.2397 - acc: 0.5700 - val_loss: 2.3000 - val_acc: 0.3265\n",
      "Epoch 973/2000\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 1.2424 - acc: 0.5681 - val_loss: 2.3394 - val_acc: 0.3170\n",
      "Epoch 974/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.2444 - acc: 0.5673 - val_loss: 2.3109 - val_acc: 0.3240\n",
      "Epoch 975/2000\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 1.2446 - acc: 0.5700 - val_loss: 2.2750 - val_acc: 0.3305\n",
      "Epoch 976/2000\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 1.2408 - acc: 0.5699 - val_loss: 2.3156 - val_acc: 0.3330\n",
      "Epoch 977/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2339 - acc: 0.5724 - val_loss: 2.3729 - val_acc: 0.3205\n",
      "Epoch 978/2000\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 1.2353 - acc: 0.5735 - val_loss: 2.4946 - val_acc: 0.3095\n",
      "Epoch 979/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2401 - acc: 0.5673 - val_loss: 2.3545 - val_acc: 0.3235\n",
      "Epoch 980/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.2422 - acc: 0.5735 - val_loss: 2.2944 - val_acc: 0.3365\n",
      "Epoch 981/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2258 - acc: 0.5766 - val_loss: 2.3637 - val_acc: 0.3240\n",
      "Epoch 982/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2358 - acc: 0.5725 - val_loss: 2.3775 - val_acc: 0.3200\n",
      "Epoch 983/2000\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 1.2362 - acc: 0.5739 - val_loss: 2.3305 - val_acc: 0.3305\n",
      "Epoch 984/2000\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 1.2339 - acc: 0.5785 - val_loss: 2.5039 - val_acc: 0.3165\n",
      "Epoch 985/2000\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 1.2376 - acc: 0.5642 - val_loss: 2.2858 - val_acc: 0.3345\n",
      "Epoch 986/2000\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: 1.2438 - acc: 0.5759 - val_loss: 2.3412 - val_acc: 0.3250\n",
      "Epoch 987/2000\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 1.2412 - acc: 0.5720 - val_loss: 2.3348 - val_acc: 0.3220\n",
      "Epoch 988/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.2330 - acc: 0.5830 - val_loss: 2.4914 - val_acc: 0.3020\n",
      "Epoch 989/2000\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 1.2369 - acc: 0.5670 - val_loss: 2.3984 - val_acc: 0.3200\n",
      "Epoch 990/2000\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 1.2352 - acc: 0.5781 - val_loss: 2.3551 - val_acc: 0.3075\n",
      "Epoch 991/2000\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 1.2413 - acc: 0.5714 - val_loss: 2.5162 - val_acc: 0.3065\n",
      "Epoch 992/2000\n",
      "8000/8000 [==============================] - 2s 287us/step - loss: 1.2367 - acc: 0.5753 - val_loss: 2.3881 - val_acc: 0.3255\n",
      "Epoch 993/2000\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 1.2402 - acc: 0.5747 - val_loss: 2.2852 - val_acc: 0.3225\n",
      "Epoch 994/2000\n",
      "8000/8000 [==============================] - 2s 289us/step - loss: 1.2559 - acc: 0.5640 - val_loss: 2.3287 - val_acc: 0.3245\n",
      "Epoch 995/2000\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 1.2397 - acc: 0.5684 - val_loss: 2.3947 - val_acc: 0.3200\n",
      "Epoch 996/2000\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: 1.2449 - acc: 0.5684 - val_loss: 2.2941 - val_acc: 0.3255\n",
      "Epoch 997/2000\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 1.2419 - acc: 0.5704 - val_loss: 2.3499 - val_acc: 0.3265\n",
      "Epoch 998/2000\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 1.2418 - acc: 0.5707 - val_loss: 2.3281 - val_acc: 0.3270\n",
      "Epoch 999/2000\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 1.2420 - acc: 0.5737 - val_loss: 2.3506 - val_acc: 0.3235\n",
      "Epoch 1000/2000\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 1.2315 - acc: 0.5733 - val_loss: 2.3523 - val_acc: 0.3330\n",
      "Epoch 1001/2000\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 1.2391 - acc: 0.5728 - val_loss: 2.3531 - val_acc: 0.3190\n",
      "Epoch 1002/2000\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 1.2329 - acc: 0.5728 - val_loss: 2.3296 - val_acc: 0.3125\n",
      "Epoch 1003/2000\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 1.2439 - acc: 0.5656 - val_loss: 2.5402 - val_acc: 0.2845\n",
      "Epoch 1004/2000\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.2344 - acc: 0.5795 - val_loss: 2.3573 - val_acc: 0.3315\n",
      "Epoch 1005/2000\n",
      "3328/8000 [===========>..................] - ETA: 1s - loss: 1.2416 - acc: 0.5730"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4afe6417a6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2357\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=data_1[b'data']/255, y=y_var, batch_size=None, epochs=2000, verbose=1, callbacks=None, validation_split=0.2, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 2.1050 - acc: 0.4802 - val_loss: 1.3734 - val_acc: 0.5052\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.5476 - acc: 0.5199 - val_loss: 5.8688 - val_acc: 0.5062\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.5787 - acc: 0.5350 - val_loss: 1.6339 - val_acc: 0.5105\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.6131 - acc: 0.5229 - val_loss: 1.2297 - val_acc: 0.5809\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7151 - acc: 0.5084 - val_loss: 1.4373 - val_acc: 0.4518\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.7538 - acc: 0.4986 - val_loss: 1.5796 - val_acc: 0.4571\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7382 - acc: 0.4879 - val_loss: 1.8798 - val_acc: 0.2684\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.6989 - acc: 0.4963 - val_loss: 2.0891 - val_acc: 0.3480\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.6956 - acc: 0.5211 - val_loss: 1.7156 - val_acc: 0.3940\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.7547 - acc: 0.4922 - val_loss: 1.3395 - val_acc: 0.5195\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 1.7661 - acc: 0.5012 - val_loss: 1.7739 - val_acc: 0.2805\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.8077 - acc: 0.4673 - val_loss: 1.4116 - val_acc: 0.4942\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.8026 - acc: 0.4785 - val_loss: 1.4955 - val_acc: 0.4779\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.8967 - acc: 0.4761 - val_loss: 1.8687 - val_acc: 0.3281\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.9481 - acc: 0.4679 - val_loss: 1.8014 - val_acc: 0.3645\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 221s 6ms/step - loss: 2.4817 - acc: 0.4419 - val_loss: 1.6660 - val_acc: 0.4063\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.9411 - acc: 0.4573 - val_loss: 1.7128 - val_acc: 0.3617\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 2.2605 - acc: 0.4466 - val_loss: 1.6590 - val_acc: 0.3708\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 1.7144 - acc: 0.4765 - val_loss: 1.8816 - val_acc: 0.2659\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 2.0784 - acc: 0.4275 - val_loss: 1.9408 - val_acc: 0.4034\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 2.0291 - acc: 0.4380 - val_loss: 1.7777 - val_acc: 0.4524\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.9495 - acc: 0.4303 - val_loss: 1.6223 - val_acc: 0.4278\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 1.8092 - acc: 0.4511 - val_loss: 1.4502 - val_acc: 0.4840\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.8653 - acc: 0.4738 - val_loss: 1.6171 - val_acc: 0.4849\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 2.0193 - acc: 0.4425 - val_loss: 1.6359 - val_acc: 0.4086\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 2.0636 - acc: 0.4261 - val_loss: 2.9927 - val_acc: 0.4399\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 408s 10ms/step - loss: 2.1964 - acc: 0.4229 - val_loss: 1.9654 - val_acc: 0.3162\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 2.1502 - acc: 0.4252 - val_loss: 1.9772 - val_acc: 0.2576\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.9303 - acc: 0.4393 - val_loss: 1.3103 - val_acc: 0.5745\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.8924 - acc: 0.4714 - val_loss: 1.4748 - val_acc: 0.5372\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 2.0894 - acc: 0.4319 - val_loss: 1.8843 - val_acc: 0.3789\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 2.0122 - acc: 0.4529 - val_loss: 2.3088 - val_acc: 0.3761\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 2.1835 - acc: 0.4330 - val_loss: 1.6600 - val_acc: 0.3835\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 3729s 93ms/step - loss: 1.9396 - acc: 0.4494 - val_loss: 1.7549 - val_acc: 0.3278\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 2.7617 - acc: 0.4203 - val_loss: 1.7028 - val_acc: 0.4192\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 2.0746 - acc: 0.4168 - val_loss: 2.1835 - val_acc: 0.2588\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 2.1953 - acc: 0.4030 - val_loss: 1.7638 - val_acc: 0.4073\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.9653 - acc: 0.4148 - val_loss: 1.6773 - val_acc: 0.4159\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 2.0763 - acc: 0.3992 - val_loss: 1.7608 - val_acc: 0.3453\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 2.3681 - acc: 0.3882 - val_loss: 2.1601 - val_acc: 0.3388\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 2.3227 - acc: 0.3807 - val_loss: 1.8729 - val_acc: 0.3844\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 2.0176 - acc: 0.4029 - val_loss: 1.8208 - val_acc: 0.3508\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.8917 - acc: 0.4181 - val_loss: 1.8933 - val_acc: 0.4392\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 2.0310 - acc: 0.4239 - val_loss: 1.7828 - val_acc: 0.3705\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 2.0941 - acc: 0.4192 - val_loss: 1.7004 - val_acc: 0.4016\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 2.1479 - acc: 0.4186 - val_loss: 1.7269 - val_acc: 0.4732\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 2.1523 - acc: 0.4071 - val_loss: 2.0474 - val_acc: 0.2596\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 2.0342 - acc: 0.4231 - val_loss: 2.0409 - val_acc: 0.4764\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 179s 4ms/step - loss: 2.1339 - acc: 0.4070 - val_loss: 2.0909 - val_acc: 0.3665\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 182s 5ms/step - loss: 2.4025 - acc: 0.3711 - val_loss: 2.9320 - val_acc: 0.2792\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 183s 5ms/step - loss: 2.2890 - acc: 0.3360 - val_loss: 2.0724 - val_acc: 0.3111\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 186s 5ms/step - loss: 2.4307 - acc: 0.3026 - val_loss: 1.8431 - val_acc: 0.2661\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 185s 5ms/step - loss: 2.4998 - acc: 0.2781 - val_loss: 1.9799 - val_acc: 0.2797\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 185s 5ms/step - loss: 2.2547 - acc: 0.2810 - val_loss: 1.8377 - val_acc: 0.3058\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 186s 5ms/step - loss: 2.4199 - acc: 0.2846 - val_loss: 2.4363 - val_acc: 0.1206\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 188s 5ms/step - loss: 2.3417 - acc: 0.2770 - val_loss: 1.9490 - val_acc: 0.2203\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 188s 5ms/step - loss: 2.2974 - acc: 0.2698 - val_loss: 1.8632 - val_acc: 0.2800\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 188s 5ms/step - loss: 2.3186 - acc: 0.2669 - val_loss: 2.1144 - val_acc: 0.1805\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 187s 5ms/step - loss: 2.3289 - acc: 0.2753 - val_loss: 2.0222 - val_acc: 0.2859\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 189s 5ms/step - loss: 2.7283 - acc: 0.2731 - val_loss: 2.0965 - val_acc: 0.2167\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.6295 - acc: 0.2674 - val_loss: 1.9213 - val_acc: 0.2428\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.6608 - acc: 0.2449 - val_loss: 2.3490 - val_acc: 0.2654\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1977 - acc: 0.2340 - val_loss: 1.9140 - val_acc: 0.2595\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 125s 3ms/step - loss: 2.4246 - acc: 0.2758 - val_loss: 1.9860 - val_acc: 0.2194\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1235 - acc: 0.2491 - val_loss: 1.9829 - val_acc: 0.1883\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.2299 - acc: 0.2569 - val_loss: 2.0297 - val_acc: 0.2169\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2490 - acc: 0.2571 - val_loss: 1.9178 - val_acc: 0.2145\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.4809 - acc: 0.2519 - val_loss: 1.8401 - val_acc: 0.2277\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.5394 - acc: 0.2621 - val_loss: 1.8394 - val_acc: 0.2393\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2434 - acc: 0.2522 - val_loss: 2.0345 - val_acc: 0.2783\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.4479 - acc: 0.2424 - val_loss: 1.9913 - val_acc: 0.2054\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2557 - acc: 0.2450 - val_loss: 1.9143 - val_acc: 0.2136\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1050 - acc: 0.2397 - val_loss: 1.9882 - val_acc: 0.1981\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1782 - acc: 0.2493 - val_loss: 1.8489 - val_acc: 0.2086\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2433 - acc: 0.2370 - val_loss: 2.1775 - val_acc: 0.1801\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.4372 - acc: 0.2359 - val_loss: 1.9136 - val_acc: 0.2455\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2290 - acc: 0.2297 - val_loss: 2.0497 - val_acc: 0.1940\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.4096 - acc: 0.2465 - val_loss: 6.7898 - val_acc: 0.2193\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.1589 - acc: 0.2384 - val_loss: 1.8803 - val_acc: 0.2130\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.3094 - acc: 0.2437 - val_loss: 1.8720 - val_acc: 0.2222\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2869 - acc: 0.2330 - val_loss: 2.3607 - val_acc: 0.2764\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.6932 - acc: 0.2410 - val_loss: 8.4946 - val_acc: 0.2406\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.4560 - acc: 0.2478 - val_loss: 1.8235 - val_acc: 0.2426\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2951 - acc: 0.2419 - val_loss: 5.7313 - val_acc: 0.2402\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2002 - acc: 0.2349 - val_loss: 1.9686 - val_acc: 0.2108\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1979 - acc: 0.2440 - val_loss: 1.9354 - val_acc: 0.2129\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.5035 - acc: 0.2502 - val_loss: 1.9720 - val_acc: 0.2108\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2558 - acc: 0.2458 - val_loss: 1.8358 - val_acc: 0.2498\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 2.0791 - acc: 0.2371 - val_loss: 2.4974 - val_acc: 0.1211\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.3747 - acc: 0.2452 - val_loss: 1.8505 - val_acc: 0.2238\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.6309 - acc: 0.2457 - val_loss: 1.8541 - val_acc: 0.2598\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.5613 - acc: 0.2466 - val_loss: 1.9033 - val_acc: 0.2416\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.0503 - acc: 0.2387 - val_loss: 1.9952 - val_acc: 0.2087\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 2.4787 - acc: 0.2538 - val_loss: 1.9338 - val_acc: 0.2091\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.3766 - acc: 0.2506 - val_loss: 2.0775 - val_acc: 0.1881\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.2905 - acc: 0.2427 - val_loss: 1.8910 - val_acc: 0.2292\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.1075 - acc: 0.2403 - val_loss: 1.8610 - val_acc: 0.2231\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 4.1635 - acc: 0.2380 - val_loss: 2.0757 - val_acc: 0.2580\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 2.8488 - acc: 0.2477 - val_loss: 2.0811 - val_acc: 0.1750\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 2.5321 - acc: 0.2486 - val_loss: 1.8712 - val_acc: 0.2615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fce96e10>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.fit(images/255, y,\n",
    "          batch_size=None,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_split=0.2,\n",
    "          validation_data=None,\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "40000/40000 [==============================] - 821s 21ms/step - loss: 1.9476 - acc: 0.2991 - val_loss: 1.6137 - val_acc: 0.4214\n",
      "Epoch 2/250\n",
      "40000/40000 [==============================] - 4084s 102ms/step - loss: 1.5424 - acc: 0.4561 - val_loss: 1.6081 - val_acc: 0.4469\n",
      "Epoch 3/250\n",
      "40000/40000 [==============================] - 822s 21ms/step - loss: 1.4483 - acc: 0.4977 - val_loss: 1.7613 - val_acc: 0.4171\n",
      "Epoch 4/250\n",
      "40000/40000 [==============================] - 3998s 100ms/step - loss: 1.3592 - acc: 0.5301 - val_loss: 1.4824 - val_acc: 0.4685\n",
      "Epoch 5/250\n",
      "40000/40000 [==============================] - 878s 22ms/step - loss: 1.3005 - acc: 0.5519 - val_loss: 1.3672 - val_acc: 0.5141\n",
      "Epoch 6/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.2575 - acc: 0.5727 - val_loss: 1.3564 - val_acc: 0.5286\n",
      "Epoch 7/250\n",
      "40000/40000 [==============================] - 803s 20ms/step - loss: 1.2309 - acc: 0.5837 - val_loss: 1.3713 - val_acc: 0.5441\n",
      "Epoch 8/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.2169 - acc: 0.5906 - val_loss: 1.6085 - val_acc: 0.5235\n",
      "Epoch 9/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.2009 - acc: 0.5996 - val_loss: 1.4393 - val_acc: 0.5037\n",
      "Epoch 10/250\n",
      "40000/40000 [==============================] - 803s 20ms/step - loss: 1.1894 - acc: 0.6046 - val_loss: 1.4135 - val_acc: 0.5229\n",
      "Epoch 11/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1726 - acc: 0.6098 - val_loss: 1.4046 - val_acc: 0.5361\n",
      "Epoch 12/250\n",
      "40000/40000 [==============================] - 812s 20ms/step - loss: 1.1596 - acc: 0.6148 - val_loss: 1.6012 - val_acc: 0.4856\n",
      "Epoch 13/250\n",
      "40000/40000 [==============================] - 4463s 112ms/step - loss: 1.1543 - acc: 0.6224 - val_loss: 1.4904 - val_acc: 0.5373\n",
      "Epoch 14/250\n",
      "40000/40000 [==============================] - 849s 21ms/step - loss: 1.1414 - acc: 0.6251 - val_loss: 1.6811 - val_acc: 0.5019\n",
      "Epoch 15/250\n",
      "40000/40000 [==============================] - 805s 20ms/step - loss: 1.1297 - acc: 0.6271 - val_loss: 1.5663 - val_acc: 0.5429\n",
      "Epoch 16/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1444 - acc: 0.6270 - val_loss: 1.5416 - val_acc: 0.4717\n",
      "Epoch 17/250\n",
      "40000/40000 [==============================] - 809s 20ms/step - loss: 1.1226 - acc: 0.6337 - val_loss: 1.7183 - val_acc: 0.4762\n",
      "Epoch 18/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1303 - acc: 0.6350 - val_loss: 1.5404 - val_acc: 0.4847\n",
      "Epoch 19/250\n",
      "40000/40000 [==============================] - 806s 20ms/step - loss: 1.1210 - acc: 0.6386 - val_loss: 1.6264 - val_acc: 0.5232\n",
      "Epoch 20/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.0991 - acc: 0.6436 - val_loss: 1.6753 - val_acc: 0.5091\n",
      "Epoch 21/250\n",
      "40000/40000 [==============================] - 806s 20ms/step - loss: 1.1291 - acc: 0.6377 - val_loss: 1.6221 - val_acc: 0.5226\n",
      "Epoch 22/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1265 - acc: 0.6403 - val_loss: 1.5659 - val_acc: 0.4925\n",
      "Epoch 23/250\n",
      "40000/40000 [==============================] - 810s 20ms/step - loss: 1.1291 - acc: 0.6375 - val_loss: 1.7380 - val_acc: 0.4623\n",
      "Epoch 24/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1389 - acc: 0.6369 - val_loss: 1.5464 - val_acc: 0.4984\n",
      "Epoch 25/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.1359 - acc: 0.6412 - val_loss: 1.6633 - val_acc: 0.5042\n",
      "Epoch 26/250\n",
      "40000/40000 [==============================] - 809s 20ms/step - loss: 1.1243 - acc: 0.6381 - val_loss: 1.7601 - val_acc: 0.5280\n",
      "Epoch 27/250\n",
      "40000/40000 [==============================] - 808s 20ms/step - loss: 1.1304 - acc: 0.6374 - val_loss: 1.7603 - val_acc: 0.4927\n",
      "Epoch 28/250\n",
      "40000/40000 [==============================] - 810s 20ms/step - loss: 1.1162 - acc: 0.6414 - val_loss: 1.5943 - val_acc: 0.4707\n",
      "Epoch 29/250\n",
      "40000/40000 [==============================] - 809s 20ms/step - loss: 1.1285 - acc: 0.6412 - val_loss: 1.6480 - val_acc: 0.4875\n",
      "Epoch 30/250\n",
      "40000/40000 [==============================] - 811s 20ms/step - loss: 1.1364 - acc: 0.6435 - val_loss: 1.5721 - val_acc: 0.4653\n",
      "Epoch 31/250\n",
      "40000/40000 [==============================] - 811s 20ms/step - loss: 1.1778 - acc: 0.6290 - val_loss: 1.6640 - val_acc: 0.4871\n",
      "Epoch 32/250\n",
      "40000/40000 [==============================] - 807s 20ms/step - loss: 1.1513 - acc: 0.6349 - val_loss: 1.8360 - val_acc: 0.4592\n",
      "Epoch 33/250\n",
      "40000/40000 [==============================] - 812s 20ms/step - loss: 1.1524 - acc: 0.6350 - val_loss: 1.5751 - val_acc: 0.4971\n",
      "Epoch 34/250\n",
      "40000/40000 [==============================] - 825s 21ms/step - loss: 1.1260 - acc: 0.6429 - val_loss: 2.2385 - val_acc: 0.4894\n",
      "Epoch 35/250\n",
      "40000/40000 [==============================] - 1438s 36ms/step - loss: 1.1484 - acc: 0.6372 - val_loss: 1.6542 - val_acc: 0.4669\n",
      "Epoch 36/250\n",
      " 8448/40000 [=====>........................] - ETA: 19:55 - loss: 1.1451 - acc: 0.6377"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-9df74fc259d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(images/255, y,\n",
    "          batch_size=None,\n",
    "          epochs=250,\n",
    "          verbose=1,\n",
    "          validation_data=None,\n",
    "          validation_split=0.2,\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.1))\n",
    "model_conv.add(Conv2D(32, (4, 4), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv.compile(loss=rms,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 239s 5ms/step - loss: 1.9623 - acc: 0.2923 - val_loss: 1.7316 - val_acc: 0.3836\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 246s 5ms/step - loss: 1.5681 - acc: 0.4399 - val_loss: 1.4504 - val_acc: 0.4677\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 247s 5ms/step - loss: 1.3896 - acc: 0.5044 - val_loss: 1.3548 - val_acc: 0.5244\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 242s 5ms/step - loss: 1.2748 - acc: 0.5495 - val_loss: 1.4874 - val_acc: 0.4896\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 245s 5ms/step - loss: 1.1773 - acc: 0.5857 - val_loss: 1.1314 - val_acc: 0.6022\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 301s 6ms/step - loss: 1.1035 - acc: 0.6129 - val_loss: 1.1418 - val_acc: 0.5987\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 221s 4ms/step - loss: 1.0520 - acc: 0.6316 - val_loss: 1.0523 - val_acc: 0.6327\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 226s 5ms/step - loss: 0.9970 - acc: 0.6512 - val_loss: 0.9412 - val_acc: 0.6686\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 226s 5ms/step - loss: 0.9498 - acc: 0.6674 - val_loss: 0.8525 - val_acc: 0.7010\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 0.9069 - acc: 0.6822 - val_loss: 0.7986 - val_acc: 0.7209\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 275s 5ms/step - loss: 0.8765 - acc: 0.6925 - val_loss: 0.7815 - val_acc: 0.7311\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 261s 5ms/step - loss: 0.8391 - acc: 0.7060 - val_loss: 0.7223 - val_acc: 0.7484\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 286s 6ms/step - loss: 0.8076 - acc: 0.7148 - val_loss: 0.7196 - val_acc: 0.7454\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 136s 3ms/step - loss: 0.7766 - acc: 0.7259 - val_loss: 0.6551 - val_acc: 0.7682\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 0.7485 - acc: 0.7367 - val_loss: 0.6277 - val_acc: 0.7819\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 148s 3ms/step - loss: 0.7211 - acc: 0.7459 - val_loss: 0.6360 - val_acc: 0.7746\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 141s 3ms/step - loss: 0.6961 - acc: 0.7547 - val_loss: 0.5745 - val_acc: 0.7992\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 1598s 32ms/step - loss: 0.6741 - acc: 0.7636 - val_loss: 0.5511 - val_acc: 0.8100\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 133s 3ms/step - loss: 0.6526 - acc: 0.7698 - val_loss: 0.5504 - val_acc: 0.8106\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 126s 3ms/step - loss: 0.6250 - acc: 0.7797 - val_loss: 0.4727 - val_acc: 0.8385\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 0.5995 - acc: 0.7882 - val_loss: 0.5371 - val_acc: 0.8055\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 127s 3ms/step - loss: 0.5883 - acc: 0.7941 - val_loss: 0.5593 - val_acc: 0.7954\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 450s 9ms/step - loss: 0.5688 - acc: 0.7964 - val_loss: 0.4406 - val_acc: 0.8427\n",
      "Epoch 24/100\n",
      "49250/50000 [============================>.] - ETA: 1s - loss: 0.5461 - acc: 0.8076"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-339-34a1d24f0c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_roll_2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_var_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(images/255, y,\n",
    "          batch_size=250,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(images_roll_2/255, y_var_2),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 170s 4ms/step - loss: 0.8410 - acc: 0.7156 - val_loss: 0.7593 - val_acc: 0.7388\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8481 - acc: 0.7155 - val_loss: 0.7761 - val_acc: 0.7332\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8478 - acc: 0.7174 - val_loss: 0.7857 - val_acc: 0.7339\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 2766s 69ms/step - loss: 0.8430 - acc: 0.7164 - val_loss: 0.8695 - val_acc: 0.7079\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8461 - acc: 0.7163 - val_loss: 0.7957 - val_acc: 0.7230\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8442 - acc: 0.7174 - val_loss: 0.7734 - val_acc: 0.7361\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8413 - acc: 0.7183 - val_loss: 0.7726 - val_acc: 0.7366\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8363 - acc: 0.7169 - val_loss: 0.9544 - val_acc: 0.6641\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.8457 - acc: 0.7187 - val_loss: 0.7116 - val_acc: 0.7581\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 178s 4ms/step - loss: 0.8430 - acc: 0.7166 - val_loss: 0.7568 - val_acc: 0.7402\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 3762s 94ms/step - loss: 0.8402 - acc: 0.7172 - val_loss: 0.7785 - val_acc: 0.7290\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8396 - acc: 0.7172 - val_loss: 0.8264 - val_acc: 0.7198\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 184s 5ms/step - loss: 0.8381 - acc: 0.7179 - val_loss: 0.7331 - val_acc: 0.7547\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 187s 5ms/step - loss: 0.8425 - acc: 0.7176 - val_loss: 0.7389 - val_acc: 0.7494\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 190s 5ms/step - loss: 0.8441 - acc: 0.7182 - val_loss: 0.7652 - val_acc: 0.7400\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 192s 5ms/step - loss: 0.8329 - acc: 0.7190 - val_loss: 0.7680 - val_acc: 0.7369\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 195s 5ms/step - loss: 0.8429 - acc: 0.7175 - val_loss: 0.7197 - val_acc: 0.7567\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 208s 5ms/step - loss: 0.8353 - acc: 0.7214 - val_loss: 0.8626 - val_acc: 0.7061\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 212s 5ms/step - loss: 0.8335 - acc: 0.7224 - val_loss: 0.7740 - val_acc: 0.7325\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 217s 5ms/step - loss: 0.8265 - acc: 0.7195 - val_loss: 0.8028 - val_acc: 0.7336\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 222s 6ms/step - loss: 0.8359 - acc: 0.7186 - val_loss: 0.8003 - val_acc: 0.7320\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 226s 6ms/step - loss: 0.8395 - acc: 0.7193 - val_loss: 0.7721 - val_acc: 0.7378\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 220s 6ms/step - loss: 0.8373 - acc: 0.7205 - val_loss: 0.8592 - val_acc: 0.7052\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 220s 5ms/step - loss: 0.8412 - acc: 0.7196 - val_loss: 0.6938 - val_acc: 0.7672\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 174s 4ms/step - loss: 0.8329 - acc: 0.7198 - val_loss: 0.7110 - val_acc: 0.7609\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8383 - acc: 0.7177 - val_loss: 0.8968 - val_acc: 0.6969\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8324 - acc: 0.7224 - val_loss: 0.7930 - val_acc: 0.7244\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8334 - acc: 0.7219 - val_loss: 0.7780 - val_acc: 0.7362\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8341 - acc: 0.7219 - val_loss: 0.8611 - val_acc: 0.7110\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 180s 5ms/step - loss: 0.8306 - acc: 0.7227 - val_loss: 0.7436 - val_acc: 0.7450\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.8339 - acc: 0.7203 - val_loss: 0.7056 - val_acc: 0.7621\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8279 - acc: 0.7219 - val_loss: 0.8374 - val_acc: 0.7162\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8344 - acc: 0.7232 - val_loss: 0.8136 - val_acc: 0.7184\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8349 - acc: 0.7210 - val_loss: 0.7277 - val_acc: 0.7529\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8310 - acc: 0.7230 - val_loss: 0.8544 - val_acc: 0.7095\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8225 - acc: 0.7254 - val_loss: 0.7066 - val_acc: 0.7588\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8278 - acc: 0.7206 - val_loss: 0.7831 - val_acc: 0.7329\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8274 - acc: 0.7228 - val_loss: 0.7903 - val_acc: 0.7351\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8255 - acc: 0.7235 - val_loss: 0.8814 - val_acc: 0.7165\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8274 - acc: 0.7260 - val_loss: 0.8096 - val_acc: 0.7296\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8292 - acc: 0.7264 - val_loss: 0.7106 - val_acc: 0.7588\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8262 - acc: 0.7268 - val_loss: 0.8164 - val_acc: 0.7242\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8325 - acc: 0.7223 - val_loss: 0.7264 - val_acc: 0.7558\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8281 - acc: 0.7231 - val_loss: 0.8070 - val_acc: 0.7261\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8291 - acc: 0.7225 - val_loss: 0.7831 - val_acc: 0.7324\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8339 - acc: 0.7230 - val_loss: 0.8058 - val_acc: 0.7271\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8338 - acc: 0.7232 - val_loss: 0.7262 - val_acc: 0.7511\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8385 - acc: 0.7231 - val_loss: 0.7896 - val_acc: 0.7343\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8262 - acc: 0.7243 - val_loss: 0.7859 - val_acc: 0.7362\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8286 - acc: 0.7268 - val_loss: 0.8372 - val_acc: 0.7132\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8237 - acc: 0.7272 - val_loss: 1.0142 - val_acc: 0.6636\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 173s 4ms/step - loss: 0.8287 - acc: 0.7290 - val_loss: 0.6981 - val_acc: 0.7622\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 180s 4ms/step - loss: 0.8340 - acc: 0.7242 - val_loss: 0.8697 - val_acc: 0.7130\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8257 - acc: 0.7246 - val_loss: 0.7123 - val_acc: 0.7557\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8317 - acc: 0.7235 - val_loss: 0.7184 - val_acc: 0.7545\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8300 - acc: 0.7258 - val_loss: 0.7804 - val_acc: 0.7372\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8196 - acc: 0.7266 - val_loss: 0.8502 - val_acc: 0.7257\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8318 - acc: 0.7253 - val_loss: 0.6898 - val_acc: 0.7708\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8432 - acc: 0.7217 - val_loss: 0.7323 - val_acc: 0.7553\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8342 - acc: 0.7260 - val_loss: 0.6976 - val_acc: 0.7681\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8425 - acc: 0.7236 - val_loss: 0.7403 - val_acc: 0.7541\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8350 - acc: 0.7244 - val_loss: 0.8478 - val_acc: 0.7341\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8350 - acc: 0.7258 - val_loss: 0.7840 - val_acc: 0.7379\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8297 - acc: 0.7253 - val_loss: 0.8624 - val_acc: 0.7120\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8200 - acc: 0.7297 - val_loss: 0.9685 - val_acc: 0.6868\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8337 - acc: 0.7250 - val_loss: 0.7464 - val_acc: 0.7524\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8405 - acc: 0.7240 - val_loss: 0.7385 - val_acc: 0.7545\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8438 - acc: 0.7239 - val_loss: 0.7323 - val_acc: 0.7492\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8470 - acc: 0.7215 - val_loss: 0.7368 - val_acc: 0.7569\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8439 - acc: 0.7239 - val_loss: 0.8599 - val_acc: 0.7145\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8498 - acc: 0.7218 - val_loss: 0.8931 - val_acc: 0.7040\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8483 - acc: 0.7209 - val_loss: 0.7795 - val_acc: 0.7392\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 176s 4ms/step - loss: 0.8418 - acc: 0.7251 - val_loss: 0.7369 - val_acc: 0.7483\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.8295 - acc: 0.7274 - val_loss: 0.7402 - val_acc: 0.7598\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8419 - acc: 0.7224 - val_loss: 0.7899 - val_acc: 0.7378\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8556 - acc: 0.7219 - val_loss: 0.8182 - val_acc: 0.7276\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8480 - acc: 0.7224 - val_loss: 0.7305 - val_acc: 0.7597\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8521 - acc: 0.7223 - val_loss: 0.7452 - val_acc: 0.7523\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8337 - acc: 0.7227 - val_loss: 0.7854 - val_acc: 0.7315\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8372 - acc: 0.7265 - val_loss: 0.7514 - val_acc: 0.7467\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8224 - acc: 0.7285 - val_loss: 0.7271 - val_acc: 0.7596\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8367 - acc: 0.7252 - val_loss: 0.7578 - val_acc: 0.7484\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8548 - acc: 0.7229 - val_loss: 0.7471 - val_acc: 0.7611\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8461 - acc: 0.7249 - val_loss: 0.7089 - val_acc: 0.7649\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 0.8444 - acc: 0.7267 - val_loss: 0.7306 - val_acc: 0.7553\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8457 - acc: 0.7258 - val_loss: 0.8161 - val_acc: 0.7253\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8479 - acc: 0.7245 - val_loss: 0.7939 - val_acc: 0.7351\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8582 - acc: 0.7225 - val_loss: 0.8227 - val_acc: 0.7173\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8510 - acc: 0.7224 - val_loss: 0.7785 - val_acc: 0.7471\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8665 - acc: 0.7210 - val_loss: 0.7447 - val_acc: 0.7515\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 0.8562 - acc: 0.7217 - val_loss: 0.7886 - val_acc: 0.7388\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 0.8495 - acc: 0.7244 - val_loss: 0.8480 - val_acc: 0.7226\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8527 - acc: 0.7211 - val_loss: 0.8353 - val_acc: 0.7175\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8533 - acc: 0.7207 - val_loss: 0.7371 - val_acc: 0.7564\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8863 - acc: 0.7161 - val_loss: 0.7326 - val_acc: 0.7556\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8626 - acc: 0.7199 - val_loss: 0.7089 - val_acc: 0.7636\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8633 - acc: 0.7199 - val_loss: 0.7195 - val_acc: 0.7555\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8717 - acc: 0.7186 - val_loss: 0.7412 - val_acc: 0.7611\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8448 - acc: 0.7229 - val_loss: 0.7775 - val_acc: 0.7456\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8893 - acc: 0.7160 - val_loss: 0.7590 - val_acc: 0.7575\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8808 - acc: 0.7179 - val_loss: 0.7208 - val_acc: 0.7565\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8798 - acc: 0.7186 - val_loss: 0.7788 - val_acc: 0.7508\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8805 - acc: 0.7195 - val_loss: 0.8732 - val_acc: 0.7156\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8896 - acc: 0.7176 - val_loss: 0.8646 - val_acc: 0.7162\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8895 - acc: 0.7163 - val_loss: 0.7448 - val_acc: 0.7498\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8782 - acc: 0.7190 - val_loss: 0.9819 - val_acc: 0.6568\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8613 - acc: 0.7224 - val_loss: 0.8584 - val_acc: 0.7226\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8582 - acc: 0.7210 - val_loss: 0.7103 - val_acc: 0.7672\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8546 - acc: 0.7262 - val_loss: 0.7385 - val_acc: 0.7533\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.9160 - acc: 0.7120 - val_loss: 0.8340 - val_acc: 0.7114\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8974 - acc: 0.7135 - val_loss: 0.7709 - val_acc: 0.7404\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.8727 - acc: 0.7225 - val_loss: 0.7335 - val_acc: 0.7484\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 0.8926 - acc: 0.7152 - val_loss: 0.7477 - val_acc: 0.7506\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.8882 - acc: 0.7151 - val_loss: 0.7803 - val_acc: 0.7346\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8859 - acc: 0.7150 - val_loss: 0.9072 - val_acc: 0.7046\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8811 - acc: 0.7179 - val_loss: 0.8418 - val_acc: 0.7284\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8959 - acc: 0.7136 - val_loss: 0.7649 - val_acc: 0.7495\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8954 - acc: 0.7118 - val_loss: 0.7740 - val_acc: 0.7423\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8903 - acc: 0.7196 - val_loss: 0.7515 - val_acc: 0.7539\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8905 - acc: 0.7189 - val_loss: 0.7041 - val_acc: 0.7659\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.9099 - acc: 0.7138 - val_loss: 0.8550 - val_acc: 0.7139\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8776 - acc: 0.7173 - val_loss: 0.7939 - val_acc: 0.7352\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.9130 - acc: 0.7140 - val_loss: 0.7246 - val_acc: 0.7612\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 163s 4ms/step - loss: 0.8995 - acc: 0.7151 - val_loss: 0.7540 - val_acc: 0.7544\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 178s 4ms/step - loss: 0.9267 - acc: 0.7095 - val_loss: 0.7524 - val_acc: 0.7512\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 166s 4ms/step - loss: 0.9084 - acc: 0.7109 - val_loss: 0.7446 - val_acc: 0.7511\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.9076 - acc: 0.7137 - val_loss: 0.7859 - val_acc: 0.7434\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 167s 4ms/step - loss: 0.8916 - acc: 0.7122 - val_loss: 0.7767 - val_acc: 0.7329\n",
      "Epoch 129/200\n",
      "11280/40000 [=======>......................] - ETA: 1:52 - loss: 0.9064 - acc: 0.7073"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-0fc953a3c7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First update Wednesday morning\n",
    "model_conv_1 = Sequential()\n",
    "model_conv_1.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv_1.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv_1.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv_1.add(Dropout(.33))\n",
    "model_conv_1.add(Flatten())\n",
    "model_conv_1.add(Dense(1000, activation='relu'))\n",
    "model_conv_1.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/75\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.8652 - acc: 0.3076 - val_loss: 1.5270 - val_acc: 0.4495\n",
      "Epoch 2/75\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.5352 - acc: 0.4412 - val_loss: 1.5286 - val_acc: 0.4593\n",
      "Epoch 3/75\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3972 - acc: 0.4933 - val_loss: 1.3406 - val_acc: 0.5202\n",
      "Epoch 4/75\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 1.3092 - acc: 0.5256 - val_loss: 1.1695 - val_acc: 0.5847\n",
      "Epoch 5/75\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.2396 - acc: 0.5565 - val_loss: 1.2034 - val_acc: 0.5733\n",
      "Epoch 6/75\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.1781 - acc: 0.5782 - val_loss: 1.2422 - val_acc: 0.5663\n",
      "Epoch 7/75\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.1306 - acc: 0.6006 - val_loss: 1.0363 - val_acc: 0.6404\n",
      "Epoch 8/75\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.0889 - acc: 0.6143 - val_loss: 1.1078 - val_acc: 0.6198\n",
      "Epoch 9/75\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 1.0513 - acc: 0.6297 - val_loss: 1.0220 - val_acc: 0.6465\n",
      "Epoch 10/75\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 1.0228 - acc: 0.6380 - val_loss: 1.0945 - val_acc: 0.6247\n",
      "Epoch 11/75\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 1.0005 - acc: 0.6471 - val_loss: 0.9843 - val_acc: 0.6546\n",
      "Epoch 12/75\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9764 - acc: 0.6564 - val_loss: 0.9152 - val_acc: 0.6760\n",
      "Epoch 13/75\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.9550 - acc: 0.6658 - val_loss: 0.9062 - val_acc: 0.6869\n",
      "Epoch 14/75\n",
      "40000/40000 [==============================] - 155s 4ms/step - loss: 0.9434 - acc: 0.6727 - val_loss: 0.8969 - val_acc: 0.6926\n",
      "Epoch 15/75\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.9298 - acc: 0.6763 - val_loss: 0.9413 - val_acc: 0.6704\n",
      "Epoch 16/75\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.9093 - acc: 0.6817 - val_loss: 1.0893 - val_acc: 0.6399\n",
      "Epoch 17/75\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.9091 - acc: 0.6866 - val_loss: 0.9886 - val_acc: 0.6645\n",
      "Epoch 18/75\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 0.8928 - acc: 0.6927 - val_loss: 0.9209 - val_acc: 0.6809\n",
      "Epoch 19/75\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 0.8884 - acc: 0.6923 - val_loss: 0.9378 - val_acc: 0.6818\n",
      "Epoch 20/75\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 0.8873 - acc: 0.6937 - val_loss: 0.8495 - val_acc: 0.7054\n",
      "Epoch 21/75\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.8747 - acc: 0.6994 - val_loss: 0.8409 - val_acc: 0.7069\n",
      "Epoch 22/75\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.8750 - acc: 0.7004 - val_loss: 0.8786 - val_acc: 0.6928\n",
      "Epoch 23/75\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.8745 - acc: 0.7003 - val_loss: 0.8132 - val_acc: 0.7124\n",
      "Epoch 24/75\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 0.8738 - acc: 0.7020 - val_loss: 0.8316 - val_acc: 0.7139\n",
      "Epoch 25/75\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 0.8692 - acc: 0.7018 - val_loss: 0.8221 - val_acc: 0.7086\n",
      "Epoch 26/75\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.8642 - acc: 0.7053 - val_loss: 0.8169 - val_acc: 0.7186\n",
      "Epoch 27/75\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 0.8720 - acc: 0.7009 - val_loss: 0.8536 - val_acc: 0.7054\n",
      "Epoch 28/75\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.8657 - acc: 0.7036 - val_loss: 0.8349 - val_acc: 0.7057\n",
      "Epoch 29/75\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 0.8785 - acc: 0.7029 - val_loss: 0.8151 - val_acc: 0.7135\n",
      "Epoch 30/75\n",
      "40000/40000 [==============================] - 171s 4ms/step - loss: 0.8775 - acc: 0.7048 - val_loss: 0.8664 - val_acc: 0.7052\n",
      "Epoch 31/75\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.8754 - acc: 0.7047 - val_loss: 0.9920 - val_acc: 0.6622\n",
      "Epoch 32/75\n",
      "40000/40000 [==============================] - 160s 4ms/step - loss: 0.8697 - acc: 0.7073 - val_loss: 0.8110 - val_acc: 0.7256\n",
      "Epoch 33/75\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 0.8714 - acc: 0.7073 - val_loss: 0.8943 - val_acc: 0.6992\n",
      "Epoch 34/75\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.8719 - acc: 0.7070 - val_loss: 0.9611 - val_acc: 0.6630\n",
      "Epoch 35/75\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.8783 - acc: 0.7057 - val_loss: 0.8713 - val_acc: 0.7051\n",
      "Epoch 36/75\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.8760 - acc: 0.7066 - val_loss: 0.8540 - val_acc: 0.7065\n",
      "Epoch 37/75\n",
      "40000/40000 [==============================] - 155s 4ms/step - loss: 0.8829 - acc: 0.7038 - val_loss: 0.9096 - val_acc: 0.6917\n",
      "Epoch 38/75\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.8897 - acc: 0.7027 - val_loss: 0.8853 - val_acc: 0.7081\n",
      "Epoch 39/75\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 0.8804 - acc: 0.7059 - val_loss: 0.9598 - val_acc: 0.6695\n",
      "Epoch 40/75\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.8947 - acc: 0.7046 - val_loss: 0.7941 - val_acc: 0.7256\n",
      "Epoch 41/75\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.8969 - acc: 0.7035 - val_loss: 0.8749 - val_acc: 0.7043\n",
      "Epoch 42/75\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 0.8882 - acc: 0.7043 - val_loss: 0.9269 - val_acc: 0.6762\n",
      "Epoch 43/75\n",
      "40000/40000 [==============================] - 175s 4ms/step - loss: 0.9017 - acc: 0.7015 - val_loss: 0.8419 - val_acc: 0.7083\n",
      "Epoch 44/75\n",
      "40000/40000 [==============================] - 173s 4ms/step - loss: 0.9101 - acc: 0.7000 - val_loss: 0.8936 - val_acc: 0.6968\n",
      "Epoch 45/75\n",
      "40000/40000 [==============================] - 171s 4ms/step - loss: 0.9102 - acc: 0.7036 - val_loss: 0.8997 - val_acc: 0.6997\n",
      "Epoch 46/75\n",
      "40000/40000 [==============================] - 181s 5ms/step - loss: 0.8958 - acc: 0.7031 - val_loss: 0.8230 - val_acc: 0.7232\n",
      "Epoch 47/75\n",
      "40000/40000 [==============================] - 177s 4ms/step - loss: 0.9065 - acc: 0.7027 - val_loss: 0.8324 - val_acc: 0.7313\n",
      "Epoch 48/75\n",
      "40000/40000 [==============================] - 177s 4ms/step - loss: 0.9104 - acc: 0.7034 - val_loss: 0.8365 - val_acc: 0.7191\n",
      "Epoch 49/75\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.9099 - acc: 0.7008 - val_loss: 0.8321 - val_acc: 0.7229\n",
      "Epoch 50/75\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9035 - acc: 0.7036 - val_loss: 1.0005 - val_acc: 0.6791\n",
      "Epoch 51/75\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9177 - acc: 0.7031 - val_loss: 0.8564 - val_acc: 0.7115\n",
      "Epoch 52/75\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.9249 - acc: 0.6990 - val_loss: 0.8646 - val_acc: 0.6936\n",
      "Epoch 53/75\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9236 - acc: 0.7007 - val_loss: 1.0494 - val_acc: 0.6550\n",
      "Epoch 54/75\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 0.9202 - acc: 0.7024 - val_loss: 0.9386 - val_acc: 0.6932\n",
      "Epoch 55/75\n",
      "40000/40000 [==============================] - 169s 4ms/step - loss: 0.9230 - acc: 0.7015 - val_loss: 0.8551 - val_acc: 0.7059\n",
      "Epoch 56/75\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 0.9314 - acc: 0.6974 - val_loss: 0.9294 - val_acc: 0.6934\n",
      "Epoch 57/75\n",
      "40000/40000 [==============================] - 164s 4ms/step - loss: 0.9316 - acc: 0.6984 - val_loss: 1.1052 - val_acc: 0.6620\n",
      "Epoch 58/75\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.9331 - acc: 0.6971 - val_loss: 1.0067 - val_acc: 0.6373\n",
      "Epoch 59/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5471s 137ms/step - loss: 0.9486 - acc: 0.6921 - val_loss: 1.0446 - val_acc: 0.6576\n",
      "Epoch 60/75\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.9507 - acc: 0.6935 - val_loss: 1.0111 - val_acc: 0.6708\n",
      "Epoch 61/75\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.9491 - acc: 0.6942 - val_loss: 0.9646 - val_acc: 0.6720\n",
      "Epoch 62/75\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.9614 - acc: 0.6900 - val_loss: 0.9404 - val_acc: 0.6813\n",
      "Epoch 63/75\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.9578 - acc: 0.6872 - val_loss: 1.0511 - val_acc: 0.6395\n",
      "Epoch 64/75\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.9724 - acc: 0.6828 - val_loss: 1.0689 - val_acc: 0.6586\n",
      "Epoch 65/75\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.9830 - acc: 0.6862 - val_loss: 1.0556 - val_acc: 0.6526\n",
      "Epoch 66/75\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.9849 - acc: 0.6836 - val_loss: 0.9763 - val_acc: 0.6656\n",
      "Epoch 67/75\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.9885 - acc: 0.6863 - val_loss: 1.1840 - val_acc: 0.6087\n",
      "Epoch 68/75\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.9708 - acc: 0.6865 - val_loss: 1.0001 - val_acc: 0.6472\n",
      "Epoch 69/75\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 0.9825 - acc: 0.6822 - val_loss: 0.8777 - val_acc: 0.7008\n",
      "Epoch 70/75\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.9865 - acc: 0.6795 - val_loss: 1.0691 - val_acc: 0.6470\n",
      "Epoch 71/75\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.9942 - acc: 0.6838 - val_loss: 1.0778 - val_acc: 0.6336\n",
      "Epoch 72/75\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.9905 - acc: 0.6793 - val_loss: 0.9791 - val_acc: 0.6689\n",
      "Epoch 73/75\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.9835 - acc: 0.6813 - val_loss: 0.9731 - val_acc: 0.6897\n",
      "Epoch 74/75\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 0.9987 - acc: 0.6811 - val_loss: 1.3891 - val_acc: 0.5675\n",
      "Epoch 75/75\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.0143 - acc: 0.6784 - val_loss: 0.9580 - val_acc: 0.6754\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_conv_1.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=75,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSb3fiVC2nL2xZVcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b32d7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['acc'])\n",
    "plt.plot(history_2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUpAk9QwKkqSeQUGS1DMoSJJ6BgVJUu8PmgsXJuhkuOQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fcacf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_2.history['loss'])\n",
    "plt.plot(history_2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.007)\n",
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 14.4952 - acc: 0.0995 - val_loss: 14.4821 - val_acc: 0.1015\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 14.5123 - acc: 0.0996 - val_loss: 14.4821 - val_acc: 0.1015\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 4.8432 - acc: 0.1145 - val_loss: 2.1935 - val_acc: 0.1698\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 2.1600 - acc: 0.1879 - val_loss: 2.2174 - val_acc: 0.1865\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 2.0495 - acc: 0.2266 - val_loss: 1.9710 - val_acc: 0.2463\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.9864 - acc: 0.2541 - val_loss: 1.8968 - val_acc: 0.2746\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.9513 - acc: 0.2663 - val_loss: 1.8637 - val_acc: 0.2831\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.9553 - acc: 0.2624 - val_loss: 1.9047 - val_acc: 0.2535\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.9330 - acc: 0.2695 - val_loss: 1.9568 - val_acc: 0.2573\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.9344 - acc: 0.2716 - val_loss: 1.8753 - val_acc: 0.2788\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.9372 - acc: 0.2712 - val_loss: 1.9178 - val_acc: 0.2605\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.9520 - acc: 0.2680 - val_loss: 2.1385 - val_acc: 0.2088\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.9590 - acc: 0.2591 - val_loss: 1.9588 - val_acc: 0.2581\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.9852 - acc: 0.2508 - val_loss: 2.1190 - val_acc: 0.2234\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.9854 - acc: 0.2386 - val_loss: 1.9435 - val_acc: 0.2676\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.9626 - acc: 0.2597 - val_loss: 1.8979 - val_acc: 0.2387\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 1.9545 - acc: 0.2637 - val_loss: 1.9369 - val_acc: 0.2576\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 267s 7ms/step - loss: 1.9604 - acc: 0.2580 - val_loss: 1.8081 - val_acc: 0.3082\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 338s 8ms/step - loss: 1.9575 - acc: 0.2591 - val_loss: 1.8677 - val_acc: 0.2667\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 308s 8ms/step - loss: 1.9433 - acc: 0.2552 - val_loss: 1.8609 - val_acc: 0.2704\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 292s 7ms/step - loss: 1.9449 - acc: 0.2559 - val_loss: 1.8251 - val_acc: 0.2952\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 339s 8ms/step - loss: 1.9903 - acc: 0.2606 - val_loss: 1.8034 - val_acc: 0.2933\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 287s 7ms/step - loss: 1.9656 - acc: 0.2537 - val_loss: 1.8410 - val_acc: 0.2721\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 281s 7ms/step - loss: 2.0008 - acc: 0.2525 - val_loss: 1.8930 - val_acc: 0.2789\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 291s 7ms/step - loss: 2.0122 - acc: 0.2404 - val_loss: 2.0194 - val_acc: 0.2031\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 246s 6ms/step - loss: 2.1504 - acc: 0.1806 - val_loss: 2.0100 - val_acc: 0.2058\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 246s 6ms/step - loss: 2.1690 - acc: 0.1731 - val_loss: 2.0835 - val_acc: 0.1922\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 321s 8ms/step - loss: 2.1281 - acc: 0.1730 - val_loss: 2.0732 - val_acc: 0.1925\n",
      "Epoch 29/200\n",
      " 9180/40000 [=====>........................] - ETA: 3:53 - loss: 2.3355 - acc: 0.1520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0fc953a3c7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second update Wednesday morning\n",
    "model_conv = Sequential()\n",
    "model_conv.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(32,32,3)))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model_conv.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_conv.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model_conv.add(MaxPooling2D(pool_size=(1, 1), strides=(1, 1)))\n",
    "model_conv.add(Flatten())\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Dense(1000, activation='relu'))\n",
    "model_conv.add(Dropout(.33))\n",
    "model_conv.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rms = keras.optimizers.RMSprop(lr=0.0005)\n",
    "model_conv.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 250s 6ms/step - loss: 1.8238 - acc: 0.3275 - val_loss: 1.5485 - val_acc: 0.4318\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 155s 4ms/step - loss: 1.4591 - acc: 0.4724 - val_loss: 1.4112 - val_acc: 0.4916\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.3075 - acc: 0.5308 - val_loss: 1.2044 - val_acc: 0.5688\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 1.1995 - acc: 0.5709 - val_loss: 1.2178 - val_acc: 0.5738\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 1.1099 - acc: 0.6070 - val_loss: 1.0817 - val_acc: 0.6162\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 1.0372 - acc: 0.6337 - val_loss: 1.1651 - val_acc: 0.5930\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 160s 4ms/step - loss: 0.9762 - acc: 0.6583 - val_loss: 1.0667 - val_acc: 0.6371\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9178 - acc: 0.6760 - val_loss: 1.0766 - val_acc: 0.6338\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 157s 4ms/step - loss: 0.8757 - acc: 0.6944 - val_loss: 1.0611 - val_acc: 0.6464\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 154s 4ms/step - loss: 0.8332 - acc: 0.7067 - val_loss: 0.9762 - val_acc: 0.6618\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.7977 - acc: 0.7203 - val_loss: 0.9556 - val_acc: 0.6780\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 162s 4ms/step - loss: 0.7638 - acc: 0.7362 - val_loss: 0.9523 - val_acc: 0.6813\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.7340 - acc: 0.7430 - val_loss: 0.9320 - val_acc: 0.6880\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 156s 4ms/step - loss: 0.7121 - acc: 0.7517 - val_loss: 0.9603 - val_acc: 0.6792\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 159s 4ms/step - loss: 0.6916 - acc: 0.7610 - val_loss: 1.1205 - val_acc: 0.6623\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.6707 - acc: 0.7693 - val_loss: 1.0262 - val_acc: 0.6775\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.6556 - acc: 0.7730 - val_loss: 1.2017 - val_acc: 0.6503\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 5856s 146ms/step - loss: 0.6403 - acc: 0.7789 - val_loss: 1.0210 - val_acc: 0.6852\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 475s 12ms/step - loss: 0.6196 - acc: 0.7883 - val_loss: 1.0837 - val_acc: 0.6927\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.6185 - acc: 0.7890 - val_loss: 1.1091 - val_acc: 0.6967\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.6093 - acc: 0.7907 - val_loss: 1.1440 - val_acc: 0.6912\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 0.6102 - acc: 0.7925 - val_loss: 1.0847 - val_acc: 0.6854\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.5910 - acc: 0.8002 - val_loss: 1.0992 - val_acc: 0.6897\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.5986 - acc: 0.8030 - val_loss: 1.2083 - val_acc: 0.6736\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.5941 - acc: 0.8036 - val_loss: 1.2195 - val_acc: 0.6896\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.5898 - acc: 0.8012 - val_loss: 1.2004 - val_acc: 0.6971\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.5837 - acc: 0.8047 - val_loss: 1.2009 - val_acc: 0.6943\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.5813 - acc: 0.8071 - val_loss: 1.0653 - val_acc: 0.6981\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.5913 - acc: 0.8071 - val_loss: 1.1822 - val_acc: 0.6961\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.6015 - acc: 0.8003 - val_loss: 1.2213 - val_acc: 0.6424\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.6127 - acc: 0.8028 - val_loss: 1.2692 - val_acc: 0.7043\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.6153 - acc: 0.7978 - val_loss: 1.4916 - val_acc: 0.6597\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 158s 4ms/step - loss: 0.6156 - acc: 0.7987 - val_loss: 1.4619 - val_acc: 0.6584\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.6267 - acc: 0.7988 - val_loss: 1.1963 - val_acc: 0.6978\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.6222 - acc: 0.7975 - val_loss: 1.2607 - val_acc: 0.6630\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.6278 - acc: 0.7980 - val_loss: 1.2544 - val_acc: 0.6872\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.6362 - acc: 0.7941 - val_loss: 1.1919 - val_acc: 0.6737\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.6362 - acc: 0.7946 - val_loss: 2.0186 - val_acc: 0.5398\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.6476 - acc: 0.7938 - val_loss: 1.4567 - val_acc: 0.6905\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.6338 - acc: 0.7968 - val_loss: 1.1410 - val_acc: 0.6937\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.6536 - acc: 0.7887 - val_loss: 1.4750 - val_acc: 0.6719\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6578 - acc: 0.7899 - val_loss: 1.1655 - val_acc: 0.6880\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6537 - acc: 0.7901 - val_loss: 1.5648 - val_acc: 0.6830\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6698 - acc: 0.7883 - val_loss: 1.1764 - val_acc: 0.7094\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.6749 - acc: 0.7870 - val_loss: 1.3104 - val_acc: 0.6865\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.6787 - acc: 0.7847 - val_loss: 1.2001 - val_acc: 0.6960\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 0.6802 - acc: 0.7851 - val_loss: 1.1171 - val_acc: 0.6920\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.6873 - acc: 0.7843 - val_loss: 1.1290 - val_acc: 0.7001\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.7070 - acc: 0.7775 - val_loss: 0.9936 - val_acc: 0.7057\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.7123 - acc: 0.7762 - val_loss: 1.0767 - val_acc: 0.6885\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.7255 - acc: 0.7745 - val_loss: 1.2881 - val_acc: 0.6382\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.7242 - acc: 0.7748 - val_loss: 1.0580 - val_acc: 0.6924\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.7452 - acc: 0.7703 - val_loss: 1.5175 - val_acc: 0.6792\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.7533 - acc: 0.7689 - val_loss: 1.0246 - val_acc: 0.6918\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 0.7451 - acc: 0.7653 - val_loss: 1.3472 - val_acc: 0.6313\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.7617 - acc: 0.7642 - val_loss: 1.1613 - val_acc: 0.6376\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 0.7857 - acc: 0.7566 - val_loss: 1.3047 - val_acc: 0.6908\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 0.7862 - acc: 0.7550 - val_loss: 1.1592 - val_acc: 0.6650\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8104 - acc: 0.7508 - val_loss: 1.2170 - val_acc: 0.5919\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.7969 - acc: 0.7521 - val_loss: 1.0348 - val_acc: 0.6700\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8057 - acc: 0.7519 - val_loss: 1.0688 - val_acc: 0.6524\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8282 - acc: 0.7427 - val_loss: 1.8857 - val_acc: 0.6700\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.8372 - acc: 0.7408 - val_loss: 1.3668 - val_acc: 0.6851\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.8420 - acc: 0.7421 - val_loss: 1.1672 - val_acc: 0.6833\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.8528 - acc: 0.7399 - val_loss: 1.0444 - val_acc: 0.6659\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 672s 17ms/step - loss: 0.8564 - acc: 0.7378 - val_loss: 1.3187 - val_acc: 0.6325\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.8777 - acc: 0.7342 - val_loss: 1.1851 - val_acc: 0.6572\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 0.8883 - acc: 0.7273 - val_loss: 1.1682 - val_acc: 0.6771\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 152s 4ms/step - loss: 0.9162 - acc: 0.7229 - val_loss: 1.1075 - val_acc: 0.6806\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 0.9291 - acc: 0.7175 - val_loss: 1.1822 - val_acc: 0.6261\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 0.9310 - acc: 0.7178 - val_loss: 1.1397 - val_acc: 0.6316\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 0.9270 - acc: 0.7234 - val_loss: 1.1243 - val_acc: 0.6790\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 0.9443 - acc: 0.7148 - val_loss: 1.6007 - val_acc: 0.6097\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.9575 - acc: 0.7146 - val_loss: 1.6907 - val_acc: 0.5297\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 0.9307 - acc: 0.7189 - val_loss: 1.0807 - val_acc: 0.6901\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 0.9922 - acc: 0.7102 - val_loss: 1.1602 - val_acc: 0.6631\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 0.9942 - acc: 0.7080 - val_loss: 1.1729 - val_acc: 0.6610\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 0.9748 - acc: 0.7062 - val_loss: 1.1788 - val_acc: 0.6702\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 0.9897 - acc: 0.7030 - val_loss: 1.1699 - val_acc: 0.6454\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 0.9886 - acc: 0.7049 - val_loss: 1.3742 - val_acc: 0.6555\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 0.9922 - acc: 0.7008 - val_loss: 1.0632 - val_acc: 0.6622\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.0374 - acc: 0.6911 - val_loss: 1.3781 - val_acc: 0.6460\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.0275 - acc: 0.6922 - val_loss: 1.4495 - val_acc: 0.5986\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.0511 - acc: 0.6879 - val_loss: 1.7541 - val_acc: 0.4196\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.0660 - acc: 0.6855 - val_loss: 1.1857 - val_acc: 0.6081\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.0447 - acc: 0.6861 - val_loss: 1.2270 - val_acc: 0.6027\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.1092 - acc: 0.6768 - val_loss: 1.2322 - val_acc: 0.6234\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.0959 - acc: 0.6706 - val_loss: 1.0951 - val_acc: 0.6362\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.1240 - acc: 0.6760 - val_loss: 1.2349 - val_acc: 0.5882\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.1547 - acc: 0.6565 - val_loss: 1.4126 - val_acc: 0.6533\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 1.2161 - acc: 0.6418 - val_loss: 2.1359 - val_acc: 0.5491\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.1747 - acc: 0.6445 - val_loss: 1.4702 - val_acc: 0.5828\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 1.2358 - acc: 0.6359 - val_loss: 1.5421 - val_acc: 0.5590\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.1783 - acc: 0.6462 - val_loss: 1.2715 - val_acc: 0.6261\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.2459 - acc: 0.6277 - val_loss: 1.4204 - val_acc: 0.4989\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.2479 - acc: 0.6232 - val_loss: 1.4036 - val_acc: 0.6466\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 161s 4ms/step - loss: 1.2699 - acc: 0.6168 - val_loss: 1.2629 - val_acc: 0.6232\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 168s 4ms/step - loss: 1.2865 - acc: 0.6217 - val_loss: 1.1754 - val_acc: 0.6204\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 165s 4ms/step - loss: 1.2989 - acc: 0.6208 - val_loss: 1.3703 - val_acc: 0.5649\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 151s 4ms/step - loss: 1.2946 - acc: 0.6224 - val_loss: 1.3524 - val_acc: 0.5431\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.2904 - acc: 0.6117 - val_loss: 1.4632 - val_acc: 0.4777\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.2742 - acc: 0.6139 - val_loss: 1.3384 - val_acc: 0.6349\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 138s 3ms/step - loss: 1.3110 - acc: 0.6054 - val_loss: 1.2978 - val_acc: 0.5882\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.3266 - acc: 0.5940 - val_loss: 1.6077 - val_acc: 0.4233\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.3377 - acc: 0.5931 - val_loss: 1.2546 - val_acc: 0.5845\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.3449 - acc: 0.6013 - val_loss: 1.5601 - val_acc: 0.4512\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3621 - acc: 0.5980 - val_loss: 1.3200 - val_acc: 0.5500\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3054 - acc: 0.6038 - val_loss: 1.2006 - val_acc: 0.6507\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 1.3756 - acc: 0.5895 - val_loss: 1.9453 - val_acc: 0.4600\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 150s 4ms/step - loss: 1.3829 - acc: 0.5870 - val_loss: 1.6768 - val_acc: 0.4080\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.3744 - acc: 0.5881 - val_loss: 1.2292 - val_acc: 0.5917\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.3850 - acc: 0.5835 - val_loss: 1.2382 - val_acc: 0.5919\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.4076 - acc: 0.5873 - val_loss: 1.4926 - val_acc: 0.4906\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.3271 - acc: 0.5853 - val_loss: 1.4754 - val_acc: 0.5498\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.4205 - acc: 0.5763 - val_loss: 1.3065 - val_acc: 0.5679\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.3642 - acc: 0.5845 - val_loss: 1.2002 - val_acc: 0.6308\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.3957 - acc: 0.5743 - val_loss: 1.4106 - val_acc: 0.5354\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.4229 - acc: 0.5733 - val_loss: 1.2989 - val_acc: 0.6080\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 1.8392 - acc: 0.5652 - val_loss: 1.5075 - val_acc: 0.6007\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 235s 6ms/step - loss: 1.5071 - acc: 0.5713 - val_loss: 1.3346 - val_acc: 0.5150\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 272s 7ms/step - loss: 1.4576 - acc: 0.5670 - val_loss: 1.4032 - val_acc: 0.5028\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 261s 7ms/step - loss: 1.4214 - acc: 0.5683 - val_loss: 1.4151 - val_acc: 0.5038\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 253s 6ms/step - loss: 1.4176 - acc: 0.5601 - val_loss: 1.5455 - val_acc: 0.4469\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 248s 6ms/step - loss: 1.4656 - acc: 0.5633 - val_loss: 1.8816 - val_acc: 0.4306\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 235s 6ms/step - loss: 1.4549 - acc: 0.5695 - val_loss: 1.5860 - val_acc: 0.5314\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 219s 5ms/step - loss: 1.4493 - acc: 0.5512 - val_loss: 1.7158 - val_acc: 0.4777\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 217s 5ms/step - loss: 1.4677 - acc: 0.5581 - val_loss: 2.0624 - val_acc: 0.5322\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 217s 5ms/step - loss: 1.4628 - acc: 0.5593 - val_loss: 4.4438 - val_acc: 0.4573\n",
      "Epoch 129/200\n",
      "40000/40000 [==============================] - 227s 6ms/step - loss: 1.4969 - acc: 0.5403 - val_loss: 1.6452 - val_acc: 0.4491\n",
      "Epoch 130/200\n",
      "40000/40000 [==============================] - 236s 6ms/step - loss: 8.1592 - acc: 0.3191 - val_loss: 1.7063 - val_acc: 0.3726\n",
      "Epoch 131/200\n",
      "40000/40000 [==============================] - 226s 6ms/step - loss: 1.5229 - acc: 0.5373 - val_loss: 1.5611 - val_acc: 0.4859\n",
      "Epoch 132/200\n",
      "40000/40000 [==============================] - 214s 5ms/step - loss: 1.5168 - acc: 0.5349 - val_loss: 1.8596 - val_acc: 0.3594\n",
      "Epoch 133/200\n",
      "40000/40000 [==============================] - 194s 5ms/step - loss: 1.4825 - acc: 0.5349 - val_loss: 1.5399 - val_acc: 0.4148\n",
      "Epoch 134/200\n",
      "40000/40000 [==============================] - 193s 5ms/step - loss: 1.5244 - acc: 0.5297 - val_loss: 1.6276 - val_acc: 0.4530\n",
      "Epoch 135/200\n",
      "40000/40000 [==============================] - 192s 5ms/step - loss: 1.5542 - acc: 0.5190 - val_loss: 1.4649 - val_acc: 0.4962\n",
      "Epoch 136/200\n",
      "40000/40000 [==============================] - 212s 5ms/step - loss: 1.5757 - acc: 0.5207 - val_loss: 2.0674 - val_acc: 0.5538\n",
      "Epoch 137/200\n",
      "40000/40000 [==============================] - 239s 6ms/step - loss: 1.6410 - acc: 0.5197 - val_loss: 1.6173 - val_acc: 0.3929\n",
      "Epoch 138/200\n",
      "40000/40000 [==============================] - 212s 5ms/step - loss: 1.6588 - acc: 0.5038 - val_loss: 1.4270 - val_acc: 0.4991\n",
      "Epoch 139/200\n",
      "40000/40000 [==============================] - 186s 5ms/step - loss: 1.6389 - acc: 0.5110 - val_loss: 3.6493 - val_acc: 0.4541\n",
      "Epoch 140/200\n",
      "40000/40000 [==============================] - 187s 5ms/step - loss: 1.5955 - acc: 0.5091 - val_loss: 2.0512 - val_acc: 0.4047\n",
      "Epoch 141/200\n",
      "40000/40000 [==============================] - 183s 5ms/step - loss: 1.6138 - acc: 0.5105 - val_loss: 1.3525 - val_acc: 0.5577\n",
      "Epoch 142/200\n",
      "40000/40000 [==============================] - 30066s 752ms/step - loss: 1.6029 - acc: 0.5075 - val_loss: 1.5340 - val_acc: 0.4789\n",
      "Epoch 143/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 1.7026 - acc: 0.4787 - val_loss: 1.6536 - val_acc: 0.3936\n",
      "Epoch 144/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.7337 - acc: 0.4928 - val_loss: 1.7835 - val_acc: 0.5520\n",
      "Epoch 145/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.6963 - acc: 0.4916 - val_loss: 1.4889 - val_acc: 0.5691\n",
      "Epoch 146/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.8118 - acc: 0.4652 - val_loss: 1.4853 - val_acc: 0.4986\n",
      "Epoch 147/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.7054 - acc: 0.4753 - val_loss: 1.5385 - val_acc: 0.4454\n",
      "Epoch 148/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.5857 - acc: 0.4913 - val_loss: 1.5979 - val_acc: 0.3798\n",
      "Epoch 149/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.6683 - acc: 0.4897 - val_loss: 2.9992 - val_acc: 0.4282\n",
      "Epoch 150/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.8244 - acc: 0.4734 - val_loss: 1.4540 - val_acc: 0.5627\n",
      "Epoch 151/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.7956 - acc: 0.4517 - val_loss: 1.8411 - val_acc: 0.3851\n",
      "Epoch 152/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 1.7110 - acc: 0.4739 - val_loss: 1.6336 - val_acc: 0.5111\n",
      "Epoch 153/200\n",
      "40000/40000 [==============================] - 140s 3ms/step - loss: 1.7328 - acc: 0.4673 - val_loss: 1.5511 - val_acc: 0.4655\n",
      "Epoch 154/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.7952 - acc: 0.4575 - val_loss: 1.3744 - val_acc: 0.5420\n",
      "Epoch 155/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.7947 - acc: 0.4562 - val_loss: 1.4210 - val_acc: 0.4826\n",
      "Epoch 156/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.7960 - acc: 0.4499 - val_loss: 1.5875 - val_acc: 0.4012\n",
      "Epoch 157/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 1.7208 - acc: 0.4443 - val_loss: 1.8756 - val_acc: 0.5015\n",
      "Epoch 158/200\n",
      "40000/40000 [==============================] - 3407s 85ms/step - loss: 2.0184 - acc: 0.4340 - val_loss: 1.4503 - val_acc: 0.4876\n",
      "Epoch 159/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7120 - acc: 0.4614 - val_loss: 1.7272 - val_acc: 0.5438\n",
      "Epoch 160/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 1.7395 - acc: 0.4661 - val_loss: 1.5958 - val_acc: 0.4377\n",
      "Epoch 161/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.8394 - acc: 0.4535 - val_loss: 1.4266 - val_acc: 0.4908\n",
      "Epoch 162/200\n",
      "40000/40000 [==============================] - 141s 4ms/step - loss: 1.7234 - acc: 0.4630 - val_loss: 1.5029 - val_acc: 0.5269\n",
      "Epoch 163/200\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 1.7479 - acc: 0.4685 - val_loss: 1.7298 - val_acc: 0.3919\n",
      "Epoch 164/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.7504 - acc: 0.4476 - val_loss: 1.5122 - val_acc: 0.5018\n",
      "Epoch 165/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.8192 - acc: 0.4610 - val_loss: 1.5730 - val_acc: 0.5074\n",
      "Epoch 166/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.7151 - acc: 0.4547 - val_loss: 1.7155 - val_acc: 0.4986\n",
      "Epoch 167/200\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.8154 - acc: 0.4657 - val_loss: 1.6586 - val_acc: 0.4329\n",
      "Epoch 168/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 1.7172 - acc: 0.4732 - val_loss: 1.7101 - val_acc: 0.4355\n",
      "Epoch 169/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.7633 - acc: 0.4679 - val_loss: 1.5857 - val_acc: 0.4514\n",
      "Epoch 170/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 1.7844 - acc: 0.4641 - val_loss: 1.5483 - val_acc: 0.4744\n",
      "Epoch 171/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 1.7548 - acc: 0.4550 - val_loss: 1.6111 - val_acc: 0.4786\n",
      "Epoch 172/200\n",
      "40000/40000 [==============================] - 136s 3ms/step - loss: 1.9340 - acc: 0.4490 - val_loss: 1.4564 - val_acc: 0.5101\n",
      "Epoch 173/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 1.9740 - acc: 0.4594 - val_loss: 1.8086 - val_acc: 0.4686\n",
      "Epoch 174/200\n",
      "40000/40000 [==============================] - 144s 4ms/step - loss: 1.7035 - acc: 0.4658 - val_loss: 1.5316 - val_acc: 0.4522\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 133s 3ms/step - loss: 1.8709 - acc: 0.4484 - val_loss: 1.3184 - val_acc: 0.5528\n",
      "Epoch 176/200\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 1.7118 - acc: 0.4701 - val_loss: 1.8058 - val_acc: 0.5221\n",
      "Epoch 177/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7123 - acc: 0.4741 - val_loss: 1.6827 - val_acc: 0.4787\n",
      "Epoch 178/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7581 - acc: 0.4638 - val_loss: 1.5107 - val_acc: 0.4576\n",
      "Epoch 179/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 1.7179 - acc: 0.4651 - val_loss: 1.6084 - val_acc: 0.4421\n",
      "Epoch 180/200\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 2.2466 - acc: 0.4355 - val_loss: 1.8357 - val_acc: 0.3727\n",
      "Epoch 181/200\n",
      "40000/40000 [==============================] - 266s 7ms/step - loss: 1.9919 - acc: 0.4556 - val_loss: 1.8881 - val_acc: 0.3848\n",
      "Epoch 182/200\n",
      "40000/40000 [==============================] - 3725s 93ms/step - loss: 3.3913 - acc: 0.4252 - val_loss: 13.0734 - val_acc: 0.1889\n",
      "Epoch 183/200\n",
      "40000/40000 [==============================] - 184s 5ms/step - loss: 13.9160 - acc: 0.1366 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 184/200\n",
      "40000/40000 [==============================] - 149s 4ms/step - loss: 14.4559 - acc: 0.1031 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 185/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 14.4563 - acc: 0.1031 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 186/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 14.4575 - acc: 0.1030 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 187/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 14.4499 - acc: 0.1035 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 188/200\n",
      "40000/40000 [==============================] - 142s 4ms/step - loss: 14.4406 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 189/200\n",
      "40000/40000 [==============================] - 146s 4ms/step - loss: 14.4398 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 190/200\n",
      "40000/40000 [==============================] - 172s 4ms/step - loss: 14.4398 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 191/200\n",
      "40000/40000 [==============================] - 153s 4ms/step - loss: 14.4467 - acc: 0.1037 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 192/200\n",
      "40000/40000 [==============================] - 137s 3ms/step - loss: 14.4378 - acc: 0.1043 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 193/200\n",
      "40000/40000 [==============================] - 139s 3ms/step - loss: 14.4551 - acc: 0.1032 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 194/200\n",
      "40000/40000 [==============================] - 148s 4ms/step - loss: 14.4523 - acc: 0.1034 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 195/200\n",
      "40000/40000 [==============================] - 140s 4ms/step - loss: 14.4599 - acc: 0.1029 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 196/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 14.4499 - acc: 0.1035 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 197/200\n",
      "40000/40000 [==============================] - 143s 4ms/step - loss: 14.4410 - acc: 0.1041 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 198/200\n",
      "40000/40000 [==============================] - 145s 4ms/step - loss: 14.4563 - acc: 0.1031 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 199/200\n",
      "40000/40000 [==============================] - 147s 4ms/step - loss: 14.4487 - acc: 0.1036 - val_loss: 14.5353 - val_acc: 0.0982\n",
      "Epoch 200/200\n",
      "40000/40000 [==============================] - 161s 4ms/step - loss: 14.4479 - acc: 0.1036 - val_loss: 14.5353 - val_acc: 0.0982\n"
     ]
    }
   ],
   "source": [
    "history = model_conv.fit(X_train/255, y_train,\n",
    "          batch_size=60,\n",
    "          epochs=200,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test/255, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVU4KZmZW5aRgZmZVTgpmZlb1/32MSsongxD/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fa75278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19fbdd7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
