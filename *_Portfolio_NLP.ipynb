{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kylehoward/Desktop/Thinkful/Thinkful-2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kylehoward/desktop/thinkful/thinkful-2') \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for scientific articles\n",
    "This project is aimes at clustering scientific articles.\n",
    "\n",
    "## Why are we interested?\n",
    "There is so much scientific information online and countless resources to gain insights into scientific problems that I wanted to create a way to organize information based topic. This will help take disjointed information on the same topic and build resource databases\n",
    "\n",
    "## Why this dataset?\n",
    "Science is one of the fields where I believe data science can dramatically increase productivity and discovery by consolidating information and making it easier to access. I also wanted to create a model that succeeds in clustering articles within the same field.\n",
    "\n",
    "## What are we trying to achieve?\n",
    "\n",
    "The goal is to create a filter for articles and/or scientific documentation that can be used to unify scientific information. Once this model is successful it would make sense to create sub categories within the parent categories and continue to do so until we can easily access specific topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All articles are from https://www.sciencenews.org\n",
    "# Load Test\n",
    "f = open(\"Article_2.txt\", \"r\")\n",
    "f = f.read()\n",
    "#print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Article_1.txt', 'Article_2.txt', 'Article_3.txt', 'Article_4.txt', 'Article_5.txt', 'Article_6.txt', 'Article_7.txt', 'Article_8.txt', 'Article_9.txt', 'Article_10.txt', 'Article_11.txt', 'Article_12.txt', 'Article_13.txt', 'Article_14.txt', 'Article_15.txt', 'Article_16.txt', 'Article_17.txt', 'Article_18.txt', 'Article_19.txt', 'Article_20.txt', 'Article_21.txt', 'Article_22.txt', 'Article_23.txt', 'Article_24.txt', 'Article_25.txt', 'Article_26.txt', 'Article_27.txt', 'Article_28.txt', 'Article_29.txt', 'Article_30.txt', 'Article_31.txt', 'Article_32.txt', 'Article_33.txt', 'Article_34.txt', 'Article_35.txt', 'Article_36.txt', 'Article_37.txt', 'Article_38.txt', 'Article_39.txt', 'Article_40.txt', 'Article_41.txt', 'Article_42.txt', 'Article_43.txt', 'Article_44.txt', 'Article_45.txt', 'Article_46.txt', 'Article_47.txt', 'Article_48.txt', 'Article_49.txt', 'Article_50.txt', 'Article_51.txt', 'Article_52.txt', 'Article_53.txt', 'Article_54.txt', 'Article_55.txt', 'Article_56.txt', 'Article_57.txt', 'Article_58.txt', 'Article_59.txt', 'Article_60.txt', 'Article_61.txt', 'Article_62.txt', 'Article_63.txt', 'Article_64.txt', 'Article_65.txt', 'Article_66.txt', 'Article_67.txt', 'Article_68.txt', 'Article_69.txt', 'Article_70.txt', 'Article_71.txt', 'Article_72.txt', 'Article_73.txt', 'Article_74.txt', 'Article_75.txt', 'Article_76.txt', 'Article_77.txt', 'Article_78.txt', 'Article_79.txt', 'Article_80.txt', 'Article_81.txt', 'Article_82.txt', 'Article_83.txt', 'Article_84.txt', 'Article_85.txt', 'Article_86.txt', 'Article_87.txt', 'Article_88.txt', 'Article_89.txt', 'Article_90.txt', 'Article_91.txt', 'Article_92.txt', 'Article_93.txt', 'Article_94.txt', 'Article_95.txt', 'Article_96.txt', 'Article_97.txt', 'Article_98.txt', 'Article_99.txt', 'Article_100.txt']\n"
     ]
    }
   ],
   "source": [
    "# Getting 100 unique article names\n",
    "text_list = []\n",
    "for x in range(101):\n",
    "    y = \"Article_\" + str(x) + \".txt\"\n",
    "    text_list.append(y)\n",
    "# Adjust list because there is no article '0'\n",
    "text_list = text_list[1:]\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading in all articles\n",
    "article_list = []\n",
    "for x in text_list:\n",
    "    f = open(x, \"r\")\n",
    "    f = f.read()\n",
    "    article_list.append(f)\n",
    "#print(article_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356922"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data. Removing all characters except for letters. Removing whitespace \n",
    "w2v = \"\"\n",
    "for art in article_list:\n",
    "    work = art\n",
    "    w2v = w2v + work\n",
    "\n",
    "\n",
    "w2v = re.sub(\"[^a-zA-Z]\",\" \", w2v)\n",
    "w2v = ' '.join(w2v.split())\n",
    "len(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizing \n",
    "nlp = spacy.load('en')\n",
    "w2v_tok = nlp(w2v)\n",
    "#print(w2v_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 356922 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Converting tokens to sentences and lemmas. Assuring punctuation is scrubbed\n",
    "sentences = []\n",
    "for sentence in w2v_tok.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "\n",
    "#print(sentences)\n",
    "print('We have {} tokens.'.format(len(w2v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------CBOW word2vec-----------------------------------------------\n",
    "# Attempting to group words together based on proximity. Using CBOW due to smaller corpus\n",
    "cbow_model = word2vec.Word2Vec(\n",
    "    sentences,     \n",
    "    min_count=5,   # Minimum word count threshold.\n",
    "    window=15,     # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity to planet\n",
      "[('dust', 0.9997695684432983), ('scientist', 0.9996869564056396), ('spacecraft', 0.9996809363365173), ('telescope', 0.9996238946914673), ('filament', 0.9995774626731873), ('phenomenon', 0.9995768070220947), ('size', 0.9995599985122681), ('difficult', 0.9995546936988831), ('universe', 0.9995542764663696), ('stream', 0.9995222091674805)]\n",
      "Cosine similatrity analysis:\n",
      "study / research\n",
      "0.999560225565\n",
      "atom / molecule\n",
      "0.999296659344\n",
      "Which doesnt fit?\n",
      "bat / bird / dinosaur / bee\n",
      "dinosaur\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'function that adds up vectors, normalize vector?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec analysis \n",
    "vocab = cbow_model.wv.vocab.keys()\n",
    "\n",
    "# Testing to see which words it will group with 'planet'\n",
    "print('Similarity to planet')\n",
    "print(cbow_model.wv.most_similar('planet'))\n",
    "\n",
    "# Testing for similarity \n",
    "print('Cosine similatrity analysis:')\n",
    "print('study / research')\n",
    "print(cbow_model.wv.similarity('study', 'research'))\n",
    "print('atom / molecule')\n",
    "print(cbow_model.wv.similarity('atom', 'molecule'))\n",
    "\n",
    "# Testing to see if it will leave out the right word (looking for 'dinosaur')\n",
    "print('Which doesnt fit?')\n",
    "print('bat / bird / dinosaur / bee')\n",
    "print(cbow_model.doesnt_match(\"bat bird dinosaur bee\".split()))\n",
    "\n",
    "'''function that adds up vectors, normalize vector?'''\n",
    "# compare\n",
    "# study / molecule\n",
    "# atom / research\n",
    "# unrelated\n",
    "# more doesn't match\n",
    "# dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3383\n",
      "Original article: ﻿The deep waters of the East Pacific hold an unprepossessing treasure trove: potato-sized lumps of rock that contain valuable metals such as manganese, cobalt and copper. Turns out, such “manganese nodules” are home to another kind of goody: a species of sponge never before seen, researchers report online September 24 in Systematics and Biodiversity. These newly discovered nodule-dwellers may help scientists monitor the impact of future deep-sea mining.\n",
      "Little is known about life in the abyssal depths of the ocean, 4,000 to 6,000 meters down. But the prospect of mining in those depths is looming: For example, the United Nation’s International Seabed Authority has granted 16 exploration contracts for mining manganese nodules. To track how mining will affect deep-sea ecosystems over time, scientists are eager to establish a baseline of existing biodiversity in regions such as the Clarion-Clipperton Zone (CCZ), an area in the eastern Pacific Ocean littered with the nodules.\n",
      "The new sponge species may be the key to that baseline. Zoologist Swee-Cheng Lim of the National University of Singapore and colleagues examined samples of manganese nodules retrieved from the CCZ in 2015 that were covered in snow-white patches of sponge. Based on the sponges’ unusual star-shaped spicules — skeletal parts that support the sponge’s soft tissues — the team suspected they’d found a new species. DNA analyses confirmed it. Dubbed Plenaster craigi, this species’ proximity to the nodules may make it the perfect canary in the coal mine.\n",
      "\n",
      "\n",
      "Tf_idf vector: {'canary': 0.12263352353261588, 'dubbed': 0.096001767050829537, 'analyses': 0.099627449625172143, 'tissues': 0.11482229759886016, 'soft': 0.12263352353261588, 'support': 0.099627449625172143, 'skeletal': 0.10876344005476327, 'unusual': 0.11482229759886016, 'white': 0.096001767050829537, 'snow': 0.12263352353261588, 'samples': 0.099627449625172143, 'key': 0.084992462436554631, 'eastern': 0.11482229759886016, 'area': 0.10381299298458525, 'zone': 0.12263352353261588, 'existing': 0.096001767050829537, 'establish': 0.12263352353261588, 'nation': 0.12263352353261588, 'example': 0.075535139427229364, 'prospect': 0.12263352353261588, 'ocean': 0.21752688010952653, 'impact': 0.12263352353261588, 'home': 0.092803688370310361, 'copper': 0.12263352353261588, 'cobalt': 0.12263352353261588, 'metals': 0.12263352353261588, 'contain': 0.10876344005476327, 'pacific': 0.21752688010952653, 'east': 0.10876344005476327, 'rock': 0.096001767050829537, 'confirmed': 0.10876344005476327, 'dna': 0.084992462436554631, 'exploration': 0.10381299298458525, 'shaped': 0.096001767050829537, 'suspected': 0.10876344005476327, 'patches': 0.11482229759886016, 'perfect': 0.10876344005476327, 'examined': 0.10876344005476327, 'meters': 0.10876344005476327, 'hold': 0.096001767050829537, 'sponges': 0.12263352353261588, 'sea': 0.19200353410165907, 'ecosystems': 0.12263352353261588, 'international': 0.092803688370310361, 'regions': 0.096001767050829537, 'baseline': 0.22964459519772032, 'seabed': 0.12263352353261588, 'waters': 0.10381299298458525, 'monitor': 0.10381299298458525, 'newly': 0.099627449625172143, 'sized': 0.096001767050829537, '24': 0.084992462436554631, 'kind': 0.077181236502798928, 'seen': 0.089942909506732643, 'united': 0.077181236502798928, 'star': 0.092803688370310361, 'turns': 0.084992462436554631, 'national': 0.072515109162336652, 'parts': 0.073983157822279738, 'species': 0.27919045785146646, '16': 0.065063521414605127, 'life': 0.063998589390470539, 'based': 0.075535139427229364, 'little': 0.080806919077141534, 'discovered': 0.072515109162336652, 'affect': 0.089942909506732643, 'future': 0.069797614462866614, 'track': 0.087355020237535136, '2015': 0.072515109162336652, 'covered': 0.10876344005476327, 'deep': 0.26206506071260538}\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------tfidf-------------------------------------------------\n",
    "\n",
    "X_train, X_test = train_test_split(article_list, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.25,          # drop words that occur in more than a quarter of the articles\n",
    "                             min_df=2,             # only use words that appear at least twice\n",
    "                             stop_words='english', # picking list of stop words\n",
    "                             lowercase=True,       # convert everything to lower case \n",
    "                             use_idf=True,         # use inverse document frequencies in our weighting\n",
    "                             norm=u'l2',           # Treat longer and shorter paragraphs equally\n",
    "                             smooth_idf=True       # Prevent divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "# Applying the vectorizer\n",
    "article_list_tfidf = vectorizer.fit_transform(article_list)\n",
    "print(\"Number of features: %d\" % article_list_tfidf.get_shape()[1])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(article_list_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "# Reshape the vectorizer output to be legible \n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "# number of articles\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "# Making a list of dictionaries, one per article\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "# List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# for each article, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "\n",
    "print('Original article:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for i in range(5):\\n    print('Component {}:'.format(i))\\n    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying SVD to reduce term document matrix into lower dimensional space (from 3417 to 260)\n",
    "svd= TruncatedSVD(260)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data and projecting the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "# Checking variance explained\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100) # Upon review seems like overfitting\n",
    "\n",
    "# Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "# It seems like some similarities exist but needs refining\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "\n",
    "#Commenting this out for easier viewing on github\n",
    "'''for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.0410816381907\n",
      "  (0, 10)\t0.0225543949874\n",
      "  (0, 11)\t0.0701455913849\n",
      "  (0, 13)\t0.021439218399\n",
      "  (0, 28)\t0.0247840006085\n",
      "  (0, 42)\t0.0242922113966\n",
      "  (0, 47)\t0.0333747145272\n",
      "  (0, 54)\t0.0410816381907\n",
      "  (0, 57)\t0.0292635099454\n",
      "  (0, 58)\t0.0225543949874\n",
      "  (0, 84)\t0.0364352272037\n",
      "  (0, 110)\t0.0364352272037\n",
      "  (0, 133)\t0.0264423764715\n",
      "  (0, 211)\t0.0264423764715\n",
      "  (0, 218)\t0.0410816381907\n",
      "  (0, 231)\t0.0333747145272\n",
      "  (0, 266)\t0.0364352272037\n",
      "  (0, 278)\t0.0410816381907\n",
      "  (0, 282)\t0.0292635099454\n",
      "  (0, 285)\t0.0333747145272\n",
      "  (0, 290)\t0.0347768513407\n",
      "  (0, 299)\t0.0333747145272\n",
      "  (0, 315)\t0.0964803851191\n",
      "  (0, 350)\t0.0333747145272\n",
      "  (0, 356)\t0.0284720644908\n",
      "  :\t:\n",
      "  (59, 1561)\t0.141547507116\n",
      "  (59, 1624)\t0.180814062888\n",
      "  (59, 1735)\t0.109082695871\n",
      "  (59, 1829)\t0.160363650358\n",
      "  (59, 1926)\t0.180814062888\n",
      "  (59, 1982)\t0.180814062888\n",
      "  (59, 1984)\t0.180814062888\n",
      "  (59, 2023)\t0.10691816667\n",
      "  (59, 2129)\t0.180814062888\n",
      "  (59, 2217)\t0.0943611879807\n",
      "  (59, 2342)\t0.169296987813\n",
      "  (59, 2343)\t0.0992694540254\n",
      "  (59, 2502)\t0.160363650358\n",
      "  (59, 2571)\t0.104864688964\n",
      "  (59, 2599)\t0.122110668678\n",
      "  (59, 2620)\t0.153064582191\n",
      "  (59, 2632)\t0.146893307988\n",
      "  (59, 2720)\t0.136832176569\n",
      "  (59, 2900)\t0.10691816667\n",
      "  (59, 2996)\t0.132614169661\n",
      "  (59, 3007)\t0.180814062888\n",
      "  (59, 3028)\t0.113798026419\n",
      "  (59, 3055)\t0.169296987813\n",
      "  (59, 3215)\t0.113798026419\n",
      "  (59, 3296)\t0.0943611879807\n"
     ]
    }
   ],
   "source": [
    "# Viewing the vectorizer output\n",
    "print(X_train_tfidf_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------LDA for clustering------------------------------------------\n",
    "# Attempting to group each article with Latent Dirichlet Allocation\n",
    "# Cleaning data with a function\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    # Remove stopwords\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    # Remove punctuation\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    # Get lemmas for all words\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    # Remove numbers and outliers\n",
    "    normalized_2 = re.sub(\"[^a-zA-Z]\",\" \", w2v)\n",
    "    # Remove extra whitespace\n",
    "    normalized_3 = ' '.join(normalized_2.split())\n",
    "    return normalized\n",
    "# Apply function\n",
    "doc_clean = [clean(doc).split() for doc in article_list]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpora' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-370acf81de0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpora' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the doc term matrix to use as our input\n",
    "doc_term_matrix = [dictionary.doc2bow(article_list) for doc in article_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the word vectors\n",
    "word_vectors = cbow_model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, \n",
    "               num_topics=10,        # With 100 articles I'm thinking 10% is a good test run\n",
    "               id2word = dictionary, \n",
    "               alpha=1,              # number of topics\n",
    "               passes=250)           # Number of passes over the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding percent certainty for every category within each article\n",
    "org = []\n",
    "def get_org():\n",
    "    for d in article_list:\n",
    "        bow = dictionary.doc2bow(d.split())\n",
    "        t = ldamodel.get_document_topics(bow)\n",
    "        org.append(t)\n",
    "        print(org)\n",
    "    return org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable to iterate over\n",
    "lst = get_org()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x = lst[0]\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(x[1])\n",
    "y = x[1]\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over the list to select the category that has the highest level of certainty and returning it to a list\n",
    "maxes = []\n",
    "for f in lst:\n",
    "    mx = 0\n",
    "    leng = len(f)\n",
    "    counter = 0\n",
    "    for x in f:\n",
    "        counter += 1\n",
    "        y = x[1]\n",
    "        if y > mx:\n",
    "            mx = y\n",
    "            mxx = x\n",
    "        if counter == leng:\n",
    "            maxes.append(mxx)\n",
    "        \n",
    "print(maxes)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of catrgories to match with the article list\n",
    "category = []\n",
    "for x in maxes:\n",
    "    y = x[0]\n",
    "    category.append(y)\n",
    "print(category)\n",
    "len(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating dataframe \n",
    "df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df continued\n",
    "df['category'] = category\n",
    "df['article'] = text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking our dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export to compare to article topic list in excel\n",
    "df.to_csv('lda_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Category 4 was 100% accurate while others need more manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
