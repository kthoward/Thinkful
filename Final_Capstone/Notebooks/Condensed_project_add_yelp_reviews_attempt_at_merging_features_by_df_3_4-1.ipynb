{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import json\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_file_names = glob.glob(\"./zip_2/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_data_list_2 = []\n",
    "fail_list = []\n",
    "for i, data in enumerate(list_of_file_names):\n",
    "    try:\n",
    "        data = pd.read_csv(data)\n",
    "        read_data_list_2.append(data)\n",
    "    except UnicodeDecodeError:\n",
    "        fail_list.append(i)\n",
    "list_of_file_names_org = list_of_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in fail_list:\n",
    "    del list_of_file_names[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./zip_2/Zip_MedianListingPricePerSqft_AllHomes.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_file_names[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build loop for data we can work with. Only dates and zip\n",
    "def build_useful_df(data_list):\n",
    "    useful_df = []\n",
    "    for dataset in range(len(data_list)):\n",
    "        data = read_data_list_2[dataset].copy()\n",
    "        region_name = data['RegionName']\n",
    "        region_name = region_name.astype(str)\n",
    "        new_df = data.select_dtypes(include=['float64'])\n",
    "        new_df.insert(loc=0, column='RegionName', value=region_name) \n",
    "        new_df = new_df.sort_values('RegionName',ascending=False)\n",
    "        new_df = new_df.set_index('RegionName')\n",
    "        useful_df.append(new_df)\n",
    "    return useful_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_df = build_useful_df(read_data_list_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sixteen_list = []\n",
    "for i, file in enumerate(useful_df):\n",
    "    if len(file) >= 10000:\n",
    "        sixteen_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sixteen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "good_columns = []\n",
    "for i, file in enumerate(useful_df):\n",
    "    columns = file.columns\n",
    "    if '2017-01' in columns:\n",
    "        good_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sixteen_good = []\n",
    "for num in sixteen_list:\n",
    "    if num in good_columns:\n",
    "        sixteen_good.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sixteen_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_data = useful_df[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data_list = []\n",
    "for i, data in enumerate(useful_df):\n",
    "    if i in sixteen_good:\n",
    "        final_data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10066"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Docs that don't need feature engineering. They are already aggregates\n",
    "pure_features = [0, 71, 75, 77, 79, 81]\n",
    "#useful_df[0].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(df, past_time_string, now_string):\n",
    "    #df.dropna(inplace=True)\n",
    "    features = pd.DataFrame()\n",
    "    #features['RegionName'] = df['RegionName']\n",
    "    mean = df.loc[:, past_time_string : now_string].mean(axis=1)\n",
    "    features['mean'] = mean\n",
    "    std = df.loc[:, past_time_string : now_string].std(axis=1)\n",
    "    features['std'] = std\n",
    "    mn = df.loc[:, past_time_string : now_string].min(axis=1)\n",
    "    features['min'] = mn\n",
    "    mx = df.loc[:, past_time_string : now_string].max(axis=1)\n",
    "    features['max'] = mx\n",
    "    features['swing'] = mx - mn\n",
    "    change = df[now_string] - df[past_time_string]\n",
    "    features['change'] = change\n",
    "    mean_swing = features['swing'].mean()\n",
    "    features['swing_pos'] = np.where(features['swing']>mean_swing, 1, 0)\n",
    "    big_swing = features['swing'].std() + mean_swing\n",
    "    features['swing_big'] = np.where(features['swing']>big_swing, 1, 0)\n",
    "    features['swing_neg'] = np.where(features['swing']<mean_swing, 1, 0)\n",
    "    swing_big_loss = mean_swing - features['swing'].std() \n",
    "    features['swing_loss_big'] = np.where(features['swing']<swing_big_loss, 1, 0)\n",
    "    return features\n",
    "    \n",
    "\n",
    "def make_modeling_data(df_list, df_for_target, now_string):\n",
    "    df_one = pd.DataFrame()\n",
    "    target = pd.DataFrame()\n",
    "    now_time = pd.to_datetime(now_string)\n",
    "    now_value = df_for_target[now_string]\n",
    "    future_time = now_time + timedelta(days=6*31)\n",
    "    future_time_string= future_time.strftime(\"%Y-%m\")\n",
    "    future_value = df_for_target[future_time_string]\n",
    "    target['target'] = future_value/now_value\n",
    "    \n",
    "    \n",
    "    past_time = now_time - timedelta(days=6*31)\n",
    "    past_time_string= past_time.strftime(\"%Y-%m\")\n",
    "    df_to_use_for_features_org= df_list[0].loc[:, :now_string]\n",
    "    features_org = make_features(df_to_use_for_features_org, past_time_string, now_string)\n",
    "    df_one = pd.merge(df_one, features_org, left_index=True, right_index=True, how = 'right')\n",
    "    for i, df in enumerate(df_list[1:]):\n",
    "        ind = str(i)\n",
    "        columns = df.columns\n",
    "        if '2011-01' in columns and '2012-01' in columns and '2013-01' in columns and '2014-01' in columns and '2015-01' in columns and '2016-01' in columns and '2017-01' in columns:\n",
    "            df_to_use_for_features= df.loc[:, :now_string]\n",
    "            features = make_features(df_to_use_for_features, past_time_string, now_string)\n",
    "            df_one = pd.merge(df_one, features, right_index=True, left_index=True, how='inner')\n",
    "    target = target.loc[df_one.index]    \n",
    "    \n",
    "\n",
    "    \n",
    "    return df_one, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_features, test_targets = make_modeling_data(final_data_list, target_data, \"2017-06\")\n",
    "train_features, train_targets = make_modeling_data(final_data_list, target_data, \"2017-01\")\n",
    "train_features = train_features.append(train_features)\n",
    "train_targets = train_targets.append(train_targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "date_counter = 0\n",
    "for year in [\"2012\", \"2013\", \"2014\", \"2015\", \"2016\"]:\n",
    "    for month in [\"06\",\"12\"]:\n",
    "            new_time = year+\"-\"+month\n",
    "            date_counter += 1\n",
    "            extra_train_features, extra_train_targets = make_modeling_data(final_data_list, target_data, new_time)\n",
    "            train_features = train_features.append(extra_train_features)\n",
    "            train_targets = train_targets.append(extra_train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w2v features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_features = pd.read_csv('real_business_w2v_features_3_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_features['postal_code'] = w2v_features['postal_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_features.index = w2v_features['postal_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_features = w2v_features.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_features = pd.read_csv('real_business_features_3_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(959, 11)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_features['postal_code'] = business_features['postal_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_features = business_features.set_index('postal_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = train_features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = test_features.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_targets = train_targets.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_targets = test_targets.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging on index for train features and targets so it doesn't multiply \n",
    "train_merge = pd.merge(train_features, train_targets, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_merge = pd.merge(test_features, test_targets, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_merge = train_merge.set_index('RegionName_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_merge = test_merge.set_index('RegionName_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_train_merge = pd.merge(train_merge, business_features, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_test_merge = pd.merge(test_merge, business_features, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_train_merge = pd.merge(train_merge, w2v_features, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_test_merge = pd.merge(test_merge, w2v_features, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_zillow_test_merge = pd.merge(test_merge, business_features, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nztmc = new_zillow_test_merge.columns\n",
    "test_merge = new_zillow_test_merge.iloc[:, :-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_zillow_train_merge = pd.merge(train_merge, business_features, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_merge = new_zillow_train_merge.iloc[:, :-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_merge = train_merge.drop('RegionName_y', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_merge = test_merge.drop('RegionName_y', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_train_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_test_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_train_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_test_merge.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating arrays and train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_merge.iloc[:, :-1].values\n",
    "y_train = train_merge.iloc[:, -1].values\n",
    "X_test = test_merge.iloc[:, :-1].values\n",
    "y_test = test_merge.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_train_merge = business_train_merge.drop('RegionName_y', 1)\n",
    "business_test_merge = business_test_merge.drop('RegionName_y', 1)\n",
    "y_train_business = business_train_merge.loc[:, 'target'].values\n",
    "y_test_business = business_test_merge.loc[:, 'target'].values\n",
    "business_train = pd.DataFrame()\n",
    "business_train_features = business_train_merge.drop('target', 1).values\n",
    "X_train_business = pd.DataFrame()\n",
    "X_train_business = business_train_features\n",
    "X_test_business_features = pd.DataFrame()\n",
    "X_test_business_features = business_test_merge.drop('target', 1).values\n",
    "X_test_business = pd.DataFrame()\n",
    "X_test_business = X_test_business_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_train_merge = w2v_train_merge.drop('RegionName_y', 1)\n",
    "w2v_test_merge = w2v_test_merge.drop('RegionName_y', 1)\n",
    "y_train_w2v = w2v_train_merge.loc[:, 'target'].values\n",
    "y_test_w2v = w2v_test_merge.loc[:, 'target'].values\n",
    "w2v_train = pd.DataFrame()\n",
    "w2v_train_features = w2v_train_merge.drop('target', 1).values\n",
    "X_train_w2v = pd.DataFrame()\n",
    "X_train_w2v = w2v_train_features\n",
    "X_test_w2v_features = pd.DataFrame()\n",
    "X_test_w2v_features = w2v_test_merge.drop('target', 1).values\n",
    "X_test_w2v = pd.DataFrame()\n",
    "X_test_w2v = X_test_w2v_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_per_column(X):\n",
    "    new_array = []\n",
    "    shape = X.shape\n",
    "    columns = shape[1]\n",
    "    for num in range(columns):\n",
    "        mx = X[:, num].max()\n",
    "        if mx == 0:\n",
    "            new_array.append(X[:, num])\n",
    "        else:\n",
    "            X[:, num] = X[:, num] / mx\n",
    "            new_array.append(X[:, num])\n",
    "    new_array = np.array(new_array)\n",
    "    new_array = np.transpose(new_array)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = norm_per_column(X_train)\n",
    "X_test = norm_per_column(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_business = norm_per_column(X_train_business)\n",
    "X_test_business = norm_per_column(X_test_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_w2v = norm_per_column(X_train_w2v)\n",
    "X_test_w2v = norm_per_column(X_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028560998107322942"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03568676443\n",
      "0.809566962437\n",
      "1.32300764356\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean())\n",
    "print(y_train.min())\n",
    "print(y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01812630226\n",
      "0.833675690212\n",
      "1.18704258259\n"
     ]
    }
   ],
   "source": [
    "print(y_test.mean())\n",
    "print(y_test.min())\n",
    "print(y_test.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating classes for targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_y_class_2(y_list):\n",
    "    y_df = pd.DataFrame()\n",
    "    y_1 = []\n",
    "    y_2 = []\n",
    "    y_3 = []\n",
    "    y_4 = []\n",
    "    y_5 = []\n",
    "    \n",
    "    for y in y_list:\n",
    "        if y <= .9:\n",
    "            y_1.append(1)\n",
    "        else:\n",
    "            y_1.append(0)\n",
    "        if y > .9 and y <= .97:\n",
    "            y_2.append(1)\n",
    "        else:\n",
    "            y_2.append(0)\n",
    "        if y > .97 and y < 1.03:\n",
    "            y_3.append(1)\n",
    "        else:\n",
    "            y_3.append(0)\n",
    "        if y >= 1.03 and y < 1.1:\n",
    "            y_4.append(1)\n",
    "        else:\n",
    "            y_4.append(0)\n",
    "        if y >= 1.1:\n",
    "            y_5.append(1)\n",
    "        else:\n",
    "            y_5.append(0)\n",
    "    y_df['1'] = y_1\n",
    "    y_df['2'] = y_2\n",
    "    y_df['3'] = y_3\n",
    "    y_df['4'] = y_4\n",
    "    y_df['5'] = y_5\n",
    "    return y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat_train = create_y_class_2(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat_test = create_y_class_2(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat_train_w2v = create_y_class_2(y_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat_test_w2v = create_y_class_2(y_test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat_train_business = create_y_class_2(y_train_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cat_test_business = create_y_class_2(y_test_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028560998107322942"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_business.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(120,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.005)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/500\n",
      "2839/2839 [==============================] - 4s 1ms/step - loss: 2.4648 - val_loss: 1.7106\n",
      "Epoch 2/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.2855 - val_loss: 0.1537\n",
      "Epoch 3/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.3109 - val_loss: 0.1096\n",
      "Epoch 4/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.2574 - val_loss: 0.1539\n",
      "Epoch 5/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.2579 - val_loss: 0.1283\n",
      "Epoch 6/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.2167 - val_loss: 0.0970\n",
      "Epoch 7/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1962 - val_loss: 0.1111\n",
      "Epoch 8/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1968 - val_loss: 0.1375\n",
      "Epoch 9/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.1932 - val_loss: 0.1611\n",
      "Epoch 10/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1972 - val_loss: 0.1335\n",
      "Epoch 11/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1687 - val_loss: 0.1073\n",
      "Epoch 12/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1510 - val_loss: 0.1419\n",
      "Epoch 13/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1655 - val_loss: 0.1780\n",
      "Epoch 14/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1719 - val_loss: 0.1590\n",
      "Epoch 15/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1576 - val_loss: 0.1539\n",
      "Epoch 16/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1512 - val_loss: 0.1357\n",
      "Epoch 17/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1372 - val_loss: 0.1861\n",
      "Epoch 18/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1562 - val_loss: 0.1777\n",
      "Epoch 19/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1614 - val_loss: 0.2200\n",
      "Epoch 20/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1699 - val_loss: 0.1103\n",
      "Epoch 21/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.1272 - val_loss: 0.1843\n",
      "Epoch 22/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.1403 - val_loss: 0.1314\n",
      "Epoch 23/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.125 - 0s 35us/step - loss: 0.1212 - val_loss: 0.1353\n",
      "Epoch 24/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.1211 - val_loss: 0.1382\n",
      "Epoch 25/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1250 - val_loss: 0.1611\n",
      "Epoch 26/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1397 - val_loss: 0.1712\n",
      "Epoch 27/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1308 - val_loss: 0.1114\n",
      "Epoch 28/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1086 - val_loss: 0.1727\n",
      "Epoch 29/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1309 - val_loss: 0.1041\n",
      "Epoch 30/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0950 - val_loss: 0.1143\n",
      "Epoch 31/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.1132 - val_loss: 0.1937\n",
      "Epoch 32/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.1387 - val_loss: 0.0824\n",
      "Epoch 33/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0849 - val_loss: 0.0788\n",
      "Epoch 34/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0866 - val_loss: 0.1122\n",
      "Epoch 35/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1069 - val_loss: 0.1915\n",
      "Epoch 36/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1402 - val_loss: 0.0771\n",
      "Epoch 37/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0780 - val_loss: 0.0931\n",
      "Epoch 38/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0912 - val_loss: 0.1346\n",
      "Epoch 39/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1165 - val_loss: 0.1183\n",
      "Epoch 40/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0958 - val_loss: 0.0655\n",
      "Epoch 41/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0732 - val_loss: 0.0834\n",
      "Epoch 42/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0801 - val_loss: 0.0769\n",
      "Epoch 43/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0807 - val_loss: 0.0958\n",
      "Epoch 44/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0836 - val_loss: 0.0828\n",
      "Epoch 45/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0801 - val_loss: 0.0660\n",
      "Epoch 46/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0685 - val_loss: 0.0766\n",
      "Epoch 47/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0713 - val_loss: 0.0580\n",
      "Epoch 48/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0643 - val_loss: 0.0618\n",
      "Epoch 49/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0618 - val_loss: 0.0576\n",
      "Epoch 50/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0669 - val_loss: 0.0717\n",
      "Epoch 51/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0642 - val_loss: 0.0417\n",
      "Epoch 52/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0522 - val_loss: 0.0550\n",
      "Epoch 53/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0591 - val_loss: 0.0600\n",
      "Epoch 54/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0682 - val_loss: 0.0487\n",
      "Epoch 55/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0566 - val_loss: 0.0402\n",
      "Epoch 56/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0482 - val_loss: 0.0212\n",
      "Epoch 57/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0420 - val_loss: 0.0283\n",
      "Epoch 58/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0466 - val_loss: 0.0445\n",
      "Epoch 59/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0518 - val_loss: 0.0455\n",
      "Epoch 60/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0502 - val_loss: 0.0276\n",
      "Epoch 61/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0378 - val_loss: 0.0179\n",
      "Epoch 62/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0357 - val_loss: 0.0155\n",
      "Epoch 63/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0330 - val_loss: 0.0129\n",
      "Epoch 64/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0327 - val_loss: 0.0169\n",
      "Epoch 65/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.0328 - val_loss: 0.0161\n",
      "Epoch 66/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.0315 - val_loss: 0.0150\n",
      "Epoch 67/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0310 - val_loss: 0.0081\n",
      "Epoch 68/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0264 - val_loss: 0.0056\n",
      "Epoch 69/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0269 - val_loss: 0.0050\n",
      "Epoch 70/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0247 - val_loss: 0.0035\n",
      "Epoch 71/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0223 - val_loss: 0.0033\n",
      "Epoch 72/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0229 - val_loss: 0.0028\n",
      "Epoch 73/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0214 - val_loss: 0.0042\n",
      "Epoch 74/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0221 - val_loss: 0.0073\n",
      "Epoch 75/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0220 - val_loss: 0.0051\n",
      "Epoch 76/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0199 - val_loss: 0.0035\n",
      "Epoch 77/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0198 - val_loss: 0.0029\n",
      "Epoch 78/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0190 - val_loss: 0.0033\n",
      "Epoch 79/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0191 - val_loss: 0.0030\n",
      "Epoch 80/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0188 - val_loss: 0.0032\n",
      "Epoch 81/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0188 - val_loss: 0.0041\n",
      "Epoch 82/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0190 - val_loss: 0.0053\n",
      "Epoch 83/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0206 - val_loss: 0.0064\n",
      "Epoch 84/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0197 - val_loss: 0.0031\n",
      "Epoch 85/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0172 - val_loss: 0.0027\n",
      "Epoch 86/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0175 - val_loss: 0.0029\n",
      "Epoch 87/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0177 - val_loss: 0.0027\n",
      "Epoch 88/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0156 - val_loss: 0.0028\n",
      "Epoch 89/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0162 - val_loss: 0.0042\n",
      "Epoch 90/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0177 - val_loss: 0.0033\n",
      "Epoch 91/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0169 - val_loss: 0.0029\n",
      "Epoch 92/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0162 - val_loss: 0.0033\n",
      "Epoch 93/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0168 - val_loss: 0.0031\n",
      "Epoch 94/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0145 - val_loss: 0.0028\n",
      "Epoch 95/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0143 - val_loss: 0.0028\n",
      "Epoch 96/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.015 - 0s 29us/step - loss: 0.0151 - val_loss: 0.0032\n",
      "Epoch 97/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0140 - val_loss: 0.0027\n",
      "Epoch 98/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0136 - val_loss: 0.0029\n",
      "Epoch 99/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0145 - val_loss: 0.0056\n",
      "Epoch 100/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0173 - val_loss: 0.0027\n",
      "Epoch 101/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.0132 - val_loss: 0.0027\n",
      "Epoch 102/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0125 - val_loss: 0.0028\n",
      "Epoch 103/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0132 - val_loss: 0.0039\n",
      "Epoch 104/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0141 - val_loss: 0.0035\n",
      "Epoch 105/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0129 - val_loss: 0.0028\n",
      "Epoch 106/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0120 - val_loss: 0.0026\n",
      "Epoch 107/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0114 - val_loss: 0.0035\n",
      "Epoch 108/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0139 - val_loss: 0.0042\n",
      "Epoch 109/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0129 - val_loss: 0.0027\n",
      "Epoch 110/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0118 - val_loss: 0.0025\n",
      "Epoch 111/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0109 - val_loss: 0.0025\n",
      "Epoch 112/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0111 - val_loss: 0.0026\n",
      "Epoch 113/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0116 - val_loss: 0.0047\n",
      "Epoch 114/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0128 - val_loss: 0.0029\n",
      "Epoch 115/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0109 - val_loss: 0.0027\n",
      "Epoch 116/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0103 - val_loss: 0.0026\n",
      "Epoch 117/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0096 - val_loss: 0.0029\n",
      "Epoch 118/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0100 - val_loss: 0.0028\n",
      "Epoch 119/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0106 - val_loss: 0.0033\n",
      "Epoch 120/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0111 - val_loss: 0.0027\n",
      "Epoch 121/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0096 - val_loss: 0.0026\n",
      "Epoch 122/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0096 - val_loss: 0.0025\n",
      "Epoch 123/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0092 - val_loss: 0.0058\n",
      "Epoch 124/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0111 - val_loss: 0.0029\n",
      "Epoch 125/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0090 - val_loss: 0.0026\n",
      "Epoch 126/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0085 - val_loss: 0.0027\n",
      "Epoch 127/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0086 - val_loss: 0.0039\n",
      "Epoch 128/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0095 - val_loss: 0.0040\n",
      "Epoch 129/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0092 - val_loss: 0.0034\n",
      "Epoch 130/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0085 - val_loss: 0.0029\n",
      "Epoch 131/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0080 - val_loss: 0.0028\n",
      "Epoch 132/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0082 - val_loss: 0.0038\n",
      "Epoch 133/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0091 - val_loss: 0.0036\n",
      "Epoch 134/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0082 - val_loss: 0.0029\n",
      "Epoch 135/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0074 - val_loss: 0.0027\n",
      "Epoch 136/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0071 - val_loss: 0.0030\n",
      "Epoch 137/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0071 - val_loss: 0.0054\n",
      "Epoch 138/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0091 - val_loss: 0.0035\n",
      "Epoch 139/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0076 - val_loss: 0.0029\n",
      "Epoch 140/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.006 - 0s 30us/step - loss: 0.0066 - val_loss: 0.0027\n",
      "Epoch 141/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 142/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0064 - val_loss: 0.0044\n",
      "Epoch 143/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0088 - val_loss: 0.0046\n",
      "Epoch 144/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0075 - val_loss: 0.0029\n",
      "Epoch 145/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0062 - val_loss: 0.0032\n",
      "Epoch 146/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0065 - val_loss: 0.0039\n",
      "Epoch 147/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0067 - val_loss: 0.0047\n",
      "Epoch 148/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0075 - val_loss: 0.0032\n",
      "Epoch 149/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0061 - val_loss: 0.0030\n",
      "Epoch 150/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0061 - val_loss: 0.0042\n",
      "Epoch 151/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0069 - val_loss: 0.0031\n",
      "Epoch 152/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 153/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0052 - val_loss: 0.0025\n",
      "Epoch 154/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0053 - val_loss: 0.0031\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0065 - val_loss: 0.0029\n",
      "Epoch 156/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0063 - val_loss: 0.0027\n",
      "Epoch 157/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0054 - val_loss: 0.0027\n",
      "Epoch 158/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0051 - val_loss: 0.0027\n",
      "Epoch 159/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0060 - val_loss: 0.0033\n",
      "Epoch 160/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0062 - val_loss: 0.0025\n",
      "Epoch 161/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0051 - val_loss: 0.0026\n",
      "Epoch 162/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0049 - val_loss: 0.0028\n",
      "Epoch 163/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0053 - val_loss: 0.0027\n",
      "Epoch 164/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0052 - val_loss: 0.0028\n",
      "Epoch 165/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0051 - val_loss: 0.0030\n",
      "Epoch 166/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 167/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 168/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0050 - val_loss: 0.0032\n",
      "Epoch 169/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0054 - val_loss: 0.0029\n",
      "Epoch 170/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0046 - val_loss: 0.0029\n",
      "Epoch 171/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0044 - val_loss: 0.0029\n",
      "Epoch 172/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0032\n",
      "Epoch 173/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0054 - val_loss: 0.0029\n",
      "Epoch 174/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0048 - val_loss: 0.0024\n",
      "Epoch 175/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0045 - val_loss: 0.0030\n",
      "Epoch 176/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0027\n",
      "Epoch 177/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 178/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0032\n",
      "Epoch 179/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0030\n",
      "Epoch 180/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0031\n",
      "Epoch 181/500\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.0049 - val_loss: 0.0028\n",
      "Epoch 182/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0046 - val_loss: 0.0029\n",
      "Epoch 183/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0032\n",
      "Epoch 184/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 185/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0031\n",
      "Epoch 186/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 187/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 188/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0030\n",
      "Epoch 189/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 190/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 191/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 192/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 193/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0033\n",
      "Epoch 194/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 195/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 196/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 197/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0035 - val_loss: 0.0036\n",
      "Epoch 198/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 199/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 200/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 201/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 202/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0031 - val_loss: 0.0036\n",
      "Epoch 203/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 204/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0037 - val_loss: 0.0031\n",
      "Epoch 205/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0037\n",
      "Epoch 206/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 207/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 208/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 209/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0030 - val_loss: 0.0037\n",
      "Epoch 210/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0028 - val_loss: 0.0041\n",
      "Epoch 211/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0026 - val_loss: 0.0040\n",
      "Epoch 212/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0026 - val_loss: 0.0035\n",
      "Epoch 213/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0026 - val_loss: 0.0043\n",
      "Epoch 214/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 215/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0034\n",
      "Epoch 216/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 217/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0029 - val_loss: 0.0034\n",
      "Epoch 218/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 219/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 220/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0027 - val_loss: 0.0035\n",
      "Epoch 221/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0026 - val_loss: 0.0033\n",
      "Epoch 222/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0025 - val_loss: 0.0034\n",
      "Epoch 223/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0028 - val_loss: 0.0032\n",
      "Epoch 224/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 225/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 226/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 227/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 228/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0025 - val_loss: 0.0033\n",
      "Epoch 229/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 230/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0030 - val_loss: 0.0030\n",
      "Epoch 231/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.0028 - val_loss: 0.0030\n",
      "Epoch 232/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 233/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 234/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 235/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0024 - val_loss: 0.0035\n",
      "Epoch 236/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 237/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0029 - val_loss: 0.0033\n",
      "Epoch 238/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 239/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 240/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 241/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0026 - val_loss: 0.0028\n",
      "Epoch 242/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 243/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 244/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 245/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 246/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 247/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0028\n",
      "Epoch 248/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0026 - val_loss: 0.0031\n",
      "Epoch 249/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0023 - val_loss: 0.0034\n",
      "Epoch 250/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 251/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0023 - val_loss: 0.0032\n",
      "Epoch 252/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 253/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 254/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 255/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0023 - val_loss: 0.0031\n",
      "Epoch 256/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 257/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 258/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 259/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0021 - val_loss: 0.0031\n",
      "Epoch 260/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0021 - val_loss: 0.0037\n",
      "Epoch 261/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0021 - val_loss: 0.0030\n",
      "Epoch 262/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0030\n",
      "Epoch 263/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0023 - val_loss: 0.0036\n",
      "Epoch 264/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 265/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0019 - val_loss: 0.0039\n",
      "Epoch 266/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 267/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0024 - val_loss: 0.0038\n",
      "Epoch 268/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0020 - val_loss: 0.0041\n",
      "Epoch 269/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 270/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 271/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0040\n",
      "Epoch 272/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 273/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0023 - val_loss: 0.0043\n",
      "Epoch 274/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0034\n",
      "Epoch 275/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 276/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0019 - val_loss: 0.0034\n",
      "Epoch 277/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0040\n",
      "Epoch 278/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 279/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 280/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0021 - val_loss: 0.0031\n",
      "Epoch 281/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 282/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 283/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 284/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0021 - val_loss: 0.0039\n",
      "Epoch 285/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0049\n",
      "Epoch 286/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0050\n",
      "Epoch 287/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0039\n",
      "Epoch 288/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 289/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0025 - val_loss: 0.0042\n",
      "Epoch 290/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 291/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0047\n",
      "Epoch 292/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 293/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0041\n",
      "Epoch 294/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0035\n",
      "Epoch 295/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0023 - val_loss: 0.0039\n",
      "Epoch 296/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0022 - val_loss: 0.0029\n",
      "Epoch 297/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0022 - val_loss: 0.0051\n",
      "Epoch 298/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0049\n",
      "Epoch 299/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0067\n",
      "Epoch 300/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0064\n",
      "Epoch 301/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0073\n",
      "Epoch 302/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0025 - val_loss: 0.0048\n",
      "Epoch 303/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0019 - val_loss: 0.0060\n",
      "Epoch 304/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 305/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0053\n",
      "Epoch 306/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0064\n",
      "Epoch 307/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0066\n",
      "Epoch 308/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0020 - val_loss: 0.0061\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0053\n",
      "Epoch 310/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 311/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 312/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 313/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0070\n",
      "Epoch 314/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 315/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0057\n",
      "Epoch 316/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0059\n",
      "Epoch 317/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 318/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 319/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0048\n",
      "Epoch 320/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0016 - val_loss: 0.0058\n",
      "Epoch 321/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 322/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 323/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0059\n",
      "Epoch 324/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0019 - val_loss: 0.0047\n",
      "Epoch 325/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0017 - val_loss: 0.0047\n",
      "Epoch 326/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0016 - val_loss: 0.0055\n",
      "Epoch 327/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0048\n",
      "Epoch 328/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0062\n",
      "Epoch 329/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0053\n",
      "Epoch 330/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0051\n",
      "Epoch 331/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 332/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 333/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 334/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 335/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0058\n",
      "Epoch 336/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0069\n",
      "Epoch 337/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 338/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.001 - 0s 29us/step - loss: 0.0019 - val_loss: 0.0054\n",
      "Epoch 339/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0056\n",
      "Epoch 340/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 341/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0055\n",
      "Epoch 342/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 343/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0045\n",
      "Epoch 344/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0051\n",
      "Epoch 345/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 346/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0050\n",
      "Epoch 347/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 348/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0019 - val_loss: 0.0051\n",
      "Epoch 349/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 350/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0047\n",
      "Epoch 351/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 352/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 353/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0018 - val_loss: 0.0057\n",
      "Epoch 354/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0059\n",
      "Epoch 355/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 356/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0063\n",
      "Epoch 357/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 358/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0015 - val_loss: 0.0039\n",
      "Epoch 359/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0015 - val_loss: 0.0041\n",
      "Epoch 360/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0016 - val_loss: 0.0042\n",
      "Epoch 361/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 362/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0040\n",
      "Epoch 363/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0035\n",
      "Epoch 364/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 365/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 366/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 367/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0019 - val_loss: 0.0042\n",
      "Epoch 368/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 369/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 370/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0041\n",
      "Epoch 371/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0033\n",
      "Epoch 372/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0021 - val_loss: 0.0027\n",
      "Epoch 373/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 374/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 375/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0016 - val_loss: 0.0035\n",
      "Epoch 376/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 377/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 378/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0047\n",
      "Epoch 379/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0016 - val_loss: 0.0038\n",
      "Epoch 380/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0036\n",
      "Epoch 381/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 382/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0022 - val_loss: 0.0042\n",
      "Epoch 383/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 384/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0015 - val_loss: 0.0052\n",
      "Epoch 385/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0015 - val_loss: 0.0063\n",
      "Epoch 386/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0019 - val_loss: 0.0072\n",
      "Epoch 387/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0021 - val_loss: 0.0046\n",
      "Epoch 388/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 389/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0055\n",
      "Epoch 390/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 391/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0019 - val_loss: 0.0059\n",
      "Epoch 392/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 393/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0015 - val_loss: 0.0051\n",
      "Epoch 394/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 395/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0062\n",
      "Epoch 396/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0042\n",
      "Epoch 397/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0052\n",
      "Epoch 398/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0064\n",
      "Epoch 399/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 400/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0015 - val_loss: 0.0069\n",
      "Epoch 401/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0020 - val_loss: 0.0044\n",
      "Epoch 402/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0015 - val_loss: 0.0046\n",
      "Epoch 403/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 404/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0057\n",
      "Epoch 405/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0066\n",
      "Epoch 406/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0020 - val_loss: 0.0048\n",
      "Epoch 407/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0052\n",
      "Epoch 408/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 409/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0067\n",
      "Epoch 410/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 411/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 412/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 413/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0015 - val_loss: 0.0054\n",
      "Epoch 414/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0065\n",
      "Epoch 415/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0057\n",
      "Epoch 416/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.001 - 0s 29us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 417/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0047\n",
      "Epoch 418/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 419/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0065\n",
      "Epoch 420/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0054\n",
      "Epoch 421/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0017 - val_loss: 0.0062\n",
      "Epoch 422/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.0016 - val_loss: 0.0060\n",
      "Epoch 423/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0018 - val_loss: 0.0062\n",
      "Epoch 424/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 425/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0052\n",
      "Epoch 426/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 427/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0053\n",
      "Epoch 428/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 429/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0017 - val_loss: 0.0052\n",
      "Epoch 430/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0047\n",
      "Epoch 431/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0055\n",
      "Epoch 432/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 433/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0060\n",
      "Epoch 434/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0046\n",
      "Epoch 435/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0049\n",
      "Epoch 436/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0044\n",
      "Epoch 437/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0058\n",
      "Epoch 438/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0018 - val_loss: 0.0041\n",
      "Epoch 439/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0046\n",
      "Epoch 440/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0013 - val_loss: 0.0047\n",
      "Epoch 441/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 442/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0050\n",
      "Epoch 443/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0070\n",
      "Epoch 444/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0023 - val_loss: 0.0035\n",
      "Epoch 445/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0017 - val_loss: 0.0044\n",
      "Epoch 446/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0013 - val_loss: 0.0051\n",
      "Epoch 447/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0067\n",
      "Epoch 448/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0020 - val_loss: 0.0060\n",
      "Epoch 449/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0019 - val_loss: 0.0056\n",
      "Epoch 450/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 451/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0067\n",
      "Epoch 452/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0019 - val_loss: 0.0046\n",
      "Epoch 453/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0055\n",
      "Epoch 454/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0058\n",
      "Epoch 455/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0058\n",
      "Epoch 456/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 457/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0057\n",
      "Epoch 458/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0064\n",
      "Epoch 459/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0019 - val_loss: 0.0048\n",
      "Epoch 460/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0059\n",
      "Epoch 461/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0017 - val_loss: 0.0048\n",
      "Epoch 462/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0015 - val_loss: 0.0046\n",
      "Epoch 463/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0014 - val_loss: 0.0057\n",
      "Epoch 464/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0014 - val_loss: 0.0047\n",
      "Epoch 465/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 466/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0069\n",
      "Epoch 467/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0022 - val_loss: 0.0032\n",
      "Epoch 468/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0043\n",
      "Epoch 469/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0014 - val_loss: 0.0053\n",
      "Epoch 470/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0015 - val_loss: 0.0080\n",
      "Epoch 471/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0021 - val_loss: 0.0046\n",
      "Epoch 472/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0055\n",
      "Epoch 473/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0015 - val_loss: 0.0052\n",
      "Epoch 474/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0057\n",
      "Epoch 475/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 476/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0055\n",
      "Epoch 477/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0015 - val_loss: 0.0057\n",
      "Epoch 478/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0016 - val_loss: 0.0062\n",
      "Epoch 479/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0047\n",
      "Epoch 480/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0054\n",
      "Epoch 481/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0014 - val_loss: 0.0060\n",
      "Epoch 482/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0015 - val_loss: 0.0064\n",
      "Epoch 483/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0015 - val_loss: 0.0060\n",
      "Epoch 484/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 485/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.001 - 0s 30us/step - loss: 0.0017 - val_loss: 0.0049\n",
      "Epoch 486/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0065\n",
      "Epoch 487/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0020 - val_loss: 0.0042\n",
      "Epoch 488/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0046\n",
      "Epoch 489/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0066\n",
      "Epoch 490/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0017 - val_loss: 0.0053\n",
      "Epoch 491/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0016 - val_loss: 0.0056\n",
      "Epoch 492/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0017 - val_loss: 0.0050\n",
      "Epoch 493/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0050\n",
      "Epoch 494/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0013 - val_loss: 0.0050\n",
      "Epoch 495/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0014 - val_loss: 0.0048\n",
      "Epoch 496/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0014 - val_loss: 0.0047\n",
      "Epoch 497/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0014 - val_loss: 0.0042\n",
      "Epoch 498/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0023 - val_loss: 0.0026\n",
      "Epoch 499/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0024 - val_loss: 0.0047\n",
      "Epoch 500/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0015 - val_loss: 0.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17891c978>"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, \n",
    "          batch_size=2000, \n",
    "          epochs=500, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(zillow_model.history['loss'])\n",
    "plt.plot(zillow_model.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(130,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.01)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 2/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0055 - val_loss: 0.0088\n",
      "Epoch 3/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0137 - val_loss: 0.0133\n",
      "Epoch 4/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0160 - val_loss: 0.0030\n",
      "Epoch 5/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 6/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 7/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 8/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 9/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 10/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0053 - val_loss: 0.1210\n",
      "Epoch 11/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5420 - val_loss: 0.1706\n",
      "Epoch 12/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2718 - val_loss: 0.0036\n",
      "Epoch 13/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0186 - val_loss: 0.0032\n",
      "Epoch 14/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0106 - val_loss: 0.0034\n",
      "Epoch 15/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0082 - val_loss: 0.0111\n",
      "Epoch 16/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0190 - val_loss: 0.0035\n",
      "Epoch 17/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0064 - val_loss: 0.0036\n",
      "Epoch 18/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0048 - val_loss: 0.0037\n",
      "Epoch 19/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 20/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 21/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 22/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 23/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 24/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 25/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 26/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 27/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 28/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 29/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 30/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 31/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 32/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 33/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 34/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 35/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 36/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 37/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 38/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 39/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0060 - val_loss: 0.0033\n",
      "Epoch 40/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 41/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 42/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 43/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 44/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 45/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 46/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 47/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 48/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0032 - val_loss: 0.0034\n",
      "Epoch 49/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 50/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0031 - val_loss: 0.0035\n",
      "Epoch 51/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0062 - val_loss: 0.4126\n",
      "Epoch 52/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.6169 - val_loss: 0.3842\n",
      "Epoch 53/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4058 - val_loss: 0.0042\n",
      "Epoch 54/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0057 - val_loss: 0.0032\n",
      "Epoch 55/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 56/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0027\n",
      "Epoch 57/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 58/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0027\n",
      "Epoch 59/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 60/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 61/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0027\n",
      "Epoch 62/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 63/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 64/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0027\n",
      "Epoch 65/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 66/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 67/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 68/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 69/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 70/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 71/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 72/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 73/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 74/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 75/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 76/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 77/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 78/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 79/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0038\n",
      "Epoch 80/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0046 - val_loss: 0.0164\n",
      "Epoch 81/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0922 - val_loss: 1.0088\n",
      "Epoch 82/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2131 - val_loss: 0.0246\n",
      "Epoch 83/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0393 - val_loss: 0.0048\n",
      "Epoch 84/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0087 - val_loss: 0.0030\n",
      "Epoch 85/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0032\n",
      "Epoch 86/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0033\n",
      "Epoch 87/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 88/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 89/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0033\n",
      "Epoch 90/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 91/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 92/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 93/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 94/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 95/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 96/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 97/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 98/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 99/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 100/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 101/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0058 - val_loss: 0.0067\n",
      "Epoch 102/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0095 - val_loss: 0.0556\n",
      "Epoch 103/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.1012 - val_loss: 0.0047\n",
      "Epoch 104/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0038\n",
      "Epoch 105/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0054 - val_loss: 0.0037\n",
      "Epoch 106/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0053 - val_loss: 0.0031\n",
      "Epoch 107/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 108/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 109/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 110/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 111/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 112/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 113/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 114/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 115/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 116/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 117/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0031 - val_loss: 0.0029\n",
      "Epoch 118/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 119/500\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 120/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0032 - val_loss: 0.0029\n",
      "Epoch 121/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 122/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0032 - val_loss: 0.0028\n",
      "Epoch 123/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 124/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0030 - val_loss: 0.0028\n",
      "Epoch 125/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0032 - val_loss: 0.0030\n",
      "Epoch 126/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0048\n",
      "Epoch 127/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0991 - val_loss: 0.8047\n",
      "Epoch 128/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 2.8528 - val_loss: 9.5942\n",
      "Epoch 129/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 11.2623 - val_loss: 1.8581\n",
      "Epoch 130/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4809 - val_loss: 0.0179\n",
      "Epoch 131/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0347 - val_loss: 0.0039\n",
      "Epoch 132/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0207 - val_loss: 0.0038\n",
      "Epoch 133/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0180 - val_loss: 0.0033\n",
      "Epoch 134/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0137 - val_loss: 0.0038\n",
      "Epoch 135/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0110 - val_loss: 0.0036\n",
      "Epoch 136/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0088 - val_loss: 0.0035\n",
      "Epoch 137/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0072 - val_loss: 0.0034\n",
      "Epoch 138/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0059 - val_loss: 0.0034\n",
      "Epoch 139/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0052 - val_loss: 0.0033\n",
      "Epoch 140/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0046 - val_loss: 0.0032\n",
      "Epoch 141/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0044 - val_loss: 0.0034\n",
      "Epoch 142/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 143/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 144/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0052\n",
      "Epoch 145/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0056 - val_loss: 0.0225\n",
      "Epoch 146/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0384 - val_loss: 0.3079\n",
      "Epoch 147/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.7884 - val_loss: 3.0779\n",
      "Epoch 148/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 2.3607 - val_loss: 0.0446\n",
      "Epoch 149/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0363 - val_loss: 0.0121\n",
      "Epoch 150/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0095 - val_loss: 0.0067\n",
      "Epoch 151/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0057 - val_loss: 0.0058\n",
      "Epoch 152/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 153/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0085 - val_loss: 0.0226\n",
      "Epoch 154/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0290 - val_loss: 0.1125\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1538 - val_loss: 0.2634\n",
      "Epoch 156/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2671 - val_loss: 0.2698\n",
      "Epoch 157/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2900 - val_loss: 0.8574\n",
      "Epoch 158/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 4.1008 - val_loss: 30.9265\n",
      "Epoch 159/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 37.3535 - val_loss: 24.0820\n",
      "Epoch 160/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 18.2658 - val_loss: 0.0029\n",
      "Epoch 161/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0098 - val_loss: 0.0029\n",
      "Epoch 162/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0082 - val_loss: 0.0029\n",
      "Epoch 163/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0070 - val_loss: 0.0029\n",
      "Epoch 164/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0069 - val_loss: 0.0030\n",
      "Epoch 165/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0062 - val_loss: 0.0029\n",
      "Epoch 166/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0056 - val_loss: 0.0030\n",
      "Epoch 167/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0053 - val_loss: 0.0029\n",
      "Epoch 168/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0051 - val_loss: 0.0031\n",
      "Epoch 169/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 170/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0069 - val_loss: 0.0214\n",
      "Epoch 171/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0375 - val_loss: 0.3485\n",
      "Epoch 172/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8947 - val_loss: 7.7936\n",
      "Epoch 173/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 12.0376 - val_loss: 19.0341\n",
      "Epoch 174/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 14.7902 - val_loss: 0.0914\n",
      "Epoch 175/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0736 - val_loss: 0.0077\n",
      "Epoch 176/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0081 - val_loss: 0.0035\n",
      "Epoch 177/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0054 - val_loss: 0.0033\n",
      "Epoch 178/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 179/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 180/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 181/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0061 - val_loss: 0.0176\n",
      "Epoch 182/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0271 - val_loss: 0.1794\n",
      "Epoch 183/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4322 - val_loss: 7.0663\n",
      "Epoch 184/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 11.6984 - val_loss: 1.5417\n",
      "Epoch 185/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.2566 - val_loss: 0.0320\n",
      "Epoch 186/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0238 - val_loss: 0.0042\n",
      "Epoch 187/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0041 - val_loss: 0.0031\n",
      "Epoch 188/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0031\n",
      "Epoch 189/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 190/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 191/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 192/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 193/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 194/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 195/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 196/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 197/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 198/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 199/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0039\n",
      "Epoch 200/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0040 - val_loss: 0.0079\n",
      "Epoch 201/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0112 - val_loss: 0.0867\n",
      "Epoch 202/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2202 - val_loss: 3.8801\n",
      "Epoch 203/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 5.0471 - val_loss: 0.0526\n",
      "Epoch 204/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0450 - val_loss: 0.0060\n",
      "Epoch 205/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0061 - val_loss: 0.0032\n",
      "Epoch 206/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0029\n",
      "Epoch 207/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0030\n",
      "Epoch 208/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 209/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 210/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 211/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 212/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 213/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 214/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 215/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 216/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 217/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 218/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 219/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 220/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 221/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0080 - val_loss: 0.0341\n",
      "Epoch 222/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.1079 - val_loss: 2.4706\n",
      "Epoch 223/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 6.7249 - val_loss: 18.3961\n",
      "Epoch 224/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 31.2690 - val_loss: 23.2112\n",
      "Epoch 225/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 18.4562 - val_loss: 0.0401\n",
      "Epoch 226/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0472 - val_loss: 0.0029\n",
      "Epoch 227/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0065 - val_loss: 0.0034\n",
      "Epoch 228/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0056 - val_loss: 0.0034\n",
      "Epoch 229/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0052 - val_loss: 0.0034\n",
      "Epoch 230/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0050 - val_loss: 0.0033\n",
      "Epoch 231/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0048 - val_loss: 0.0034\n",
      "Epoch 232/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 233/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0045 - val_loss: 0.0035\n",
      "Epoch 234/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0034\n",
      "Epoch 235/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0034\n",
      "Epoch 236/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0033\n",
      "Epoch 237/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0031\n",
      "Epoch 238/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 239/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0040 - val_loss: 0.0029\n",
      "Epoch 240/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0058\n",
      "Epoch 241/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0168 - val_loss: 0.1399\n",
      "Epoch 242/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.4274 - val_loss: 5.2691\n",
      "Epoch 243/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 11.3014 - val_loss: 67.5562\n",
      "Epoch 244/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 60.0220 - val_loss: 0.3208\n",
      "Epoch 245/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2505 - val_loss: 0.0073\n",
      "Epoch 246/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0082 - val_loss: 0.0033\n",
      "Epoch 247/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0056 - val_loss: 0.0032\n",
      "Epoch 248/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0057 - val_loss: 0.0031\n",
      "Epoch 249/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0050 - val_loss: 0.0031\n",
      "Epoch 250/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0031\n",
      "Epoch 251/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0044 - val_loss: 0.0032\n",
      "Epoch 252/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0042 - val_loss: 0.0031\n",
      "Epoch 253/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0041 - val_loss: 0.0032\n",
      "Epoch 254/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 255/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 256/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 257/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 258/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0045 - val_loss: 0.0096\n",
      "Epoch 259/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0126 - val_loss: 0.0939\n",
      "Epoch 260/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1889 - val_loss: 2.0366\n",
      "Epoch 261/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 5.0588 - val_loss: 16.2810\n",
      "Epoch 262/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 12.2137 - val_loss: 0.5355\n",
      "Epoch 263/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5032 - val_loss: 0.2358\n",
      "Epoch 264/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2301 - val_loss: 0.1611\n",
      "Epoch 265/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.1714 - val_loss: 0.2214\n",
      "Epoch 266/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2493 - val_loss: 0.4269\n",
      "Epoch 267/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5756 - val_loss: 2.5519\n",
      "Epoch 268/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 6.1397 - val_loss: 19.9508\n",
      "Epoch 269/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 15.5076 - val_loss: 0.0581\n",
      "Epoch 270/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0629 - val_loss: 0.0029\n",
      "Epoch 271/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0057 - val_loss: 0.0032\n",
      "Epoch 272/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 273/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 274/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0041 - val_loss: 0.0032\n",
      "Epoch 275/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 276/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 277/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 278/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0036 - val_loss: 0.0031\n",
      "Epoch 279/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 280/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 281/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 282/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 283/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 284/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0104 - val_loss: 0.0328\n",
      "Epoch 285/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0726 - val_loss: 0.5266\n",
      "Epoch 286/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5459 - val_loss: 23.9728\n",
      "Epoch 287/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 34.7491 - val_loss: 20.9841\n",
      "Epoch 288/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 16.6981 - val_loss: 0.0029\n",
      "Epoch 289/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0192 - val_loss: 0.0029\n",
      "Epoch 290/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0133 - val_loss: 0.0029\n",
      "Epoch 291/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0095 - val_loss: 0.0029\n",
      "Epoch 292/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0063 - val_loss: 0.0030\n",
      "Epoch 293/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0052 - val_loss: 0.0030\n",
      "Epoch 294/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 295/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0031\n",
      "Epoch 296/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0032\n",
      "Epoch 297/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0033\n",
      "Epoch 298/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0048\n",
      "Epoch 299/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0052 - val_loss: 0.0224\n",
      "Epoch 300/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0499 - val_loss: 0.8239\n",
      "Epoch 301/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 2.6580 - val_loss: 13.1671\n",
      "Epoch 302/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 10.5746 - val_loss: 0.0645\n",
      "Epoch 303/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0595 - val_loss: 0.0029\n",
      "Epoch 304/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 305/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 306/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 307/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 308/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 310/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 311/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0034 - val_loss: 0.0033\n",
      "Epoch 312/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 313/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 314/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 315/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 316/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 317/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0039\n",
      "Epoch 318/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0103 - val_loss: 0.1003\n",
      "Epoch 319/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.4130 - val_loss: 4.4048\n",
      "Epoch 320/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 4.0120 - val_loss: 0.0670\n",
      "Epoch 321/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0672 - val_loss: 0.0059\n",
      "Epoch 322/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0075 - val_loss: 0.0030\n",
      "Epoch 323/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0050 - val_loss: 0.0029\n",
      "Epoch 324/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0029\n",
      "Epoch 325/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0041 - val_loss: 0.0029\n",
      "Epoch 326/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0029\n",
      "Epoch 327/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 328/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0038 - val_loss: 0.0029\n",
      "Epoch 329/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 330/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 331/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 332/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 333/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 334/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 335/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 336/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 337/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 338/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 339/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 340/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0058\n",
      "Epoch 341/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0174 - val_loss: 0.1153\n",
      "Epoch 342/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.2134 - val_loss: 0.0084\n",
      "Epoch 343/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0142 - val_loss: 0.0028\n",
      "Epoch 344/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0030\n",
      "Epoch 345/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 346/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 347/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 348/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 349/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 350/500\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 351/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 352/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 353/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 354/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 355/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 356/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0029\n",
      "Epoch 357/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 358/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 359/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0051\n",
      "Epoch 360/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0135 - val_loss: 0.0028\n",
      "Epoch 361/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0062 - val_loss: 0.0032\n",
      "Epoch 362/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 363/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 364/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 365/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 366/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 367/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 368/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 369/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 370/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 371/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 372/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0047\n",
      "Epoch 373/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0388 - val_loss: 0.0664\n",
      "Epoch 374/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2143 - val_loss: 0.9015\n",
      "Epoch 375/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.6988 - val_loss: 5.7502\n",
      "Epoch 376/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 8.9211 - val_loss: 5.0362\n",
      "Epoch 377/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 6.955 - 0s 29us/step - loss: 4.9079 - val_loss: 0.0048\n",
      "Epoch 378/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0186 - val_loss: 0.0035\n",
      "Epoch 379/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0162 - val_loss: 0.0031\n",
      "Epoch 380/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0137 - val_loss: 0.0032\n",
      "Epoch 381/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0109 - val_loss: 0.0029\n",
      "Epoch 382/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0100 - val_loss: 0.0029\n",
      "Epoch 383/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0087 - val_loss: 0.0027\n",
      "Epoch 384/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0075 - val_loss: 0.0029\n",
      "Epoch 385/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0063 - val_loss: 0.0028\n",
      "Epoch 386/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0061 - val_loss: 0.0028\n",
      "Epoch 387/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0053 - val_loss: 0.0026\n",
      "Epoch 388/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0048 - val_loss: 0.0027\n",
      "Epoch 389/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0046 - val_loss: 0.0029\n",
      "Epoch 390/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0050 - val_loss: 0.0041\n",
      "Epoch 391/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0085 - val_loss: 0.0236\n",
      "Epoch 392/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0608 - val_loss: 0.4231\n",
      "Epoch 393/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4826 - val_loss: 2.4886\n",
      "Epoch 394/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 2.7621 - val_loss: 0.0174\n",
      "Epoch 395/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0204 - val_loss: 0.0031\n",
      "Epoch 396/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0043 - val_loss: 0.0027\n",
      "Epoch 397/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0040 - val_loss: 0.0027\n",
      "Epoch 398/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 399/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 400/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 401/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0041 - val_loss: 0.0033\n",
      "Epoch 402/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0056 - val_loss: 0.0159\n",
      "Epoch 403/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0566 - val_loss: 0.5200\n",
      "Epoch 404/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.8449 - val_loss: 2.5200\n",
      "Epoch 405/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 2.7800 - val_loss: 0.0028\n",
      "Epoch 406/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0078 - val_loss: 0.0028\n",
      "Epoch 407/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0066 - val_loss: 0.0028\n",
      "Epoch 408/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0063 - val_loss: 0.0028\n",
      "Epoch 409/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0058 - val_loss: 0.0028\n",
      "Epoch 410/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0054 - val_loss: 0.0029\n",
      "Epoch 411/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0051 - val_loss: 0.0029\n",
      "Epoch 412/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0048 - val_loss: 0.0030\n",
      "Epoch 413/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0049 - val_loss: 0.0033\n",
      "Epoch 414/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 415/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0106 - val_loss: 0.0345\n",
      "Epoch 416/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0952 - val_loss: 0.3965\n",
      "Epoch 417/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.5595 - val_loss: 0.0336\n",
      "Epoch 418/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0377 - val_loss: 0.0028\n",
      "Epoch 419/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0037 - val_loss: 0.0028\n",
      "Epoch 420/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 421/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0038 - val_loss: 0.0028\n",
      "Epoch 422/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 423/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 424/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 425/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0028\n",
      "Epoch 426/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Epoch 427/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Epoch 428/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 429/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.0045\n",
      "Epoch 430/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0109 - val_loss: 0.0508\n",
      "Epoch 431/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2127 - val_loss: 0.5418\n",
      "Epoch 432/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6122 - val_loss: 0.0032\n",
      "Epoch 433/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 434/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 435/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 436/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 437/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 438/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 439/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 440/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 441/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 442/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 443/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 444/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 445/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 446/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 447/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 448/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 449/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 450/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 451/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 452/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 453/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0070 - val_loss: 0.0123\n",
      "Epoch 454/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0224 - val_loss: 0.0033\n",
      "Epoch 455/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0030\n",
      "Epoch 456/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 457/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 458/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 459/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 460/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 461/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 462/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 463/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 464/500\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 465/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 466/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 467/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 468/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 469/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 470/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 471/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0074 - val_loss: 0.0031\n",
      "Epoch 472/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 473/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0204 - val_loss: 0.1557\n",
      "Epoch 474/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2628 - val_loss: 0.0043\n",
      "Epoch 475/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0134 - val_loss: 0.0031\n",
      "Epoch 476/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0063 - val_loss: 0.0030\n",
      "Epoch 477/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0045 - val_loss: 0.0029\n",
      "Epoch 478/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 479/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 480/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 481/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 482/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0030\n",
      "Epoch 483/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 484/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 485/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 486/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 487/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 488/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0056 - val_loss: 0.0031\n",
      "Epoch 489/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 490/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 491/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0035 - val_loss: 0.0031\n",
      "Epoch 492/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 493/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 494/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 495/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 496/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 497/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 498/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 499/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 500/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.0034 - val_loss: 0.0032\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train_business, y=y_train_business, \n",
    "          batch_size=2000, \n",
    "          epochs=500, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_business, y_test_business),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(631,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.002)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/500\n",
      "2839/2839 [==============================] - 4s 1ms/step - loss: 3.1253 - val_loss: 0.3215\n",
      "Epoch 2/500\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3927 - val_loss: 0.2637\n",
      "Epoch 3/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.3240 - val_loss: 0.2399\n",
      "Epoch 4/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2909 - val_loss: 0.1291\n",
      "Epoch 5/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2447 - val_loss: 0.1072\n",
      "Epoch 6/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2297 - val_loss: 0.1186\n",
      "Epoch 7/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2025 - val_loss: 0.2005\n",
      "Epoch 8/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.2366 - val_loss: 0.2338\n",
      "Epoch 9/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2277 - val_loss: 0.1693\n",
      "Epoch 10/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.1877 - val_loss: 0.1282\n",
      "Epoch 11/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.1745 - val_loss: 0.1411\n",
      "Epoch 12/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.1660 - val_loss: 0.1104\n",
      "Epoch 13/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.1478 - val_loss: 0.1603\n",
      "Epoch 14/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.1639 - val_loss: 0.2238\n",
      "Epoch 15/500\n",
      "2839/2839 [==============================] - 0s 92us/step - loss: 0.1798 - val_loss: 0.2185\n",
      "Epoch 16/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.1586 - val_loss: 0.1309\n",
      "Epoch 17/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.1269 - val_loss: 0.1460\n",
      "Epoch 18/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.1226 - val_loss: 0.1419\n",
      "Epoch 19/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.1188 - val_loss: 0.1322\n",
      "Epoch 20/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.1142 - val_loss: 0.2100\n",
      "Epoch 21/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.1477 - val_loss: 0.1941\n",
      "Epoch 22/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.1351 - val_loss: 0.1434\n",
      "Epoch 23/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.1089 - val_loss: 0.1333\n",
      "Epoch 24/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.1087 - val_loss: 0.1734\n",
      "Epoch 25/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.1242 - val_loss: 0.1681\n",
      "Epoch 26/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.1236 - val_loss: 0.1397\n",
      "Epoch 27/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.1041 - val_loss: 0.1265\n",
      "Epoch 28/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0930 - val_loss: 0.1414\n",
      "Epoch 29/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.1073 - val_loss: 0.1549\n",
      "Epoch 30/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.1063 - val_loss: 0.1308\n",
      "Epoch 31/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0955 - val_loss: 0.1328\n",
      "Epoch 32/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0950 - val_loss: 0.1184\n",
      "Epoch 33/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0840 - val_loss: 0.1127\n",
      "Epoch 34/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0848 - val_loss: 0.1371\n",
      "Epoch 35/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0962 - val_loss: 0.1347\n",
      "Epoch 36/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0994 - val_loss: 0.1492\n",
      "Epoch 37/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0920 - val_loss: 0.1038\n",
      "Epoch 38/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0761 - val_loss: 0.0920\n",
      "Epoch 39/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0710 - val_loss: 0.0986\n",
      "Epoch 40/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0798 - val_loss: 0.1339\n",
      "Epoch 41/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0883 - val_loss: 0.1173\n",
      "Epoch 42/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0795 - val_loss: 0.0971\n",
      "Epoch 43/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0691 - val_loss: 0.0996\n",
      "Epoch 44/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0750 - val_loss: 0.1234\n",
      "Epoch 45/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0834 - val_loss: 0.1221\n",
      "Epoch 46/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0818 - val_loss: 0.1055\n",
      "Epoch 47/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0705 - val_loss: 0.0837\n",
      "Epoch 48/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0635 - val_loss: 0.0809\n",
      "Epoch 49/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0631 - val_loss: 0.0942\n",
      "Epoch 50/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0679 - val_loss: 0.0971\n",
      "Epoch 51/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0705 - val_loss: 0.0994\n",
      "Epoch 52/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0712 - val_loss: 0.0786\n",
      "Epoch 53/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0592 - val_loss: 0.0679\n",
      "Epoch 54/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0567 - val_loss: 0.0868\n",
      "Epoch 55/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0620 - val_loss: 0.0766\n",
      "Epoch 56/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0638 - val_loss: 0.0971\n",
      "Epoch 57/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0646 - val_loss: 0.0700\n",
      "Epoch 58/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0572 - val_loss: 0.0794\n",
      "Epoch 59/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0543 - val_loss: 0.0706\n",
      "Epoch 60/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0554 - val_loss: 0.0651\n",
      "Epoch 61/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0532 - val_loss: 0.0694\n",
      "Epoch 62/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0542 - val_loss: 0.0654\n",
      "Epoch 63/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0492 - val_loss: 0.0531\n",
      "Epoch 64/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0471 - val_loss: 0.0646\n",
      "Epoch 65/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.0525 - val_loss: 0.0713\n",
      "Epoch 66/500\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.0543 - val_loss: 0.0587\n",
      "Epoch 67/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0473 - val_loss: 0.0570\n",
      "Epoch 68/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0506 - val_loss: 0.0612\n",
      "Epoch 69/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0480 - val_loss: 0.0486\n",
      "Epoch 70/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0421 - val_loss: 0.0495\n",
      "Epoch 71/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0411 - val_loss: 0.0367\n",
      "Epoch 72/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0426 - val_loss: 0.0547\n",
      "Epoch 73/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0467 - val_loss: 0.0652\n",
      "Epoch 74/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0512 - val_loss: 0.0473\n",
      "Epoch 75/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0435 - val_loss: 0.0434\n",
      "Epoch 76/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0426 - val_loss: 0.0540\n",
      "Epoch 77/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0461 - val_loss: 0.0432\n",
      "Epoch 78/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0405 - val_loss: 0.0397\n",
      "Epoch 79/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0409 - val_loss: 0.0388\n",
      "Epoch 80/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0398 - val_loss: 0.0345\n",
      "Epoch 81/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0397 - val_loss: 0.0410\n",
      "Epoch 82/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0402 - val_loss: 0.0232\n",
      "Epoch 83/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0334 - val_loss: 0.0252\n",
      "Epoch 84/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0347 - val_loss: 0.0287\n",
      "Epoch 85/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0369 - val_loss: 0.0343\n",
      "Epoch 86/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0397 - val_loss: 0.0283\n",
      "Epoch 87/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0375 - val_loss: 0.0205\n",
      "Epoch 88/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0336 - val_loss: 0.0163\n",
      "Epoch 89/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0323 - val_loss: 0.0138\n",
      "Epoch 90/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0308 - val_loss: 0.0162\n",
      "Epoch 91/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0325 - val_loss: 0.0127\n",
      "Epoch 92/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0316 - val_loss: 0.0122\n",
      "Epoch 93/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0298 - val_loss: 0.0091\n",
      "Epoch 94/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0280 - val_loss: 0.0096\n",
      "Epoch 95/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0303 - val_loss: 0.0185\n",
      "Epoch 96/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0353 - val_loss: 0.0118\n",
      "Epoch 97/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0315 - val_loss: 0.0122\n",
      "Epoch 98/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0300 - val_loss: 0.0083\n",
      "Epoch 99/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0289 - val_loss: 0.0099\n",
      "Epoch 100/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0297 - val_loss: 0.0118\n",
      "Epoch 101/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0289 - val_loss: 0.0102\n",
      "Epoch 102/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0298 - val_loss: 0.0130\n",
      "Epoch 103/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0317 - val_loss: 0.0071\n",
      "Epoch 104/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0276 - val_loss: 0.0070\n",
      "Epoch 105/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0258 - val_loss: 0.0045\n",
      "Epoch 106/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0256 - val_loss: 0.0035\n",
      "Epoch 107/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0262 - val_loss: 0.0028\n",
      "Epoch 108/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0267 - val_loss: 0.0028\n",
      "Epoch 109/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0265 - val_loss: 0.0028\n",
      "Epoch 110/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0280 - val_loss: 0.0032\n",
      "Epoch 111/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0266 - val_loss: 0.0030\n",
      "Epoch 112/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0248 - val_loss: 0.0033\n",
      "Epoch 113/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0248 - val_loss: 0.0028\n",
      "Epoch 114/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0243 - val_loss: 0.0028\n",
      "Epoch 115/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0261 - val_loss: 0.0030\n",
      "Epoch 116/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0241 - val_loss: 0.0030\n",
      "Epoch 117/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0244 - val_loss: 0.0027\n",
      "Epoch 118/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0254 - val_loss: 0.0030\n",
      "Epoch 119/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0259 - val_loss: 0.0031\n",
      "Epoch 120/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0246 - val_loss: 0.0030\n",
      "Epoch 121/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0234 - val_loss: 0.0045\n",
      "Epoch 122/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0246 - val_loss: 0.0070\n",
      "Epoch 123/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0249 - val_loss: 0.0037\n",
      "Epoch 124/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0230 - val_loss: 0.0038\n",
      "Epoch 125/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0220 - val_loss: 0.0029\n",
      "Epoch 126/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0240 - val_loss: 0.0032\n",
      "Epoch 127/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0256 - val_loss: 0.0029\n",
      "Epoch 128/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0233 - val_loss: 0.0034\n",
      "Epoch 129/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0240 - val_loss: 0.0028\n",
      "Epoch 130/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0220 - val_loss: 0.0029\n",
      "Epoch 131/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0219 - val_loss: 0.0034\n",
      "Epoch 132/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0223 - val_loss: 0.0039\n",
      "Epoch 133/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0229 - val_loss: 0.0040\n",
      "Epoch 134/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0219 - val_loss: 0.0068\n",
      "Epoch 135/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0240 - val_loss: 0.0035\n",
      "Epoch 136/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0211 - val_loss: 0.0039\n",
      "Epoch 137/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0218 - val_loss: 0.0035\n",
      "Epoch 138/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0218 - val_loss: 0.0049\n",
      "Epoch 139/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0227 - val_loss: 0.0059\n",
      "Epoch 140/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0232 - val_loss: 0.0054\n",
      "Epoch 141/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0234 - val_loss: 0.0033\n",
      "Epoch 142/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0207 - val_loss: 0.0037\n",
      "Epoch 143/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0210 - val_loss: 0.0057\n",
      "Epoch 144/500\n",
      "2839/2839 [==============================] - 0s 93us/step - loss: 0.0216 - val_loss: 0.0045\n",
      "Epoch 145/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0204 - val_loss: 0.0030\n",
      "Epoch 146/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0206 - val_loss: 0.0029\n",
      "Epoch 147/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0211 - val_loss: 0.0028\n",
      "Epoch 148/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0212 - val_loss: 0.0029\n",
      "Epoch 149/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0203 - val_loss: 0.0028\n",
      "Epoch 150/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0195 - val_loss: 0.0028\n",
      "Epoch 151/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0199 - val_loss: 0.0028\n",
      "Epoch 152/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0196 - val_loss: 0.0032\n",
      "Epoch 153/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0210 - val_loss: 0.0034\n",
      "Epoch 154/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0207 - val_loss: 0.0028\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0199 - val_loss: 0.0030\n",
      "Epoch 156/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0184 - val_loss: 0.0029\n",
      "Epoch 157/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0193 - val_loss: 0.0029\n",
      "Epoch 158/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0183 - val_loss: 0.0029\n",
      "Epoch 159/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0184 - val_loss: 0.0027\n",
      "Epoch 160/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0198 - val_loss: 0.0031\n",
      "Epoch 161/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0197 - val_loss: 0.0031\n",
      "Epoch 162/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0185 - val_loss: 0.0030\n",
      "Epoch 163/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0189 - val_loss: 0.0029\n",
      "Epoch 164/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0177 - val_loss: 0.0028\n",
      "Epoch 165/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0187 - val_loss: 0.0028\n",
      "Epoch 166/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0177 - val_loss: 0.0033\n",
      "Epoch 167/500\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.0199 - val_loss: 0.0032\n",
      "Epoch 168/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0178 - val_loss: 0.0029\n",
      "Epoch 169/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0177 - val_loss: 0.0029\n",
      "Epoch 170/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0176 - val_loss: 0.0028\n",
      "Epoch 171/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0179 - val_loss: 0.0029\n",
      "Epoch 172/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0177 - val_loss: 0.0028\n",
      "Epoch 173/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0176 - val_loss: 0.0028\n",
      "Epoch 174/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0178 - val_loss: 0.0030\n",
      "Epoch 175/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0180 - val_loss: 0.0031\n",
      "Epoch 176/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0166 - val_loss: 0.0030\n",
      "Epoch 177/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0175 - val_loss: 0.0030\n",
      "Epoch 178/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0161 - val_loss: 0.0028\n",
      "Epoch 179/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0159 - val_loss: 0.0029\n",
      "Epoch 180/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0171 - val_loss: 0.0031\n",
      "Epoch 181/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0166 - val_loss: 0.0028\n",
      "Epoch 182/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0161 - val_loss: 0.0038\n",
      "Epoch 183/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0169 - val_loss: 0.0045\n",
      "Epoch 184/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0168 - val_loss: 0.0034\n",
      "Epoch 185/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0157 - val_loss: 0.0029\n",
      "Epoch 186/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0158 - val_loss: 0.0030\n",
      "Epoch 187/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0152 - val_loss: 0.0034\n",
      "Epoch 188/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0163 - val_loss: 0.0051\n",
      "Epoch 189/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0171 - val_loss: 0.0030\n",
      "Epoch 190/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0157 - val_loss: 0.0030\n",
      "Epoch 191/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0152 - val_loss: 0.0046\n",
      "Epoch 192/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0164 - val_loss: 0.0031\n",
      "Epoch 193/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0147 - val_loss: 0.0030\n",
      "Epoch 194/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0143 - val_loss: 0.0036\n",
      "Epoch 195/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0153 - val_loss: 0.0045\n",
      "Epoch 196/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0153 - val_loss: 0.0029\n",
      "Epoch 197/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0140 - val_loss: 0.0030\n",
      "Epoch 198/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0144 - val_loss: 0.0027\n",
      "Epoch 199/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0146 - val_loss: 0.0031\n",
      "Epoch 200/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0140 - val_loss: 0.0029\n",
      "Epoch 201/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0132 - val_loss: 0.0027\n",
      "Epoch 202/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0143 - val_loss: 0.0030\n",
      "Epoch 203/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0139 - val_loss: 0.0031\n",
      "Epoch 204/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0142 - val_loss: 0.0031\n",
      "Epoch 205/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0141 - val_loss: 0.0029\n",
      "Epoch 206/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0137 - val_loss: 0.0028\n",
      "Epoch 207/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0136 - val_loss: 0.0035\n",
      "Epoch 208/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0141 - val_loss: 0.0038\n",
      "Epoch 209/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0138 - val_loss: 0.0031\n",
      "Epoch 210/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0127 - val_loss: 0.0028\n",
      "Epoch 211/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0128 - val_loss: 0.0027\n",
      "Epoch 212/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0126 - val_loss: 0.0029\n",
      "Epoch 213/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0137 - val_loss: 0.0034\n",
      "Epoch 214/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0134 - val_loss: 0.0030\n",
      "Epoch 215/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0139 - val_loss: 0.0033\n",
      "Epoch 216/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0127 - val_loss: 0.0028\n",
      "Epoch 217/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0125 - val_loss: 0.0027\n",
      "Epoch 218/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0127 - val_loss: 0.0027\n",
      "Epoch 219/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0129 - val_loss: 0.0032\n",
      "Epoch 220/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0129 - val_loss: 0.0029\n",
      "Epoch 221/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0126 - val_loss: 0.0028\n",
      "Epoch 222/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0123 - val_loss: 0.0029\n",
      "Epoch 223/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0123 - val_loss: 0.0029\n",
      "Epoch 224/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0125 - val_loss: 0.0030\n",
      "Epoch 225/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0120 - val_loss: 0.0029\n",
      "Epoch 226/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0114 - val_loss: 0.0027\n",
      "Epoch 227/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0119 - val_loss: 0.0029\n",
      "Epoch 228/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0123 - val_loss: 0.0043\n",
      "Epoch 229/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0126 - val_loss: 0.0028\n",
      "Epoch 230/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0117 - val_loss: 0.0028\n",
      "Epoch 231/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0113 - val_loss: 0.0028\n",
      "Epoch 232/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0113 - val_loss: 0.0027\n",
      "Epoch 233/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0109 - val_loss: 0.0033\n",
      "Epoch 234/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0121 - val_loss: 0.0030\n",
      "Epoch 235/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0109 - val_loss: 0.0028\n",
      "Epoch 236/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0109 - val_loss: 0.0030\n",
      "Epoch 237/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0120 - val_loss: 0.0031\n",
      "Epoch 238/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0111 - val_loss: 0.0029\n",
      "Epoch 239/500\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.0109 - val_loss: 0.0029\n",
      "Epoch 240/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0102 - val_loss: 0.0034\n",
      "Epoch 241/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0114 - val_loss: 0.0028\n",
      "Epoch 242/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0100 - val_loss: 0.0027\n",
      "Epoch 243/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0099 - val_loss: 0.0028\n",
      "Epoch 244/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0109 - val_loss: 0.0033\n",
      "Epoch 245/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0107 - val_loss: 0.0030\n",
      "Epoch 246/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0104 - val_loss: 0.0029\n",
      "Epoch 247/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0104 - val_loss: 0.0031\n",
      "Epoch 248/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0100 - val_loss: 0.0031\n",
      "Epoch 249/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0098 - val_loss: 0.0028\n",
      "Epoch 250/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0098 - val_loss: 0.0037\n",
      "Epoch 251/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0113 - val_loss: 0.0030\n",
      "Epoch 252/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0098 - val_loss: 0.0028\n",
      "Epoch 253/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0098 - val_loss: 0.0027\n",
      "Epoch 254/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0095 - val_loss: 0.0029\n",
      "Epoch 255/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0097 - val_loss: 0.0034\n",
      "Epoch 256/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0106 - val_loss: 0.0031\n",
      "Epoch 257/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0096 - val_loss: 0.0028\n",
      "Epoch 258/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0095 - val_loss: 0.0027\n",
      "Epoch 259/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0092 - val_loss: 0.0034\n",
      "Epoch 260/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0100 - val_loss: 0.0027\n",
      "Epoch 261/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0092 - val_loss: 0.0027\n",
      "Epoch 262/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0092 - val_loss: 0.0026\n",
      "Epoch 263/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0088 - val_loss: 0.0030\n",
      "Epoch 264/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0101 - val_loss: 0.0041\n",
      "Epoch 265/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0104 - val_loss: 0.0027\n",
      "Epoch 266/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0086 - val_loss: 0.0026\n",
      "Epoch 267/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0086 - val_loss: 0.0026\n",
      "Epoch 268/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0086 - val_loss: 0.0029\n",
      "Epoch 269/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0090 - val_loss: 0.0031\n",
      "Epoch 270/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0093 - val_loss: 0.0028\n",
      "Epoch 271/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0087 - val_loss: 0.0026\n",
      "Epoch 272/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.008 - 0s 70us/step - loss: 0.0085 - val_loss: 0.0028\n",
      "Epoch 273/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0086 - val_loss: 0.0032\n",
      "Epoch 274/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0089 - val_loss: 0.0028\n",
      "Epoch 275/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0086 - val_loss: 0.0026\n",
      "Epoch 276/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0081 - val_loss: 0.0029\n",
      "Epoch 277/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0084 - val_loss: 0.0031\n",
      "Epoch 278/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.0090 - val_loss: 0.0027\n",
      "Epoch 279/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0084 - val_loss: 0.0027\n",
      "Epoch 280/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0082 - val_loss: 0.0031\n",
      "Epoch 281/500\n",
      "2839/2839 [==============================] - 0s 97us/step - loss: 0.0089 - val_loss: 0.0029\n",
      "Epoch 282/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.0083 - val_loss: 0.0026\n",
      "Epoch 283/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0078 - val_loss: 0.0026\n",
      "Epoch 284/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0075 - val_loss: 0.0031\n",
      "Epoch 285/500\n",
      "2839/2839 [==============================] - 0s 107us/step - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 286/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0081 - val_loss: 0.0029\n",
      "Epoch 287/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0076 - val_loss: 0.0028\n",
      "Epoch 288/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0073 - val_loss: 0.0028\n",
      "Epoch 289/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0080 - val_loss: 0.0035\n",
      "Epoch 290/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0084 - val_loss: 0.0029\n",
      "Epoch 291/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0077 - val_loss: 0.0027\n",
      "Epoch 292/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0072 - val_loss: 0.0027\n",
      "Epoch 293/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0073 - val_loss: 0.0027\n",
      "Epoch 294/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0072 - val_loss: 0.0041\n",
      "Epoch 295/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0086 - val_loss: 0.0028\n",
      "Epoch 296/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0073 - val_loss: 0.0026\n",
      "Epoch 297/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 298/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0068 - val_loss: 0.0026\n",
      "Epoch 299/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0070 - val_loss: 0.0026\n",
      "Epoch 300/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0070 - val_loss: 0.0046\n",
      "Epoch 301/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0098 - val_loss: 0.0027\n",
      "Epoch 302/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0070 - val_loss: 0.0025\n",
      "Epoch 303/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 304/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 305/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0071 - val_loss: 0.0025\n",
      "Epoch 306/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0068 - val_loss: 0.0028\n",
      "Epoch 307/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0073 - val_loss: 0.0027\n",
      "Epoch 308/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0072 - val_loss: 0.0025\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0067 - val_loss: 0.0026\n",
      "Epoch 310/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0069 - val_loss: 0.0028\n",
      "Epoch 311/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0067 - val_loss: 0.0027\n",
      "Epoch 312/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0069 - val_loss: 0.0025\n",
      "Epoch 313/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0063 - val_loss: 0.0027\n",
      "Epoch 314/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0068 - val_loss: 0.0028\n",
      "Epoch 315/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0067 - val_loss: 0.0025\n",
      "Epoch 316/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0062 - val_loss: 0.0025\n",
      "Epoch 317/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0063 - val_loss: 0.0031\n",
      "Epoch 318/500\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.0067 - val_loss: 0.0025\n",
      "Epoch 319/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 320/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0063 - val_loss: 0.0030\n",
      "Epoch 321/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0071 - val_loss: 0.0025\n",
      "Epoch 322/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 323/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0058 - val_loss: 0.0027\n",
      "Epoch 324/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 325/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0060 - val_loss: 0.0025\n",
      "Epoch 326/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0060 - val_loss: 0.0027\n",
      "Epoch 327/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0063 - val_loss: 0.0026\n",
      "Epoch 328/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0061 - val_loss: 0.0026\n",
      "Epoch 329/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0062 - val_loss: 0.0025\n",
      "Epoch 330/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0059 - val_loss: 0.0028\n",
      "Epoch 331/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0064 - val_loss: 0.0024\n",
      "Epoch 332/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0056 - val_loss: 0.0025\n",
      "Epoch 333/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0056 - val_loss: 0.0028\n",
      "Epoch 334/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0061 - val_loss: 0.0025\n",
      "Epoch 335/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0055 - val_loss: 0.0025\n",
      "Epoch 336/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0056 - val_loss: 0.0028\n",
      "Epoch 337/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0062 - val_loss: 0.0026\n",
      "Epoch 338/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0055 - val_loss: 0.0025\n",
      "Epoch 339/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0053 - val_loss: 0.0026\n",
      "Epoch 340/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0061 - val_loss: 0.0025\n",
      "Epoch 341/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0053 - val_loss: 0.0025\n",
      "Epoch 342/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0051 - val_loss: 0.0028\n",
      "Epoch 343/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0061 - val_loss: 0.0027\n",
      "Epoch 344/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0057 - val_loss: 0.0025\n",
      "Epoch 345/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0053 - val_loss: 0.0024\n",
      "Epoch 346/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0051 - val_loss: 0.0026\n",
      "Epoch 347/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0059 - val_loss: 0.0025\n",
      "Epoch 348/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0055 - val_loss: 0.0025\n",
      "Epoch 349/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0054 - val_loss: 0.0027\n",
      "Epoch 350/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0057 - val_loss: 0.0025\n",
      "Epoch 351/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0049 - val_loss: 0.0024\n",
      "Epoch 352/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0048 - val_loss: 0.0024\n",
      "Epoch 353/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0031\n",
      "Epoch 354/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0061 - val_loss: 0.0025\n",
      "Epoch 355/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0050 - val_loss: 0.0024\n",
      "Epoch 356/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0049 - val_loss: 0.0024\n",
      "Epoch 357/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0051 - val_loss: 0.0028\n",
      "Epoch 358/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0056 - val_loss: 0.0025\n",
      "Epoch 359/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0048 - val_loss: 0.0024\n",
      "Epoch 360/500\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.0049 - val_loss: 0.0025\n",
      "Epoch 361/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0053 - val_loss: 0.0026\n",
      "Epoch 362/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0050 - val_loss: 0.0024\n",
      "Epoch 363/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 364/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 365/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0048 - val_loss: 0.0026\n",
      "Epoch 366/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.0050 - val_loss: 0.0024\n",
      "Epoch 367/500\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.0046 - val_loss: 0.0024\n",
      "Epoch 368/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 369/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0052 - val_loss: 0.0027\n",
      "Epoch 370/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0051 - val_loss: 0.0024\n",
      "Epoch 371/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 372/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 373/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 374/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 375/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0052 - val_loss: 0.0024\n",
      "Epoch 376/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0045 - val_loss: 0.0024\n",
      "Epoch 377/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0047 - val_loss: 0.0023\n",
      "Epoch 378/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0043 - val_loss: 0.0025\n",
      "Epoch 379/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 380/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0043 - val_loss: 0.0024\n",
      "Epoch 381/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 382/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 383/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.0044 - val_loss: 0.0023\n",
      "Epoch 384/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 385/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0042 - val_loss: 0.0024\n",
      "Epoch 386/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0042 - val_loss: 0.0022\n",
      "Epoch 387/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0040 - val_loss: 0.0024\n",
      "Epoch 388/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0045 - val_loss: 0.0026\n",
      "Epoch 389/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0043 - val_loss: 0.0023\n",
      "Epoch 390/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 391/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 392/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0042 - val_loss: 0.0027\n",
      "Epoch 393/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0046 - val_loss: 0.0024\n",
      "Epoch 394/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0038 - val_loss: 0.0023\n",
      "Epoch 395/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 396/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0043 - val_loss: 0.0022\n",
      "Epoch 397/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0036 - val_loss: 0.0022\n",
      "Epoch 398/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0024\n",
      "Epoch 399/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0042 - val_loss: 0.0026\n",
      "Epoch 400/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0044 - val_loss: 0.0024\n",
      "Epoch 401/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 402/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0037 - val_loss: 0.0026\n",
      "Epoch 403/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 404/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 405/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 406/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0041 - val_loss: 0.0025\n",
      "Epoch 407/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 408/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0038 - val_loss: 0.0032\n",
      "Epoch 409/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0042 - val_loss: 0.0026\n",
      "Epoch 410/500\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.0034 - val_loss: 0.0026\n",
      "Epoch 411/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0037 - val_loss: 0.0027\n",
      "Epoch 412/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0037 - val_loss: 0.0029\n",
      "Epoch 413/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 414/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0035 - val_loss: 0.0028\n",
      "Epoch 415/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 416/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0033 - val_loss: 0.0024\n",
      "Epoch 417/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0033 - val_loss: 0.0026\n",
      "Epoch 418/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 419/500\n",
      "2839/2839 [==============================] - 0s 100us/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 420/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.0033 - val_loss: 0.0027\n",
      "Epoch 421/500\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.0036 - val_loss: 0.0029\n",
      "Epoch 422/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 423/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 424/500\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 425/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 426/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0033 - val_loss: 0.0030\n",
      "Epoch 427/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0046 - val_loss: 0.0025\n",
      "Epoch 428/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0031 - val_loss: 0.0022\n",
      "Epoch 429/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0030 - val_loss: 0.0022\n",
      "Epoch 430/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0030 - val_loss: 0.0024\n",
      "Epoch 431/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 432/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0038 - val_loss: 0.0026\n",
      "Epoch 433/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 434/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0032 - val_loss: 0.0027\n",
      "Epoch 435/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 436/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0032 - val_loss: 0.0023\n",
      "Epoch 437/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 438/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 439/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 440/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0031 - val_loss: 0.0027\n",
      "Epoch 441/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 442/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0032 - val_loss: 0.0024\n",
      "Epoch 443/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 444/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 445/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 446/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 447/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 448/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 449/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0036 - val_loss: 0.0027\n",
      "Epoch 450/500\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 451/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 452/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0033\n",
      "Epoch 453/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 454/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 455/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 456/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 457/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0032 - val_loss: 0.0026\n",
      "Epoch 458/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 459/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0025 - val_loss: 0.0027\n",
      "Epoch 460/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.0033 - val_loss: 0.0028\n",
      "Epoch 461/500\n",
      "2839/2839 [==============================] - 0s 87us/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 462/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0027 - val_loss: 0.0026\n",
      "Epoch 463/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0031 - val_loss: 0.0028\n",
      "Epoch 464/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0031 - val_loss: 0.0026\n",
      "Epoch 465/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.0027 - val_loss: 0.0024\n",
      "Epoch 466/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 467/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0025 - val_loss: 0.0022\n",
      "Epoch 468/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 469/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 470/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 471/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 472/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 473/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0030 - val_loss: 0.0023\n",
      "Epoch 474/500\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 475/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.0024 - val_loss: 0.0022\n",
      "Epoch 476/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0032\n",
      "Epoch 477/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0035 - val_loss: 0.0022\n",
      "Epoch 478/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.0026 - val_loss: 0.0022\n",
      "Epoch 479/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 480/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 481/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0027 - val_loss: 0.0022\n",
      "Epoch 482/500\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 483/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0028 - val_loss: 0.0026\n",
      "Epoch 484/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 485/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 486/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 487/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 488/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 489/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 490/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0024 - val_loss: 0.0027\n",
      "Epoch 491/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.0029 - val_loss: 0.0023\n",
      "Epoch 492/500\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 493/500\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 494/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0026 - val_loss: 0.0023\n",
      "Epoch 495/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 496/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0029 - val_loss: 0.0022\n",
      "Epoch 497/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 498/500\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 499/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 500/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.0028 - val_loss: 0.0031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b561c50>"
      ]
     },
     "execution_count": 746,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train_w2v, y=y_train_w2v, \n",
    "          batch_size=2000, \n",
    "          epochs=500, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_w2v, y_test_w2v),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switched top 10% below (will notate where) to 15%. The only instance where it has changed is in the weighted w2v model cell.\n",
    "#### This is where we generate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_metrics(predictions, y_test):\n",
    "    df = pd.DataFrame(predictions)\n",
    "    predicted_classes = predictions.argmax(axis=1)\n",
    "    # predicting a loss 3% or larger by seeing if max loss probability is higher then slight gain\n",
    "    df['loss'] = np.where(df[0] > df[3], 1, 0)\n",
    "    # predicting a gain 3% or larger by seeing if max gain probability is higher then slight loss\n",
    "    df['gain'] = np.where(df[4] > df[1], 1, 0)\n",
    "    df = pd.merge(df, y_test, left_index=True, right_index=True, how='inner')\n",
    "    # predicting a loss 3% or larger by seeing if max loss probability is higher then mean probibility \n",
    "    df['loss_large'] = np.where(df[0] > df[2], 1, 0)\n",
    "    # predicting a gain 3% or larger by seeing if max gain probability is higher then mean probibility \n",
    "    df['gain_large'] = np.where(df[4] > df[2], 1, 0)\n",
    "    # Cheching if loss is predicted and a loss of 3% or more occurs\n",
    "    df['loss_true_true'] = np.where((df['loss'] == 1) & ((df['2'] == 1) | (df['1'] == 1)), 1, 0)\n",
    "    # Checking if loss is predicted and price stays at the mean\n",
    "    df['loss_stay'] = np.where((df['loss'] == 1) & (df['3'] == 1), 1, 0)\n",
    "    # Cheching if large loss is predicted and a loss of 3% or more occurs\n",
    "    df['loss_large_true'] = np.where((df['loss_large'] == 1) & ((df['2'] == 1) | (df['1'] == 1)), 1, 0)\n",
    "    # Cheching if large loss is predicted and price stays at the mean\n",
    "    df['loss_large_stay'] = np.where((df['loss_large'] == 1) & (df['3'] == 1), 1, 0)\n",
    "    # Cheching if gain is predicted and a loss of 3% or more occurs\n",
    "    df['gain_true_true'] = np.where((df['gain'] == 1) & ((df['4'] == 1) | (df['5'] == 1)), 1, 0)\n",
    "    # Checking if gain is predicted and price stays at the mean\n",
    "    df['gain_stay'] = np.where((df['gain'] == 1) & (df['3'] == 1), 1, 0)\n",
    "    # Cheching if large gain is predicted and a gain of 3% or more occurs\n",
    "    df['gain_large_true'] = np.where((df['gain_large'] == 1) & ((df['4'] == 1) | (df['5'] == 1)), 1, 0)\n",
    "    # Cheching if large gain is predicted and price stays at the mean\n",
    "    df['gain_large_stay'] = np.where((df['gain_large'] == 1) & (df['3'] == 1), 1, 0)\n",
    "    #df['gain_large_large'] = np.where((df['gain_large'] == 1) & (df['5'] == 1), 1, 0)\n",
    "    # Checking if both loss and gain are selected. This appears to mean high volitility\n",
    "    df['volitile'] = np.where((df['loss'] == 1) & (df['gain'] == 1), 1, 0)\n",
    "    df['top_vs_mean_low'] = (df[4] + df[3]) - (df[2] + df[1])\n",
    "    df = df.sort_values('top_vs_mean_low', ascending=False)\n",
    "    top_10 = []\n",
    "    # Here is where I switched 10% to 15%\n",
    "    top = round(len(df) * .15)\n",
    "    for num in range(top):\n",
    "        top_10.append(1)\n",
    "    finish_list = len(df) - len(top_10)\n",
    "    for num in range(finish_list):\n",
    "        top_10.append(0)\n",
    "    df['top_vs_mean_low_select'] = top_10\n",
    "    df['top_vs_mean_low_select_true'] = np.where((df['top_vs_mean_low_select'] == 1) & ((df['4'] == 1) | (df['5'] == 1)), 1, 0)\n",
    "    df['top_vs_mean_low_select_large'] = np.where((df['top_vs_mean_low_select'] == 1) & (df['5'] == 1), 1, 0)\n",
    "    df['gain_large_true_large'] = np.where((df['gain'] == 1) & (df['5'] == 1), 1, 0)\n",
    "    df['top_vs_mean_low_select_mean'] = np.where((df['top_vs_mean_low_select'] == 1) & (df['3'] == 1), 1, 0)\n",
    "    df['predicted_classes'] = predicted_classes\n",
    "    df['predicted_classes_gain'] = np.where((df['predicted_classes'] == 3) | (df['predicted_classes'] == 4) , 1, 0)\n",
    "    df['predicted_classes_gain_true'] = np.where((df['predicted_classes_gain'] == 1)  & ((df['4'] == 1) | (df['5'] == 1)), 1, 0)\n",
    "    df['predicted_classes_gain_large_true'] = np.where((df['predicted_classes_gain'] == 1) & (df['5'] == 1), 1, 0)\n",
    "    df['predicted_classes_gain_mean'] = np.where((df['predicted_classes_gain'] == 1) & (df['3'] == 1), 1, 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    gain_pred = df['gain'].sum()\n",
    "    gain_large_pred = df['gain_large'].sum()\n",
    "    \n",
    "    gain_stay_true = (df['gain_true_true'].sum() + df['gain_stay'].sum()) / df['gain'].sum()\n",
    "    \n",
    "    #top_vs_mid = \n",
    "    gain_true = df['gain_true_true'].sum() / df['gain'].sum()\n",
    "    #loss_stay_true = (df['loss_true_true'].sum() + df['loss_stay'].sum()) / df['loss'].sum()\n",
    "    #loss_stay_true = df['loss_true_true'].sum() / df['loss'].sum()\n",
    "    gain_stay_large_true = (df['gain_large_true'].sum() + df['gain_large_stay'].sum()) / df['gain_large'].sum()\n",
    "    gain_large_true = df['gain_large_true'].sum() / df['gain_large'].sum()\n",
    "    #loss_stay_large_true = (df['loss_large_true'].sum() + df['loss_large_stay']) / df['loss_large'].sum()\n",
    "    #loss_large_true = df['loss_large_true'].sum() / df['loss_large'].sum()\n",
    "    volitile = df['volitile'].sum()\n",
    "    four_five_split = df['top_vs_mean_low_select_true'].sum() / df['top_vs_mean_low_select'].sum()\n",
    "    four_five_split_large = df['top_vs_mean_low_select_large'].sum() / df['top_vs_mean_low_select'].sum()\n",
    "    four_five_split_mean = (df['top_vs_mean_low_select_true'].sum() + df['top_vs_mean_low_select_mean'].sum()) / df['top_vs_mean_low_select'].sum()\n",
    "    predicted_classes_gain_true = df['predicted_classes_gain_true'].sum() / df['predicted_classes_gain'].sum()\n",
    "    predicted_classes_gain_large_true = df['predicted_classes_gain_large_true'].sum() / df['predicted_classes_gain'].sum()\n",
    "    predicted_classes_gain_mean = (df['predicted_classes_gain_true'].sum() + df['predicted_classes_gain_mean'].sum()) / df['predicted_classes_gain'].sum()\n",
    "    gain_true_large = df['gain_large_true_large'].sum() / df['gain'].sum()\n",
    "    #print(gain_pred)\n",
    "    #print(gain_true)\n",
    "    #print(gain_stay_true)\n",
    "    #print(gain_large_pred)\n",
    "    #print(gain_large_true)\n",
    "    #print(gain_stay_large_true)\n",
    "    #print(four_five_split)\n",
    "    #print(gain_large_true_large)\n",
    "    return gain_pred, gain_true_large, gain_true, gain_stay_true, four_five_split_large, four_five_split, four_five_split_mean, predicted_classes_gain_true, predicted_classes_gain_large_true, predicted_classes_gain_mean \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 4, 1, 2, 4, 4, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 2, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3,\n",
       "       3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       2, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 4,\n",
       "       2, 3, 1, 1, 0, 3, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 4, 1, 3, 1, 3, 0,\n",
       "       3, 3, 3, 1, 3, 1, 3, 3, 1, 1, 1, 1, 3, 3, 3, 1, 1, 3, 0, 1, 3, 3, 3,\n",
       "       3, 4, 1, 4, 1, 1, 1, 3, 1, 4, 4, 3, 3, 4, 2, 0, 4, 0, 3, 4, 0, 4, 1,\n",
       "       3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 4, 3, 3, 3, 2, 2, 3, 3, 2, 3, 1, 3, 3,\n",
       "       3, 2, 3, 0, 1, 2, 4, 2, 2, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 1,\n",
       "       2, 2, 3, 2, 4, 2, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3,\n",
       "       3, 3, 3, 2, 3, 3, 1, 4, 0, 0, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 3, 3, 1,\n",
       "       1, 3, 1, 0, 0, 1, 1, 4, 0, 0, 4, 3, 3, 3, 1, 2, 0, 1, 1, 1, 4, 0, 3,\n",
       "       1, 1, 1, 1, 2, 3, 2, 1, 1])"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = predictions.argmax(axis=1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(120,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.00001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/500\n",
      "2839/2839 [==============================] - 6s 2ms/step - loss: 1.6362 - acc: 0.1786 - val_loss: 1.6403 - val_acc: 0.0785\n",
      "Epoch 2/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6369 - acc: 0.1803 - val_loss: 1.6390 - val_acc: 0.0785\n",
      "Epoch 3/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.6321 - acc: 0.1923 - val_loss: 1.6380 - val_acc: 0.0785\n",
      "Epoch 4/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6380 - acc: 0.1934 - val_loss: 1.6370 - val_acc: 0.0816\n",
      "Epoch 5/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6394 - acc: 0.1789 - val_loss: 1.6361 - val_acc: 0.0846\n",
      "Epoch 6/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6386 - acc: 0.1744 - val_loss: 1.6353 - val_acc: 0.0876\n",
      "Epoch 7/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.6299 - acc: 0.1934 - val_loss: 1.6344 - val_acc: 0.0906\n",
      "Epoch 8/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6398 - acc: 0.1832 - val_loss: 1.6336 - val_acc: 0.0937\n",
      "Epoch 9/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6286 - acc: 0.1913 - val_loss: 1.6329 - val_acc: 0.0967\n",
      "Epoch 10/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6290 - acc: 0.1902 - val_loss: 1.6322 - val_acc: 0.1057\n",
      "Epoch 11/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.6338 - acc: 0.1888 - val_loss: 1.6315 - val_acc: 0.1057\n",
      "Epoch 12/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6300 - acc: 0.1892 - val_loss: 1.6309 - val_acc: 0.1118\n",
      "Epoch 13/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6278 - acc: 0.1920 - val_loss: 1.6302 - val_acc: 0.1148\n",
      "Epoch 14/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6253 - acc: 0.1895 - val_loss: 1.6295 - val_acc: 0.1178\n",
      "Epoch 15/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6290 - acc: 0.1899 - val_loss: 1.6289 - val_acc: 0.1178\n",
      "Epoch 16/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.6247 - acc: 0.1828 - val_loss: 1.6282 - val_acc: 0.1178\n",
      "Epoch 17/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.6235 - acc: 0.1983 - val_loss: 1.6276 - val_acc: 0.1178\n",
      "Epoch 18/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6227 - acc: 0.1941 - val_loss: 1.6269 - val_acc: 0.1208\n",
      "Epoch 19/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6241 - acc: 0.1983 - val_loss: 1.6263 - val_acc: 0.1239\n",
      "Epoch 20/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6243 - acc: 0.1930 - val_loss: 1.6257 - val_acc: 0.1269\n",
      "Epoch 21/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6215 - acc: 0.2022 - val_loss: 1.6251 - val_acc: 0.1269\n",
      "Epoch 22/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6217 - acc: 0.1916 - val_loss: 1.6244 - val_acc: 0.1299\n",
      "Epoch 23/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6153 - acc: 0.1965 - val_loss: 1.6238 - val_acc: 0.1360\n",
      "Epoch 24/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.6191 - acc: 0.2015 - val_loss: 1.6232 - val_acc: 0.1360\n",
      "Epoch 25/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6185 - acc: 0.2018 - val_loss: 1.6227 - val_acc: 0.1420\n",
      "Epoch 26/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6246 - acc: 0.1818 - val_loss: 1.6221 - val_acc: 0.1480\n",
      "Epoch 27/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6228 - acc: 0.1881 - val_loss: 1.6215 - val_acc: 0.1511\n",
      "Epoch 28/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6206 - acc: 0.1951 - val_loss: 1.6209 - val_acc: 0.1541\n",
      "Epoch 29/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6258 - acc: 0.1980 - val_loss: 1.6203 - val_acc: 0.1571\n",
      "Epoch 30/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6164 - acc: 0.2128 - val_loss: 1.6197 - val_acc: 0.1601\n",
      "Epoch 31/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6154 - acc: 0.2036 - val_loss: 1.6190 - val_acc: 0.1601\n",
      "Epoch 32/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.6239 - acc: 0.1892 - val_loss: 1.6185 - val_acc: 0.1601\n",
      "Epoch 33/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.6074 - acc: 0.2068 - val_loss: 1.6179 - val_acc: 0.1662\n",
      "Epoch 34/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6227 - acc: 0.1930 - val_loss: 1.6173 - val_acc: 0.1631\n",
      "Epoch 35/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6138 - acc: 0.2194 - val_loss: 1.6167 - val_acc: 0.1631\n",
      "Epoch 36/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.6118 - acc: 0.2061 - val_loss: 1.6161 - val_acc: 0.1692\n",
      "Epoch 37/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6113 - acc: 0.2156 - val_loss: 1.6155 - val_acc: 0.1722\n",
      "Epoch 38/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6091 - acc: 0.2050 - val_loss: 1.6150 - val_acc: 0.1782\n",
      "Epoch 39/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6084 - acc: 0.2029 - val_loss: 1.6144 - val_acc: 0.1843\n",
      "Epoch 40/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6055 - acc: 0.2120 - val_loss: 1.6138 - val_acc: 0.1903\n",
      "Epoch 41/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.6008 - acc: 0.2247 - val_loss: 1.6131 - val_acc: 0.1934\n",
      "Epoch 42/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.6104 - acc: 0.2022 - val_loss: 1.6125 - val_acc: 0.1934\n",
      "Epoch 43/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6143 - acc: 0.2089 - val_loss: 1.6120 - val_acc: 0.1994\n",
      "Epoch 44/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6083 - acc: 0.2103 - val_loss: 1.6114 - val_acc: 0.1994\n",
      "Epoch 45/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6047 - acc: 0.2205 - val_loss: 1.6108 - val_acc: 0.2024\n",
      "Epoch 46/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6085 - acc: 0.2082 - val_loss: 1.6103 - val_acc: 0.2054\n",
      "Epoch 47/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.6100 - acc: 0.2068 - val_loss: 1.6097 - val_acc: 0.2085\n",
      "Epoch 48/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6049 - acc: 0.2216 - val_loss: 1.6091 - val_acc: 0.2085\n",
      "Epoch 49/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6025 - acc: 0.2128 - val_loss: 1.6086 - val_acc: 0.2115\n",
      "Epoch 50/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6069 - acc: 0.2117 - val_loss: 1.6080 - val_acc: 0.2175\n",
      "Epoch 51/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6005 - acc: 0.2307 - val_loss: 1.6075 - val_acc: 0.2205\n",
      "Epoch 52/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6098 - acc: 0.2163 - val_loss: 1.6069 - val_acc: 0.2266\n",
      "Epoch 53/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.6016 - acc: 0.2085 - val_loss: 1.6063 - val_acc: 0.2296\n",
      "Epoch 54/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.6032 - acc: 0.2152 - val_loss: 1.6057 - val_acc: 0.2356\n",
      "Epoch 55/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6067 - acc: 0.1923 - val_loss: 1.6052 - val_acc: 0.2356\n",
      "Epoch 56/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.5988 - acc: 0.2395 - val_loss: 1.6047 - val_acc: 0.2447\n",
      "Epoch 57/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5993 - acc: 0.2180 - val_loss: 1.6041 - val_acc: 0.2447\n",
      "Epoch 58/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6011 - acc: 0.2187 - val_loss: 1.6036 - val_acc: 0.2508\n",
      "Epoch 59/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.5930 - acc: 0.2290 - val_loss: 1.6031 - val_acc: 0.2508\n",
      "Epoch 60/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.5995 - acc: 0.2209 - val_loss: 1.6025 - val_acc: 0.2508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5978 - acc: 0.2145 - val_loss: 1.6019 - val_acc: 0.2538\n",
      "Epoch 62/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5990 - acc: 0.2219 - val_loss: 1.6014 - val_acc: 0.2508\n",
      "Epoch 63/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.6002 - acc: 0.2321 - val_loss: 1.6008 - val_acc: 0.2477\n",
      "Epoch 64/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5976 - acc: 0.2230 - val_loss: 1.6003 - val_acc: 0.2538\n",
      "Epoch 65/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.6062 - acc: 0.2135 - val_loss: 1.5998 - val_acc: 0.2568\n",
      "Epoch 66/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5947 - acc: 0.2318 - val_loss: 1.5992 - val_acc: 0.2568\n",
      "Epoch 67/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5976 - acc: 0.2342 - val_loss: 1.5987 - val_acc: 0.2568\n",
      "Epoch 68/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5930 - acc: 0.2216 - val_loss: 1.5982 - val_acc: 0.2628\n",
      "Epoch 69/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5976 - acc: 0.2254 - val_loss: 1.5978 - val_acc: 0.2659\n",
      "Epoch 70/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.6017 - acc: 0.2191 - val_loss: 1.5972 - val_acc: 0.2689\n",
      "Epoch 71/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.6018 - acc: 0.2304 - val_loss: 1.5967 - val_acc: 0.2779\n",
      "Epoch 72/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5996 - acc: 0.2258 - val_loss: 1.5962 - val_acc: 0.2779\n",
      "Epoch 73/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5929 - acc: 0.2275 - val_loss: 1.5957 - val_acc: 0.2779\n",
      "Epoch 74/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5920 - acc: 0.2212 - val_loss: 1.5952 - val_acc: 0.2779\n",
      "Epoch 75/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5904 - acc: 0.2318 - val_loss: 1.5947 - val_acc: 0.2810\n",
      "Epoch 76/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5857 - acc: 0.2290 - val_loss: 1.5942 - val_acc: 0.2840\n",
      "Epoch 77/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5947 - acc: 0.2290 - val_loss: 1.5937 - val_acc: 0.2840\n",
      "Epoch 78/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5862 - acc: 0.2349 - val_loss: 1.5931 - val_acc: 0.2870\n",
      "Epoch 79/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5865 - acc: 0.2416 - val_loss: 1.5926 - val_acc: 0.2900\n",
      "Epoch 80/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5905 - acc: 0.2349 - val_loss: 1.5921 - val_acc: 0.2900\n",
      "Epoch 81/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5884 - acc: 0.2258 - val_loss: 1.5916 - val_acc: 0.2931\n",
      "Epoch 82/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5918 - acc: 0.2325 - val_loss: 1.5911 - val_acc: 0.2991\n",
      "Epoch 83/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5888 - acc: 0.2469 - val_loss: 1.5905 - val_acc: 0.3051\n",
      "Epoch 84/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5916 - acc: 0.2335 - val_loss: 1.5900 - val_acc: 0.3051\n",
      "Epoch 85/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5907 - acc: 0.2240 - val_loss: 1.5895 - val_acc: 0.3082\n",
      "Epoch 86/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5849 - acc: 0.2402 - val_loss: 1.5890 - val_acc: 0.3142\n",
      "Epoch 87/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5884 - acc: 0.2194 - val_loss: 1.5885 - val_acc: 0.3172\n",
      "Epoch 88/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5836 - acc: 0.2420 - val_loss: 1.5880 - val_acc: 0.3112\n",
      "Epoch 89/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5844 - acc: 0.2364 - val_loss: 1.5875 - val_acc: 0.3142\n",
      "Epoch 90/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5805 - acc: 0.2409 - val_loss: 1.5871 - val_acc: 0.3142\n",
      "Epoch 91/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5779 - acc: 0.2466 - val_loss: 1.5865 - val_acc: 0.3172\n",
      "Epoch 92/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5717 - acc: 0.2466 - val_loss: 1.5859 - val_acc: 0.3172\n",
      "Epoch 93/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5878 - acc: 0.2371 - val_loss: 1.5855 - val_acc: 0.3202\n",
      "Epoch 94/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5812 - acc: 0.2511 - val_loss: 1.5850 - val_acc: 0.3233\n",
      "Epoch 95/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5799 - acc: 0.2614 - val_loss: 1.5844 - val_acc: 0.3233\n",
      "Epoch 96/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5842 - acc: 0.2346 - val_loss: 1.5839 - val_acc: 0.3263\n",
      "Epoch 97/500\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 1.5761 - acc: 0.2445 - val_loss: 1.5835 - val_acc: 0.3293\n",
      "Epoch 98/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.5811 - acc: 0.2392 - val_loss: 1.5830 - val_acc: 0.3323\n",
      "Epoch 99/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.5891 - acc: 0.2413 - val_loss: 1.5825 - val_acc: 0.3263\n",
      "Epoch 100/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5793 - acc: 0.2434 - val_loss: 1.5821 - val_acc: 0.3293\n",
      "Epoch 101/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5831 - acc: 0.2360 - val_loss: 1.5816 - val_acc: 0.3293\n",
      "Epoch 102/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5802 - acc: 0.2388 - val_loss: 1.5812 - val_acc: 0.3323\n",
      "Epoch 103/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5778 - acc: 0.2445 - val_loss: 1.5806 - val_acc: 0.3323\n",
      "Epoch 104/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5742 - acc: 0.2487 - val_loss: 1.5801 - val_acc: 0.3353\n",
      "Epoch 105/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5777 - acc: 0.2448 - val_loss: 1.5796 - val_acc: 0.3353\n",
      "Epoch 106/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5821 - acc: 0.2367 - val_loss: 1.5791 - val_acc: 0.3384\n",
      "Epoch 107/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5701 - acc: 0.2617 - val_loss: 1.5786 - val_acc: 0.3384\n",
      "Epoch 108/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5818 - acc: 0.2445 - val_loss: 1.5781 - val_acc: 0.3384\n",
      "Epoch 109/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5712 - acc: 0.2434 - val_loss: 1.5776 - val_acc: 0.3384\n",
      "Epoch 110/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5723 - acc: 0.2526 - val_loss: 1.5771 - val_acc: 0.3384\n",
      "Epoch 111/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5810 - acc: 0.2374 - val_loss: 1.5767 - val_acc: 0.3384\n",
      "Epoch 112/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5784 - acc: 0.2476 - val_loss: 1.5762 - val_acc: 0.3353\n",
      "Epoch 113/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5765 - acc: 0.2589 - val_loss: 1.5757 - val_acc: 0.3353\n",
      "Epoch 114/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5721 - acc: 0.2617 - val_loss: 1.5752 - val_acc: 0.3353\n",
      "Epoch 115/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5759 - acc: 0.2582 - val_loss: 1.5748 - val_acc: 0.3353\n",
      "Epoch 116/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5787 - acc: 0.2430 - val_loss: 1.5742 - val_acc: 0.3353\n",
      "Epoch 117/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5728 - acc: 0.2536 - val_loss: 1.5738 - val_acc: 0.3263\n",
      "Epoch 118/500\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.5731 - acc: 0.2434 - val_loss: 1.5733 - val_acc: 0.3263\n",
      "Epoch 119/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5742 - acc: 0.2497 - val_loss: 1.5728 - val_acc: 0.3233\n",
      "Epoch 120/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.5640 - acc: 0.2762 - val_loss: 1.5723 - val_acc: 0.3263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5725 - acc: 0.2652 - val_loss: 1.5718 - val_acc: 0.3293\n",
      "Epoch 122/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.5681 - acc: 0.2571 - val_loss: 1.5713 - val_acc: 0.3384\n",
      "Epoch 123/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5677 - acc: 0.2635 - val_loss: 1.5708 - val_acc: 0.3384\n",
      "Epoch 124/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.5636 - acc: 0.2603 - val_loss: 1.5702 - val_acc: 0.3384\n",
      "Epoch 125/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.5601 - acc: 0.2730 - val_loss: 1.5697 - val_acc: 0.3353\n",
      "Epoch 126/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5672 - acc: 0.2642 - val_loss: 1.5692 - val_acc: 0.3353\n",
      "Epoch 127/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5657 - acc: 0.2557 - val_loss: 1.5687 - val_acc: 0.3384\n",
      "Epoch 128/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5624 - acc: 0.2603 - val_loss: 1.5682 - val_acc: 0.3384\n",
      "Epoch 129/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5576 - acc: 0.2716 - val_loss: 1.5677 - val_acc: 0.3384\n",
      "Epoch 130/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5571 - acc: 0.2726 - val_loss: 1.5672 - val_acc: 0.3384\n",
      "Epoch 131/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.5629 - acc: 0.2666 - val_loss: 1.5667 - val_acc: 0.3414\n",
      "Epoch 132/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5689 - acc: 0.2712 - val_loss: 1.5661 - val_acc: 0.3414\n",
      "Epoch 133/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5608 - acc: 0.2730 - val_loss: 1.5656 - val_acc: 0.3414\n",
      "Epoch 134/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5617 - acc: 0.2684 - val_loss: 1.5651 - val_acc: 0.3414\n",
      "Epoch 135/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5662 - acc: 0.2638 - val_loss: 1.5646 - val_acc: 0.3414\n",
      "Epoch 136/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5575 - acc: 0.2709 - val_loss: 1.5640 - val_acc: 0.3414\n",
      "Epoch 137/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5598 - acc: 0.2783 - val_loss: 1.5636 - val_acc: 0.3444\n",
      "Epoch 138/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5580 - acc: 0.2585 - val_loss: 1.5630 - val_acc: 0.3474\n",
      "Epoch 139/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5593 - acc: 0.2719 - val_loss: 1.5625 - val_acc: 0.3505\n",
      "Epoch 140/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5662 - acc: 0.2691 - val_loss: 1.5621 - val_acc: 0.3505\n",
      "Epoch 141/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5613 - acc: 0.2600 - val_loss: 1.5615 - val_acc: 0.3474\n",
      "Epoch 142/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5559 - acc: 0.2712 - val_loss: 1.5610 - val_acc: 0.3474\n",
      "Epoch 143/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.5589 - acc: 0.2772 - val_loss: 1.5605 - val_acc: 0.3444\n",
      "Epoch 144/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5541 - acc: 0.2821 - val_loss: 1.5600 - val_acc: 0.3444\n",
      "Epoch 145/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5671 - acc: 0.2600 - val_loss: 1.5595 - val_acc: 0.3474\n",
      "Epoch 146/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5627 - acc: 0.2709 - val_loss: 1.5590 - val_acc: 0.3444\n",
      "Epoch 147/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5512 - acc: 0.2765 - val_loss: 1.5585 - val_acc: 0.3505\n",
      "Epoch 148/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5584 - acc: 0.2705 - val_loss: 1.5580 - val_acc: 0.3505\n",
      "Epoch 149/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5575 - acc: 0.2762 - val_loss: 1.5574 - val_acc: 0.3505\n",
      "Epoch 150/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5608 - acc: 0.2607 - val_loss: 1.5569 - val_acc: 0.3505\n",
      "Epoch 151/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5562 - acc: 0.2839 - val_loss: 1.5564 - val_acc: 0.3535\n",
      "Epoch 152/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5528 - acc: 0.2733 - val_loss: 1.5559 - val_acc: 0.3535\n",
      "Epoch 153/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5542 - acc: 0.2716 - val_loss: 1.5554 - val_acc: 0.3565\n",
      "Epoch 154/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5452 - acc: 0.2892 - val_loss: 1.5548 - val_acc: 0.3565\n",
      "Epoch 155/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5583 - acc: 0.2621 - val_loss: 1.5543 - val_acc: 0.3565\n",
      "Epoch 156/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5535 - acc: 0.2814 - val_loss: 1.5538 - val_acc: 0.3565\n",
      "Epoch 157/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5551 - acc: 0.2758 - val_loss: 1.5533 - val_acc: 0.3595\n",
      "Epoch 158/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5539 - acc: 0.2881 - val_loss: 1.5528 - val_acc: 0.3625\n",
      "Epoch 159/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5608 - acc: 0.2589 - val_loss: 1.5523 - val_acc: 0.3656\n",
      "Epoch 160/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5504 - acc: 0.2874 - val_loss: 1.5518 - val_acc: 0.3656\n",
      "Epoch 161/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5523 - acc: 0.2705 - val_loss: 1.5513 - val_acc: 0.3656\n",
      "Epoch 162/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5493 - acc: 0.2747 - val_loss: 1.5508 - val_acc: 0.3656\n",
      "Epoch 163/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5549 - acc: 0.2786 - val_loss: 1.5502 - val_acc: 0.3656\n",
      "Epoch 164/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5437 - acc: 0.2913 - val_loss: 1.5497 - val_acc: 0.3656\n",
      "Epoch 165/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5559 - acc: 0.2783 - val_loss: 1.5492 - val_acc: 0.3656\n",
      "Epoch 166/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5468 - acc: 0.2843 - val_loss: 1.5486 - val_acc: 0.3656\n",
      "Epoch 167/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5480 - acc: 0.2899 - val_loss: 1.5481 - val_acc: 0.3686\n",
      "Epoch 168/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5464 - acc: 0.2874 - val_loss: 1.5475 - val_acc: 0.3686\n",
      "Epoch 169/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5445 - acc: 0.2917 - val_loss: 1.5470 - val_acc: 0.3686\n",
      "Epoch 170/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5454 - acc: 0.2878 - val_loss: 1.5465 - val_acc: 0.3686\n",
      "Epoch 171/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5541 - acc: 0.2744 - val_loss: 1.5460 - val_acc: 0.3686\n",
      "Epoch 172/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5470 - acc: 0.2931 - val_loss: 1.5455 - val_acc: 0.3716\n",
      "Epoch 173/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5427 - acc: 0.2917 - val_loss: 1.5450 - val_acc: 0.3686\n",
      "Epoch 174/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5452 - acc: 0.2888 - val_loss: 1.5445 - val_acc: 0.3686\n",
      "Epoch 175/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5431 - acc: 0.2853 - val_loss: 1.5439 - val_acc: 0.3686\n",
      "Epoch 176/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5397 - acc: 0.2948 - val_loss: 1.5434 - val_acc: 0.3686\n",
      "Epoch 177/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5422 - acc: 0.2885 - val_loss: 1.5428 - val_acc: 0.3686\n",
      "Epoch 178/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5484 - acc: 0.2832 - val_loss: 1.5423 - val_acc: 0.3716\n",
      "Epoch 179/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5402 - acc: 0.2895 - val_loss: 1.5418 - val_acc: 0.3716\n",
      "Epoch 180/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5568 - acc: 0.2790 - val_loss: 1.5414 - val_acc: 0.3686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5356 - acc: 0.2994 - val_loss: 1.5408 - val_acc: 0.3656\n",
      "Epoch 182/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5517 - acc: 0.2969 - val_loss: 1.5403 - val_acc: 0.3656\n",
      "Epoch 183/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5369 - acc: 0.2853 - val_loss: 1.5398 - val_acc: 0.3686\n",
      "Epoch 184/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5356 - acc: 0.3086 - val_loss: 1.5393 - val_acc: 0.3686\n",
      "Epoch 185/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5377 - acc: 0.3015 - val_loss: 1.5387 - val_acc: 0.3686\n",
      "Epoch 186/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5384 - acc: 0.2976 - val_loss: 1.5383 - val_acc: 0.3746\n",
      "Epoch 187/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5325 - acc: 0.2998 - val_loss: 1.5377 - val_acc: 0.3746\n",
      "Epoch 188/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5287 - acc: 0.3064 - val_loss: 1.5372 - val_acc: 0.3716\n",
      "Epoch 189/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5461 - acc: 0.2895 - val_loss: 1.5367 - val_acc: 0.3716\n",
      "Epoch 190/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5388 - acc: 0.2934 - val_loss: 1.5362 - val_acc: 0.3716\n",
      "Epoch 191/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5318 - acc: 0.3047 - val_loss: 1.5356 - val_acc: 0.3716\n",
      "Epoch 192/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5359 - acc: 0.2973 - val_loss: 1.5351 - val_acc: 0.3716\n",
      "Epoch 193/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5335 - acc: 0.3054 - val_loss: 1.5346 - val_acc: 0.3716\n",
      "Epoch 194/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5320 - acc: 0.3043 - val_loss: 1.5340 - val_acc: 0.3716\n",
      "Epoch 195/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5381 - acc: 0.3029 - val_loss: 1.5335 - val_acc: 0.3656\n",
      "Epoch 196/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5344 - acc: 0.2955 - val_loss: 1.5330 - val_acc: 0.3656\n",
      "Epoch 197/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5389 - acc: 0.2902 - val_loss: 1.5326 - val_acc: 0.3686\n",
      "Epoch 198/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5381 - acc: 0.3036 - val_loss: 1.5321 - val_acc: 0.3686\n",
      "Epoch 199/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5332 - acc: 0.2998 - val_loss: 1.5316 - val_acc: 0.3686\n",
      "Epoch 200/500\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 1.5327 - acc: 0.3061 - val_loss: 1.5311 - val_acc: 0.3716\n",
      "Epoch 201/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5217 - acc: 0.3188 - val_loss: 1.5306 - val_acc: 0.3686\n",
      "Epoch 202/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5254 - acc: 0.3050 - val_loss: 1.5301 - val_acc: 0.3686\n",
      "Epoch 203/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.5347 - acc: 0.2959 - val_loss: 1.5296 - val_acc: 0.3686\n",
      "Epoch 204/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.5248 - acc: 0.3064 - val_loss: 1.5290 - val_acc: 0.3686\n",
      "Epoch 205/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5279 - acc: 0.3026 - val_loss: 1.5286 - val_acc: 0.3716\n",
      "Epoch 206/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5335 - acc: 0.3057 - val_loss: 1.5281 - val_acc: 0.3716\n",
      "Epoch 207/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.5344 - acc: 0.2976 - val_loss: 1.5276 - val_acc: 0.3716\n",
      "Epoch 208/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.5296 - acc: 0.3005 - val_loss: 1.5271 - val_acc: 0.3716\n",
      "Epoch 209/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5286 - acc: 0.3110 - val_loss: 1.5266 - val_acc: 0.3746\n",
      "Epoch 210/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5314 - acc: 0.2980 - val_loss: 1.5261 - val_acc: 0.3746\n",
      "Epoch 211/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5268 - acc: 0.3230 - val_loss: 1.5256 - val_acc: 0.3776\n",
      "Epoch 212/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5314 - acc: 0.3072 - val_loss: 1.5251 - val_acc: 0.3807\n",
      "Epoch 213/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5315 - acc: 0.3019 - val_loss: 1.5245 - val_acc: 0.3776\n",
      "Epoch 214/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5286 - acc: 0.3216 - val_loss: 1.5240 - val_acc: 0.3776\n",
      "Epoch 215/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5264 - acc: 0.3050 - val_loss: 1.5235 - val_acc: 0.3776\n",
      "Epoch 216/500\n",
      "2839/2839 [==============================] - 0s 22us/step - loss: 1.5305 - acc: 0.3145 - val_loss: 1.5230 - val_acc: 0.3776\n",
      "Epoch 217/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5298 - acc: 0.3079 - val_loss: 1.5225 - val_acc: 0.3776\n",
      "Epoch 218/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5281 - acc: 0.3040 - val_loss: 1.5220 - val_acc: 0.3776\n",
      "Epoch 219/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5291 - acc: 0.2994 - val_loss: 1.5215 - val_acc: 0.3776\n",
      "Epoch 220/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5352 - acc: 0.2955 - val_loss: 1.5210 - val_acc: 0.3776\n",
      "Epoch 221/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5277 - acc: 0.3124 - val_loss: 1.5205 - val_acc: 0.3776\n",
      "Epoch 222/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5224 - acc: 0.3209 - val_loss: 1.5201 - val_acc: 0.3776\n",
      "Epoch 223/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.5207 - acc: 0.3167 - val_loss: 1.5195 - val_acc: 0.3776\n",
      "Epoch 224/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.5255 - acc: 0.3145 - val_loss: 1.5191 - val_acc: 0.3776\n",
      "Epoch 225/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5242 - acc: 0.3135 - val_loss: 1.5186 - val_acc: 0.3776\n",
      "Epoch 226/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.5308 - acc: 0.3075 - val_loss: 1.5181 - val_acc: 0.3776\n",
      "Epoch 227/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5291 - acc: 0.3156 - val_loss: 1.5176 - val_acc: 0.3776\n",
      "Epoch 228/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.5200 - acc: 0.3188 - val_loss: 1.5171 - val_acc: 0.3776\n",
      "Epoch 229/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5176 - acc: 0.3156 - val_loss: 1.5166 - val_acc: 0.3776\n",
      "Epoch 230/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5130 - acc: 0.3195 - val_loss: 1.5161 - val_acc: 0.3776\n",
      "Epoch 231/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5192 - acc: 0.3064 - val_loss: 1.5156 - val_acc: 0.3776\n",
      "Epoch 232/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5226 - acc: 0.3096 - val_loss: 1.5152 - val_acc: 0.3776\n",
      "Epoch 233/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5141 - acc: 0.3209 - val_loss: 1.5147 - val_acc: 0.3776\n",
      "Epoch 234/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5146 - acc: 0.3300 - val_loss: 1.5142 - val_acc: 0.3776\n",
      "Epoch 235/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5180 - acc: 0.3230 - val_loss: 1.5137 - val_acc: 0.3776\n",
      "Epoch 236/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5105 - acc: 0.3209 - val_loss: 1.5133 - val_acc: 0.3807\n",
      "Epoch 237/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5334 - acc: 0.2941 - val_loss: 1.5129 - val_acc: 0.3807\n",
      "Epoch 238/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5080 - acc: 0.3374 - val_loss: 1.5124 - val_acc: 0.3807\n",
      "Epoch 239/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5155 - acc: 0.3191 - val_loss: 1.5119 - val_acc: 0.3837\n",
      "Epoch 240/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5170 - acc: 0.3093 - val_loss: 1.5115 - val_acc: 0.3837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5123 - acc: 0.3241 - val_loss: 1.5110 - val_acc: 0.3807\n",
      "Epoch 242/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5136 - acc: 0.3286 - val_loss: 1.5105 - val_acc: 0.3776\n",
      "Epoch 243/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5186 - acc: 0.3219 - val_loss: 1.5101 - val_acc: 0.3776\n",
      "Epoch 244/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5087 - acc: 0.3315 - val_loss: 1.5096 - val_acc: 0.3807\n",
      "Epoch 245/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5089 - acc: 0.3318 - val_loss: 1.5091 - val_acc: 0.3807\n",
      "Epoch 246/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5147 - acc: 0.3300 - val_loss: 1.5087 - val_acc: 0.3837\n",
      "Epoch 247/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5095 - acc: 0.3269 - val_loss: 1.5081 - val_acc: 0.3807\n",
      "Epoch 248/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.5107 - acc: 0.3315 - val_loss: 1.5077 - val_acc: 0.3807\n",
      "Epoch 249/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5083 - acc: 0.3262 - val_loss: 1.5072 - val_acc: 0.3776\n",
      "Epoch 250/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5128 - acc: 0.3202 - val_loss: 1.5067 - val_acc: 0.3807\n",
      "Epoch 251/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5126 - acc: 0.3269 - val_loss: 1.5062 - val_acc: 0.3807\n",
      "Epoch 252/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5173 - acc: 0.3188 - val_loss: 1.5058 - val_acc: 0.3807\n",
      "Epoch 253/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5130 - acc: 0.3212 - val_loss: 1.5053 - val_acc: 0.3837\n",
      "Epoch 254/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5096 - acc: 0.3255 - val_loss: 1.5047 - val_acc: 0.3807\n",
      "Epoch 255/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5114 - acc: 0.3300 - val_loss: 1.5043 - val_acc: 0.3807\n",
      "Epoch 256/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5046 - acc: 0.3357 - val_loss: 1.5038 - val_acc: 0.3807\n",
      "Epoch 257/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4980 - acc: 0.3593 - val_loss: 1.5033 - val_acc: 0.3807\n",
      "Epoch 258/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5052 - acc: 0.3403 - val_loss: 1.5029 - val_acc: 0.3776\n",
      "Epoch 259/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5081 - acc: 0.3293 - val_loss: 1.5024 - val_acc: 0.3776\n",
      "Epoch 260/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5137 - acc: 0.3234 - val_loss: 1.5020 - val_acc: 0.3776\n",
      "Epoch 261/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5096 - acc: 0.3290 - val_loss: 1.5015 - val_acc: 0.3776\n",
      "Epoch 262/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5010 - acc: 0.3385 - val_loss: 1.5010 - val_acc: 0.3776\n",
      "Epoch 263/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5050 - acc: 0.3350 - val_loss: 1.5005 - val_acc: 0.3746\n",
      "Epoch 264/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5174 - acc: 0.3279 - val_loss: 1.5001 - val_acc: 0.3746\n",
      "Epoch 265/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5131 - acc: 0.3167 - val_loss: 1.4997 - val_acc: 0.3746\n",
      "Epoch 266/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5038 - acc: 0.3195 - val_loss: 1.4992 - val_acc: 0.3746\n",
      "Epoch 267/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5095 - acc: 0.3226 - val_loss: 1.4988 - val_acc: 0.3746\n",
      "Epoch 268/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5077 - acc: 0.3255 - val_loss: 1.4983 - val_acc: 0.3746\n",
      "Epoch 269/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5126 - acc: 0.3145 - val_loss: 1.4979 - val_acc: 0.3746\n",
      "Epoch 270/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4946 - acc: 0.3389 - val_loss: 1.4974 - val_acc: 0.3716\n",
      "Epoch 271/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5046 - acc: 0.3156 - val_loss: 1.4970 - val_acc: 0.3716\n",
      "Epoch 272/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5026 - acc: 0.3410 - val_loss: 1.4965 - val_acc: 0.3716\n",
      "Epoch 273/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4949 - acc: 0.3434 - val_loss: 1.4960 - val_acc: 0.3686\n",
      "Epoch 274/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4973 - acc: 0.3371 - val_loss: 1.4955 - val_acc: 0.3656\n",
      "Epoch 275/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5059 - acc: 0.3205 - val_loss: 1.4950 - val_acc: 0.3656\n",
      "Epoch 276/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5035 - acc: 0.3191 - val_loss: 1.4946 - val_acc: 0.3625\n",
      "Epoch 277/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4970 - acc: 0.3374 - val_loss: 1.4941 - val_acc: 0.3625\n",
      "Epoch 278/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5155 - acc: 0.3223 - val_loss: 1.4937 - val_acc: 0.3625\n",
      "Epoch 279/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5015 - acc: 0.3318 - val_loss: 1.4932 - val_acc: 0.3625\n",
      "Epoch 280/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4969 - acc: 0.3512 - val_loss: 1.4926 - val_acc: 0.3625\n",
      "Epoch 281/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5074 - acc: 0.3336 - val_loss: 1.4922 - val_acc: 0.3595\n",
      "Epoch 282/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4982 - acc: 0.3293 - val_loss: 1.4917 - val_acc: 0.3595\n",
      "Epoch 283/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4925 - acc: 0.3431 - val_loss: 1.4913 - val_acc: 0.3595\n",
      "Epoch 284/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5050 - acc: 0.3167 - val_loss: 1.4908 - val_acc: 0.3595\n",
      "Epoch 285/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4958 - acc: 0.3508 - val_loss: 1.4904 - val_acc: 0.3595\n",
      "Epoch 286/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4976 - acc: 0.3297 - val_loss: 1.4899 - val_acc: 0.3595\n",
      "Epoch 287/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4897 - acc: 0.3434 - val_loss: 1.4895 - val_acc: 0.3595\n",
      "Epoch 288/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4938 - acc: 0.3219 - val_loss: 1.4890 - val_acc: 0.3595\n",
      "Epoch 289/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5035 - acc: 0.3269 - val_loss: 1.4885 - val_acc: 0.3595\n",
      "Epoch 290/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.5077 - acc: 0.3367 - val_loss: 1.4881 - val_acc: 0.3595\n",
      "Epoch 291/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4929 - acc: 0.3438 - val_loss: 1.4876 - val_acc: 0.3595\n",
      "Epoch 292/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4901 - acc: 0.3357 - val_loss: 1.4871 - val_acc: 0.3595\n",
      "Epoch 293/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.5043 - acc: 0.3223 - val_loss: 1.4867 - val_acc: 0.3595\n",
      "Epoch 294/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4911 - acc: 0.3494 - val_loss: 1.4862 - val_acc: 0.3595\n",
      "Epoch 295/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4865 - acc: 0.3410 - val_loss: 1.4857 - val_acc: 0.3595\n",
      "Epoch 296/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4959 - acc: 0.3332 - val_loss: 1.4853 - val_acc: 0.3625\n",
      "Epoch 297/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5000 - acc: 0.3322 - val_loss: 1.4849 - val_acc: 0.3625\n",
      "Epoch 298/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4822 - acc: 0.3522 - val_loss: 1.4844 - val_acc: 0.3595\n",
      "Epoch 299/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4880 - acc: 0.3536 - val_loss: 1.4839 - val_acc: 0.3595\n",
      "Epoch 300/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4927 - acc: 0.3364 - val_loss: 1.4835 - val_acc: 0.3595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4929 - acc: 0.3484 - val_loss: 1.4831 - val_acc: 0.3595\n",
      "Epoch 302/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4878 - acc: 0.3561 - val_loss: 1.4826 - val_acc: 0.3595\n",
      "Epoch 303/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4979 - acc: 0.3237 - val_loss: 1.4822 - val_acc: 0.3595\n",
      "Epoch 304/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4855 - acc: 0.3452 - val_loss: 1.4817 - val_acc: 0.3565\n",
      "Epoch 305/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4936 - acc: 0.3237 - val_loss: 1.4812 - val_acc: 0.3595\n",
      "Epoch 306/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.5012 - acc: 0.3272 - val_loss: 1.4808 - val_acc: 0.3595\n",
      "Epoch 307/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4754 - acc: 0.3512 - val_loss: 1.4803 - val_acc: 0.3595\n",
      "Epoch 308/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4831 - acc: 0.3536 - val_loss: 1.4798 - val_acc: 0.3625\n",
      "Epoch 309/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4744 - acc: 0.3607 - val_loss: 1.4793 - val_acc: 0.3595\n",
      "Epoch 310/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4854 - acc: 0.3529 - val_loss: 1.4788 - val_acc: 0.3625\n",
      "Epoch 311/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.5029 - acc: 0.3251 - val_loss: 1.4784 - val_acc: 0.3595\n",
      "Epoch 312/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4872 - acc: 0.3505 - val_loss: 1.4780 - val_acc: 0.3595\n",
      "Epoch 313/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4853 - acc: 0.3455 - val_loss: 1.4775 - val_acc: 0.3565\n",
      "Epoch 314/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4812 - acc: 0.3512 - val_loss: 1.4770 - val_acc: 0.3595\n",
      "Epoch 315/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4941 - acc: 0.3248 - val_loss: 1.4767 - val_acc: 0.3595\n",
      "Epoch 316/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4862 - acc: 0.3389 - val_loss: 1.4762 - val_acc: 0.3595\n",
      "Epoch 317/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4799 - acc: 0.3494 - val_loss: 1.4758 - val_acc: 0.3595\n",
      "Epoch 318/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4773 - acc: 0.3586 - val_loss: 1.4753 - val_acc: 0.3565\n",
      "Epoch 319/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4824 - acc: 0.3501 - val_loss: 1.4749 - val_acc: 0.3565\n",
      "Epoch 320/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4947 - acc: 0.3480 - val_loss: 1.4745 - val_acc: 0.3595\n",
      "Epoch 321/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4866 - acc: 0.3498 - val_loss: 1.4741 - val_acc: 0.3595\n",
      "Epoch 322/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4834 - acc: 0.3455 - val_loss: 1.4737 - val_acc: 0.3595\n",
      "Epoch 323/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4915 - acc: 0.3360 - val_loss: 1.4733 - val_acc: 0.3595\n",
      "Epoch 324/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4769 - acc: 0.3473 - val_loss: 1.4728 - val_acc: 0.3595\n",
      "Epoch 325/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4776 - acc: 0.3445 - val_loss: 1.4724 - val_acc: 0.3595\n",
      "Epoch 326/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4931 - acc: 0.3350 - val_loss: 1.4719 - val_acc: 0.3565\n",
      "Epoch 327/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4964 - acc: 0.3480 - val_loss: 1.4715 - val_acc: 0.3565\n",
      "Epoch 328/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4749 - acc: 0.3639 - val_loss: 1.4711 - val_acc: 0.3565\n",
      "Epoch 329/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4800 - acc: 0.3484 - val_loss: 1.4707 - val_acc: 0.3565\n",
      "Epoch 330/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4769 - acc: 0.3526 - val_loss: 1.4702 - val_acc: 0.3565\n",
      "Epoch 331/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4828 - acc: 0.3371 - val_loss: 1.4698 - val_acc: 0.3565\n",
      "Epoch 332/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4758 - acc: 0.3498 - val_loss: 1.4693 - val_acc: 0.3565\n",
      "Epoch 333/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4836 - acc: 0.3526 - val_loss: 1.4689 - val_acc: 0.3535\n",
      "Epoch 334/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4873 - acc: 0.3438 - val_loss: 1.4684 - val_acc: 0.3535\n",
      "Epoch 335/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4829 - acc: 0.3480 - val_loss: 1.4680 - val_acc: 0.3505\n",
      "Epoch 336/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4909 - acc: 0.3396 - val_loss: 1.4676 - val_acc: 0.3505\n",
      "Epoch 337/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4688 - acc: 0.3656 - val_loss: 1.4673 - val_acc: 0.3474\n",
      "Epoch 338/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4717 - acc: 0.3434 - val_loss: 1.4669 - val_acc: 0.3474\n",
      "Epoch 339/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4764 - acc: 0.3536 - val_loss: 1.4665 - val_acc: 0.3474\n",
      "Epoch 340/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4794 - acc: 0.3487 - val_loss: 1.4661 - val_acc: 0.3474\n",
      "Epoch 341/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4619 - acc: 0.3691 - val_loss: 1.4656 - val_acc: 0.3474\n",
      "Epoch 342/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4885 - acc: 0.3480 - val_loss: 1.4653 - val_acc: 0.3474\n",
      "Epoch 343/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4728 - acc: 0.3565 - val_loss: 1.4649 - val_acc: 0.3474\n",
      "Epoch 344/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4824 - acc: 0.3413 - val_loss: 1.4644 - val_acc: 0.3474\n",
      "Epoch 345/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4695 - acc: 0.3610 - val_loss: 1.4641 - val_acc: 0.3474\n",
      "Epoch 346/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4884 - acc: 0.3445 - val_loss: 1.4637 - val_acc: 0.3474\n",
      "Epoch 347/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4708 - acc: 0.3505 - val_loss: 1.4633 - val_acc: 0.3444\n",
      "Epoch 348/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4841 - acc: 0.3551 - val_loss: 1.4629 - val_acc: 0.3444\n",
      "Epoch 349/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4758 - acc: 0.3480 - val_loss: 1.4624 - val_acc: 0.3444\n",
      "Epoch 350/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4823 - acc: 0.3325 - val_loss: 1.4621 - val_acc: 0.3444\n",
      "Epoch 351/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4747 - acc: 0.3614 - val_loss: 1.4617 - val_acc: 0.3444\n",
      "Epoch 352/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4719 - acc: 0.3512 - val_loss: 1.4612 - val_acc: 0.3444\n",
      "Epoch 353/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4749 - acc: 0.3515 - val_loss: 1.4608 - val_acc: 0.3444\n",
      "Epoch 354/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4738 - acc: 0.3561 - val_loss: 1.4605 - val_acc: 0.3444\n",
      "Epoch 355/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4654 - acc: 0.3522 - val_loss: 1.4600 - val_acc: 0.3444\n",
      "Epoch 356/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4743 - acc: 0.3572 - val_loss: 1.4596 - val_acc: 0.3444\n",
      "Epoch 357/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4718 - acc: 0.3691 - val_loss: 1.4592 - val_acc: 0.3444\n",
      "Epoch 358/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4758 - acc: 0.3484 - val_loss: 1.4588 - val_acc: 0.3444\n",
      "Epoch 359/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4798 - acc: 0.3526 - val_loss: 1.4585 - val_acc: 0.3474\n",
      "Epoch 360/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4681 - acc: 0.3572 - val_loss: 1.4581 - val_acc: 0.3474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4755 - acc: 0.3470 - val_loss: 1.4577 - val_acc: 0.3474\n",
      "Epoch 362/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4680 - acc: 0.3667 - val_loss: 1.4573 - val_acc: 0.3474\n",
      "Epoch 363/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4590 - acc: 0.3575 - val_loss: 1.4569 - val_acc: 0.3474\n",
      "Epoch 364/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4571 - acc: 0.3751 - val_loss: 1.4565 - val_acc: 0.3474\n",
      "Epoch 365/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4699 - acc: 0.3551 - val_loss: 1.4562 - val_acc: 0.3474\n",
      "Epoch 366/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4682 - acc: 0.3512 - val_loss: 1.4558 - val_acc: 0.3474\n",
      "Epoch 367/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4567 - acc: 0.3663 - val_loss: 1.4554 - val_acc: 0.3474\n",
      "Epoch 368/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4714 - acc: 0.3603 - val_loss: 1.4550 - val_acc: 0.3444\n",
      "Epoch 369/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4709 - acc: 0.3487 - val_loss: 1.4545 - val_acc: 0.3414\n",
      "Epoch 370/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4715 - acc: 0.3420 - val_loss: 1.4541 - val_acc: 0.3414\n",
      "Epoch 371/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4608 - acc: 0.3589 - val_loss: 1.4537 - val_acc: 0.3414\n",
      "Epoch 372/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4625 - acc: 0.3716 - val_loss: 1.4533 - val_acc: 0.3444\n",
      "Epoch 373/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4621 - acc: 0.3568 - val_loss: 1.4529 - val_acc: 0.3444\n",
      "Epoch 374/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4565 - acc: 0.3621 - val_loss: 1.4525 - val_acc: 0.3444\n",
      "Epoch 375/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4678 - acc: 0.3519 - val_loss: 1.4522 - val_acc: 0.3444\n",
      "Epoch 376/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4733 - acc: 0.3381 - val_loss: 1.4518 - val_acc: 0.3444\n",
      "Epoch 377/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4667 - acc: 0.3593 - val_loss: 1.4514 - val_acc: 0.3444\n",
      "Epoch 378/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4709 - acc: 0.3508 - val_loss: 1.4510 - val_acc: 0.3444\n",
      "Epoch 379/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4591 - acc: 0.3529 - val_loss: 1.4506 - val_acc: 0.3444\n",
      "Epoch 380/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4683 - acc: 0.3554 - val_loss: 1.4503 - val_acc: 0.3444\n",
      "Epoch 381/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4627 - acc: 0.3713 - val_loss: 1.4499 - val_acc: 0.3444\n",
      "Epoch 382/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4626 - acc: 0.3579 - val_loss: 1.4495 - val_acc: 0.3444\n",
      "Epoch 383/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4662 - acc: 0.3558 - val_loss: 1.4491 - val_acc: 0.3444\n",
      "Epoch 384/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4635 - acc: 0.3603 - val_loss: 1.4488 - val_acc: 0.3444\n",
      "Epoch 385/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4494 - acc: 0.3617 - val_loss: 1.4483 - val_acc: 0.3444\n",
      "Epoch 386/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4693 - acc: 0.3544 - val_loss: 1.4479 - val_acc: 0.3444\n",
      "Epoch 387/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4672 - acc: 0.3603 - val_loss: 1.4476 - val_acc: 0.3444\n",
      "Epoch 388/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4574 - acc: 0.3667 - val_loss: 1.4471 - val_acc: 0.3444\n",
      "Epoch 389/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4731 - acc: 0.3614 - val_loss: 1.4468 - val_acc: 0.3444\n",
      "Epoch 390/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4602 - acc: 0.3639 - val_loss: 1.4464 - val_acc: 0.3444\n",
      "Epoch 391/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4789 - acc: 0.3420 - val_loss: 1.4460 - val_acc: 0.3444\n",
      "Epoch 392/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4581 - acc: 0.3681 - val_loss: 1.4457 - val_acc: 0.3444\n",
      "Epoch 393/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4535 - acc: 0.3593 - val_loss: 1.4453 - val_acc: 0.3444\n",
      "Epoch 394/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4614 - acc: 0.3698 - val_loss: 1.4449 - val_acc: 0.3414\n",
      "Epoch 395/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4539 - acc: 0.3603 - val_loss: 1.4445 - val_acc: 0.3414\n",
      "Epoch 396/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4576 - acc: 0.3632 - val_loss: 1.4441 - val_acc: 0.3414\n",
      "Epoch 397/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4542 - acc: 0.3561 - val_loss: 1.4437 - val_acc: 0.3414\n",
      "Epoch 398/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4623 - acc: 0.3529 - val_loss: 1.4434 - val_acc: 0.3414\n",
      "Epoch 399/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4557 - acc: 0.3575 - val_loss: 1.4430 - val_acc: 0.3414\n",
      "Epoch 400/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.4466 - acc: 0.3688 - val_loss: 1.4426 - val_acc: 0.3414\n",
      "Epoch 401/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4572 - acc: 0.3709 - val_loss: 1.4422 - val_acc: 0.3414\n",
      "Epoch 402/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4494 - acc: 0.3572 - val_loss: 1.4419 - val_acc: 0.3414\n",
      "Epoch 403/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4584 - acc: 0.3667 - val_loss: 1.4415 - val_acc: 0.3414\n",
      "Epoch 404/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4604 - acc: 0.3730 - val_loss: 1.4412 - val_acc: 0.3414\n",
      "Epoch 405/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4673 - acc: 0.3501 - val_loss: 1.4409 - val_acc: 0.3414\n",
      "Epoch 406/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4591 - acc: 0.3582 - val_loss: 1.4405 - val_acc: 0.3414\n",
      "Epoch 407/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4565 - acc: 0.3734 - val_loss: 1.4402 - val_acc: 0.3414\n",
      "Epoch 408/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4608 - acc: 0.3582 - val_loss: 1.4398 - val_acc: 0.3414\n",
      "Epoch 409/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4522 - acc: 0.3684 - val_loss: 1.4395 - val_acc: 0.3414\n",
      "Epoch 410/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4604 - acc: 0.3702 - val_loss: 1.4391 - val_acc: 0.3414\n",
      "Epoch 411/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4550 - acc: 0.3681 - val_loss: 1.4388 - val_acc: 0.3414\n",
      "Epoch 412/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4566 - acc: 0.3565 - val_loss: 1.4384 - val_acc: 0.3414\n",
      "Epoch 413/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4594 - acc: 0.3625 - val_loss: 1.4381 - val_acc: 0.3414\n",
      "Epoch 414/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4686 - acc: 0.3533 - val_loss: 1.4378 - val_acc: 0.3414\n",
      "Epoch 415/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4675 - acc: 0.3582 - val_loss: 1.4374 - val_acc: 0.3414\n",
      "Epoch 416/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.4661 - acc: 0.3505 - val_loss: 1.4371 - val_acc: 0.3414\n",
      "Epoch 417/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.4644 - acc: 0.3625 - val_loss: 1.4368 - val_acc: 0.3414\n",
      "Epoch 418/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.4576 - acc: 0.3639 - val_loss: 1.4364 - val_acc: 0.3414\n",
      "Epoch 419/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.4503 - acc: 0.3730 - val_loss: 1.4360 - val_acc: 0.3414\n",
      "Epoch 420/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4544 - acc: 0.3607 - val_loss: 1.4356 - val_acc: 0.3414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 421/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4590 - acc: 0.3519 - val_loss: 1.4352 - val_acc: 0.3414\n",
      "Epoch 422/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4610 - acc: 0.3695 - val_loss: 1.4348 - val_acc: 0.3444\n",
      "Epoch 423/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4471 - acc: 0.3751 - val_loss: 1.4345 - val_acc: 0.3444\n",
      "Epoch 424/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4553 - acc: 0.3737 - val_loss: 1.4342 - val_acc: 0.3444\n",
      "Epoch 425/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4447 - acc: 0.3625 - val_loss: 1.4339 - val_acc: 0.3444\n",
      "Epoch 426/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4451 - acc: 0.3734 - val_loss: 1.4335 - val_acc: 0.3444\n",
      "Epoch 427/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4416 - acc: 0.3681 - val_loss: 1.4331 - val_acc: 0.3444\n",
      "Epoch 428/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4545 - acc: 0.3709 - val_loss: 1.4327 - val_acc: 0.3444\n",
      "Epoch 429/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4456 - acc: 0.3674 - val_loss: 1.4324 - val_acc: 0.3444\n",
      "Epoch 430/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4466 - acc: 0.3734 - val_loss: 1.4320 - val_acc: 0.3444\n",
      "Epoch 431/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4561 - acc: 0.3593 - val_loss: 1.4317 - val_acc: 0.3444\n",
      "Epoch 432/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4455 - acc: 0.3765 - val_loss: 1.4314 - val_acc: 0.3444\n",
      "Epoch 433/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4578 - acc: 0.3603 - val_loss: 1.4310 - val_acc: 0.3444\n",
      "Epoch 434/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4506 - acc: 0.3635 - val_loss: 1.4307 - val_acc: 0.3444\n",
      "Epoch 435/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4534 - acc: 0.3544 - val_loss: 1.4304 - val_acc: 0.3444\n",
      "Epoch 436/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4482 - acc: 0.3737 - val_loss: 1.4300 - val_acc: 0.3444\n",
      "Epoch 437/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4526 - acc: 0.3674 - val_loss: 1.4298 - val_acc: 0.3444\n",
      "Epoch 438/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4577 - acc: 0.3551 - val_loss: 1.4295 - val_acc: 0.3444\n",
      "Epoch 439/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4380 - acc: 0.3688 - val_loss: 1.4291 - val_acc: 0.3444\n",
      "Epoch 440/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.4491 - acc: 0.3702 - val_loss: 1.4288 - val_acc: 0.3414\n",
      "Epoch 441/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4602 - acc: 0.3603 - val_loss: 1.4284 - val_acc: 0.3414\n",
      "Epoch 442/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4407 - acc: 0.3727 - val_loss: 1.4281 - val_acc: 0.3414\n",
      "Epoch 443/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4484 - acc: 0.3706 - val_loss: 1.4278 - val_acc: 0.3414\n",
      "Epoch 444/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4497 - acc: 0.3691 - val_loss: 1.4274 - val_acc: 0.3414\n",
      "Epoch 445/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4413 - acc: 0.3765 - val_loss: 1.4271 - val_acc: 0.3414\n",
      "Epoch 446/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4491 - acc: 0.3660 - val_loss: 1.4269 - val_acc: 0.3414\n",
      "Epoch 447/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.4593 - acc: 0.3660 - val_loss: 1.4265 - val_acc: 0.3414\n",
      "Epoch 448/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4344 - acc: 0.3797 - val_loss: 1.4261 - val_acc: 0.3414\n",
      "Epoch 449/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4397 - acc: 0.3765 - val_loss: 1.4257 - val_acc: 0.3414\n",
      "Epoch 450/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4335 - acc: 0.3864 - val_loss: 1.4253 - val_acc: 0.3414\n",
      "Epoch 451/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4622 - acc: 0.3642 - val_loss: 1.4251 - val_acc: 0.3414\n",
      "Epoch 452/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4495 - acc: 0.3677 - val_loss: 1.4248 - val_acc: 0.3414\n",
      "Epoch 453/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4506 - acc: 0.3730 - val_loss: 1.4245 - val_acc: 0.3414\n",
      "Epoch 454/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4517 - acc: 0.3649 - val_loss: 1.4242 - val_acc: 0.3414\n",
      "Epoch 455/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4380 - acc: 0.3748 - val_loss: 1.4239 - val_acc: 0.3414\n",
      "Epoch 456/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4407 - acc: 0.3674 - val_loss: 1.4236 - val_acc: 0.3414\n",
      "Epoch 457/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4454 - acc: 0.3758 - val_loss: 1.4233 - val_acc: 0.3414\n",
      "Epoch 458/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4464 - acc: 0.3755 - val_loss: 1.4230 - val_acc: 0.3384\n",
      "Epoch 459/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4451 - acc: 0.3723 - val_loss: 1.4227 - val_acc: 0.3384\n",
      "Epoch 460/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4334 - acc: 0.3787 - val_loss: 1.4223 - val_acc: 0.3384\n",
      "Epoch 461/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4433 - acc: 0.3797 - val_loss: 1.4221 - val_acc: 0.3384\n",
      "Epoch 462/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4376 - acc: 0.3691 - val_loss: 1.4218 - val_acc: 0.3414\n",
      "Epoch 463/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4351 - acc: 0.3674 - val_loss: 1.4214 - val_acc: 0.3414\n",
      "Epoch 464/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.4480 - acc: 0.3702 - val_loss: 1.4211 - val_acc: 0.3414\n",
      "Epoch 465/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4370 - acc: 0.3765 - val_loss: 1.4208 - val_acc: 0.3414\n",
      "Epoch 466/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4420 - acc: 0.3762 - val_loss: 1.4205 - val_acc: 0.3414\n",
      "Epoch 467/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4303 - acc: 0.3744 - val_loss: 1.4201 - val_acc: 0.3414\n",
      "Epoch 468/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.4596 - acc: 0.3586 - val_loss: 1.4198 - val_acc: 0.3384\n",
      "Epoch 469/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4526 - acc: 0.3610 - val_loss: 1.4195 - val_acc: 0.3444\n",
      "Epoch 470/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4414 - acc: 0.3776 - val_loss: 1.4193 - val_acc: 0.3444\n",
      "Epoch 471/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4455 - acc: 0.3670 - val_loss: 1.4190 - val_acc: 0.3414\n",
      "Epoch 472/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4453 - acc: 0.3533 - val_loss: 1.4188 - val_acc: 0.3414\n",
      "Epoch 473/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4300 - acc: 0.3850 - val_loss: 1.4185 - val_acc: 0.3414\n",
      "Epoch 474/500\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.4391 - acc: 0.3723 - val_loss: 1.4183 - val_acc: 0.3414\n",
      "Epoch 475/500\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4441 - acc: 0.3706 - val_loss: 1.4180 - val_acc: 0.3414\n",
      "Epoch 476/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4345 - acc: 0.3818 - val_loss: 1.4177 - val_acc: 0.3444\n",
      "Epoch 477/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4427 - acc: 0.3674 - val_loss: 1.4173 - val_acc: 0.3444\n",
      "Epoch 478/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4485 - acc: 0.3607 - val_loss: 1.4171 - val_acc: 0.3444\n",
      "Epoch 479/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4418 - acc: 0.3695 - val_loss: 1.4169 - val_acc: 0.3444\n",
      "Epoch 480/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4429 - acc: 0.3610 - val_loss: 1.4166 - val_acc: 0.3444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4309 - acc: 0.3765 - val_loss: 1.4163 - val_acc: 0.3444\n",
      "Epoch 482/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4394 - acc: 0.3607 - val_loss: 1.4160 - val_acc: 0.3444\n",
      "Epoch 483/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4350 - acc: 0.3737 - val_loss: 1.4156 - val_acc: 0.3444\n",
      "Epoch 484/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4573 - acc: 0.3610 - val_loss: 1.4154 - val_acc: 0.3444\n",
      "Epoch 485/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4372 - acc: 0.3656 - val_loss: 1.4150 - val_acc: 0.3444\n",
      "Epoch 486/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4246 - acc: 0.3861 - val_loss: 1.4147 - val_acc: 0.3444\n",
      "Epoch 487/500\n",
      "2839/2839 [==============================] - 0s 28us/step - loss: 1.4316 - acc: 0.3769 - val_loss: 1.4144 - val_acc: 0.3444\n",
      "Epoch 488/500\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 1.4364 - acc: 0.3741 - val_loss: 1.4142 - val_acc: 0.3444\n",
      "Epoch 489/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.4408 - acc: 0.3762 - val_loss: 1.4140 - val_acc: 0.3444\n",
      "Epoch 490/500\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.4408 - acc: 0.3628 - val_loss: 1.4136 - val_acc: 0.3444\n",
      "Epoch 491/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4380 - acc: 0.3769 - val_loss: 1.4133 - val_acc: 0.3444\n",
      "Epoch 492/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4360 - acc: 0.3677 - val_loss: 1.4130 - val_acc: 0.3444\n",
      "Epoch 493/500\n",
      "2839/2839 [==============================] - 0s 26us/step - loss: 1.4281 - acc: 0.3769 - val_loss: 1.4128 - val_acc: 0.3444\n",
      "Epoch 494/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4358 - acc: 0.3769 - val_loss: 1.4126 - val_acc: 0.3444\n",
      "Epoch 495/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4256 - acc: 0.3868 - val_loss: 1.4123 - val_acc: 0.3444\n",
      "Epoch 496/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4251 - acc: 0.3815 - val_loss: 1.4120 - val_acc: 0.3444\n",
      "Epoch 497/500\n",
      "2839/2839 [==============================] - 0s 25us/step - loss: 1.4374 - acc: 0.3801 - val_loss: 1.4118 - val_acc: 0.3384\n",
      "Epoch 498/500\n",
      "2839/2839 [==============================] - 0s 24us/step - loss: 1.4405 - acc: 0.3635 - val_loss: 1.4116 - val_acc: 0.3384\n",
      "Epoch 499/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4391 - acc: 0.3639 - val_loss: 1.4114 - val_acc: 0.3414\n",
      "Epoch 500/500\n",
      "2839/2839 [==============================] - 0s 23us/step - loss: 1.4330 - acc: 0.3628 - val_loss: 1.4111 - val_acc: 0.3414\n"
     ]
    }
   ],
   "source": [
    "zillow_model = model.fit(x=X_train, y=y_cat_train, \n",
    "          batch_size=20000, \n",
    "          epochs=500, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_cat_test),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-782-5bac093a14be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-730-9d1a245b8a9e>\u001b[0m in \u001b[0;36mmodel_metrics\u001b[0;34m(predictions, y_test)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mgain_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain_true_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss_stay_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_true_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_stay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mloss_stay_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_true_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mgain_stay_large_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain_large_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain_large_stay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain_large'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "model_metrics(predictions, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0],\n",
       "       [  1,   8,  16,  13,   0],\n",
       "       [  4,  44, 128,  97,  20],\n",
       "       [  0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEkHGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1838b67f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(zillow_model.history['acc'])\n",
    "plt.plot(zillow_model.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(130,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/500\n",
      "2839/2839 [==============================] - 4s 2ms/step - loss: 1.5033 - acc: 0.3716 - val_loss: 1.3798 - val_acc: 0.3323\n",
      "Epoch 2/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3634 - acc: 0.3853 - val_loss: 1.3100 - val_acc: 0.3444\n",
      "Epoch 3/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3411 - acc: 0.4181 - val_loss: 1.3122 - val_acc: 0.3293\n",
      "Epoch 4/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3114 - acc: 0.4068 - val_loss: 1.3019 - val_acc: 0.4079\n",
      "Epoch 5/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.3128 - acc: 0.4086 - val_loss: 1.2902 - val_acc: 0.3414\n",
      "Epoch 6/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3031 - acc: 0.4001 - val_loss: 1.2780 - val_acc: 0.3565\n",
      "Epoch 7/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2880 - acc: 0.4178 - val_loss: 1.2635 - val_acc: 0.4773\n",
      "Epoch 8/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2884 - acc: 0.4054 - val_loss: 1.2504 - val_acc: 0.5045\n",
      "Epoch 9/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2674 - acc: 0.4040 - val_loss: 1.2408 - val_acc: 0.4713\n",
      "Epoch 10/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.2608 - acc: 0.4181 - val_loss: 1.2386 - val_acc: 0.4773\n",
      "Epoch 11/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2499 - acc: 0.4227 - val_loss: 1.2269 - val_acc: 0.4743\n",
      "Epoch 12/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2456 - acc: 0.4156 - val_loss: 1.2084 - val_acc: 0.4773\n",
      "Epoch 13/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2438 - acc: 0.4146 - val_loss: 1.2156 - val_acc: 0.5015\n",
      "Epoch 14/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2483 - acc: 0.4174 - val_loss: 1.2171 - val_acc: 0.5166\n",
      "Epoch 15/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2317 - acc: 0.4428 - val_loss: 1.2243 - val_acc: 0.4260\n",
      "Epoch 16/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2127 - acc: 0.4526 - val_loss: 1.1775 - val_acc: 0.5227\n",
      "Epoch 17/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2310 - acc: 0.4371 - val_loss: 1.1959 - val_acc: 0.5166\n",
      "Epoch 18/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2056 - acc: 0.4579 - val_loss: 1.1586 - val_acc: 0.4985\n",
      "Epoch 19/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2030 - acc: 0.4544 - val_loss: 1.1544 - val_acc: 0.5227\n",
      "Epoch 20/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1970 - acc: 0.4847 - val_loss: 1.1838 - val_acc: 0.4260\n",
      "Epoch 21/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1957 - acc: 0.4618 - val_loss: 1.1452 - val_acc: 0.5196\n",
      "Epoch 22/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1906 - acc: 0.4505 - val_loss: 1.1328 - val_acc: 0.5378\n",
      "Epoch 23/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1768 - acc: 0.4618 - val_loss: 1.1371 - val_acc: 0.5378\n",
      "Epoch 24/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1844 - acc: 0.4572 - val_loss: 1.1588 - val_acc: 0.4894\n",
      "Epoch 25/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1538 - acc: 0.4815 - val_loss: 1.1176 - val_acc: 0.5257\n",
      "Epoch 26/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1683 - acc: 0.4607 - val_loss: 1.1305 - val_acc: 0.5347\n",
      "Epoch 27/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1521 - acc: 0.4840 - val_loss: 1.1088 - val_acc: 0.5106\n",
      "Epoch 28/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1393 - acc: 0.4829 - val_loss: 1.1100 - val_acc: 0.5529\n",
      "Epoch 29/500\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.1439 - acc: 0.4910 - val_loss: 1.1427 - val_acc: 0.5076\n",
      "Epoch 30/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1379 - acc: 0.4914 - val_loss: 1.0935 - val_acc: 0.5378\n",
      "Epoch 31/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1311 - acc: 0.4959 - val_loss: 1.1047 - val_acc: 0.5468\n",
      "Epoch 32/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1255 - acc: 0.4942 - val_loss: 1.1158 - val_acc: 0.5257\n",
      "Epoch 33/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1327 - acc: 0.5023 - val_loss: 1.0896 - val_acc: 0.5408\n",
      "Epoch 34/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1166 - acc: 0.5037 - val_loss: 1.0883 - val_acc: 0.5438\n",
      "Epoch 35/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1191 - acc: 0.5012 - val_loss: 1.1111 - val_acc: 0.5378\n",
      "Epoch 36/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1040 - acc: 0.5107 - val_loss: 1.1071 - val_acc: 0.5589\n",
      "Epoch 37/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0929 - acc: 0.5164 - val_loss: 1.0892 - val_acc: 0.5498\n",
      "Epoch 38/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1105 - acc: 0.5005 - val_loss: 1.0821 - val_acc: 0.5529\n",
      "Epoch 39/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0786 - acc: 0.5100 - val_loss: 1.0510 - val_acc: 0.5619\n",
      "Epoch 40/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0989 - acc: 0.5262 - val_loss: 1.0951 - val_acc: 0.5650\n",
      "Epoch 41/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0782 - acc: 0.5291 - val_loss: 1.0620 - val_acc: 0.5619\n",
      "Epoch 42/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0826 - acc: 0.5259 - val_loss: 1.0555 - val_acc: 0.5468\n",
      "Epoch 43/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0728 - acc: 0.5213 - val_loss: 1.0327 - val_acc: 0.5650\n",
      "Epoch 44/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0617 - acc: 0.5269 - val_loss: 1.0459 - val_acc: 0.5650\n",
      "Epoch 45/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0657 - acc: 0.5298 - val_loss: 1.0522 - val_acc: 0.5257\n",
      "Epoch 46/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0650 - acc: 0.5255 - val_loss: 1.0419 - val_acc: 0.5529\n",
      "Epoch 47/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0588 - acc: 0.5358 - val_loss: 1.0402 - val_acc: 0.5589\n",
      "Epoch 48/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0489 - acc: 0.5305 - val_loss: 1.0579 - val_acc: 0.5589\n",
      "Epoch 49/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0402 - acc: 0.5326 - val_loss: 1.0381 - val_acc: 0.5619\n",
      "Epoch 50/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0490 - acc: 0.5308 - val_loss: 1.0335 - val_acc: 0.5559\n",
      "Epoch 51/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0306 - acc: 0.5460 - val_loss: 1.0410 - val_acc: 0.5650\n",
      "Epoch 52/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0484 - acc: 0.5308 - val_loss: 1.0489 - val_acc: 0.5770\n",
      "Epoch 53/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0230 - acc: 0.5586 - val_loss: 1.0654 - val_acc: 0.5861\n",
      "Epoch 54/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0214 - acc: 0.5481 - val_loss: 1.0483 - val_acc: 0.5166\n",
      "Epoch 55/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0222 - acc: 0.5407 - val_loss: 1.0231 - val_acc: 0.5529\n",
      "Epoch 56/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0073 - acc: 0.5491 - val_loss: 1.0435 - val_acc: 0.5468\n",
      "Epoch 57/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0253 - acc: 0.5336 - val_loss: 1.0383 - val_acc: 0.5559\n",
      "Epoch 58/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0076 - acc: 0.5446 - val_loss: 1.0171 - val_acc: 0.5740\n",
      "Epoch 59/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0026 - acc: 0.5643 - val_loss: 1.0520 - val_acc: 0.5770\n",
      "Epoch 60/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9952 - acc: 0.5551 - val_loss: 1.0451 - val_acc: 0.5468\n",
      "Epoch 61/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9801 - acc: 0.5576 - val_loss: 1.0226 - val_acc: 0.5801\n",
      "Epoch 62/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9923 - acc: 0.5682 - val_loss: 1.0331 - val_acc: 0.5891\n",
      "Epoch 63/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9769 - acc: 0.5720 - val_loss: 1.0904 - val_acc: 0.5770\n",
      "Epoch 64/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9866 - acc: 0.5675 - val_loss: 1.0621 - val_acc: 0.5921\n",
      "Epoch 65/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9894 - acc: 0.5534 - val_loss: 1.0365 - val_acc: 0.5589\n",
      "Epoch 66/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9700 - acc: 0.5731 - val_loss: 1.0412 - val_acc: 0.5831\n",
      "Epoch 67/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9814 - acc: 0.5502 - val_loss: 1.0467 - val_acc: 0.5559\n",
      "Epoch 68/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9870 - acc: 0.5576 - val_loss: 1.0675 - val_acc: 0.5619\n",
      "Epoch 69/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9606 - acc: 0.5756 - val_loss: 1.0624 - val_acc: 0.5559\n",
      "Epoch 70/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9606 - acc: 0.5657 - val_loss: 1.0912 - val_acc: 0.5408\n",
      "Epoch 71/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9690 - acc: 0.5738 - val_loss: 1.0912 - val_acc: 0.5287\n",
      "Epoch 72/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9419 - acc: 0.5808 - val_loss: 1.0916 - val_acc: 0.5408\n",
      "Epoch 73/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9678 - acc: 0.5611 - val_loss: 1.0387 - val_acc: 0.5650\n",
      "Epoch 74/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9498 - acc: 0.5770 - val_loss: 1.0654 - val_acc: 0.5921\n",
      "Epoch 75/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9411 - acc: 0.5808 - val_loss: 1.0453 - val_acc: 0.5831\n",
      "Epoch 76/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9353 - acc: 0.5889 - val_loss: 1.0490 - val_acc: 0.5891\n",
      "Epoch 77/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9484 - acc: 0.5763 - val_loss: 1.0919 - val_acc: 0.5378\n",
      "Epoch 78/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9308 - acc: 0.5879 - val_loss: 1.0879 - val_acc: 0.5408\n",
      "Epoch 79/500\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.9040 - acc: 0.6062 - val_loss: 1.0933 - val_acc: 0.5740\n",
      "Epoch 80/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.8976 - acc: 0.603 - 0s 51us/step - loss: 0.9019 - acc: 0.6034 - val_loss: 1.0822 - val_acc: 0.5680\n",
      "Epoch 81/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9274 - acc: 0.5777 - val_loss: 1.1012 - val_acc: 0.5498\n",
      "Epoch 82/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9152 - acc: 0.6044 - val_loss: 1.0644 - val_acc: 0.6012\n",
      "Epoch 83/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9166 - acc: 0.5918 - val_loss: 1.0600 - val_acc: 0.5861\n",
      "Epoch 84/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9111 - acc: 0.5988 - val_loss: 1.1024 - val_acc: 0.5498\n",
      "Epoch 85/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8939 - acc: 0.6122 - val_loss: 1.1224 - val_acc: 0.5438\n",
      "Epoch 86/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9128 - acc: 0.5854 - val_loss: 1.1479 - val_acc: 0.5196\n",
      "Epoch 87/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8957 - acc: 0.6076 - val_loss: 1.1416 - val_acc: 0.5498\n",
      "Epoch 88/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9146 - acc: 0.5992 - val_loss: 1.0692 - val_acc: 0.5952\n",
      "Epoch 89/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8771 - acc: 0.5981 - val_loss: 1.0892 - val_acc: 0.5952\n",
      "Epoch 90/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9176 - acc: 0.5903 - val_loss: 1.0846 - val_acc: 0.5831\n",
      "Epoch 91/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8673 - acc: 0.6136 - val_loss: 1.1470 - val_acc: 0.5559\n",
      "Epoch 92/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8964 - acc: 0.6002 - val_loss: 1.1066 - val_acc: 0.5498\n",
      "Epoch 93/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8743 - acc: 0.6150 - val_loss: 1.0950 - val_acc: 0.6103\n",
      "Epoch 94/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8836 - acc: 0.6132 - val_loss: 1.0989 - val_acc: 0.5982\n",
      "Epoch 95/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8612 - acc: 0.6192 - val_loss: 1.2392 - val_acc: 0.5227\n",
      "Epoch 96/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8790 - acc: 0.6122 - val_loss: 1.1933 - val_acc: 0.5347\n",
      "Epoch 97/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8698 - acc: 0.6115 - val_loss: 1.1771 - val_acc: 0.5468\n",
      "Epoch 98/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8405 - acc: 0.6287 - val_loss: 1.1909 - val_acc: 0.5438\n",
      "Epoch 99/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8950 - acc: 0.6051 - val_loss: 1.2008 - val_acc: 0.5317\n",
      "Epoch 100/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8600 - acc: 0.6263 - val_loss: 1.1590 - val_acc: 0.5408\n",
      "Epoch 101/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8529 - acc: 0.6326 - val_loss: 1.1268 - val_acc: 0.5619\n",
      "Epoch 102/500\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.8819 - acc: 0.6150 - val_loss: 1.1625 - val_acc: 0.5378\n",
      "Epoch 103/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8554 - acc: 0.6273 - val_loss: 1.1487 - val_acc: 0.5529\n",
      "Epoch 104/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8383 - acc: 0.6270 - val_loss: 1.2056 - val_acc: 0.5227\n",
      "Epoch 105/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8170 - acc: 0.6390 - val_loss: 1.1277 - val_acc: 0.5831\n",
      "Epoch 106/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8605 - acc: 0.6221 - val_loss: 1.1711 - val_acc: 0.5559\n",
      "Epoch 107/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8423 - acc: 0.6358 - val_loss: 1.1557 - val_acc: 0.5589\n",
      "Epoch 108/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8138 - acc: 0.6516 - val_loss: 1.1453 - val_acc: 0.5589\n",
      "Epoch 109/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8627 - acc: 0.6125 - val_loss: 1.1895 - val_acc: 0.5468\n",
      "Epoch 110/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8058 - acc: 0.6516 - val_loss: 1.2550 - val_acc: 0.5166\n",
      "Epoch 111/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8295 - acc: 0.6249 - val_loss: 1.1640 - val_acc: 0.5438\n",
      "Epoch 112/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8035 - acc: 0.6340 - val_loss: 1.2644 - val_acc: 0.5076\n",
      "Epoch 113/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8342 - acc: 0.6411 - val_loss: 1.2321 - val_acc: 0.5257\n",
      "Epoch 114/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8419 - acc: 0.6316 - val_loss: 1.2296 - val_acc: 0.5196\n",
      "Epoch 115/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8202 - acc: 0.6361 - val_loss: 1.1847 - val_acc: 0.5650\n",
      "Epoch 116/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8226 - acc: 0.6485 - val_loss: 1.2126 - val_acc: 0.5347\n",
      "Epoch 117/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8011 - acc: 0.6460 - val_loss: 1.3419 - val_acc: 0.4955\n",
      "Epoch 118/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8402 - acc: 0.6397 - val_loss: 1.2601 - val_acc: 0.5257\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.7938 - acc: 0.6375 - val_loss: 1.3074 - val_acc: 0.5257\n",
      "Epoch 120/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8180 - acc: 0.6323 - val_loss: 1.2888 - val_acc: 0.4955\n",
      "Epoch 121/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7835 - acc: 0.6552 - val_loss: 1.2142 - val_acc: 0.5468\n",
      "Epoch 122/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7940 - acc: 0.6562 - val_loss: 1.2368 - val_acc: 0.5317\n",
      "Epoch 123/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8201 - acc: 0.6323 - val_loss: 1.2557 - val_acc: 0.5257\n",
      "Epoch 124/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7745 - acc: 0.6622 - val_loss: 1.2163 - val_acc: 0.5589\n",
      "Epoch 125/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7715 - acc: 0.6640 - val_loss: 1.2688 - val_acc: 0.5408\n",
      "Epoch 126/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8057 - acc: 0.6432 - val_loss: 1.2521 - val_acc: 0.5257\n",
      "Epoch 127/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7681 - acc: 0.6566 - val_loss: 1.3050 - val_acc: 0.5317\n",
      "Epoch 128/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.7705 - acc: 0.6527 - val_loss: 1.3105 - val_acc: 0.5257\n",
      "Epoch 129/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.7875 - acc: 0.6654 - val_loss: 1.2323 - val_acc: 0.5801\n",
      "Epoch 130/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.7661 - acc: 0.6689 - val_loss: 1.2402 - val_acc: 0.5770\n",
      "Epoch 131/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7760 - acc: 0.6548 - val_loss: 1.2365 - val_acc: 0.5650\n",
      "Epoch 132/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.7459 - acc: 0.6798 - val_loss: 1.3073 - val_acc: 0.5287\n",
      "Epoch 133/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7819 - acc: 0.6545 - val_loss: 1.3036 - val_acc: 0.5257\n",
      "Epoch 134/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7497 - acc: 0.6830 - val_loss: 1.3908 - val_acc: 0.5136\n",
      "Epoch 135/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7917 - acc: 0.6509 - val_loss: 1.3013 - val_acc: 0.5166\n",
      "Epoch 136/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7438 - acc: 0.6692 - val_loss: 1.2418 - val_acc: 0.5589\n",
      "Epoch 137/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7642 - acc: 0.6654 - val_loss: 1.2460 - val_acc: 0.5438\n",
      "Epoch 138/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.7457 - acc: 0.6731 - val_loss: 1.2405 - val_acc: 0.5650\n",
      "Epoch 139/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7516 - acc: 0.6619 - val_loss: 1.2358 - val_acc: 0.5589\n",
      "Epoch 140/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7433 - acc: 0.6692 - val_loss: 1.2915 - val_acc: 0.5438\n",
      "Epoch 141/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7736 - acc: 0.6633 - val_loss: 1.3656 - val_acc: 0.5136\n",
      "Epoch 142/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.7417 - acc: 0.6840 - val_loss: 1.4215 - val_acc: 0.5015\n",
      "Epoch 143/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7209 - acc: 0.6876 - val_loss: 1.3119 - val_acc: 0.5106\n",
      "Epoch 144/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7967 - acc: 0.6509 - val_loss: 1.2550 - val_acc: 0.5650\n",
      "Epoch 145/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.7147 - acc: 0.6950 - val_loss: 1.3982 - val_acc: 0.5378\n",
      "Epoch 146/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7600 - acc: 0.6689 - val_loss: 1.3979 - val_acc: 0.4985\n",
      "Epoch 147/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7218 - acc: 0.6911 - val_loss: 1.3278 - val_acc: 0.5317\n",
      "Epoch 148/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7175 - acc: 0.6953 - val_loss: 1.3186 - val_acc: 0.5408\n",
      "Epoch 149/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7336 - acc: 0.6826 - val_loss: 1.2948 - val_acc: 0.5468\n",
      "Epoch 150/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7390 - acc: 0.6781 - val_loss: 1.3530 - val_acc: 0.5076\n",
      "Epoch 151/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7237 - acc: 0.6858 - val_loss: 1.3367 - val_acc: 0.5529\n",
      "Epoch 152/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7289 - acc: 0.6791 - val_loss: 1.3060 - val_acc: 0.5468\n",
      "Epoch 153/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7311 - acc: 0.6766 - val_loss: 1.2617 - val_acc: 0.5740\n",
      "Epoch 154/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7373 - acc: 0.6837 - val_loss: 1.4141 - val_acc: 0.5076\n",
      "Epoch 155/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6890 - acc: 0.6918 - val_loss: 1.3299 - val_acc: 0.5529\n",
      "Epoch 156/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.7591 - acc: 0.6738 - val_loss: 1.3650 - val_acc: 0.4924\n",
      "Epoch 157/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.6990 - acc: 0.7052 - val_loss: 1.4927 - val_acc: 0.4773\n",
      "Epoch 158/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7024 - acc: 0.6988 - val_loss: 1.5240 - val_acc: 0.4713\n",
      "Epoch 159/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6912 - acc: 0.7062 - val_loss: 1.4885 - val_acc: 0.4804\n",
      "Epoch 160/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.7101 - acc: 0.6921 - val_loss: 1.2911 - val_acc: 0.5529\n",
      "Epoch 161/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6877 - acc: 0.7059 - val_loss: 1.4083 - val_acc: 0.5347\n",
      "Epoch 162/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7077 - acc: 0.6823 - val_loss: 1.3494 - val_acc: 0.5468\n",
      "Epoch 163/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6741 - acc: 0.7017 - val_loss: 1.3609 - val_acc: 0.5166\n",
      "Epoch 164/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6991 - acc: 0.6936 - val_loss: 1.3665 - val_acc: 0.5347\n",
      "Epoch 165/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6801 - acc: 0.7010 - val_loss: 1.4190 - val_acc: 0.5257\n",
      "Epoch 166/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.6802 - acc: 0.7098 - val_loss: 1.4343 - val_acc: 0.4985\n",
      "Epoch 167/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.7047 - acc: 0.6869 - val_loss: 1.3145 - val_acc: 0.5650\n",
      "Epoch 168/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.6814 - acc: 0.7143 - val_loss: 1.4693 - val_acc: 0.4864\n",
      "Epoch 169/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.6810 - acc: 0.7055 - val_loss: 1.3838 - val_acc: 0.5347\n",
      "Epoch 170/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6946 - acc: 0.6971 - val_loss: 1.4374 - val_acc: 0.5166\n",
      "Epoch 171/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6600 - acc: 0.7122 - val_loss: 1.5357 - val_acc: 0.4955\n",
      "Epoch 172/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6737 - acc: 0.7157 - val_loss: 1.5650 - val_acc: 0.5106\n",
      "Epoch 173/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6664 - acc: 0.7126 - val_loss: 1.5491 - val_acc: 0.5076\n",
      "Epoch 174/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6711 - acc: 0.7143 - val_loss: 1.4359 - val_acc: 0.5438\n",
      "Epoch 175/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6657 - acc: 0.7136 - val_loss: 1.4036 - val_acc: 0.5257\n",
      "Epoch 176/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6420 - acc: 0.7288 - val_loss: 1.4724 - val_acc: 0.5347\n",
      "Epoch 177/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6770 - acc: 0.7140 - val_loss: 1.5045 - val_acc: 0.4804\n",
      "Epoch 178/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6465 - acc: 0.7274 - val_loss: 1.5838 - val_acc: 0.5076\n",
      "Epoch 179/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6652 - acc: 0.7098 - val_loss: 1.5395 - val_acc: 0.5166\n",
      "Epoch 180/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6684 - acc: 0.7133 - val_loss: 1.6847 - val_acc: 0.4804\n",
      "Epoch 181/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6396 - acc: 0.7175 - val_loss: 1.6796 - val_acc: 0.4773\n",
      "Epoch 182/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6587 - acc: 0.7256 - val_loss: 1.5097 - val_acc: 0.5106\n",
      "Epoch 183/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6653 - acc: 0.7260 - val_loss: 1.5263 - val_acc: 0.5166\n",
      "Epoch 184/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.6496 - acc: 0.7207 - val_loss: 1.6920 - val_acc: 0.4562\n",
      "Epoch 185/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6235 - acc: 0.7281 - val_loss: 1.6080 - val_acc: 0.5166\n",
      "Epoch 186/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6344 - acc: 0.7305 - val_loss: 1.6509 - val_acc: 0.4834\n",
      "Epoch 187/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6607 - acc: 0.7172 - val_loss: 1.5724 - val_acc: 0.4955\n",
      "Epoch 188/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6237 - acc: 0.7418 - val_loss: 1.5272 - val_acc: 0.5287\n",
      "Epoch 189/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6405 - acc: 0.7267 - val_loss: 1.5565 - val_acc: 0.5287\n",
      "Epoch 190/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6741 - acc: 0.7002 - val_loss: 1.5302 - val_acc: 0.5166\n",
      "Epoch 191/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6080 - acc: 0.7334 - val_loss: 1.4570 - val_acc: 0.5378\n",
      "Epoch 192/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6203 - acc: 0.7390 - val_loss: 1.6790 - val_acc: 0.5076\n",
      "Epoch 193/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6752 - acc: 0.7133 - val_loss: 1.5911 - val_acc: 0.5045\n",
      "Epoch 194/500\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.5972 - acc: 0.7369 - val_loss: 1.5121 - val_acc: 0.5257\n",
      "Epoch 195/500\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.6445 - acc: 0.7168 - val_loss: 1.5082 - val_acc: 0.5136\n",
      "Epoch 196/500\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.6075 - acc: 0.7369 - val_loss: 1.5074 - val_acc: 0.5015\n",
      "Epoch 197/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6052 - acc: 0.7489 - val_loss: 1.5763 - val_acc: 0.5227\n",
      "Epoch 198/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.6880 - acc: 0.7055 - val_loss: 1.6324 - val_acc: 0.5045\n",
      "Epoch 199/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.6049 - acc: 0.7499 - val_loss: 1.6751 - val_acc: 0.4955\n",
      "Epoch 200/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5966 - acc: 0.7492 - val_loss: 1.7802 - val_acc: 0.4562\n",
      "Epoch 201/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6457 - acc: 0.7231 - val_loss: 1.6716 - val_acc: 0.4955\n",
      "Epoch 202/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6107 - acc: 0.7510 - val_loss: 1.7724 - val_acc: 0.4985\n",
      "Epoch 203/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6068 - acc: 0.7457 - val_loss: 1.7214 - val_acc: 0.4773\n",
      "Epoch 204/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6109 - acc: 0.7362 - val_loss: 1.5455 - val_acc: 0.4804\n",
      "Epoch 205/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.6048 - acc: 0.7415 - val_loss: 1.5111 - val_acc: 0.5257\n",
      "Epoch 206/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5795 - acc: 0.7619 - val_loss: 1.6386 - val_acc: 0.4713\n",
      "Epoch 207/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6308 - acc: 0.7242 - val_loss: 1.5242 - val_acc: 0.4985\n",
      "Epoch 208/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5858 - acc: 0.7538 - val_loss: 1.6359 - val_acc: 0.4834\n",
      "Epoch 209/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5914 - acc: 0.7577 - val_loss: 1.5762 - val_acc: 0.5015\n",
      "Epoch 210/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5842 - acc: 0.7527 - val_loss: 1.6789 - val_acc: 0.4924\n",
      "Epoch 211/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6060 - acc: 0.7415 - val_loss: 1.6085 - val_acc: 0.4592\n",
      "Epoch 212/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5966 - acc: 0.7415 - val_loss: 1.6721 - val_acc: 0.5015\n",
      "Epoch 213/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5912 - acc: 0.7436 - val_loss: 1.5451 - val_acc: 0.5015\n",
      "Epoch 214/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5981 - acc: 0.7446 - val_loss: 1.6285 - val_acc: 0.4924\n",
      "Epoch 215/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.6079 - acc: 0.7439 - val_loss: 1.5933 - val_acc: 0.4804\n",
      "Epoch 216/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5635 - acc: 0.7548 - val_loss: 1.6992 - val_acc: 0.5076\n",
      "Epoch 217/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6096 - acc: 0.7372 - val_loss: 1.6173 - val_acc: 0.5045\n",
      "Epoch 218/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5928 - acc: 0.7482 - val_loss: 1.6052 - val_acc: 0.4804\n",
      "Epoch 219/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5931 - acc: 0.7563 - val_loss: 1.5424 - val_acc: 0.5106\n",
      "Epoch 220/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.5655 - acc: 0.7527 - val_loss: 1.6730 - val_acc: 0.4985\n",
      "Epoch 221/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.5527 - acc: 0.7644 - val_loss: 1.5659 - val_acc: 0.5257\n",
      "Epoch 222/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.6029 - acc: 0.7362 - val_loss: 1.6899 - val_acc: 0.5076\n",
      "Epoch 223/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5450 - acc: 0.7756 - val_loss: 1.8407 - val_acc: 0.4955\n",
      "Epoch 224/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5805 - acc: 0.7541 - val_loss: 1.9174 - val_acc: 0.4864\n",
      "Epoch 225/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5733 - acc: 0.7626 - val_loss: 1.9072 - val_acc: 0.4773\n",
      "Epoch 226/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5782 - acc: 0.7598 - val_loss: 1.7557 - val_acc: 0.4894\n",
      "Epoch 227/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5613 - acc: 0.7707 - val_loss: 1.8368 - val_acc: 0.4955\n",
      "Epoch 228/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5753 - acc: 0.7587 - val_loss: 1.6663 - val_acc: 0.5045\n",
      "Epoch 229/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5556 - acc: 0.7636 - val_loss: 1.7423 - val_acc: 0.4743\n",
      "Epoch 230/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5294 - acc: 0.7823 - val_loss: 1.8404 - val_acc: 0.4773\n",
      "Epoch 231/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5618 - acc: 0.7714 - val_loss: 1.7592 - val_acc: 0.4955\n",
      "Epoch 232/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.5862 - acc: 0.7647 - val_loss: 1.7933 - val_acc: 0.4955\n",
      "Epoch 233/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.5526 - acc: 0.7707 - val_loss: 1.6842 - val_acc: 0.5196\n",
      "Epoch 234/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5429 - acc: 0.7816 - val_loss: 1.7718 - val_acc: 0.5045\n",
      "Epoch 235/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5591 - acc: 0.7696 - val_loss: 1.9563 - val_acc: 0.4562\n",
      "Epoch 236/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.5445 - acc: 0.7707 - val_loss: 1.8242 - val_acc: 0.4894\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5134 - acc: 0.7869 - val_loss: 1.7977 - val_acc: 0.5015\n",
      "Epoch 238/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.5274 - acc: 0.7841 - val_loss: 1.7531 - val_acc: 0.4924\n",
      "Epoch 239/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.6088 - acc: 0.7369 - val_loss: 1.8138 - val_acc: 0.4773\n",
      "Epoch 240/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5263 - acc: 0.7834 - val_loss: 1.8974 - val_acc: 0.4804\n",
      "Epoch 241/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5723 - acc: 0.7640 - val_loss: 1.8715 - val_acc: 0.4864\n",
      "Epoch 242/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5331 - acc: 0.7795 - val_loss: 1.8751 - val_acc: 0.4683\n",
      "Epoch 243/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5346 - acc: 0.7770 - val_loss: 1.7585 - val_acc: 0.5045\n",
      "Epoch 244/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5375 - acc: 0.7806 - val_loss: 1.8055 - val_acc: 0.4955\n",
      "Epoch 245/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5730 - acc: 0.7591 - val_loss: 1.8006 - val_acc: 0.4773\n",
      "Epoch 246/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.5039 - acc: 0.7869 - val_loss: 1.9892 - val_acc: 0.4653\n",
      "Epoch 247/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5494 - acc: 0.7629 - val_loss: 1.8676 - val_acc: 0.4743\n",
      "Epoch 248/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5063 - acc: 0.7834 - val_loss: 2.0418 - val_acc: 0.4924\n",
      "Epoch 249/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5361 - acc: 0.7806 - val_loss: 1.8854 - val_acc: 0.4985\n",
      "Epoch 250/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5212 - acc: 0.7844 - val_loss: 2.0715 - val_acc: 0.4683\n",
      "Epoch 251/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5562 - acc: 0.7615 - val_loss: 1.8587 - val_acc: 0.4773\n",
      "Epoch 252/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4944 - acc: 0.7929 - val_loss: 1.7795 - val_acc: 0.4622\n",
      "Epoch 253/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5342 - acc: 0.7686 - val_loss: 1.8202 - val_acc: 0.5106\n",
      "Epoch 254/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5359 - acc: 0.7735 - val_loss: 1.9017 - val_acc: 0.4924\n",
      "Epoch 255/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5224 - acc: 0.7767 - val_loss: 1.8576 - val_acc: 0.4864\n",
      "Epoch 256/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5254 - acc: 0.7725 - val_loss: 1.7532 - val_acc: 0.5076\n",
      "Epoch 257/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4901 - acc: 0.8031 - val_loss: 1.7747 - val_acc: 0.5196\n",
      "Epoch 258/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.5102 - acc: 0.7954 - val_loss: 2.0039 - val_acc: 0.4622\n",
      "Epoch 259/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4900 - acc: 0.7946 - val_loss: 1.8957 - val_acc: 0.4894\n",
      "Epoch 260/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5543 - acc: 0.7725 - val_loss: 1.9167 - val_acc: 0.4713\n",
      "Epoch 261/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5052 - acc: 0.7915 - val_loss: 2.0345 - val_acc: 0.4743\n",
      "Epoch 262/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.5184 - acc: 0.7911 - val_loss: 1.7969 - val_acc: 0.5015\n",
      "Epoch 263/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5186 - acc: 0.7806 - val_loss: 1.8651 - val_acc: 0.4592\n",
      "Epoch 264/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5495 - acc: 0.7781 - val_loss: 1.7561 - val_acc: 0.4773\n",
      "Epoch 265/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5033 - acc: 0.7869 - val_loss: 1.8157 - val_acc: 0.4743\n",
      "Epoch 266/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.4915 - acc: 0.8035 - val_loss: 1.9105 - val_acc: 0.4804\n",
      "Epoch 267/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4831 - acc: 0.7996 - val_loss: 2.1363 - val_acc: 0.4622\n",
      "Epoch 268/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5449 - acc: 0.7806 - val_loss: 1.9870 - val_acc: 0.4532\n",
      "Epoch 269/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4911 - acc: 0.7999 - val_loss: 1.9400 - val_acc: 0.4924\n",
      "Epoch 270/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4843 - acc: 0.8006 - val_loss: 1.7839 - val_acc: 0.5166\n",
      "Epoch 271/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5335 - acc: 0.7753 - val_loss: 1.9594 - val_acc: 0.4441\n",
      "Epoch 272/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.5230 - acc: 0.7816 - val_loss: 2.0247 - val_acc: 0.4592\n",
      "Epoch 273/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4636 - acc: 0.8077 - val_loss: 1.9516 - val_acc: 0.4592\n",
      "Epoch 274/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5360 - acc: 0.7728 - val_loss: 1.8426 - val_acc: 0.4864\n",
      "Epoch 275/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.4805 - acc: 0.7985 - val_loss: 1.8560 - val_acc: 0.4713\n",
      "Epoch 276/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.5078 - acc: 0.7855 - val_loss: 1.9433 - val_acc: 0.4804\n",
      "Epoch 277/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4513 - acc: 0.8108 - val_loss: 1.7999 - val_acc: 0.5076\n",
      "Epoch 278/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5181 - acc: 0.7791 - val_loss: 2.0348 - val_acc: 0.4834\n",
      "Epoch 279/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4570 - acc: 0.8112 - val_loss: 2.1466 - val_acc: 0.4653\n",
      "Epoch 280/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5119 - acc: 0.7911 - val_loss: 2.0359 - val_acc: 0.4924\n",
      "Epoch 281/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4821 - acc: 0.8010 - val_loss: 1.9086 - val_acc: 0.4804\n",
      "Epoch 282/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.5383 - acc: 0.7813 - val_loss: 1.8380 - val_acc: 0.4804\n",
      "Epoch 283/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.4746 - acc: 0.8105 - val_loss: 1.8767 - val_acc: 0.4894\n",
      "Epoch 284/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4906 - acc: 0.7985 - val_loss: 1.9483 - val_acc: 0.4743\n",
      "Epoch 285/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4549 - acc: 0.8052 - val_loss: 2.1284 - val_acc: 0.4894\n",
      "Epoch 286/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5092 - acc: 0.8003 - val_loss: 1.9555 - val_acc: 0.4622\n",
      "Epoch 287/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4523 - acc: 0.8179 - val_loss: 1.9769 - val_acc: 0.4713\n",
      "Epoch 288/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.4971 - acc: 0.7890 - val_loss: 1.9795 - val_acc: 0.4864\n",
      "Epoch 289/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4532 - acc: 0.8130 - val_loss: 1.9954 - val_acc: 0.4894\n",
      "Epoch 290/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.5067 - acc: 0.7957 - val_loss: 2.1503 - val_acc: 0.4471\n",
      "Epoch 291/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4591 - acc: 0.8116 - val_loss: 2.0735 - val_acc: 0.4743\n",
      "Epoch 292/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4791 - acc: 0.8098 - val_loss: 1.9614 - val_acc: 0.4864\n",
      "Epoch 293/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4463 - acc: 0.8190 - val_loss: 2.1118 - val_acc: 0.4653\n",
      "Epoch 294/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4534 - acc: 0.8179 - val_loss: 2.7018 - val_acc: 0.4411\n",
      "Epoch 295/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5083 - acc: 0.7929 - val_loss: 2.0738 - val_acc: 0.4622\n",
      "Epoch 296/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4404 - acc: 0.8211 - val_loss: 2.1317 - val_acc: 0.4773\n",
      "Epoch 297/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4634 - acc: 0.8091 - val_loss: 1.9978 - val_acc: 0.4864\n",
      "Epoch 298/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4839 - acc: 0.7954 - val_loss: 1.9729 - val_acc: 0.4713\n",
      "Epoch 299/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4424 - acc: 0.8218 - val_loss: 1.9806 - val_acc: 0.4804\n",
      "Epoch 300/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4839 - acc: 0.8084 - val_loss: 1.9379 - val_acc: 0.4894\n",
      "Epoch 301/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4405 - acc: 0.8239 - val_loss: 2.4860 - val_acc: 0.4562\n",
      "Epoch 302/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4846 - acc: 0.8013 - val_loss: 2.1531 - val_acc: 0.4804\n",
      "Epoch 303/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4753 - acc: 0.8031 - val_loss: 2.2076 - val_acc: 0.4773\n",
      "Epoch 304/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4278 - acc: 0.8267 - val_loss: 2.2036 - val_acc: 0.4834\n",
      "Epoch 305/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4985 - acc: 0.7971 - val_loss: 2.2041 - val_acc: 0.4532\n",
      "Epoch 306/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4333 - acc: 0.8190 - val_loss: 2.3404 - val_acc: 0.4532\n",
      "Epoch 307/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4415 - acc: 0.8242 - val_loss: 2.3179 - val_acc: 0.4592\n",
      "Epoch 308/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4552 - acc: 0.8126 - val_loss: 2.3566 - val_acc: 0.4622\n",
      "Epoch 309/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4638 - acc: 0.8151 - val_loss: 2.3164 - val_acc: 0.4713\n",
      "Epoch 310/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4235 - acc: 0.8285 - val_loss: 2.4150 - val_acc: 0.4562\n",
      "Epoch 311/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4685 - acc: 0.7999 - val_loss: 2.2213 - val_acc: 0.4502\n",
      "Epoch 312/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4453 - acc: 0.8211 - val_loss: 2.0063 - val_acc: 0.4834\n",
      "Epoch 313/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4353 - acc: 0.8179 - val_loss: 1.9840 - val_acc: 0.4834\n",
      "Epoch 314/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4564 - acc: 0.8207 - val_loss: 2.2142 - val_acc: 0.4502\n",
      "Epoch 315/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4679 - acc: 0.8101 - val_loss: 2.1774 - val_acc: 0.4562\n",
      "Epoch 316/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4341 - acc: 0.8221 - val_loss: 2.1301 - val_acc: 0.4743\n",
      "Epoch 317/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4683 - acc: 0.8137 - val_loss: 2.0810 - val_acc: 0.4592\n",
      "Epoch 318/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4150 - acc: 0.8267 - val_loss: 2.0854 - val_acc: 0.4773\n",
      "Epoch 319/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4612 - acc: 0.8190 - val_loss: 2.3585 - val_acc: 0.4411\n",
      "Epoch 320/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4392 - acc: 0.8221 - val_loss: 2.3770 - val_acc: 0.4502\n",
      "Epoch 321/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4363 - acc: 0.8302 - val_loss: 2.3213 - val_acc: 0.4743\n",
      "Epoch 322/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4348 - acc: 0.8207 - val_loss: 2.1588 - val_acc: 0.4955\n",
      "Epoch 323/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4392 - acc: 0.8158 - val_loss: 2.0578 - val_acc: 0.4924\n",
      "Epoch 324/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.4507 - acc: 0.8186 - val_loss: 2.2434 - val_acc: 0.4532\n",
      "Epoch 325/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4501 - acc: 0.8200 - val_loss: 2.0060 - val_acc: 0.4653\n",
      "Epoch 326/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4279 - acc: 0.8281 - val_loss: 2.1578 - val_acc: 0.4562\n",
      "Epoch 327/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4056 - acc: 0.8344 - val_loss: 2.1685 - val_acc: 0.4773\n",
      "Epoch 328/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4297 - acc: 0.8253 - val_loss: 1.9941 - val_acc: 0.4955\n",
      "Epoch 329/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3960 - acc: 0.8390 - val_loss: 2.1017 - val_acc: 0.5106\n",
      "Epoch 330/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4403 - acc: 0.8182 - val_loss: 2.1976 - val_acc: 0.5045\n",
      "Epoch 331/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4425 - acc: 0.8221 - val_loss: 2.1725 - val_acc: 0.4985\n",
      "Epoch 332/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4370 - acc: 0.8292 - val_loss: 2.3989 - val_acc: 0.4622\n",
      "Epoch 333/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4048 - acc: 0.8341 - val_loss: 2.0898 - val_acc: 0.5015\n",
      "Epoch 334/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4286 - acc: 0.8239 - val_loss: 2.3896 - val_acc: 0.4683\n",
      "Epoch 335/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4222 - acc: 0.8260 - val_loss: 2.1598 - val_acc: 0.4834\n",
      "Epoch 336/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3988 - acc: 0.8408 - val_loss: 2.3171 - val_acc: 0.4653\n",
      "Epoch 337/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4301 - acc: 0.8316 - val_loss: 2.4800 - val_acc: 0.4350\n",
      "Epoch 338/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.4401 - acc: 0.8218 - val_loss: 2.3039 - val_acc: 0.4562\n",
      "Epoch 339/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4221 - acc: 0.8327 - val_loss: 2.3129 - val_acc: 0.4562\n",
      "Epoch 340/500\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.4438 - acc: 0.8271 - val_loss: 2.2316 - val_acc: 0.4773\n",
      "Epoch 341/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.4244 - acc: 0.8295 - val_loss: 2.3629 - val_acc: 0.4924\n",
      "Epoch 342/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4022 - acc: 0.8440 - val_loss: 2.5467 - val_acc: 0.4592\n",
      "Epoch 343/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3923 - acc: 0.8443 - val_loss: 2.4999 - val_acc: 0.4683\n",
      "Epoch 344/500\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4420 - acc: 0.8211 - val_loss: 2.2894 - val_acc: 0.4622\n",
      "Epoch 345/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3702 - acc: 0.8552 - val_loss: 2.3445 - val_acc: 0.4743\n",
      "Epoch 346/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.4576 - acc: 0.8154 - val_loss: 2.6191 - val_acc: 0.4441\n",
      "Epoch 347/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4433 - acc: 0.8267 - val_loss: 2.3021 - val_acc: 0.4502\n",
      "Epoch 348/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3829 - acc: 0.8397 - val_loss: 2.2489 - val_acc: 0.4894\n",
      "Epoch 349/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4073 - acc: 0.8380 - val_loss: 2.1873 - val_acc: 0.5136\n",
      "Epoch 350/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4086 - acc: 0.8355 - val_loss: 2.3758 - val_acc: 0.4924\n",
      "Epoch 351/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3937 - acc: 0.8359 - val_loss: 2.4696 - val_acc: 0.4592\n",
      "Epoch 352/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4321 - acc: 0.8256 - val_loss: 2.3861 - val_acc: 0.4713\n",
      "Epoch 353/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3695 - acc: 0.8538 - val_loss: 2.7296 - val_acc: 0.4350\n",
      "Epoch 354/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4587 - acc: 0.8186 - val_loss: 2.3298 - val_acc: 0.4622\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3955 - acc: 0.8475 - val_loss: 2.4165 - val_acc: 0.4381\n",
      "Epoch 356/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.3721 - acc: 0.8492 - val_loss: 2.1627 - val_acc: 0.4622\n",
      "Epoch 357/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4189 - acc: 0.8330 - val_loss: 2.3412 - val_acc: 0.4834\n",
      "Epoch 358/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4018 - acc: 0.8408 - val_loss: 2.3807 - val_acc: 0.4653\n",
      "Epoch 359/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4106 - acc: 0.8362 - val_loss: 2.2662 - val_acc: 0.4894\n",
      "Epoch 360/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4236 - acc: 0.8302 - val_loss: 2.4143 - val_acc: 0.4592\n",
      "Epoch 361/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3619 - acc: 0.8570 - val_loss: 2.4279 - val_acc: 0.4411\n",
      "Epoch 362/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4038 - acc: 0.8397 - val_loss: 2.3776 - val_acc: 0.4683\n",
      "Epoch 363/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4010 - acc: 0.8433 - val_loss: 2.5267 - val_acc: 0.4773\n",
      "Epoch 364/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3584 - acc: 0.8637 - val_loss: 2.3795 - val_acc: 0.4894\n",
      "Epoch 365/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4217 - acc: 0.8267 - val_loss: 2.5005 - val_acc: 0.4471\n",
      "Epoch 366/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4200 - acc: 0.8288 - val_loss: 2.5887 - val_acc: 0.4653\n",
      "Epoch 367/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3839 - acc: 0.8440 - val_loss: 2.3660 - val_acc: 0.4955\n",
      "Epoch 368/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3709 - acc: 0.8619 - val_loss: 2.4236 - val_acc: 0.4411\n",
      "Epoch 369/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3879 - acc: 0.8464 - val_loss: 2.4692 - val_acc: 0.4773\n",
      "Epoch 370/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3569 - acc: 0.8524 - val_loss: 2.4220 - val_acc: 0.4804\n",
      "Epoch 371/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3996 - acc: 0.8383 - val_loss: 2.2477 - val_acc: 0.4532\n",
      "Epoch 372/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4219 - acc: 0.8281 - val_loss: 2.5141 - val_acc: 0.4773\n",
      "Epoch 373/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4043 - acc: 0.8436 - val_loss: 2.5881 - val_acc: 0.4502\n",
      "Epoch 374/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3686 - acc: 0.8549 - val_loss: 2.4900 - val_acc: 0.4592\n",
      "Epoch 375/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3686 - acc: 0.8580 - val_loss: 2.3353 - val_acc: 0.4350\n",
      "Epoch 376/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.3864 - acc: 0.8482 - val_loss: 2.1838 - val_acc: 0.5106\n",
      "Epoch 377/500\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.3886 - acc: 0.8499 - val_loss: 2.4323 - val_acc: 0.4834\n",
      "Epoch 378/500\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.3898 - acc: 0.8464 - val_loss: 2.4921 - val_acc: 0.4834\n",
      "Epoch 379/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3772 - acc: 0.8485 - val_loss: 2.4960 - val_acc: 0.5045\n",
      "Epoch 380/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3459 - acc: 0.8616 - val_loss: 2.4007 - val_acc: 0.4743\n",
      "Epoch 381/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4026 - acc: 0.8440 - val_loss: 2.4460 - val_acc: 0.4381\n",
      "Epoch 382/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3764 - acc: 0.8489 - val_loss: 2.6408 - val_acc: 0.4169\n",
      "Epoch 383/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3984 - acc: 0.8510 - val_loss: 2.5472 - val_acc: 0.4743\n",
      "Epoch 384/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3582 - acc: 0.8637 - val_loss: 2.5856 - val_acc: 0.4562\n",
      "Epoch 385/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3838 - acc: 0.8454 - val_loss: 2.6222 - val_acc: 0.4562\n",
      "Epoch 386/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3585 - acc: 0.8609 - val_loss: 2.7509 - val_acc: 0.4441\n",
      "Epoch 387/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.4123 - acc: 0.8288 - val_loss: 2.4659 - val_acc: 0.4683\n",
      "Epoch 388/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3686 - acc: 0.8443 - val_loss: 2.5711 - val_acc: 0.4713\n",
      "Epoch 389/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3590 - acc: 0.8619 - val_loss: 2.7196 - val_acc: 0.4139\n",
      "Epoch 390/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3856 - acc: 0.8482 - val_loss: 2.8979 - val_acc: 0.4532\n",
      "Epoch 391/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3935 - acc: 0.8359 - val_loss: 2.5678 - val_acc: 0.4532\n",
      "Epoch 392/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3688 - acc: 0.8524 - val_loss: 2.6153 - val_acc: 0.4411\n",
      "Epoch 393/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3467 - acc: 0.8697 - val_loss: 2.6208 - val_acc: 0.4773\n",
      "Epoch 394/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3652 - acc: 0.8573 - val_loss: 2.4037 - val_acc: 0.5015\n",
      "Epoch 395/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3946 - acc: 0.8394 - val_loss: 2.3642 - val_acc: 0.5015\n",
      "Epoch 396/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3521 - acc: 0.8623 - val_loss: 2.5985 - val_acc: 0.4471\n",
      "Epoch 397/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3632 - acc: 0.8535 - val_loss: 2.6446 - val_acc: 0.4773\n",
      "Epoch 398/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3625 - acc: 0.8584 - val_loss: 2.5231 - val_acc: 0.4864\n",
      "Epoch 399/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3800 - acc: 0.8440 - val_loss: 2.4219 - val_acc: 0.4683\n",
      "Epoch 400/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3353 - acc: 0.8707 - val_loss: 2.5353 - val_acc: 0.4894\n",
      "Epoch 401/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4061 - acc: 0.8397 - val_loss: 2.2342 - val_acc: 0.4743\n",
      "Epoch 402/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3672 - acc: 0.8591 - val_loss: 2.4519 - val_acc: 0.4804\n",
      "Epoch 403/500\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.3403 - acc: 0.8697 - val_loss: 2.4585 - val_acc: 0.5045\n",
      "Epoch 404/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.3604 - acc: 0.8545 - val_loss: 2.6542 - val_acc: 0.4924\n",
      "Epoch 405/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3823 - acc: 0.8482 - val_loss: 2.4612 - val_acc: 0.5076\n",
      "Epoch 406/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3377 - acc: 0.8739 - val_loss: 2.5922 - val_acc: 0.4653\n",
      "Epoch 407/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3415 - acc: 0.8595 - val_loss: 2.7330 - val_acc: 0.4773\n",
      "Epoch 408/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3619 - acc: 0.8507 - val_loss: 2.5142 - val_acc: 0.4864\n",
      "Epoch 409/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.4177 - acc: 0.8362 - val_loss: 2.5475 - val_acc: 0.4773\n",
      "Epoch 410/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3524 - acc: 0.8640 - val_loss: 2.4692 - val_acc: 0.4955\n",
      "Epoch 411/500\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3351 - acc: 0.8679 - val_loss: 2.5319 - val_acc: 0.4985\n",
      "Epoch 412/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3415 - acc: 0.8718 - val_loss: 2.5815 - val_acc: 0.4743\n",
      "Epoch 413/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3349 - acc: 0.8746 - val_loss: 2.7288 - val_acc: 0.4864\n",
      "Epoch 414/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3446 - acc: 0.8662 - val_loss: 2.6635 - val_acc: 0.4592\n",
      "Epoch 415/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3300 - acc: 0.8676 - val_loss: 2.8455 - val_acc: 0.4713\n",
      "Epoch 416/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3865 - acc: 0.8464 - val_loss: 2.7135 - val_acc: 0.4653\n",
      "Epoch 417/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3694 - acc: 0.8496 - val_loss: 2.5008 - val_acc: 0.5015\n",
      "Epoch 418/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3268 - acc: 0.8764 - val_loss: 2.5428 - val_acc: 0.4834\n",
      "Epoch 419/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3978 - acc: 0.8355 - val_loss: 2.4266 - val_acc: 0.4834\n",
      "Epoch 420/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3335 - acc: 0.8714 - val_loss: 2.6145 - val_acc: 0.4985\n",
      "Epoch 421/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3446 - acc: 0.8654 - val_loss: 2.8687 - val_acc: 0.4653\n",
      "Epoch 422/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3546 - acc: 0.8623 - val_loss: 2.5668 - val_acc: 0.4441\n",
      "Epoch 423/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3635 - acc: 0.8605 - val_loss: 2.3426 - val_acc: 0.5106\n",
      "Epoch 424/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3615 - acc: 0.8580 - val_loss: 2.6770 - val_acc: 0.4683\n",
      "Epoch 425/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3328 - acc: 0.8753 - val_loss: 2.3794 - val_acc: 0.5257\n",
      "Epoch 426/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3149 - acc: 0.8746 - val_loss: 2.7717 - val_acc: 0.4894\n",
      "Epoch 427/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3497 - acc: 0.8605 - val_loss: 2.7507 - val_acc: 0.4653\n",
      "Epoch 428/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3377 - acc: 0.8658 - val_loss: 2.7586 - val_acc: 0.4562\n",
      "Epoch 429/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.3439 - acc: 0.8644 - val_loss: 2.4850 - val_acc: 0.4834\n",
      "Epoch 430/500\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.3149 - acc: 0.8778 - val_loss: 2.9257 - val_acc: 0.4260\n",
      "Epoch 431/500\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.3533 - acc: 0.8521 - val_loss: 2.8266 - val_acc: 0.4653\n",
      "Epoch 432/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3455 - acc: 0.8669 - val_loss: 2.5742 - val_acc: 0.4864\n",
      "Epoch 433/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3161 - acc: 0.8774 - val_loss: 2.7250 - val_acc: 0.4804\n",
      "Epoch 434/500\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3531 - acc: 0.8619 - val_loss: 2.6076 - val_acc: 0.4834\n",
      "Epoch 435/500\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3053 - acc: 0.8845 - val_loss: 3.0149 - val_acc: 0.4562\n",
      "Epoch 436/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3713 - acc: 0.8492 - val_loss: 2.7491 - val_acc: 0.4502\n",
      "Epoch 437/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3077 - acc: 0.8813 - val_loss: 3.0193 - val_acc: 0.4592\n",
      "Epoch 438/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.3667 - acc: 0.8521 - val_loss: 2.6571 - val_acc: 0.4411\n",
      "Epoch 439/500\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.3268 - acc: 0.8700 - val_loss: 2.7060 - val_acc: 0.4773\n",
      "Epoch 440/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3147 - acc: 0.8795 - val_loss: 2.9900 - val_acc: 0.4411\n",
      "Epoch 441/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3202 - acc: 0.8728 - val_loss: 2.9776 - val_acc: 0.4562\n",
      "Epoch 442/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3476 - acc: 0.8669 - val_loss: 2.8225 - val_acc: 0.4653\n",
      "Epoch 443/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.3250 - acc: 0.8711 - val_loss: 2.5354 - val_acc: 0.4985\n",
      "Epoch 444/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3428 - acc: 0.8658 - val_loss: 2.7538 - val_acc: 0.4713\n",
      "Epoch 445/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3174 - acc: 0.8750 - val_loss: 2.6246 - val_acc: 0.4834\n",
      "Epoch 446/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3518 - acc: 0.8552 - val_loss: 2.6030 - val_acc: 0.4532\n",
      "Epoch 447/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3513 - acc: 0.8640 - val_loss: 2.7918 - val_acc: 0.4622\n",
      "Epoch 448/500\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.3336 - acc: 0.8683 - val_loss: 2.7870 - val_acc: 0.4713\n",
      "Epoch 449/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2950 - acc: 0.8866 - val_loss: 2.7239 - val_acc: 0.4834\n",
      "Epoch 450/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3529 - acc: 0.8549 - val_loss: 2.4548 - val_acc: 0.4985\n",
      "Epoch 451/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3382 - acc: 0.8658 - val_loss: 2.6224 - val_acc: 0.4894\n",
      "Epoch 452/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3314 - acc: 0.8700 - val_loss: 2.7047 - val_acc: 0.4955\n",
      "Epoch 453/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.2838 - acc: 0.8901 - val_loss: 2.8846 - val_acc: 0.4653\n",
      "Epoch 454/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3614 - acc: 0.8580 - val_loss: 2.5883 - val_acc: 0.4653\n",
      "Epoch 455/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3251 - acc: 0.8662 - val_loss: 2.8170 - val_acc: 0.4804\n",
      "Epoch 456/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3009 - acc: 0.8816 - val_loss: 2.9005 - val_acc: 0.4622\n",
      "Epoch 457/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3278 - acc: 0.8764 - val_loss: 2.8249 - val_acc: 0.4592\n",
      "Epoch 458/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3404 - acc: 0.8637 - val_loss: 2.5351 - val_acc: 0.4924\n",
      "Epoch 459/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3116 - acc: 0.8802 - val_loss: 2.5696 - val_acc: 0.4985\n",
      "Epoch 460/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3267 - acc: 0.8707 - val_loss: 2.6625 - val_acc: 0.4924\n",
      "Epoch 461/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3071 - acc: 0.8799 - val_loss: 2.8286 - val_acc: 0.4653\n",
      "Epoch 462/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2818 - acc: 0.8869 - val_loss: 3.1686 - val_acc: 0.4683\n",
      "Epoch 463/500\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.3353 - acc: 0.8697 - val_loss: 3.0814 - val_acc: 0.4562\n",
      "Epoch 464/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3579 - acc: 0.8686 - val_loss: 2.6807 - val_acc: 0.4713\n",
      "Epoch 465/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3034 - acc: 0.8838 - val_loss: 3.1181 - val_acc: 0.4502\n",
      "Epoch 466/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.3464 - acc: 0.8647 - val_loss: 2.9178 - val_acc: 0.4381\n",
      "Epoch 467/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.2949 - acc: 0.8880 - val_loss: 2.6864 - val_acc: 0.5136\n",
      "Epoch 468/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3082 - acc: 0.8866 - val_loss: 2.8047 - val_acc: 0.4894\n",
      "Epoch 469/500\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3256 - acc: 0.8686 - val_loss: 3.0435 - val_acc: 0.4441\n",
      "Epoch 470/500\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.3545 - acc: 0.8609 - val_loss: 2.8427 - val_acc: 0.4773\n",
      "Epoch 471/500\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.2758 - acc: 0.8905 - val_loss: 3.1187 - val_acc: 0.4592\n",
      "Epoch 472/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3094 - acc: 0.8760 - val_loss: 2.9720 - val_acc: 0.4653\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3234 - acc: 0.8753 - val_loss: 3.0210 - val_acc: 0.4804\n",
      "Epoch 474/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3418 - acc: 0.8672 - val_loss: 2.8961 - val_acc: 0.4713\n",
      "Epoch 475/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.2836 - acc: 0.8926 - val_loss: 3.0902 - val_acc: 0.4773\n",
      "Epoch 476/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3010 - acc: 0.8834 - val_loss: 3.1999 - val_acc: 0.4622\n",
      "Epoch 477/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3515 - acc: 0.8591 - val_loss: 3.1376 - val_acc: 0.4713\n",
      "Epoch 478/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3106 - acc: 0.8788 - val_loss: 2.5768 - val_acc: 0.5136\n",
      "Epoch 479/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3260 - acc: 0.8672 - val_loss: 2.7457 - val_acc: 0.4924\n",
      "Epoch 480/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2774 - acc: 0.8975 - val_loss: 3.0244 - val_acc: 0.4713\n",
      "Epoch 481/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3103 - acc: 0.8816 - val_loss: 3.0400 - val_acc: 0.4592\n",
      "Epoch 482/500\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3075 - acc: 0.8757 - val_loss: 2.9588 - val_acc: 0.4804\n",
      "Epoch 483/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3010 - acc: 0.8887 - val_loss: 2.8643 - val_acc: 0.4834\n",
      "Epoch 484/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3350 - acc: 0.8714 - val_loss: 2.8241 - val_acc: 0.4924\n",
      "Epoch 485/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.2797 - acc: 0.8908 - val_loss: 2.7871 - val_acc: 0.4773\n",
      "Epoch 486/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3081 - acc: 0.8845 - val_loss: 2.5867 - val_acc: 0.4804\n",
      "Epoch 487/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3288 - acc: 0.8725 - val_loss: 2.7218 - val_acc: 0.4713\n",
      "Epoch 488/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2741 - acc: 0.8901 - val_loss: 3.1495 - val_acc: 0.4622\n",
      "Epoch 489/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.2936 - acc: 0.8894 - val_loss: 2.7339 - val_acc: 0.4532\n",
      "Epoch 490/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2937 - acc: 0.8792 - val_loss: 3.0871 - val_acc: 0.4653\n",
      "Epoch 491/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3154 - acc: 0.8760 - val_loss: 3.1320 - val_acc: 0.4502\n",
      "Epoch 492/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3124 - acc: 0.8785 - val_loss: 3.0045 - val_acc: 0.4502\n",
      "Epoch 493/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2921 - acc: 0.8838 - val_loss: 2.9571 - val_acc: 0.4653\n",
      "Epoch 494/500\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.3083 - acc: 0.8834 - val_loss: 2.7397 - val_acc: 0.5015\n",
      "Epoch 495/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3109 - acc: 0.8799 - val_loss: 2.8075 - val_acc: 0.5045\n",
      "Epoch 496/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.3204 - acc: 0.8771 - val_loss: 2.4771 - val_acc: 0.5166\n",
      "Epoch 497/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3150 - acc: 0.8686 - val_loss: 2.8582 - val_acc: 0.5106\n",
      "Epoch 498/500\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.2730 - acc: 0.8964 - val_loss: 3.0094 - val_acc: 0.4834\n",
      "Epoch 499/500\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.2889 - acc: 0.8957 - val_loss: 3.0405 - val_acc: 0.4864\n",
      "Epoch 500/500\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.2678 - acc: 0.8869 - val_loss: 3.2644 - val_acc: 0.4653\n"
     ]
    }
   ],
   "source": [
    "business_model = model.fit(x=X_train_business, y=y_cat_train_business, \n",
    "          batch_size=500, \n",
    "          epochs=500, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_business, y_cat_test_business),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "0.6933333333333334\n",
      "0.9466666666666667\n",
      "26\n",
      "0.6538461538461539\n",
      "0.8846153846153846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0],\n",
       "       [  4,  32,  30,   8,   4],\n",
       "       [  1,  16, 100,  76,   5],\n",
       "       [  0,   2,  11,  18,   7],\n",
       "       [  0,   2,   3,   8,   4]])"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_business)\n",
    "model_metrics(predictions, y_cat_test_business)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test_business.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x175411a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(business_model.history['acc'])\n",
    "plt.plot(business_model.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(631,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/500\n",
      "2839/2839 [==============================] - 5s 2ms/step - loss: 1.5258 - acc: 0.3364 - val_loss: 1.4371 - val_acc: 0.4109\n",
      "Epoch 2/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.3853 - acc: 0.3913 - val_loss: 1.3264 - val_acc: 0.3505\n",
      "Epoch 3/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.3349 - acc: 0.4089 - val_loss: 1.2662 - val_acc: 0.3686\n",
      "Epoch 4/500\n",
      "2839/2839 [==============================] - 0s 88us/step - loss: 1.3340 - acc: 0.4015 - val_loss: 1.2256 - val_acc: 0.4834\n",
      "Epoch 5/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.3127 - acc: 0.3959 - val_loss: 1.2376 - val_acc: 0.4622\n",
      "Epoch 6/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.3010 - acc: 0.3998 - val_loss: 1.2260 - val_acc: 0.4441\n",
      "Epoch 7/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2891 - acc: 0.3984 - val_loss: 1.2199 - val_acc: 0.4471\n",
      "Epoch 8/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.2680 - acc: 0.4216 - val_loss: 1.2456 - val_acc: 0.3565\n",
      "Epoch 9/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2629 - acc: 0.4255 - val_loss: 1.1913 - val_acc: 0.5015\n",
      "Epoch 10/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2564 - acc: 0.4371 - val_loss: 1.1978 - val_acc: 0.4441\n",
      "Epoch 11/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.2501 - acc: 0.4287 - val_loss: 1.2327 - val_acc: 0.3746\n",
      "Epoch 12/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.2511 - acc: 0.4294 - val_loss: 1.1800 - val_acc: 0.4713\n",
      "Epoch 13/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.2223 - acc: 0.4414 - val_loss: 1.2343 - val_acc: 0.4199\n",
      "Epoch 14/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2349 - acc: 0.4410 - val_loss: 1.2068 - val_acc: 0.4079\n",
      "Epoch 15/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2190 - acc: 0.4350 - val_loss: 1.1872 - val_acc: 0.4411\n",
      "Epoch 16/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2116 - acc: 0.4544 - val_loss: 1.1882 - val_acc: 0.4955\n",
      "Epoch 17/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.2102 - acc: 0.4414 - val_loss: 1.1893 - val_acc: 0.4260\n",
      "Epoch 18/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.1941 - acc: 0.4509 - val_loss: 1.1727 - val_acc: 0.4622\n",
      "Epoch 19/500\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 1.1744 - acc: 0.4618 - val_loss: 1.2000 - val_acc: 0.4230\n",
      "Epoch 20/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.1982 - acc: 0.4625 - val_loss: 1.1892 - val_acc: 0.4592\n",
      "Epoch 21/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1668 - acc: 0.4783 - val_loss: 1.1693 - val_acc: 0.4894\n",
      "Epoch 22/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1831 - acc: 0.4593 - val_loss: 1.1850 - val_acc: 0.4199\n",
      "Epoch 23/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1671 - acc: 0.4748 - val_loss: 1.1637 - val_acc: 0.4290\n",
      "Epoch 24/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1543 - acc: 0.4797 - val_loss: 1.1548 - val_acc: 0.4653\n",
      "Epoch 25/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1547 - acc: 0.4702 - val_loss: 1.2051 - val_acc: 0.4260\n",
      "Epoch 26/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1807 - acc: 0.4586 - val_loss: 1.1560 - val_acc: 0.4502\n",
      "Epoch 27/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1571 - acc: 0.4716 - val_loss: 1.1848 - val_acc: 0.4502\n",
      "Epoch 28/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1468 - acc: 0.4597 - val_loss: 1.1546 - val_acc: 0.4502\n",
      "Epoch 29/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1414 - acc: 0.4787 - val_loss: 1.1395 - val_acc: 0.4683\n",
      "Epoch 30/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1320 - acc: 0.4949 - val_loss: 1.1608 - val_acc: 0.4441\n",
      "Epoch 31/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1425 - acc: 0.4685 - val_loss: 1.1332 - val_acc: 0.4834\n",
      "Epoch 32/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1192 - acc: 0.4910 - val_loss: 1.2317 - val_acc: 0.4048\n",
      "Epoch 33/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1320 - acc: 0.4826 - val_loss: 1.2685 - val_acc: 0.4018\n",
      "Epoch 34/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1250 - acc: 0.5019 - val_loss: 1.1196 - val_acc: 0.4985\n",
      "Epoch 35/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.1240 - acc: 0.4847 - val_loss: 1.2113 - val_acc: 0.4230\n",
      "Epoch 36/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.1068 - acc: 0.5136 - val_loss: 1.1175 - val_acc: 0.4894\n",
      "Epoch 37/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1192 - acc: 0.4882 - val_loss: 1.1447 - val_acc: 0.4713\n",
      "Epoch 38/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1062 - acc: 0.4875 - val_loss: 1.1562 - val_acc: 0.4532\n",
      "Epoch 39/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1156 - acc: 0.4952 - val_loss: 1.1080 - val_acc: 0.5196\n",
      "Epoch 40/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1014 - acc: 0.4875 - val_loss: 1.1466 - val_acc: 0.4562\n",
      "Epoch 41/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0982 - acc: 0.5171 - val_loss: 1.1207 - val_acc: 0.4924\n",
      "Epoch 42/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0936 - acc: 0.5076 - val_loss: 1.1730 - val_acc: 0.4592\n",
      "Epoch 43/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0803 - acc: 0.5188 - val_loss: 1.1107 - val_acc: 0.5106\n",
      "Epoch 44/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0668 - acc: 0.5125 - val_loss: 1.1304 - val_acc: 0.4592\n",
      "Epoch 45/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0771 - acc: 0.5058 - val_loss: 1.1088 - val_acc: 0.4985\n",
      "Epoch 46/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0791 - acc: 0.5076 - val_loss: 1.1563 - val_acc: 0.4441\n",
      "Epoch 47/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0823 - acc: 0.5118 - val_loss: 1.1008 - val_acc: 0.5076\n",
      "Epoch 48/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0877 - acc: 0.5037 - val_loss: 1.1125 - val_acc: 0.4955\n",
      "Epoch 49/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0543 - acc: 0.5185 - val_loss: 1.0961 - val_acc: 0.5257\n",
      "Epoch 50/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0562 - acc: 0.5206 - val_loss: 1.1359 - val_acc: 0.4743\n",
      "Epoch 51/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0846 - acc: 0.5122 - val_loss: 1.0965 - val_acc: 0.5287\n",
      "Epoch 52/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0459 - acc: 0.5333 - val_loss: 1.1377 - val_acc: 0.4622\n",
      "Epoch 53/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0415 - acc: 0.5308 - val_loss: 1.0993 - val_acc: 0.4894\n",
      "Epoch 54/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0802 - acc: 0.5114 - val_loss: 1.0845 - val_acc: 0.5287\n",
      "Epoch 55/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0431 - acc: 0.5358 - val_loss: 1.1049 - val_acc: 0.4894\n",
      "Epoch 56/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0316 - acc: 0.5407 - val_loss: 1.2327 - val_acc: 0.4199\n",
      "Epoch 57/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0449 - acc: 0.5203 - val_loss: 1.1281 - val_acc: 0.5136\n",
      "Epoch 58/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0631 - acc: 0.5379 - val_loss: 1.1208 - val_acc: 0.4804\n",
      "Epoch 59/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0492 - acc: 0.5083 - val_loss: 1.1111 - val_acc: 0.4955\n",
      "Epoch 60/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0306 - acc: 0.5477 - val_loss: 1.1191 - val_acc: 0.4773\n",
      "Epoch 61/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0407 - acc: 0.5294 - val_loss: 1.1693 - val_acc: 0.4411\n",
      "Epoch 62/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0328 - acc: 0.5379 - val_loss: 1.0964 - val_acc: 0.5287\n",
      "Epoch 63/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0447 - acc: 0.5171 - val_loss: 1.1017 - val_acc: 0.5106\n",
      "Epoch 64/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0146 - acc: 0.5505 - val_loss: 1.0860 - val_acc: 0.4924\n",
      "Epoch 65/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0413 - acc: 0.5252 - val_loss: 1.0913 - val_acc: 0.5227\n",
      "Epoch 66/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0084 - acc: 0.5435 - val_loss: 1.0984 - val_acc: 0.4924\n",
      "Epoch 67/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0300 - acc: 0.5379 - val_loss: 1.0828 - val_acc: 0.5015\n",
      "Epoch 68/500\n",
      "2839/2839 [==============================] - 0s 88us/step - loss: 1.0034 - acc: 0.5470 - val_loss: 1.0963 - val_acc: 0.5227\n",
      "Epoch 69/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.0103 - acc: 0.5460 - val_loss: 1.1246 - val_acc: 0.4743\n",
      "Epoch 70/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0143 - acc: 0.5470 - val_loss: 1.1395 - val_acc: 0.5287\n",
      "Epoch 71/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0150 - acc: 0.5396 - val_loss: 1.0659 - val_acc: 0.5196\n",
      "Epoch 72/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0039 - acc: 0.5509 - val_loss: 1.0950 - val_acc: 0.4864\n",
      "Epoch 73/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9878 - acc: 0.5604 - val_loss: 1.1309 - val_acc: 0.5076\n",
      "Epoch 74/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0059 - acc: 0.5541 - val_loss: 1.1396 - val_acc: 0.5196\n",
      "Epoch 75/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0375 - acc: 0.5354 - val_loss: 1.1037 - val_acc: 0.5378\n",
      "Epoch 76/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9749 - acc: 0.5558 - val_loss: 1.1813 - val_acc: 0.5076\n",
      "Epoch 77/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0154 - acc: 0.5460 - val_loss: 1.1097 - val_acc: 0.5045\n",
      "Epoch 78/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9866 - acc: 0.5502 - val_loss: 1.0807 - val_acc: 0.5136\n",
      "Epoch 79/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9886 - acc: 0.5576 - val_loss: 1.0738 - val_acc: 0.5076\n",
      "Epoch 80/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9799 - acc: 0.5548 - val_loss: 1.0782 - val_acc: 0.5529\n",
      "Epoch 81/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0025 - acc: 0.5414 - val_loss: 1.1066 - val_acc: 0.5015\n",
      "Epoch 82/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9967 - acc: 0.5572 - val_loss: 1.1018 - val_acc: 0.5045\n",
      "Epoch 83/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9937 - acc: 0.5505 - val_loss: 1.0641 - val_acc: 0.5287\n",
      "Epoch 84/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.9540 - acc: 0.5738 - val_loss: 1.1323 - val_acc: 0.5257\n",
      "Epoch 85/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.9667 - acc: 0.5741 - val_loss: 1.0849 - val_acc: 0.5136\n",
      "Epoch 86/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9685 - acc: 0.5650 - val_loss: 1.1241 - val_acc: 0.5045\n",
      "Epoch 87/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9700 - acc: 0.5678 - val_loss: 1.1127 - val_acc: 0.4894\n",
      "Epoch 88/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9704 - acc: 0.5639 - val_loss: 1.1208 - val_acc: 0.5347\n",
      "Epoch 89/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9622 - acc: 0.5794 - val_loss: 1.0978 - val_acc: 0.5347\n",
      "Epoch 90/500\n",
      "2839/2839 [==============================] - 0s 87us/step - loss: 0.9777 - acc: 0.5625 - val_loss: 1.0898 - val_acc: 0.5680\n",
      "Epoch 91/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9740 - acc: 0.5678 - val_loss: 1.1104 - val_acc: 0.5287\n",
      "Epoch 92/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9599 - acc: 0.5657 - val_loss: 1.1004 - val_acc: 0.5196\n",
      "Epoch 93/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.9323 - acc: 0.5819 - val_loss: 1.1154 - val_acc: 0.5347\n",
      "Epoch 94/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9883 - acc: 0.5470 - val_loss: 1.0907 - val_acc: 0.5347\n",
      "Epoch 95/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9203 - acc: 0.5974 - val_loss: 1.1123 - val_acc: 0.5287\n",
      "Epoch 96/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.9494 - acc: 0.5787 - val_loss: 1.0757 - val_acc: 0.5498\n",
      "Epoch 97/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.9415 - acc: 0.5756 - val_loss: 1.0875 - val_acc: 0.5468\n",
      "Epoch 98/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9417 - acc: 0.5763 - val_loss: 1.1224 - val_acc: 0.5287\n",
      "Epoch 99/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9519 - acc: 0.5903 - val_loss: 1.0846 - val_acc: 0.5498\n",
      "Epoch 100/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9688 - acc: 0.5660 - val_loss: 1.0927 - val_acc: 0.5166\n",
      "Epoch 101/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9422 - acc: 0.5889 - val_loss: 1.0749 - val_acc: 0.5257\n",
      "Epoch 102/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8936 - acc: 0.6034 - val_loss: 1.1509 - val_acc: 0.5196\n",
      "Epoch 103/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9705 - acc: 0.5632 - val_loss: 1.0936 - val_acc: 0.5317\n",
      "Epoch 104/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9121 - acc: 0.5946 - val_loss: 1.1066 - val_acc: 0.5227\n",
      "Epoch 105/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9181 - acc: 0.6034 - val_loss: 1.1160 - val_acc: 0.5166\n",
      "Epoch 106/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.9302 - acc: 0.5932 - val_loss: 1.1274 - val_acc: 0.5287\n",
      "Epoch 107/500\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 0.9538 - acc: 0.5784 - val_loss: 1.0962 - val_acc: 0.5317\n",
      "Epoch 108/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.9218 - acc: 0.6002 - val_loss: 1.1234 - val_acc: 0.5287\n",
      "Epoch 109/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.9200 - acc: 0.5889 - val_loss: 1.1080 - val_acc: 0.5438\n",
      "Epoch 110/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8767 - acc: 0.6076 - val_loss: 1.1203 - val_acc: 0.5287\n",
      "Epoch 111/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.9249 - acc: 0.5900 - val_loss: 1.1915 - val_acc: 0.5136\n",
      "Epoch 112/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.9196 - acc: 0.6006 - val_loss: 1.1016 - val_acc: 0.5529\n",
      "Epoch 113/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9199 - acc: 0.5858 - val_loss: 1.1139 - val_acc: 0.5498\n",
      "Epoch 114/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.8598 - acc: 0.6185 - val_loss: 1.1385 - val_acc: 0.5408\n",
      "Epoch 115/500\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.8880 - acc: 0.6104 - val_loss: 1.1731 - val_acc: 0.5136\n",
      "Epoch 116/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.9469 - acc: 0.5805 - val_loss: 1.1444 - val_acc: 0.5680\n",
      "Epoch 117/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.8908 - acc: 0.6090 - val_loss: 1.1120 - val_acc: 0.5136\n",
      "Epoch 118/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.8933 - acc: 0.6058 - val_loss: 1.1193 - val_acc: 0.5559\n",
      "Epoch 119/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.8897 - acc: 0.6164 - val_loss: 1.1666 - val_acc: 0.5257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.8703 - acc: 0.6150 - val_loss: 1.1655 - val_acc: 0.5166\n",
      "Epoch 121/500\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.9008 - acc: 0.5738 - val_loss: 1.1591 - val_acc: 0.4864\n",
      "Epoch 122/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.8980 - acc: 0.6069 - val_loss: 1.1493 - val_acc: 0.5257\n",
      "Epoch 123/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.8798 - acc: 0.6122 - val_loss: 1.1784 - val_acc: 0.4804\n",
      "Epoch 124/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8743 - acc: 0.6178 - val_loss: 1.1536 - val_acc: 0.5287\n",
      "Epoch 125/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8883 - acc: 0.6013 - val_loss: 1.1557 - val_acc: 0.5589\n",
      "Epoch 126/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.8552 - acc: 0.6245 - val_loss: 1.1489 - val_acc: 0.5498\n",
      "Epoch 127/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.8838 - acc: 0.6002 - val_loss: 1.1181 - val_acc: 0.5589\n",
      "Epoch 128/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.8352 - acc: 0.6368 - val_loss: 1.1468 - val_acc: 0.5529\n",
      "Epoch 129/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.8790 - acc: 0.6009 - val_loss: 1.1608 - val_acc: 0.5680\n",
      "Epoch 130/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.8635 - acc: 0.6284 - val_loss: 1.2485 - val_acc: 0.5106\n",
      "Epoch 131/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.8773 - acc: 0.6066 - val_loss: 1.2018 - val_acc: 0.5045\n",
      "Epoch 132/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.8621 - acc: 0.6266 - val_loss: 1.1850 - val_acc: 0.5347\n",
      "Epoch 133/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.8752 - acc: 0.6224 - val_loss: 1.1837 - val_acc: 0.5106\n",
      "Epoch 134/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8468 - acc: 0.6326 - val_loss: 1.2334 - val_acc: 0.5136\n",
      "Epoch 135/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.8769 - acc: 0.6125 - val_loss: 1.1305 - val_acc: 0.5619\n",
      "Epoch 136/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.8260 - acc: 0.6421 - val_loss: 1.1642 - val_acc: 0.5166\n",
      "Epoch 137/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8496 - acc: 0.6270 - val_loss: 1.2481 - val_acc: 0.5408\n",
      "Epoch 138/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8668 - acc: 0.6203 - val_loss: 1.1842 - val_acc: 0.4955\n",
      "Epoch 139/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8407 - acc: 0.6249 - val_loss: 1.2214 - val_acc: 0.4894\n",
      "Epoch 140/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8104 - acc: 0.6358 - val_loss: 1.1728 - val_acc: 0.5529\n",
      "Epoch 141/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.8441 - acc: 0.6287 - val_loss: 1.2534 - val_acc: 0.5287\n",
      "Epoch 142/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8293 - acc: 0.6383 - val_loss: 1.2846 - val_acc: 0.5136\n",
      "Epoch 143/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.8366 - acc: 0.6435 - val_loss: 1.1927 - val_acc: 0.5589\n",
      "Epoch 144/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8370 - acc: 0.6252 - val_loss: 1.2662 - val_acc: 0.5166\n",
      "Epoch 145/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8119 - acc: 0.6361 - val_loss: 1.2461 - val_acc: 0.4532\n",
      "Epoch 146/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.8589 - acc: 0.6245 - val_loss: 1.2343 - val_acc: 0.4773\n",
      "Epoch 147/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8239 - acc: 0.6492 - val_loss: 1.3130 - val_acc: 0.5680\n",
      "Epoch 148/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8409 - acc: 0.6354 - val_loss: 1.2647 - val_acc: 0.4924\n",
      "Epoch 149/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7876 - acc: 0.6559 - val_loss: 1.2474 - val_acc: 0.4592\n",
      "Epoch 150/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8180 - acc: 0.6552 - val_loss: 1.2305 - val_acc: 0.5468\n",
      "Epoch 151/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.8171 - acc: 0.6351 - val_loss: 1.1803 - val_acc: 0.5106\n",
      "Epoch 152/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7808 - acc: 0.6534 - val_loss: 1.3220 - val_acc: 0.5619\n",
      "Epoch 153/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8159 - acc: 0.6513 - val_loss: 1.2214 - val_acc: 0.5619\n",
      "Epoch 154/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8266 - acc: 0.6291 - val_loss: 1.2902 - val_acc: 0.5347\n",
      "Epoch 155/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8324 - acc: 0.6302 - val_loss: 1.1854 - val_acc: 0.5498\n",
      "Epoch 156/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.7857 - acc: 0.6619 - val_loss: 1.2384 - val_acc: 0.5740\n",
      "Epoch 157/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8037 - acc: 0.6439 - val_loss: 1.2884 - val_acc: 0.5227\n",
      "Epoch 158/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8051 - acc: 0.6576 - val_loss: 1.2720 - val_acc: 0.4955\n",
      "Epoch 159/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.8208 - acc: 0.6323 - val_loss: 1.2335 - val_acc: 0.5076\n",
      "Epoch 160/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8255 - acc: 0.6460 - val_loss: 1.2860 - val_acc: 0.5136\n",
      "Epoch 161/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7653 - acc: 0.6615 - val_loss: 1.3349 - val_acc: 0.4864\n",
      "Epoch 162/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.8132 - acc: 0.6495 - val_loss: 1.3250 - val_acc: 0.5136\n",
      "Epoch 163/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.7925 - acc: 0.6404 - val_loss: 1.3112 - val_acc: 0.5136\n",
      "Epoch 164/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.7846 - acc: 0.6689 - val_loss: 1.4045 - val_acc: 0.4985\n",
      "Epoch 165/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7990 - acc: 0.6375 - val_loss: 1.3404 - val_acc: 0.5317\n",
      "Epoch 166/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.7863 - acc: 0.6692 - val_loss: 1.3411 - val_acc: 0.5196\n",
      "Epoch 167/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7699 - acc: 0.6633 - val_loss: 1.2989 - val_acc: 0.5589\n",
      "Epoch 168/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7614 - acc: 0.6721 - val_loss: 1.3167 - val_acc: 0.5498\n",
      "Epoch 169/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.8561 - acc: 0.6256 - val_loss: 1.2859 - val_acc: 0.5166\n",
      "Epoch 170/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7512 - acc: 0.6774 - val_loss: 1.4745 - val_acc: 0.4864\n",
      "Epoch 171/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7405 - acc: 0.6805 - val_loss: 1.3735 - val_acc: 0.4924\n",
      "Epoch 172/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.8014 - acc: 0.6471 - val_loss: 1.4463 - val_acc: 0.4411\n",
      "Epoch 173/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7767 - acc: 0.6576 - val_loss: 1.3530 - val_acc: 0.5136\n",
      "Epoch 174/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7716 - acc: 0.6611 - val_loss: 1.3890 - val_acc: 0.5136\n",
      "Epoch 175/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7838 - acc: 0.6488 - val_loss: 1.3940 - val_acc: 0.4834\n",
      "Epoch 176/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7544 - acc: 0.6664 - val_loss: 1.5077 - val_acc: 0.4260\n",
      "Epoch 177/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7705 - acc: 0.6700 - val_loss: 1.4317 - val_acc: 0.4804\n",
      "Epoch 178/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.7252 - acc: 0.6890 - val_loss: 1.3746 - val_acc: 0.4955\n",
      "Epoch 179/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7783 - acc: 0.6520 - val_loss: 1.2998 - val_acc: 0.5468\n",
      "Epoch 180/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7337 - acc: 0.6717 - val_loss: 1.3982 - val_acc: 0.5045\n",
      "Epoch 181/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7777 - acc: 0.6604 - val_loss: 1.3855 - val_acc: 0.4924\n",
      "Epoch 182/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7657 - acc: 0.6555 - val_loss: 1.3584 - val_acc: 0.4743\n",
      "Epoch 183/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7103 - acc: 0.6999 - val_loss: 1.3367 - val_acc: 0.5166\n",
      "Epoch 184/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7533 - acc: 0.6738 - val_loss: 1.3705 - val_acc: 0.5680\n",
      "Epoch 185/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7368 - acc: 0.6805 - val_loss: 1.4714 - val_acc: 0.4653\n",
      "Epoch 186/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7329 - acc: 0.6837 - val_loss: 1.5355 - val_acc: 0.4502\n",
      "Epoch 187/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.7571 - acc: 0.6678 - val_loss: 1.4483 - val_acc: 0.5045\n",
      "Epoch 188/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.7372 - acc: 0.6816 - val_loss: 1.5073 - val_acc: 0.5227\n",
      "Epoch 189/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.7745 - acc: 0.6590 - val_loss: 1.4076 - val_acc: 0.5166\n",
      "Epoch 190/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6834 - acc: 0.7098 - val_loss: 1.4719 - val_acc: 0.5015\n",
      "Epoch 191/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7492 - acc: 0.6749 - val_loss: 1.4952 - val_acc: 0.5408\n",
      "Epoch 192/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7493 - acc: 0.6692 - val_loss: 1.5085 - val_acc: 0.4592\n",
      "Epoch 193/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.7370 - acc: 0.6904 - val_loss: 1.5174 - val_acc: 0.4743\n",
      "Epoch 194/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.6848 - acc: 0.7013 - val_loss: 1.6110 - val_acc: 0.4532\n",
      "Epoch 195/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7363 - acc: 0.6816 - val_loss: 1.3791 - val_acc: 0.4955\n",
      "Epoch 196/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7193 - acc: 0.6819 - val_loss: 1.3669 - val_acc: 0.4924\n",
      "Epoch 197/500\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.7236 - acc: 0.6851 - val_loss: 1.5068 - val_acc: 0.4471\n",
      "Epoch 198/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.7375 - acc: 0.6830 - val_loss: 1.4021 - val_acc: 0.5287\n",
      "Epoch 199/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6917 - acc: 0.6971 - val_loss: 1.4865 - val_acc: 0.4622\n",
      "Epoch 200/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7232 - acc: 0.6925 - val_loss: 1.6137 - val_acc: 0.4592\n",
      "Epoch 201/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.7125 - acc: 0.6978 - val_loss: 1.4591 - val_acc: 0.5801\n",
      "Epoch 202/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6776 - acc: 0.7115 - val_loss: 1.5311 - val_acc: 0.4955\n",
      "Epoch 203/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7258 - acc: 0.6967 - val_loss: 1.4116 - val_acc: 0.5196\n",
      "Epoch 204/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6796 - acc: 0.7024 - val_loss: 1.5452 - val_acc: 0.5498\n",
      "Epoch 205/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7085 - acc: 0.7002 - val_loss: 1.7072 - val_acc: 0.4743\n",
      "Epoch 206/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.7291 - acc: 0.6809 - val_loss: 1.7331 - val_acc: 0.4230\n",
      "Epoch 207/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.6975 - acc: 0.7038 - val_loss: 1.5704 - val_acc: 0.5347\n",
      "Epoch 208/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7052 - acc: 0.6995 - val_loss: 1.5984 - val_acc: 0.4713\n",
      "Epoch 209/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7141 - acc: 0.6978 - val_loss: 1.6114 - val_acc: 0.4502\n",
      "Epoch 210/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6690 - acc: 0.7080 - val_loss: 1.6135 - val_acc: 0.4955\n",
      "Epoch 211/500\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.7046 - acc: 0.6886 - val_loss: 1.7742 - val_acc: 0.4381\n",
      "Epoch 212/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.6835 - acc: 0.6981 - val_loss: 1.5681 - val_acc: 0.4743\n",
      "Epoch 213/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.7128 - acc: 0.6988 - val_loss: 1.6193 - val_acc: 0.5166\n",
      "Epoch 214/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6996 - acc: 0.7038 - val_loss: 1.5784 - val_acc: 0.4773\n",
      "Epoch 215/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.6705 - acc: 0.7154 - val_loss: 1.7040 - val_acc: 0.5015\n",
      "Epoch 216/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6943 - acc: 0.6971 - val_loss: 1.5860 - val_acc: 0.4683\n",
      "Epoch 217/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.7125 - acc: 0.6928 - val_loss: 1.5263 - val_acc: 0.5045\n",
      "Epoch 218/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6399 - acc: 0.7305 - val_loss: 1.6285 - val_acc: 0.4592\n",
      "Epoch 219/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7603 - acc: 0.6904 - val_loss: 1.4689 - val_acc: 0.5015\n",
      "Epoch 220/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6161 - acc: 0.7425 - val_loss: 1.4381 - val_acc: 0.5347\n",
      "Epoch 221/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6930 - acc: 0.7020 - val_loss: 1.4845 - val_acc: 0.4894\n",
      "Epoch 222/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6963 - acc: 0.6872 - val_loss: 1.4968 - val_acc: 0.5136\n",
      "Epoch 223/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6419 - acc: 0.7260 - val_loss: 1.5971 - val_acc: 0.5287\n",
      "Epoch 224/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6471 - acc: 0.7260 - val_loss: 1.4707 - val_acc: 0.5076\n",
      "Epoch 225/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6693 - acc: 0.7112 - val_loss: 1.5884 - val_acc: 0.5015\n",
      "Epoch 226/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6737 - acc: 0.7122 - val_loss: 1.5454 - val_acc: 0.4653\n",
      "Epoch 227/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6602 - acc: 0.7161 - val_loss: 1.4816 - val_acc: 0.5196\n",
      "Epoch 228/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6532 - acc: 0.7147 - val_loss: 1.4747 - val_acc: 0.5378\n",
      "Epoch 229/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6637 - acc: 0.7055 - val_loss: 1.6061 - val_acc: 0.4773\n",
      "Epoch 230/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6869 - acc: 0.6995 - val_loss: 1.6636 - val_acc: 0.5015\n",
      "Epoch 231/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6372 - acc: 0.7270 - val_loss: 1.5003 - val_acc: 0.5498\n",
      "Epoch 232/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6529 - acc: 0.7200 - val_loss: 1.5405 - val_acc: 0.5045\n",
      "Epoch 233/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.6770 - acc: 0.7048 - val_loss: 1.6870 - val_acc: 0.4683\n",
      "Epoch 234/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6688 - acc: 0.7214 - val_loss: 1.5758 - val_acc: 0.4622\n",
      "Epoch 235/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.7049 - acc: 0.6974 - val_loss: 1.5469 - val_acc: 0.5559\n",
      "Epoch 236/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6189 - acc: 0.7372 - val_loss: 1.9967 - val_acc: 0.4199\n",
      "Epoch 237/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6743 - acc: 0.7062 - val_loss: 1.7114 - val_acc: 0.4381\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6427 - acc: 0.7302 - val_loss: 1.7539 - val_acc: 0.5045\n",
      "Epoch 239/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6389 - acc: 0.7319 - val_loss: 1.8631 - val_acc: 0.4985\n",
      "Epoch 240/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6481 - acc: 0.7193 - val_loss: 1.6074 - val_acc: 0.5106\n",
      "Epoch 241/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5993 - acc: 0.7422 - val_loss: 1.7651 - val_acc: 0.5106\n",
      "Epoch 242/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6939 - acc: 0.7094 - val_loss: 1.8605 - val_acc: 0.4018\n",
      "Epoch 243/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6457 - acc: 0.7203 - val_loss: 1.7802 - val_acc: 0.5166\n",
      "Epoch 244/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6269 - acc: 0.7231 - val_loss: 1.8056 - val_acc: 0.4894\n",
      "Epoch 245/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6680 - acc: 0.7094 - val_loss: 1.7607 - val_acc: 0.4683\n",
      "Epoch 246/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6200 - acc: 0.7425 - val_loss: 1.8539 - val_acc: 0.4471\n",
      "Epoch 247/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.6367 - acc: 0.7196 - val_loss: 1.7689 - val_acc: 0.4532\n",
      "Epoch 248/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.6148 - acc: 0.7411 - val_loss: 1.8639 - val_acc: 0.4955\n",
      "Epoch 249/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6468 - acc: 0.7256 - val_loss: 1.8137 - val_acc: 0.4290\n",
      "Epoch 250/500\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.6279 - acc: 0.7408 - val_loss: 1.7785 - val_acc: 0.4864\n",
      "Epoch 251/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6250 - acc: 0.7260 - val_loss: 2.0320 - val_acc: 0.4562\n",
      "Epoch 252/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6369 - acc: 0.7238 - val_loss: 1.7850 - val_acc: 0.5166\n",
      "Epoch 253/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.6649 - acc: 0.7242 - val_loss: 1.7004 - val_acc: 0.5136\n",
      "Epoch 254/500\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.5623 - acc: 0.758 - 0s 71us/step - loss: 0.5645 - acc: 0.7559 - val_loss: 1.7354 - val_acc: 0.5136\n",
      "Epoch 255/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.6717 - acc: 0.7186 - val_loss: 1.5574 - val_acc: 0.5408\n",
      "Epoch 256/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.6109 - acc: 0.7457 - val_loss: 1.7196 - val_acc: 0.4924\n",
      "Epoch 257/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5607 - acc: 0.7665 - val_loss: 1.8260 - val_acc: 0.5196\n",
      "Epoch 258/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6200 - acc: 0.7400 - val_loss: 1.6950 - val_acc: 0.5076\n",
      "Epoch 259/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6283 - acc: 0.7327 - val_loss: 1.8046 - val_acc: 0.5015\n",
      "Epoch 260/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5975 - acc: 0.7376 - val_loss: 1.5951 - val_acc: 0.5106\n",
      "Epoch 261/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.5487 - acc: 0.7626 - val_loss: 1.8886 - val_acc: 0.4411\n",
      "Epoch 262/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.6466 - acc: 0.7238 - val_loss: 1.6932 - val_acc: 0.5106\n",
      "Epoch 263/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5986 - acc: 0.7425 - val_loss: 1.6972 - val_acc: 0.4924\n",
      "Epoch 264/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6384 - acc: 0.7231 - val_loss: 1.8978 - val_acc: 0.5076\n",
      "Epoch 265/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.5832 - acc: 0.7622 - val_loss: 1.9515 - val_acc: 0.4985\n",
      "Epoch 266/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.6269 - acc: 0.7224 - val_loss: 2.0843 - val_acc: 0.4502\n",
      "Epoch 267/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5781 - acc: 0.7482 - val_loss: 1.9429 - val_acc: 0.4924\n",
      "Epoch 268/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6025 - acc: 0.7506 - val_loss: 2.1234 - val_acc: 0.4804\n",
      "Epoch 269/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.5964 - acc: 0.7439 - val_loss: 1.9148 - val_acc: 0.4864\n",
      "Epoch 270/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5964 - acc: 0.7446 - val_loss: 1.9563 - val_acc: 0.4532\n",
      "Epoch 271/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5769 - acc: 0.7541 - val_loss: 1.9391 - val_acc: 0.5015\n",
      "Epoch 272/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6172 - acc: 0.7517 - val_loss: 1.8390 - val_acc: 0.5076\n",
      "Epoch 273/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6020 - acc: 0.7467 - val_loss: 1.8740 - val_acc: 0.4743\n",
      "Epoch 274/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5460 - acc: 0.7739 - val_loss: 2.1107 - val_acc: 0.4139\n",
      "Epoch 275/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6022 - acc: 0.7408 - val_loss: 2.0345 - val_acc: 0.4773\n",
      "Epoch 276/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6000 - acc: 0.7517 - val_loss: 1.8174 - val_acc: 0.5106\n",
      "Epoch 277/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5431 - acc: 0.7784 - val_loss: 1.7750 - val_acc: 0.4773\n",
      "Epoch 278/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6114 - acc: 0.7341 - val_loss: 1.6828 - val_acc: 0.5498\n",
      "Epoch 279/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5448 - acc: 0.7665 - val_loss: 1.8038 - val_acc: 0.5196\n",
      "Epoch 280/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.5945 - acc: 0.7460 - val_loss: 1.8498 - val_acc: 0.5015\n",
      "Epoch 281/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.5282 - acc: 0.7795 - val_loss: 1.9563 - val_acc: 0.5257\n",
      "Epoch 282/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6069 - acc: 0.7524 - val_loss: 1.9004 - val_acc: 0.4864\n",
      "Epoch 283/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6212 - acc: 0.7260 - val_loss: 1.9195 - val_acc: 0.4562\n",
      "Epoch 284/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5746 - acc: 0.7629 - val_loss: 2.1419 - val_acc: 0.4683\n",
      "Epoch 285/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5620 - acc: 0.7577 - val_loss: 2.1153 - val_acc: 0.4773\n",
      "Epoch 286/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5767 - acc: 0.7552 - val_loss: 1.7477 - val_acc: 0.5378\n",
      "Epoch 287/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5746 - acc: 0.7587 - val_loss: 1.8436 - val_acc: 0.4894\n",
      "Epoch 288/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5199 - acc: 0.7802 - val_loss: 1.9738 - val_acc: 0.5408\n",
      "Epoch 289/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.6303 - acc: 0.7298 - val_loss: 1.7426 - val_acc: 0.5076\n",
      "Epoch 290/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5475 - acc: 0.7749 - val_loss: 1.8175 - val_acc: 0.5045\n",
      "Epoch 291/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.5715 - acc: 0.7566 - val_loss: 1.6978 - val_acc: 0.4713\n",
      "Epoch 292/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5713 - acc: 0.7538 - val_loss: 1.9378 - val_acc: 0.4381\n",
      "Epoch 293/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5681 - acc: 0.7714 - val_loss: 1.9440 - val_acc: 0.5015\n",
      "Epoch 294/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5570 - acc: 0.7710 - val_loss: 1.7789 - val_acc: 0.5227\n",
      "Epoch 295/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5817 - acc: 0.7478 - val_loss: 1.8579 - val_acc: 0.4804\n",
      "Epoch 296/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5260 - acc: 0.7848 - val_loss: 1.8572 - val_acc: 0.5076\n",
      "Epoch 297/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5597 - acc: 0.7746 - val_loss: 1.9937 - val_acc: 0.4834\n",
      "Epoch 298/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5727 - acc: 0.7563 - val_loss: 1.8865 - val_acc: 0.5045\n",
      "Epoch 299/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5760 - acc: 0.7633 - val_loss: 1.9737 - val_acc: 0.4894\n",
      "Epoch 300/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5304 - acc: 0.7844 - val_loss: 1.9783 - val_acc: 0.4804\n",
      "Epoch 301/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5021 - acc: 0.7883 - val_loss: 2.1685 - val_acc: 0.4985\n",
      "Epoch 302/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5486 - acc: 0.7689 - val_loss: 2.2915 - val_acc: 0.4562\n",
      "Epoch 303/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5857 - acc: 0.7450 - val_loss: 2.0974 - val_acc: 0.4622\n",
      "Epoch 304/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5337 - acc: 0.7746 - val_loss: 2.3328 - val_acc: 0.4350\n",
      "Epoch 305/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5217 - acc: 0.7742 - val_loss: 1.8750 - val_acc: 0.5106\n",
      "Epoch 306/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5706 - acc: 0.7559 - val_loss: 1.8523 - val_acc: 0.4653\n",
      "Epoch 307/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5001 - acc: 0.7915 - val_loss: 1.8084 - val_acc: 0.5076\n",
      "Epoch 308/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.6111 - acc: 0.7460 - val_loss: 1.8257 - val_acc: 0.5468\n",
      "Epoch 309/500\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 0.5051 - acc: 0.7890 - val_loss: 2.0514 - val_acc: 0.5166\n",
      "Epoch 310/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5669 - acc: 0.7605 - val_loss: 2.2816 - val_acc: 0.4350\n",
      "Epoch 311/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5186 - acc: 0.7760 - val_loss: 2.0864 - val_acc: 0.4955\n",
      "Epoch 312/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5053 - acc: 0.7883 - val_loss: 1.7980 - val_acc: 0.5287\n",
      "Epoch 313/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4991 - acc: 0.7911 - val_loss: 2.0456 - val_acc: 0.4924\n",
      "Epoch 314/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5097 - acc: 0.7890 - val_loss: 2.0788 - val_acc: 0.4411\n",
      "Epoch 315/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5480 - acc: 0.7647 - val_loss: 1.7907 - val_acc: 0.5045\n",
      "Epoch 316/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.5173 - acc: 0.7841 - val_loss: 2.0424 - val_acc: 0.5227\n",
      "Epoch 317/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5756 - acc: 0.7573 - val_loss: 2.0322 - val_acc: 0.4260\n",
      "Epoch 318/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5267 - acc: 0.7855 - val_loss: 2.2314 - val_acc: 0.4320\n",
      "Epoch 319/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4769 - acc: 0.8119 - val_loss: 2.6472 - val_acc: 0.4653\n",
      "Epoch 320/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5512 - acc: 0.7721 - val_loss: 2.0682 - val_acc: 0.4713\n",
      "Epoch 321/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5296 - acc: 0.7841 - val_loss: 2.3055 - val_acc: 0.4985\n",
      "Epoch 322/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5335 - acc: 0.7696 - val_loss: 2.1159 - val_acc: 0.4713\n",
      "Epoch 323/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4977 - acc: 0.7915 - val_loss: 1.9078 - val_acc: 0.5257\n",
      "Epoch 324/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5636 - acc: 0.7651 - val_loss: 2.0432 - val_acc: 0.4894\n",
      "Epoch 325/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5192 - acc: 0.7894 - val_loss: 2.2433 - val_acc: 0.4653\n",
      "Epoch 326/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.5235 - acc: 0.7774 - val_loss: 2.1085 - val_acc: 0.4230\n",
      "Epoch 327/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.5063 - acc: 0.7932 - val_loss: 2.3733 - val_acc: 0.4079\n",
      "Epoch 328/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.5090 - acc: 0.8013 - val_loss: 2.1220 - val_acc: 0.4350\n",
      "Epoch 329/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.5403 - acc: 0.7791 - val_loss: 1.8149 - val_acc: 0.4713\n",
      "Epoch 330/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5031 - acc: 0.7915 - val_loss: 2.0905 - val_acc: 0.4743\n",
      "Epoch 331/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.5084 - acc: 0.8031 - val_loss: 2.1715 - val_acc: 0.4260\n",
      "Epoch 332/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.5162 - acc: 0.7809 - val_loss: 2.0697 - val_acc: 0.5136\n",
      "Epoch 333/500\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.5398 - acc: 0.7728 - val_loss: 2.2606 - val_acc: 0.4502\n",
      "Epoch 334/500\n",
      "2839/2839 [==============================] - 0s 97us/step - loss: 0.4607 - acc: 0.8101 - val_loss: 2.4759 - val_acc: 0.4683\n",
      "Epoch 335/500\n",
      "2839/2839 [==============================] - 0s 97us/step - loss: 0.5384 - acc: 0.7696 - val_loss: 2.2520 - val_acc: 0.4381\n",
      "Epoch 336/500\n",
      "2839/2839 [==============================] - 0s 88us/step - loss: 0.4854 - acc: 0.8010 - val_loss: 2.2719 - val_acc: 0.4048\n",
      "Epoch 337/500\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.5023 - acc: 0.7894 - val_loss: 2.1056 - val_acc: 0.4471\n",
      "Epoch 338/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.4612 - acc: 0.8147 - val_loss: 2.0009 - val_acc: 0.4773\n",
      "Epoch 339/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5168 - acc: 0.7802 - val_loss: 2.1432 - val_acc: 0.4622\n",
      "Epoch 340/500\n",
      "2839/2839 [==============================] - 0s 101us/step - loss: 0.5351 - acc: 0.7714 - val_loss: 1.9061 - val_acc: 0.4773\n",
      "Epoch 341/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.4988 - acc: 0.7961 - val_loss: 2.1585 - val_acc: 0.4834\n",
      "Epoch 342/500\n",
      "2839/2839 [==============================] - 0s 88us/step - loss: 0.4472 - acc: 0.8207 - val_loss: 2.9242 - val_acc: 0.4230\n",
      "Epoch 343/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.5521 - acc: 0.7827 - val_loss: 2.6172 - val_acc: 0.4834\n",
      "Epoch 344/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.5361 - acc: 0.7784 - val_loss: 2.2271 - val_acc: 0.4804\n",
      "Epoch 345/500\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.4577 - acc: 0.8288 - val_loss: 2.5311 - val_acc: 0.4471\n",
      "Epoch 346/500\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.4876 - acc: 0.7978 - val_loss: 2.4000 - val_acc: 0.4290\n",
      "Epoch 347/500\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.5132 - acc: 0.7781 - val_loss: 2.2654 - val_acc: 0.4773\n",
      "Epoch 348/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5445 - acc: 0.7770 - val_loss: 2.4667 - val_acc: 0.4441\n",
      "Epoch 349/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4417 - acc: 0.8214 - val_loss: 2.4565 - val_acc: 0.4955\n",
      "Epoch 350/500\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.4866 - acc: 0.7922 - val_loss: 2.2199 - val_acc: 0.5136\n",
      "Epoch 351/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.4612 - acc: 0.8077 - val_loss: 2.1585 - val_acc: 0.5136\n",
      "Epoch 352/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5026 - acc: 0.7834 - val_loss: 2.0691 - val_acc: 0.5106\n",
      "Epoch 353/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5029 - acc: 0.7844 - val_loss: 2.2337 - val_acc: 0.4713\n",
      "Epoch 354/500\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 0.4189 - acc: 0.8320 - val_loss: 2.5551 - val_acc: 0.4502\n",
      "Epoch 355/500\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.5164 - acc: 0.7770 - val_loss: 2.4252 - val_acc: 0.4713\n",
      "Epoch 356/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.4789 - acc: 0.8137 - val_loss: 2.7821 - val_acc: 0.4471\n",
      "Epoch 357/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4678 - acc: 0.8112 - val_loss: 2.1480 - val_acc: 0.5045\n",
      "Epoch 358/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4535 - acc: 0.8094 - val_loss: 2.0908 - val_acc: 0.4743\n",
      "Epoch 359/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5030 - acc: 0.7876 - val_loss: 2.1826 - val_acc: 0.5196\n",
      "Epoch 360/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4846 - acc: 0.8123 - val_loss: 2.3796 - val_acc: 0.4713\n",
      "Epoch 361/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5290 - acc: 0.7848 - val_loss: 2.0491 - val_acc: 0.4985\n",
      "Epoch 362/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.4387 - acc: 0.8263 - val_loss: 2.1797 - val_acc: 0.4834\n",
      "Epoch 363/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4925 - acc: 0.7950 - val_loss: 2.2366 - val_acc: 0.5106\n",
      "Epoch 364/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5054 - acc: 0.8038 - val_loss: 2.5726 - val_acc: 0.4592\n",
      "Epoch 365/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4296 - acc: 0.8232 - val_loss: 2.2780 - val_acc: 0.4592\n",
      "Epoch 366/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4914 - acc: 0.8024 - val_loss: 2.2136 - val_acc: 0.5106\n",
      "Epoch 367/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4756 - acc: 0.8052 - val_loss: 2.2147 - val_acc: 0.5196\n",
      "Epoch 368/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4367 - acc: 0.8295 - val_loss: 2.4412 - val_acc: 0.4834\n",
      "Epoch 369/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5063 - acc: 0.7869 - val_loss: 2.1422 - val_acc: 0.4743\n",
      "Epoch 370/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4109 - acc: 0.8348 - val_loss: 2.1981 - val_acc: 0.5317\n",
      "Epoch 371/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5104 - acc: 0.7837 - val_loss: 2.3241 - val_acc: 0.4955\n",
      "Epoch 372/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4095 - acc: 0.8411 - val_loss: 2.8449 - val_acc: 0.4290\n",
      "Epoch 373/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5620 - acc: 0.7594 - val_loss: 2.4797 - val_acc: 0.4743\n",
      "Epoch 374/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4409 - acc: 0.8281 - val_loss: 2.2555 - val_acc: 0.4955\n",
      "Epoch 375/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4122 - acc: 0.8380 - val_loss: 2.2849 - val_acc: 0.4622\n",
      "Epoch 376/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4825 - acc: 0.8013 - val_loss: 2.4244 - val_acc: 0.4381\n",
      "Epoch 377/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4835 - acc: 0.7978 - val_loss: 2.4549 - val_acc: 0.4804\n",
      "Epoch 378/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4499 - acc: 0.8119 - val_loss: 2.0950 - val_acc: 0.5287\n",
      "Epoch 379/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4556 - acc: 0.8084 - val_loss: 2.2599 - val_acc: 0.4985\n",
      "Epoch 380/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4273 - acc: 0.8288 - val_loss: 2.3825 - val_acc: 0.4683\n",
      "Epoch 381/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4443 - acc: 0.8267 - val_loss: 2.6344 - val_acc: 0.4260\n",
      "Epoch 382/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4581 - acc: 0.8172 - val_loss: 2.2843 - val_acc: 0.4683\n",
      "Epoch 383/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4661 - acc: 0.8063 - val_loss: 2.2187 - val_acc: 0.4713\n",
      "Epoch 384/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3949 - acc: 0.8464 - val_loss: 2.7758 - val_acc: 0.4230\n",
      "Epoch 385/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4863 - acc: 0.8020 - val_loss: 2.6056 - val_acc: 0.4350\n",
      "Epoch 386/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4199 - acc: 0.8278 - val_loss: 3.1043 - val_acc: 0.4683\n",
      "Epoch 387/500\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.5183 - acc: 0.7918 - val_loss: 2.4135 - val_acc: 0.5045\n",
      "Epoch 388/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.3836 - acc: 0.8531 - val_loss: 2.2597 - val_acc: 0.4713\n",
      "Epoch 389/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4633 - acc: 0.8087 - val_loss: 2.5937 - val_acc: 0.4381\n",
      "Epoch 390/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3877 - acc: 0.8359 - val_loss: 2.8319 - val_acc: 0.4139\n",
      "Epoch 391/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4479 - acc: 0.8066 - val_loss: 2.7484 - val_acc: 0.4411\n",
      "Epoch 392/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3819 - acc: 0.8457 - val_loss: 2.5350 - val_acc: 0.4260\n",
      "Epoch 393/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5022 - acc: 0.7982 - val_loss: 2.4021 - val_acc: 0.4653\n",
      "Epoch 394/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4045 - acc: 0.8429 - val_loss: 2.4063 - val_acc: 0.4653\n",
      "Epoch 395/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4513 - acc: 0.8116 - val_loss: 2.5015 - val_acc: 0.4924\n",
      "Epoch 396/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.4537 - acc: 0.8253 - val_loss: 2.2771 - val_acc: 0.4743\n",
      "Epoch 397/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3905 - acc: 0.8454 - val_loss: 2.6742 - val_acc: 0.4471\n",
      "Epoch 398/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3952 - acc: 0.8440 - val_loss: 2.2754 - val_acc: 0.4743\n",
      "Epoch 399/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5223 - acc: 0.7774 - val_loss: 2.5183 - val_acc: 0.4683\n",
      "Epoch 400/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3940 - acc: 0.8471 - val_loss: 2.7899 - val_acc: 0.4743\n",
      "Epoch 401/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4837 - acc: 0.8020 - val_loss: 2.6362 - val_acc: 0.4622\n",
      "Epoch 402/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4075 - acc: 0.8397 - val_loss: 2.3120 - val_acc: 0.5196\n",
      "Epoch 403/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4577 - acc: 0.8352 - val_loss: 2.6603 - val_acc: 0.4562\n",
      "Epoch 404/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3927 - acc: 0.8510 - val_loss: 2.4280 - val_acc: 0.4653\n",
      "Epoch 405/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4399 - acc: 0.8144 - val_loss: 2.3479 - val_acc: 0.4471\n",
      "Epoch 406/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3773 - acc: 0.8507 - val_loss: 2.5865 - val_acc: 0.4562\n",
      "Epoch 407/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4022 - acc: 0.8390 - val_loss: 2.2560 - val_acc: 0.4743\n",
      "Epoch 408/500\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.5065 - acc: 0.7922 - val_loss: 2.4518 - val_acc: 0.4471\n",
      "Epoch 409/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3928 - acc: 0.8436 - val_loss: 2.4785 - val_acc: 0.5045\n",
      "Epoch 410/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3626 - acc: 0.8538 - val_loss: 2.9901 - val_acc: 0.4562\n",
      "Epoch 411/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4577 - acc: 0.8098 - val_loss: 2.7092 - val_acc: 0.4622\n",
      "Epoch 412/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3714 - acc: 0.8503 - val_loss: 3.0206 - val_acc: 0.4864\n",
      "Epoch 413/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4647 - acc: 0.8108 - val_loss: 2.5734 - val_acc: 0.4713\n",
      "Epoch 414/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3808 - acc: 0.8542 - val_loss: 2.5640 - val_acc: 0.4864\n",
      "Epoch 415/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4802 - acc: 0.7939 - val_loss: 2.4928 - val_acc: 0.4502\n",
      "Epoch 416/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4029 - acc: 0.8415 - val_loss: 2.6369 - val_acc: 0.4653\n",
      "Epoch 417/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3876 - acc: 0.8482 - val_loss: 2.7925 - val_acc: 0.4834\n",
      "Epoch 418/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4364 - acc: 0.8161 - val_loss: 3.1048 - val_acc: 0.4350\n",
      "Epoch 419/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3986 - acc: 0.8404 - val_loss: 3.1970 - val_acc: 0.4471\n",
      "Epoch 420/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4193 - acc: 0.8218 - val_loss: 3.1052 - val_acc: 0.4592\n",
      "Epoch 421/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4868 - acc: 0.8045 - val_loss: 2.7952 - val_acc: 0.4985\n",
      "Epoch 422/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.3856 - acc: 0.8503 - val_loss: 2.6730 - val_acc: 0.5166\n",
      "Epoch 423/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3823 - acc: 0.8447 - val_loss: 3.2170 - val_acc: 0.4471\n",
      "Epoch 424/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4378 - acc: 0.8197 - val_loss: 2.8479 - val_acc: 0.4169\n",
      "Epoch 425/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3743 - acc: 0.8489 - val_loss: 2.5873 - val_acc: 0.4985\n",
      "Epoch 426/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4639 - acc: 0.8101 - val_loss: 2.3695 - val_acc: 0.5106\n",
      "Epoch 427/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3758 - acc: 0.8584 - val_loss: 3.1325 - val_acc: 0.4743\n",
      "Epoch 428/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4865 - acc: 0.8010 - val_loss: 2.5692 - val_acc: 0.4683\n",
      "Epoch 429/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3609 - acc: 0.8584 - val_loss: 3.0148 - val_acc: 0.4955\n",
      "Epoch 430/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4081 - acc: 0.8344 - val_loss: 3.0336 - val_acc: 0.4653\n",
      "Epoch 431/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4139 - acc: 0.8316 - val_loss: 2.8482 - val_acc: 0.4743\n",
      "Epoch 432/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3744 - acc: 0.8580 - val_loss: 3.1191 - val_acc: 0.4592\n",
      "Epoch 433/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4124 - acc: 0.8528 - val_loss: 2.8308 - val_acc: 0.4713\n",
      "Epoch 434/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4579 - acc: 0.8070 - val_loss: 2.4476 - val_acc: 0.4804\n",
      "Epoch 435/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.3597 - acc: 0.8633 - val_loss: 2.8053 - val_acc: 0.4894\n",
      "Epoch 436/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3927 - acc: 0.8369 - val_loss: 3.5460 - val_acc: 0.4411\n",
      "Epoch 437/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4409 - acc: 0.8172 - val_loss: 2.8761 - val_acc: 0.4894\n",
      "Epoch 438/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3290 - acc: 0.8609 - val_loss: 2.7493 - val_acc: 0.4924\n",
      "Epoch 439/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4153 - acc: 0.8313 - val_loss: 3.3664 - val_acc: 0.4562\n",
      "Epoch 440/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4557 - acc: 0.8186 - val_loss: 3.0549 - val_acc: 0.4592\n",
      "Epoch 441/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3391 - acc: 0.8679 - val_loss: 2.7226 - val_acc: 0.4562\n",
      "Epoch 442/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4212 - acc: 0.8337 - val_loss: 2.2866 - val_acc: 0.4834\n",
      "Epoch 443/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4468 - acc: 0.8355 - val_loss: 3.2350 - val_acc: 0.4592\n",
      "Epoch 444/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3927 - acc: 0.8404 - val_loss: 2.9468 - val_acc: 0.4532\n",
      "Epoch 445/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3620 - acc: 0.8510 - val_loss: 3.6206 - val_acc: 0.4653\n",
      "Epoch 446/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4564 - acc: 0.8207 - val_loss: 2.9530 - val_acc: 0.4381\n",
      "Epoch 447/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3406 - acc: 0.8750 - val_loss: 2.7576 - val_acc: 0.4804\n",
      "Epoch 448/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3612 - acc: 0.8545 - val_loss: 3.1436 - val_acc: 0.4169\n",
      "Epoch 449/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4745 - acc: 0.8038 - val_loss: 3.0946 - val_acc: 0.4622\n",
      "Epoch 450/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3668 - acc: 0.8584 - val_loss: 3.0342 - val_acc: 0.4592\n",
      "Epoch 451/500\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.3721 - acc: 0.8521 - val_loss: 3.3629 - val_acc: 0.4592\n",
      "Epoch 452/500\n",
      "2839/2839 [==============================] - 0s 88us/step - loss: 0.4940 - acc: 0.7992 - val_loss: 3.0706 - val_acc: 0.4713\n",
      "Epoch 453/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3175 - acc: 0.8750 - val_loss: 2.8020 - val_acc: 0.4804\n",
      "Epoch 454/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3732 - acc: 0.8556 - val_loss: 2.3914 - val_acc: 0.5106\n",
      "Epoch 455/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.4195 - acc: 0.8401 - val_loss: 2.4722 - val_acc: 0.4743\n",
      "Epoch 456/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4043 - acc: 0.8387 - val_loss: 2.7622 - val_acc: 0.4502\n",
      "Epoch 457/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3387 - acc: 0.8728 - val_loss: 3.0896 - val_acc: 0.4381\n",
      "Epoch 458/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4050 - acc: 0.8457 - val_loss: 3.3021 - val_acc: 0.4290\n",
      "Epoch 459/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3699 - acc: 0.8570 - val_loss: 3.2427 - val_acc: 0.4713\n",
      "Epoch 460/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3584 - acc: 0.8507 - val_loss: 3.0069 - val_acc: 0.4683\n",
      "Epoch 461/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3438 - acc: 0.8619 - val_loss: 3.0142 - val_acc: 0.5166\n",
      "Epoch 462/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4373 - acc: 0.8263 - val_loss: 2.6591 - val_acc: 0.4773\n",
      "Epoch 463/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3513 - acc: 0.8690 - val_loss: 3.1470 - val_acc: 0.4834\n",
      "Epoch 464/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3748 - acc: 0.8517 - val_loss: 2.5880 - val_acc: 0.4592\n",
      "Epoch 465/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4039 - acc: 0.8373 - val_loss: 2.7799 - val_acc: 0.4169\n",
      "Epoch 466/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3371 - acc: 0.8714 - val_loss: 3.3749 - val_acc: 0.4864\n",
      "Epoch 467/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4501 - acc: 0.8182 - val_loss: 2.8463 - val_acc: 0.4894\n",
      "Epoch 468/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.3132 - acc: 0.8795 - val_loss: 2.8384 - val_acc: 0.5076\n",
      "Epoch 469/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4240 - acc: 0.8411 - val_loss: 2.5565 - val_acc: 0.4743\n",
      "Epoch 470/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3593 - acc: 0.8538 - val_loss: 2.7369 - val_acc: 0.4653\n",
      "Epoch 471/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3411 - acc: 0.8725 - val_loss: 2.9723 - val_acc: 0.4562\n",
      "Epoch 472/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3104 - acc: 0.8831 - val_loss: 2.8605 - val_acc: 0.4562\n",
      "Epoch 473/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3956 - acc: 0.8429 - val_loss: 2.7602 - val_acc: 0.4441\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.3922 - acc: 0.8482 - val_loss: 3.2471 - val_acc: 0.4471\n",
      "Epoch 475/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3808 - acc: 0.8450 - val_loss: 3.2240 - val_acc: 0.4381\n",
      "Epoch 476/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3084 - acc: 0.8820 - val_loss: 3.3952 - val_acc: 0.4622\n",
      "Epoch 477/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3531 - acc: 0.8626 - val_loss: 3.1762 - val_acc: 0.4773\n",
      "Epoch 478/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4516 - acc: 0.8140 - val_loss: 3.0197 - val_acc: 0.4411\n",
      "Epoch 479/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3139 - acc: 0.8792 - val_loss: 2.9773 - val_acc: 0.4955\n",
      "Epoch 480/500\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3908 - acc: 0.8510 - val_loss: 3.1405 - val_acc: 0.4592\n",
      "Epoch 481/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3403 - acc: 0.8598 - val_loss: 2.9155 - val_acc: 0.4894\n",
      "Epoch 482/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3028 - acc: 0.8820 - val_loss: 2.8484 - val_acc: 0.4773\n",
      "Epoch 483/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3441 - acc: 0.8605 - val_loss: 3.8006 - val_acc: 0.4894\n",
      "Epoch 484/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.4411 - acc: 0.8193 - val_loss: 3.2814 - val_acc: 0.4653\n",
      "Epoch 485/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3442 - acc: 0.8616 - val_loss: 3.2383 - val_acc: 0.4894\n",
      "Epoch 486/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3415 - acc: 0.8623 - val_loss: 3.1772 - val_acc: 0.4773\n",
      "Epoch 487/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.2999 - acc: 0.8841 - val_loss: 3.5965 - val_acc: 0.4804\n",
      "Epoch 488/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4293 - acc: 0.8359 - val_loss: 3.3201 - val_acc: 0.4199\n",
      "Epoch 489/500\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3246 - acc: 0.8725 - val_loss: 3.3242 - val_acc: 0.4411\n",
      "Epoch 490/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3377 - acc: 0.8746 - val_loss: 2.8557 - val_acc: 0.4471\n",
      "Epoch 491/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5092 - acc: 0.8070 - val_loss: 3.1835 - val_acc: 0.4592\n",
      "Epoch 492/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3020 - acc: 0.8831 - val_loss: 3.5324 - val_acc: 0.4653\n",
      "Epoch 493/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3399 - acc: 0.8693 - val_loss: 3.4226 - val_acc: 0.4320\n",
      "Epoch 494/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3032 - acc: 0.8859 - val_loss: 3.3043 - val_acc: 0.4834\n",
      "Epoch 495/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3363 - acc: 0.8669 - val_loss: 3.7608 - val_acc: 0.4562\n",
      "Epoch 496/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.4293 - acc: 0.8292 - val_loss: 3.1688 - val_acc: 0.4683\n",
      "Epoch 497/500\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3524 - acc: 0.8700 - val_loss: 3.2207 - val_acc: 0.4653\n",
      "Epoch 498/500\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3201 - acc: 0.8848 - val_loss: 3.1216 - val_acc: 0.5076\n",
      "Epoch 499/500\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.4287 - acc: 0.8179 - val_loss: 2.7759 - val_acc: 0.4411\n",
      "Epoch 500/500\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.3336 - acc: 0.8746 - val_loss: 3.0001 - val_acc: 0.4502\n"
     ]
    }
   ],
   "source": [
    "w2v_model = model.fit(x=X_train_w2v, y=y_cat_train_w2v, \n",
    "          batch_size=500, \n",
    "          epochs=500, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_w2v, y_cat_test_w2v),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "0.7432432432432432\n",
      "0.9594594594594594\n",
      "39\n",
      "0.7692307692307693\n",
      "0.9487179487179487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  0,  1,  0],\n",
       "       [ 4, 34, 54, 17,  4],\n",
       "       [ 1, 15, 73, 48,  5],\n",
       "       [ 0,  1, 14, 36,  5],\n",
       "       [ 0,  1,  3,  8,  6]])"
      ]
     },
     "execution_count": 791,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_w2v)\n",
    "model_metrics(predictions, y_cat_test_w2v)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test_w2v.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njAMLBYZhGMbh/wHjbM0FgsqNrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x188d03860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w2v_model.history['acc'])\n",
    "plt.plot(w2v_model.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyhq2XVYv89IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x188d81978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(zillow_model.history['val_acc'])\n",
    "plt.plot(business_model.history['val_acc'])\n",
    "plt.plot(w2v_model.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['zillow', 'business', 'w2v'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding weights to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weight = {0 : 2,\n",
    "    1: 1,\n",
    "    2: .5,\n",
    "    3: 1,\n",
    "    4: 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(120,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/1000\n",
      "2839/2839 [==============================] - 11s 4ms/step - loss: 1.4156 - acc: 0.3131 - val_loss: 1.4839 - val_acc: 0.3323\n",
      "Epoch 2/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 1.3225 - acc: 0.3709 - val_loss: 1.4642 - val_acc: 0.3323\n",
      "Epoch 3/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.3063 - acc: 0.3857 - val_loss: 1.4650 - val_acc: 0.3323\n",
      "Epoch 4/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.2904 - acc: 0.3868 - val_loss: 1.4872 - val_acc: 0.3323\n",
      "Epoch 5/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 1.2755 - acc: 0.3843 - val_loss: 1.4356 - val_acc: 0.3323\n",
      "Epoch 6/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.2673 - acc: 0.3956 - val_loss: 1.4294 - val_acc: 0.3293\n",
      "Epoch 7/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 1.2554 - acc: 0.4033 - val_loss: 1.4331 - val_acc: 0.3293\n",
      "Epoch 8/1000\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 1.2440 - acc: 0.3942 - val_loss: 1.4235 - val_acc: 0.3323\n",
      "Epoch 9/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 1.2273 - acc: 0.4033 - val_loss: 1.4281 - val_acc: 0.3293\n",
      "Epoch 10/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.2361 - acc: 0.3857 - val_loss: 1.4453 - val_acc: 0.3293\n",
      "Epoch 11/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 1.2255 - acc: 0.3927 - val_loss: 1.3791 - val_acc: 0.3323\n",
      "Epoch 12/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 1.2200 - acc: 0.4040 - val_loss: 1.3580 - val_acc: 0.3353\n",
      "Epoch 13/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 1.2072 - acc: 0.4114 - val_loss: 1.3302 - val_acc: 0.3505\n",
      "Epoch 14/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2095 - acc: 0.4167 - val_loss: 1.3782 - val_acc: 0.3293\n",
      "Epoch 15/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1987 - acc: 0.3956 - val_loss: 1.3946 - val_acc: 0.3323\n",
      "Epoch 16/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.1964 - acc: 0.4040 - val_loss: 1.4066 - val_acc: 0.3293\n",
      "Epoch 17/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.1751 - acc: 0.4192 - val_loss: 1.3436 - val_acc: 0.3323\n",
      "Epoch 18/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.1811 - acc: 0.4160 - val_loss: 1.3928 - val_acc: 0.3353\n",
      "Epoch 19/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.1793 - acc: 0.4086 - val_loss: 1.3383 - val_acc: 0.3384\n",
      "Epoch 20/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.1717 - acc: 0.4237 - val_loss: 1.3662 - val_acc: 0.3263\n",
      "Epoch 21/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1559 - acc: 0.4273 - val_loss: 1.3137 - val_acc: 0.3686\n",
      "Epoch 22/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.1744 - acc: 0.4153 - val_loss: 1.3611 - val_acc: 0.3293\n",
      "Epoch 23/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.1679 - acc: 0.4220 - val_loss: 1.4633 - val_acc: 0.3323\n",
      "Epoch 24/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.1608 - acc: 0.4283 - val_loss: 1.3888 - val_acc: 0.3353\n",
      "Epoch 25/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.1558 - acc: 0.4125 - val_loss: 1.3676 - val_acc: 0.3384\n",
      "Epoch 26/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 1.1369 - acc: 0.4350 - val_loss: 1.2674 - val_acc: 0.4048\n",
      "Epoch 27/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.1472 - acc: 0.4350 - val_loss: 1.3050 - val_acc: 0.3746\n",
      "Epoch 28/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.1363 - acc: 0.4340 - val_loss: 1.3126 - val_acc: 0.3565\n",
      "Epoch 29/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1217 - acc: 0.4244 - val_loss: 1.3596 - val_acc: 0.3444\n",
      "Epoch 30/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 1.1192 - acc: 0.4333 - val_loss: 1.3292 - val_acc: 0.3535\n",
      "Epoch 31/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 1.1228 - acc: 0.4484 - val_loss: 1.3689 - val_acc: 0.3625\n",
      "Epoch 32/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.1145 - acc: 0.4304 - val_loss: 1.3848 - val_acc: 0.3353\n",
      "Epoch 33/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 1.1289 - acc: 0.4406 - val_loss: 1.4096 - val_acc: 0.3202\n",
      "Epoch 34/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 1.1158 - acc: 0.4118 - val_loss: 1.3437 - val_acc: 0.3384\n",
      "Epoch 35/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0978 - acc: 0.4371 - val_loss: 1.3114 - val_acc: 0.3716\n",
      "Epoch 36/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0992 - acc: 0.4473 - val_loss: 1.4205 - val_acc: 0.3112\n",
      "Epoch 37/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.0864 - acc: 0.4421 - val_loss: 1.3398 - val_acc: 0.3474\n",
      "Epoch 38/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0886 - acc: 0.4399 - val_loss: 1.2540 - val_acc: 0.3988\n",
      "Epoch 39/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 1.0990 - acc: 0.4414 - val_loss: 1.2204 - val_acc: 0.3867\n",
      "Epoch 40/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0935 - acc: 0.4371 - val_loss: 1.2323 - val_acc: 0.4018\n",
      "Epoch 41/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0794 - acc: 0.4544 - val_loss: 1.2841 - val_acc: 0.3867\n",
      "Epoch 42/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0795 - acc: 0.4516 - val_loss: 1.3093 - val_acc: 0.3746\n",
      "Epoch 43/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0781 - acc: 0.4491 - val_loss: 1.2499 - val_acc: 0.4079\n",
      "Epoch 44/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 1.0746 - acc: 0.4431 - val_loss: 1.1759 - val_acc: 0.4532\n",
      "Epoch 45/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 1.0798 - acc: 0.4790 - val_loss: 1.2271 - val_acc: 0.4260\n",
      "Epoch 46/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0714 - acc: 0.4646 - val_loss: 1.2399 - val_acc: 0.4320\n",
      "Epoch 47/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 1.0523 - acc: 0.4755 - val_loss: 1.2140 - val_acc: 0.4743\n",
      "Epoch 48/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0524 - acc: 0.4812 - val_loss: 1.1680 - val_acc: 0.4743\n",
      "Epoch 49/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0651 - acc: 0.4766 - val_loss: 1.1538 - val_acc: 0.4955\n",
      "Epoch 50/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0581 - acc: 0.4797 - val_loss: 1.2201 - val_acc: 0.4381\n",
      "Epoch 51/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 1.0411 - acc: 0.4674 - val_loss: 1.1968 - val_acc: 0.4532\n",
      "Epoch 52/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 1.0378 - acc: 0.4766 - val_loss: 1.2337 - val_acc: 0.4471\n",
      "Epoch 53/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0315 - acc: 0.4741 - val_loss: 1.3245 - val_acc: 0.3807\n",
      "Epoch 54/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 1.0325 - acc: 0.4625 - val_loss: 1.2350 - val_acc: 0.4230\n",
      "Epoch 55/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0337 - acc: 0.4759 - val_loss: 1.2762 - val_acc: 0.4018\n",
      "Epoch 56/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0180 - acc: 0.4773 - val_loss: 1.2210 - val_acc: 0.4622\n",
      "Epoch 57/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0211 - acc: 0.4755 - val_loss: 1.2150 - val_acc: 0.4743\n",
      "Epoch 58/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 1.0149 - acc: 0.4907 - val_loss: 1.1189 - val_acc: 0.5498\n",
      "Epoch 59/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0403 - acc: 0.4829 - val_loss: 1.1253 - val_acc: 0.5136\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0379 - acc: 0.5019 - val_loss: 1.1848 - val_acc: 0.4773\n",
      "Epoch 61/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 1.0154 - acc: 0.4868 - val_loss: 1.2053 - val_acc: 0.4562\n",
      "Epoch 62/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 1.0020 - acc: 0.4917 - val_loss: 1.2435 - val_acc: 0.4502\n",
      "Epoch 63/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 1.0054 - acc: 0.4854 - val_loss: 1.2872 - val_acc: 0.4350\n",
      "Epoch 64/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 1.0154 - acc: 0.4882 - val_loss: 1.3260 - val_acc: 0.4139\n",
      "Epoch 65/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.0199 - acc: 0.4664 - val_loss: 1.2977 - val_acc: 0.4139\n",
      "Epoch 66/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0044 - acc: 0.4741 - val_loss: 1.2520 - val_acc: 0.4230\n",
      "Epoch 67/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.9886 - acc: 0.4910 - val_loss: 1.2972 - val_acc: 0.4290\n",
      "Epoch 68/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.9866 - acc: 0.4903 - val_loss: 1.3155 - val_acc: 0.4199\n",
      "Epoch 69/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.9843 - acc: 0.4783 - val_loss: 1.2685 - val_acc: 0.4290\n",
      "Epoch 70/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.9841 - acc: 0.4822 - val_loss: 1.2812 - val_acc: 0.4230\n",
      "Epoch 71/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.9803 - acc: 0.4826 - val_loss: 1.3495 - val_acc: 0.3837\n",
      "Epoch 72/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.9868 - acc: 0.4861 - val_loss: 1.2685 - val_acc: 0.4320\n",
      "Epoch 73/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.9730 - acc: 0.4854 - val_loss: 1.2515 - val_acc: 0.4502\n",
      "Epoch 74/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.9602 - acc: 0.5125 - val_loss: 1.2894 - val_acc: 0.4199\n",
      "Epoch 75/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9793 - acc: 0.4861 - val_loss: 1.2783 - val_acc: 0.4381\n",
      "Epoch 76/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.9701 - acc: 0.4907 - val_loss: 1.3255 - val_acc: 0.4139\n",
      "Epoch 77/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.9822 - acc: 0.465 - 0s 70us/step - loss: 0.9815 - acc: 0.4847 - val_loss: 1.2680 - val_acc: 0.4441\n",
      "Epoch 78/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.9557 - acc: 0.5002 - val_loss: 1.1653 - val_acc: 0.5438\n",
      "Epoch 79/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9415 - acc: 0.5181 - val_loss: 1.1784 - val_acc: 0.5257\n",
      "Epoch 80/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.9291 - acc: 0.5231 - val_loss: 1.2591 - val_acc: 0.4562\n",
      "Epoch 81/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.9642 - acc: 0.4956 - val_loss: 1.3151 - val_acc: 0.4079\n",
      "Epoch 82/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9969 - acc: 0.4759 - val_loss: 1.3466 - val_acc: 0.3958\n",
      "Epoch 83/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.9694 - acc: 0.4935 - val_loss: 1.1962 - val_acc: 0.5438\n",
      "Epoch 84/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.9258 - acc: 0.5157 - val_loss: 1.2054 - val_acc: 0.4924\n",
      "Epoch 85/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.9262 - acc: 0.5294 - val_loss: 1.2205 - val_acc: 0.4955\n",
      "Epoch 86/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9500 - acc: 0.5178 - val_loss: 1.1934 - val_acc: 0.5257\n",
      "Epoch 87/1000\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.9416 - acc: 0.5072 - val_loss: 1.2329 - val_acc: 0.4743\n",
      "Epoch 88/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.9274 - acc: 0.5111 - val_loss: 1.2540 - val_acc: 0.4441\n",
      "Epoch 89/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9387 - acc: 0.5086 - val_loss: 1.3886 - val_acc: 0.3958\n",
      "Epoch 90/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.9804 - acc: 0.4674 - val_loss: 1.2710 - val_acc: 0.4350\n",
      "Epoch 91/1000\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.9350 - acc: 0.5023 - val_loss: 1.2359 - val_acc: 0.4804\n",
      "Epoch 92/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.9158 - acc: 0.5298 - val_loss: 1.2705 - val_acc: 0.4471\n",
      "Epoch 93/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.9343 - acc: 0.5107 - val_loss: 1.2545 - val_acc: 0.4562\n",
      "Epoch 94/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.9084 - acc: 0.5164 - val_loss: 1.2152 - val_acc: 0.4924\n",
      "Epoch 95/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.9290 - acc: 0.5185 - val_loss: 1.3625 - val_acc: 0.4350\n",
      "Epoch 96/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.9511 - acc: 0.4886 - val_loss: 1.1963 - val_acc: 0.5378\n",
      "Epoch 97/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.9036 - acc: 0.5354 - val_loss: 1.2081 - val_acc: 0.5257\n",
      "Epoch 98/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.9282 - acc: 0.5213 - val_loss: 1.1757 - val_acc: 0.5498\n",
      "Epoch 99/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.8979 - acc: 0.5329 - val_loss: 1.2466 - val_acc: 0.4743\n",
      "Epoch 100/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.8964 - acc: 0.5322 - val_loss: 1.2354 - val_acc: 0.4713\n",
      "Epoch 101/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.9014 - acc: 0.5350 - val_loss: 1.3395 - val_acc: 0.4260\n",
      "Epoch 102/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.9327 - acc: 0.4896 - val_loss: 1.2331 - val_acc: 0.4713\n",
      "Epoch 103/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.8896 - acc: 0.5361 - val_loss: 1.2052 - val_acc: 0.4804\n",
      "Epoch 104/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.8890 - acc: 0.5305 - val_loss: 1.1762 - val_acc: 0.5106\n",
      "Epoch 105/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.8658 - acc: 0.5442 - val_loss: 1.2000 - val_acc: 0.5166\n",
      "Epoch 106/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.8957 - acc: 0.5301 - val_loss: 1.2896 - val_acc: 0.4653\n",
      "Epoch 107/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.9197 - acc: 0.5072 - val_loss: 1.2503 - val_acc: 0.4743\n",
      "Epoch 108/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.8790 - acc: 0.5269 - val_loss: 1.2954 - val_acc: 0.4411\n",
      "Epoch 109/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.8987 - acc: 0.5224 - val_loss: 1.2663 - val_acc: 0.4532\n",
      "Epoch 110/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.8732 - acc: 0.5502 - val_loss: 1.2107 - val_acc: 0.4985\n",
      "Epoch 111/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.8678 - acc: 0.5340 - val_loss: 1.1914 - val_acc: 0.5378\n",
      "Epoch 112/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.8736 - acc: 0.5495 - val_loss: 1.1815 - val_acc: 0.5408\n",
      "Epoch 113/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.8599 - acc: 0.5481 - val_loss: 1.1418 - val_acc: 0.5680\n",
      "Epoch 114/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.9043 - acc: 0.5262 - val_loss: 1.0731 - val_acc: 0.5740\n",
      "Epoch 115/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.9286 - acc: 0.5322 - val_loss: 1.1382 - val_acc: 0.5227\n",
      "Epoch 116/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.8487 - acc: 0.5646 - val_loss: 1.1573 - val_acc: 0.5106\n",
      "Epoch 117/1000\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.8548 - acc: 0.5572 - val_loss: 1.1251 - val_acc: 0.5317\n",
      "Epoch 118/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.8581 - acc: 0.5502 - val_loss: 1.1155 - val_acc: 0.5408\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.8561 - acc: 0.5505 - val_loss: 1.1264 - val_acc: 0.5136\n",
      "Epoch 120/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.8743 - acc: 0.5523 - val_loss: 1.1574 - val_acc: 0.5076\n",
      "Epoch 121/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.8585 - acc: 0.5484 - val_loss: 1.1684 - val_acc: 0.5015\n",
      "Epoch 122/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.8316 - acc: 0.5611 - val_loss: 1.2272 - val_acc: 0.4955\n",
      "Epoch 123/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.8350 - acc: 0.5449 - val_loss: 1.2799 - val_acc: 0.4411\n",
      "Epoch 124/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.8690 - acc: 0.5372 - val_loss: 1.3184 - val_acc: 0.4260\n",
      "Epoch 125/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.8606 - acc: 0.5301 - val_loss: 1.2453 - val_acc: 0.4804\n",
      "Epoch 126/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.8532 - acc: 0.5312 - val_loss: 1.2040 - val_acc: 0.5106\n",
      "Epoch 127/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.8638 - acc: 0.5439 - val_loss: 1.2848 - val_acc: 0.4804\n",
      "Epoch 128/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.8574 - acc: 0.5361 - val_loss: 1.2095 - val_acc: 0.5166\n",
      "Epoch 129/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.8222 - acc: 0.5442 - val_loss: 1.1745 - val_acc: 0.5438\n",
      "Epoch 130/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.8169 - acc: 0.5569 - val_loss: 1.1643 - val_acc: 0.5196\n",
      "Epoch 131/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.8205 - acc: 0.5675 - val_loss: 1.1226 - val_acc: 0.5378\n",
      "Epoch 132/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.8642 - acc: 0.5463 - val_loss: 1.0817 - val_acc: 0.5740\n",
      "Epoch 133/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.8941 - acc: 0.5625 - val_loss: 1.1581 - val_acc: 0.5378\n",
      "Epoch 134/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.8111 - acc: 0.5643 - val_loss: 1.2210 - val_acc: 0.5196\n",
      "Epoch 135/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.8259 - acc: 0.5569 - val_loss: 1.2253 - val_acc: 0.4985\n",
      "Epoch 136/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.8249 - acc: 0.5565 - val_loss: 1.3448 - val_acc: 0.4199\n",
      "Epoch 137/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.8597 - acc: 0.5298 - val_loss: 1.3156 - val_acc: 0.4411\n",
      "Epoch 138/1000\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.8608 - acc: 0.5375 - val_loss: 1.3087 - val_acc: 0.4532\n",
      "Epoch 139/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.8386 - acc: 0.5431 - val_loss: 1.2654 - val_acc: 0.4713\n",
      "Epoch 140/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.8129 - acc: 0.5572 - val_loss: 1.1868 - val_acc: 0.5287\n",
      "Epoch 141/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.8096 - acc: 0.5671 - val_loss: 1.3392 - val_acc: 0.4320\n",
      "Epoch 142/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.8478 - acc: 0.5449 - val_loss: 1.3001 - val_acc: 0.4562\n",
      "Epoch 143/1000\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8203 - acc: 0.5639 - val_loss: 1.2143 - val_acc: 0.5227\n",
      "Epoch 144/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.7850 - acc: 0.5766 - val_loss: 1.2438 - val_acc: 0.5166\n",
      "Epoch 145/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.8090 - acc: 0.5706 - val_loss: 1.2871 - val_acc: 0.4773\n",
      "Epoch 146/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.8276 - acc: 0.5488 - val_loss: 1.2631 - val_acc: 0.4773\n",
      "Epoch 147/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.8012 - acc: 0.5639 - val_loss: 1.2315 - val_acc: 0.5257\n",
      "Epoch 148/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.7861 - acc: 0.5696 - val_loss: 1.1927 - val_acc: 0.5227\n",
      "Epoch 149/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.7607 - acc: 0.5847 - val_loss: 1.1684 - val_acc: 0.5498\n",
      "Epoch 150/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.7913 - acc: 0.5720 - val_loss: 1.1098 - val_acc: 0.5861\n",
      "Epoch 151/1000\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.8582 - acc: 0.5548 - val_loss: 1.1212 - val_acc: 0.5559\n",
      "Epoch 152/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.8071 - acc: 0.5770 - val_loss: 1.1710 - val_acc: 0.5106\n",
      "Epoch 153/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.7721 - acc: 0.5946 - val_loss: 1.2048 - val_acc: 0.4985\n",
      "Epoch 154/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.7677 - acc: 0.5893 - val_loss: 1.3092 - val_acc: 0.4622\n",
      "Epoch 155/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.8021 - acc: 0.5660 - val_loss: 1.2559 - val_acc: 0.4834\n",
      "Epoch 156/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.7784 - acc: 0.5650 - val_loss: 1.2336 - val_acc: 0.5076\n",
      "Epoch 157/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.8054 - acc: 0.5601 - val_loss: 1.2128 - val_acc: 0.5227\n",
      "Epoch 158/1000\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.7928 - acc: 0.5643 - val_loss: 1.1421 - val_acc: 0.5650\n",
      "Epoch 159/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.7883 - acc: 0.5928 - val_loss: 1.1358 - val_acc: 0.5619\n",
      "Epoch 160/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.7873 - acc: 0.5939 - val_loss: 1.1583 - val_acc: 0.5378\n",
      "Epoch 161/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.7694 - acc: 0.5925 - val_loss: 1.1403 - val_acc: 0.5347\n",
      "Epoch 162/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.7959 - acc: 0.5844 - val_loss: 1.1710 - val_acc: 0.5347\n",
      "Epoch 163/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.8286 - acc: 0.5724 - val_loss: 1.1879 - val_acc: 0.5106\n",
      "Epoch 164/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.7756 - acc: 0.5854 - val_loss: 1.2791 - val_acc: 0.4683\n",
      "Epoch 165/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.7642 - acc: 0.5787 - val_loss: 1.2642 - val_acc: 0.4773\n",
      "Epoch 166/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.7698 - acc: 0.5963 - val_loss: 1.2727 - val_acc: 0.4804\n",
      "Epoch 167/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.7650 - acc: 0.5759 - val_loss: 1.2837 - val_acc: 0.4834\n",
      "Epoch 168/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.7505 - acc: 0.5858 - val_loss: 1.2510 - val_acc: 0.5136\n",
      "Epoch 169/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.7581 - acc: 0.5932 - val_loss: 1.3686 - val_acc: 0.4532\n",
      "Epoch 170/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.8085 - acc: 0.5720 - val_loss: 1.2921 - val_acc: 0.4834\n",
      "Epoch 171/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.7748 - acc: 0.5667 - val_loss: 1.2686 - val_acc: 0.4864\n",
      "Epoch 172/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.7694 - acc: 0.5875 - val_loss: 1.3674 - val_acc: 0.4290\n",
      "Epoch 173/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.7859 - acc: 0.5717 - val_loss: 1.2946 - val_acc: 0.4653\n",
      "Epoch 174/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.7600 - acc: 0.5749 - val_loss: 1.2881 - val_acc: 0.4683\n",
      "Epoch 175/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.7509 - acc: 0.6027 - val_loss: 1.2449 - val_acc: 0.4955\n",
      "Epoch 176/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.7476 - acc: 0.5932 - val_loss: 1.2875 - val_acc: 0.4743\n",
      "Epoch 177/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.7491 - acc: 0.5942 - val_loss: 1.4148 - val_acc: 0.4350\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.8216 - acc: 0.5410 - val_loss: 1.3008 - val_acc: 0.4743\n",
      "Epoch 179/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.7633 - acc: 0.5738 - val_loss: 1.2943 - val_acc: 0.4743\n",
      "Epoch 180/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.7418 - acc: 0.5840 - val_loss: 1.2274 - val_acc: 0.5106\n",
      "Epoch 181/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.7102 - acc: 0.6051 - val_loss: 1.1987 - val_acc: 0.5106\n",
      "Epoch 182/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.7399 - acc: 0.6066 - val_loss: 1.1942 - val_acc: 0.5378\n",
      "Epoch 183/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.7378 - acc: 0.6076 - val_loss: 1.1546 - val_acc: 0.5589\n",
      "Epoch 184/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.7833 - acc: 0.5840 - val_loss: 1.1638 - val_acc: 0.5347\n",
      "Epoch 185/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.7458 - acc: 0.5953 - val_loss: 1.2132 - val_acc: 0.5136\n",
      "Epoch 186/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.7646 - acc: 0.6020 - val_loss: 1.2723 - val_acc: 0.4924\n",
      "Epoch 187/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.7687 - acc: 0.6016 - val_loss: 1.2446 - val_acc: 0.4713\n",
      "Epoch 188/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.7234 - acc: 0.6058 - val_loss: 1.2487 - val_acc: 0.4955\n",
      "Epoch 189/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.7206 - acc: 0.6196 - val_loss: 1.3586 - val_acc: 0.4411\n",
      "Epoch 190/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.7341 - acc: 0.6037 - val_loss: 1.3516 - val_acc: 0.4622\n",
      "Epoch 191/1000\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.7491 - acc: 0.5830 - val_loss: 1.4400 - val_acc: 0.3988\n",
      "Epoch 192/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.7875 - acc: 0.5608 - val_loss: 1.2950 - val_acc: 0.4683\n",
      "Epoch 193/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.7240 - acc: 0.5974 - val_loss: 1.2361 - val_acc: 0.5166\n",
      "Epoch 194/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.7174 - acc: 0.6161 - val_loss: 1.2150 - val_acc: 0.5227\n",
      "Epoch 195/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.6894 - acc: 0.6199 - val_loss: 1.2467 - val_acc: 0.5347\n",
      "Epoch 196/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.7348 - acc: 0.6041 - val_loss: 1.2946 - val_acc: 0.5045\n",
      "Epoch 197/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.7282 - acc: 0.5960 - val_loss: 1.3095 - val_acc: 0.4864\n",
      "Epoch 198/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.7197 - acc: 0.5953 - val_loss: 1.3189 - val_acc: 0.4743\n",
      "Epoch 199/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.7153 - acc: 0.5977 - val_loss: 1.3691 - val_acc: 0.4683\n",
      "Epoch 200/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.7354 - acc: 0.5879 - val_loss: 1.4677 - val_acc: 0.4109\n",
      "Epoch 201/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.7796 - acc: 0.5689 - val_loss: 1.3253 - val_acc: 0.4441\n",
      "Epoch 202/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.7204 - acc: 0.6083 - val_loss: 1.3357 - val_acc: 0.4502\n",
      "Epoch 203/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.7106 - acc: 0.6027 - val_loss: 1.2836 - val_acc: 0.4773\n",
      "Epoch 204/1000\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.6862 - acc: 0.6252 - val_loss: 1.3365 - val_acc: 0.4562\n",
      "Epoch 205/1000\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.7062 - acc: 0.6034 - val_loss: 1.4392 - val_acc: 0.4230\n",
      "Epoch 206/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.7535 - acc: 0.5784 - val_loss: 1.4099 - val_acc: 0.4199\n",
      "Epoch 207/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.7259 - acc: 0.6016 - val_loss: 1.3408 - val_acc: 0.4592\n",
      "Epoch 208/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.7003 - acc: 0.6164 - val_loss: 1.2942 - val_acc: 0.4894\n",
      "Epoch 209/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6957 - acc: 0.6168 - val_loss: 1.3355 - val_acc: 0.4804\n",
      "Epoch 210/1000\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.7330 - acc: 0.5914 - val_loss: 1.3995 - val_acc: 0.4532\n",
      "Epoch 211/1000\n",
      "2839/2839 [==============================] - 0s 88us/step - loss: 0.7422 - acc: 0.5865 - val_loss: 1.3706 - val_acc: 0.4502\n",
      "Epoch 212/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.7203 - acc: 0.5981 - val_loss: 1.3870 - val_acc: 0.4320\n",
      "Epoch 213/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.6992 - acc: 0.6136 - val_loss: 1.3230 - val_acc: 0.4622\n",
      "Epoch 214/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.6835 - acc: 0.6249 - val_loss: 1.2924 - val_acc: 0.4804\n",
      "Epoch 215/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.6660 - acc: 0.6372 - val_loss: 1.3012 - val_acc: 0.4743\n",
      "Epoch 216/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.6700 - acc: 0.6319 - val_loss: 1.2233 - val_acc: 0.5317\n",
      "Epoch 217/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.6904 - acc: 0.6284 - val_loss: 1.1985 - val_acc: 0.5619\n",
      "Epoch 218/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.7419 - acc: 0.6087 - val_loss: 1.2370 - val_acc: 0.5347\n",
      "Epoch 219/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.6943 - acc: 0.6270 - val_loss: 1.2738 - val_acc: 0.5015\n",
      "Epoch 220/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6838 - acc: 0.6224 - val_loss: 1.2357 - val_acc: 0.5196\n",
      "Epoch 221/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.6853 - acc: 0.6256 - val_loss: 1.2286 - val_acc: 0.5317\n",
      "Epoch 222/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.6770 - acc: 0.6390 - val_loss: 1.2690 - val_acc: 0.5015\n",
      "Epoch 223/1000\n",
      "2839/2839 [==============================] - 0s 94us/step - loss: 0.6824 - acc: 0.6291 - val_loss: 1.3086 - val_acc: 0.4864\n",
      "Epoch 224/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.6680 - acc: 0.6294 - val_loss: 1.3192 - val_acc: 0.4804\n",
      "Epoch 225/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6950 - acc: 0.6270 - val_loss: 1.4084 - val_acc: 0.4441\n",
      "Epoch 226/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.7217 - acc: 0.6076 - val_loss: 1.4411 - val_acc: 0.4320\n",
      "Epoch 227/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.7226 - acc: 0.5949 - val_loss: 1.4056 - val_acc: 0.4532\n",
      "Epoch 228/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.7016 - acc: 0.6115 - val_loss: 1.2958 - val_acc: 0.4985\n",
      "Epoch 229/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.6558 - acc: 0.6390 - val_loss: 1.3250 - val_acc: 0.4683\n",
      "Epoch 230/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.6654 - acc: 0.6344 - val_loss: 1.3570 - val_acc: 0.4713\n",
      "Epoch 231/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.6753 - acc: 0.6213 - val_loss: 1.3615 - val_acc: 0.4592\n",
      "Epoch 232/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.6769 - acc: 0.6147 - val_loss: 1.3272 - val_acc: 0.4804\n",
      "Epoch 233/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.6647 - acc: 0.6280 - val_loss: 1.4260 - val_acc: 0.4320\n",
      "Epoch 234/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.7006 - acc: 0.6097 - val_loss: 1.4292 - val_acc: 0.4350\n",
      "Epoch 235/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.7046 - acc: 0.6051 - val_loss: 1.4092 - val_acc: 0.4441\n",
      "Epoch 236/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.6849 - acc: 0.6164 - val_loss: 1.4418 - val_acc: 0.4320\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.6875 - acc: 0.6083 - val_loss: 1.3487 - val_acc: 0.4592\n",
      "Epoch 238/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.6451 - acc: 0.6471 - val_loss: 1.3211 - val_acc: 0.4804\n",
      "Epoch 239/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.6462 - acc: 0.6418 - val_loss: 1.3112 - val_acc: 0.4985\n",
      "Epoch 240/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.6571 - acc: 0.6231 - val_loss: 1.3452 - val_acc: 0.4955\n",
      "Epoch 241/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.6977 - acc: 0.6150 - val_loss: 1.2927 - val_acc: 0.5076\n",
      "Epoch 242/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6476 - acc: 0.6407 - val_loss: 1.3617 - val_acc: 0.4743\n",
      "Epoch 243/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.6631 - acc: 0.6270 - val_loss: 1.4081 - val_acc: 0.4713\n",
      "Epoch 244/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.6652 - acc: 0.6287 - val_loss: 1.4895 - val_acc: 0.4048\n",
      "Epoch 245/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.6951 - acc: 0.6041 - val_loss: 1.4309 - val_acc: 0.4260\n",
      "Epoch 246/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.6642 - acc: 0.6266 - val_loss: 1.3563 - val_acc: 0.4502\n",
      "Epoch 247/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.6199 - acc: 0.6467 - val_loss: 1.4249 - val_acc: 0.4260\n",
      "Epoch 248/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6720 - acc: 0.6199 - val_loss: 1.4447 - val_acc: 0.4320\n",
      "Epoch 249/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.6605 - acc: 0.6199 - val_loss: 1.4174 - val_acc: 0.4592\n",
      "Epoch 250/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.6454 - acc: 0.6414 - val_loss: 1.3901 - val_acc: 0.4683\n",
      "Epoch 251/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.6332 - acc: 0.6530 - val_loss: 1.4522 - val_acc: 0.4350\n",
      "Epoch 252/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.6779 - acc: 0.6266 - val_loss: 1.5488 - val_acc: 0.3897\n",
      "Epoch 253/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.7317 - acc: 0.5953 - val_loss: 1.4115 - val_acc: 0.4411\n",
      "Epoch 254/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.6750 - acc: 0.6228 - val_loss: 1.3555 - val_acc: 0.4683\n",
      "Epoch 255/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.6390 - acc: 0.6337 - val_loss: 1.3259 - val_acc: 0.4804\n",
      "Epoch 256/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.6158 - acc: 0.6523 - val_loss: 1.3186 - val_acc: 0.5076\n",
      "Epoch 257/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.6086 - acc: 0.6668 - val_loss: 1.3140 - val_acc: 0.5136\n",
      "Epoch 258/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.6132 - acc: 0.6615 - val_loss: 1.2903 - val_acc: 0.5287\n",
      "Epoch 259/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6781 - acc: 0.6309 - val_loss: 1.2688 - val_acc: 0.5378\n",
      "Epoch 260/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.6797 - acc: 0.6400 - val_loss: 1.2975 - val_acc: 0.5136\n",
      "Epoch 261/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.6320 - acc: 0.6569 - val_loss: 1.3251 - val_acc: 0.4804\n",
      "Epoch 262/1000\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.6458 - acc: 0.6446 - val_loss: 1.4187 - val_acc: 0.4955\n",
      "Epoch 263/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.6716 - acc: 0.6206 - val_loss: 1.3648 - val_acc: 0.4924\n",
      "Epoch 264/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6201 - acc: 0.6545 - val_loss: 1.3909 - val_acc: 0.4773\n",
      "Epoch 265/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.6203 - acc: 0.6552 - val_loss: 1.5064 - val_acc: 0.4199\n",
      "Epoch 266/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.6768 - acc: 0.6302 - val_loss: 1.5571 - val_acc: 0.3897\n",
      "Epoch 267/1000\n",
      "2839/2839 [==============================] - 0s 97us/step - loss: 0.7123 - acc: 0.6147 - val_loss: 1.4933 - val_acc: 0.4199\n",
      "Epoch 268/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.6657 - acc: 0.6220 - val_loss: 1.4022 - val_acc: 0.4622\n",
      "Epoch 269/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.6158 - acc: 0.6566 - val_loss: 1.3423 - val_acc: 0.4894\n",
      "Epoch 270/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.6052 - acc: 0.6738 - val_loss: 1.3289 - val_acc: 0.4924\n",
      "Epoch 271/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.6124 - acc: 0.6657 - val_loss: 1.3459 - val_acc: 0.5136\n",
      "Epoch 272/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.6264 - acc: 0.6615 - val_loss: 1.3095 - val_acc: 0.5287\n",
      "Epoch 273/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.7062 - acc: 0.6368 - val_loss: 1.3354 - val_acc: 0.5015\n",
      "Epoch 274/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.6474 - acc: 0.6611 - val_loss: 1.4021 - val_acc: 0.4713\n",
      "Epoch 275/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.6109 - acc: 0.6569 - val_loss: 1.3886 - val_acc: 0.4773\n",
      "Epoch 276/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.6206 - acc: 0.6587 - val_loss: 1.4079 - val_acc: 0.4804\n",
      "Epoch 277/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.6203 - acc: 0.6552 - val_loss: 1.4429 - val_acc: 0.4683\n",
      "Epoch 278/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.6143 - acc: 0.6601 - val_loss: 1.4001 - val_acc: 0.4773\n",
      "Epoch 279/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.6083 - acc: 0.6654 - val_loss: 1.4466 - val_acc: 0.4622\n",
      "Epoch 280/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.6117 - acc: 0.6626 - val_loss: 1.4776 - val_acc: 0.4411\n",
      "Epoch 281/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.6219 - acc: 0.6534 - val_loss: 1.5109 - val_acc: 0.4260\n",
      "Epoch 282/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.6471 - acc: 0.6386 - val_loss: 1.5169 - val_acc: 0.4169\n",
      "Epoch 283/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.6431 - acc: 0.6273 - val_loss: 1.4558 - val_acc: 0.4532\n",
      "Epoch 284/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.6168 - acc: 0.6495 - val_loss: 1.4399 - val_acc: 0.4411\n",
      "Epoch 285/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.5981 - acc: 0.6668 - val_loss: 1.3778 - val_acc: 0.4924\n",
      "Epoch 286/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6043 - acc: 0.6633 - val_loss: 1.3835 - val_acc: 0.4955\n",
      "Epoch 287/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.5988 - acc: 0.670 - 0s 69us/step - loss: 0.6061 - acc: 0.6615 - val_loss: 1.4553 - val_acc: 0.4683\n",
      "Epoch 288/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.6439 - acc: 0.6520 - val_loss: 1.4401 - val_acc: 0.4743\n",
      "Epoch 289/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.6298 - acc: 0.6576 - val_loss: 1.4292 - val_acc: 0.4713\n",
      "Epoch 290/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.5959 - acc: 0.6689 - val_loss: 1.4933 - val_acc: 0.4320\n",
      "Epoch 291/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.6123 - acc: 0.6654 - val_loss: 1.6093 - val_acc: 0.3927\n",
      "Epoch 292/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.7174 - acc: 0.6002 - val_loss: 1.5322 - val_acc: 0.4048\n",
      "Epoch 293/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.6257 - acc: 0.6263 - val_loss: 1.4123 - val_acc: 0.4592\n",
      "Epoch 294/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.5685 - acc: 0.6777 - val_loss: 1.4131 - val_acc: 0.4713\n",
      "Epoch 295/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.5837 - acc: 0.6728 - val_loss: 1.4107 - val_acc: 0.4773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5751 - acc: 0.6735 - val_loss: 1.4885 - val_acc: 0.4381\n",
      "Epoch 297/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.6080 - acc: 0.6442 - val_loss: 1.5358 - val_acc: 0.4079\n",
      "Epoch 298/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6114 - acc: 0.6446 - val_loss: 1.5102 - val_acc: 0.4320\n",
      "Epoch 299/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.6331 - acc: 0.6365 - val_loss: 1.4240 - val_acc: 0.4683\n",
      "Epoch 300/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.6427 - acc: 0.6340 - val_loss: 1.3605 - val_acc: 0.4804\n",
      "Epoch 301/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.5833 - acc: 0.6717 - val_loss: 1.3643 - val_acc: 0.4985\n",
      "Epoch 302/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.6024 - acc: 0.6566 - val_loss: 1.3279 - val_acc: 0.5257\n",
      "Epoch 303/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.6405 - acc: 0.6594 - val_loss: 1.3656 - val_acc: 0.5045\n",
      "Epoch 304/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.6039 - acc: 0.6657 - val_loss: 1.3678 - val_acc: 0.4955\n",
      "Epoch 305/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.5725 - acc: 0.6865 - val_loss: 1.4243 - val_acc: 0.4653\n",
      "Epoch 306/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5807 - acc: 0.6756 - val_loss: 1.4202 - val_acc: 0.4834\n",
      "Epoch 307/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.5698 - acc: 0.6647 - val_loss: 1.4212 - val_acc: 0.4894\n",
      "Epoch 308/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.5579 - acc: 0.6809 - val_loss: 1.4656 - val_acc: 0.4834\n",
      "Epoch 309/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.5749 - acc: 0.6816 - val_loss: 1.5706 - val_acc: 0.4290\n",
      "Epoch 310/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.6175 - acc: 0.6580 - val_loss: 1.5742 - val_acc: 0.4381\n",
      "Epoch 311/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.6078 - acc: 0.6735 - val_loss: 1.5132 - val_acc: 0.4471\n",
      "Epoch 312/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.5940 - acc: 0.6756 - val_loss: 1.6772 - val_acc: 0.3958\n",
      "Epoch 313/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.7057 - acc: 0.6094 - val_loss: 1.5437 - val_acc: 0.4079\n",
      "Epoch 314/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.5922 - acc: 0.6562 - val_loss: 1.4650 - val_acc: 0.4471\n",
      "Epoch 315/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.5643 - acc: 0.6710 - val_loss: 1.4292 - val_acc: 0.4743\n",
      "Epoch 316/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5606 - acc: 0.6717 - val_loss: 1.4347 - val_acc: 0.4834\n",
      "Epoch 317/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.5665 - acc: 0.6678 - val_loss: 1.4104 - val_acc: 0.4894\n",
      "Epoch 318/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.5685 - acc: 0.6738 - val_loss: 1.4104 - val_acc: 0.4924\n",
      "Epoch 319/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.5545 - acc: 0.6904 - val_loss: 1.3996 - val_acc: 0.5045\n",
      "Epoch 320/1000\n",
      "2839/2839 [==============================] - 0s 90us/step - loss: 0.5667 - acc: 0.6862 - val_loss: 1.4223 - val_acc: 0.4985\n",
      "Epoch 321/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.5846 - acc: 0.6756 - val_loss: 1.3931 - val_acc: 0.5136\n",
      "Epoch 322/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6156 - acc: 0.6742 - val_loss: 1.3502 - val_acc: 0.5227\n",
      "Epoch 323/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.6523 - acc: 0.6583 - val_loss: 1.4232 - val_acc: 0.4773\n",
      "Epoch 324/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5614 - acc: 0.6879 - val_loss: 1.5127 - val_acc: 0.4471\n",
      "Epoch 325/1000\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.5630 - acc: 0.6960 - val_loss: 1.5298 - val_acc: 0.4502\n",
      "Epoch 326/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5876 - acc: 0.6742 - val_loss: 1.4877 - val_acc: 0.4713\n",
      "Epoch 327/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5694 - acc: 0.6728 - val_loss: 1.4294 - val_acc: 0.4894\n",
      "Epoch 328/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.5502 - acc: 0.7034 - val_loss: 1.4347 - val_acc: 0.4894\n",
      "Epoch 329/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.5733 - acc: 0.6844 - val_loss: 1.4161 - val_acc: 0.4955\n",
      "Epoch 330/1000\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.5786 - acc: 0.6865 - val_loss: 1.4078 - val_acc: 0.5076\n",
      "Epoch 331/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.6015 - acc: 0.6823 - val_loss: 1.4124 - val_acc: 0.4955\n",
      "Epoch 332/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.5753 - acc: 0.6840 - val_loss: 1.4469 - val_acc: 0.4985\n",
      "Epoch 333/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.5312 - acc: 0.6974 - val_loss: 1.4783 - val_acc: 0.4683\n",
      "Epoch 334/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.5244 - acc: 0.7031 - val_loss: 1.4798 - val_acc: 0.4864\n",
      "Epoch 335/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.5600 - acc: 0.6869 - val_loss: 1.4376 - val_acc: 0.4985\n",
      "Epoch 336/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6234 - acc: 0.6435 - val_loss: 1.3945 - val_acc: 0.5136\n",
      "Epoch 337/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.6111 - acc: 0.6763 - val_loss: 1.4376 - val_acc: 0.4924\n",
      "Epoch 338/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.5613 - acc: 0.6812 - val_loss: 1.4696 - val_acc: 0.4864\n",
      "Epoch 339/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.5422 - acc: 0.6900 - val_loss: 1.5386 - val_acc: 0.4622\n",
      "Epoch 340/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.5976 - acc: 0.6770 - val_loss: 1.5131 - val_acc: 0.4804\n",
      "Epoch 341/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.5920 - acc: 0.6936 - val_loss: 1.5358 - val_acc: 0.4592\n",
      "Epoch 342/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.5617 - acc: 0.7031 - val_loss: 1.4986 - val_acc: 0.4502\n",
      "Epoch 343/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5370 - acc: 0.7002 - val_loss: 1.4621 - val_acc: 0.4985\n",
      "Epoch 344/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5321 - acc: 0.7034 - val_loss: 1.4624 - val_acc: 0.5166\n",
      "Epoch 345/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.6102 - acc: 0.6802 - val_loss: 1.4133 - val_acc: 0.5106\n",
      "Epoch 346/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.5894 - acc: 0.6851 - val_loss: 1.4319 - val_acc: 0.4864\n",
      "Epoch 347/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.5350 - acc: 0.6950 - val_loss: 1.4549 - val_acc: 0.4773\n",
      "Epoch 348/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.5160 - acc: 0.6988 - val_loss: 1.4804 - val_acc: 0.4683\n",
      "Epoch 349/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.5114 - acc: 0.6988 - val_loss: 1.4920 - val_acc: 0.4653\n",
      "Epoch 350/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.5240 - acc: 0.6918 - val_loss: 1.6138 - val_acc: 0.4260\n",
      "Epoch 351/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.5656 - acc: 0.6742 - val_loss: 1.6508 - val_acc: 0.4169\n",
      "Epoch 352/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.5818 - acc: 0.6710 - val_loss: 1.7312 - val_acc: 0.4018\n",
      "Epoch 353/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.5880 - acc: 0.6664 - val_loss: 1.5962 - val_acc: 0.4260\n",
      "Epoch 354/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.5258 - acc: 0.6971 - val_loss: 1.6028 - val_acc: 0.4350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.5207 - acc: 0.6985 - val_loss: 1.5691 - val_acc: 0.4653\n",
      "Epoch 356/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.5349 - acc: 0.6999 - val_loss: 1.5491 - val_acc: 0.4713\n",
      "Epoch 357/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.5304 - acc: 0.720 - 0s 68us/step - loss: 0.5321 - acc: 0.7094 - val_loss: 1.5232 - val_acc: 0.4834\n",
      "Epoch 358/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5422 - acc: 0.7010 - val_loss: 1.5650 - val_acc: 0.4653\n",
      "Epoch 359/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.5529 - acc: 0.7024 - val_loss: 1.4502 - val_acc: 0.5106\n",
      "Epoch 360/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.5579 - acc: 0.7013 - val_loss: 1.4629 - val_acc: 0.4924\n",
      "Epoch 361/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.5305 - acc: 0.7073 - val_loss: 1.4687 - val_acc: 0.4924\n",
      "Epoch 362/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.5368 - acc: 0.7136 - val_loss: 1.4569 - val_acc: 0.5106\n",
      "Epoch 363/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.5931 - acc: 0.6837 - val_loss: 1.4503 - val_acc: 0.4924\n",
      "Epoch 364/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.5325 - acc: 0.7087 - val_loss: 1.5293 - val_acc: 0.4683\n",
      "Epoch 365/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4961 - acc: 0.7076 - val_loss: 1.5240 - val_acc: 0.4743\n",
      "Epoch 366/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4978 - acc: 0.7087 - val_loss: 1.5804 - val_acc: 0.4502\n",
      "Epoch 367/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.5079 - acc: 0.7073 - val_loss: 1.6284 - val_acc: 0.4411\n",
      "Epoch 368/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.5445 - acc: 0.6781 - val_loss: 1.6880 - val_acc: 0.4199\n",
      "Epoch 369/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5654 - acc: 0.6611 - val_loss: 1.6043 - val_acc: 0.4230\n",
      "Epoch 370/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.5314 - acc: 0.6791 - val_loss: 1.4966 - val_acc: 0.4743\n",
      "Epoch 371/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.5222 - acc: 0.7052 - val_loss: 1.4495 - val_acc: 0.5076\n",
      "Epoch 372/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5514 - acc: 0.6992 - val_loss: 1.4530 - val_acc: 0.5136\n",
      "Epoch 373/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.5316 - acc: 0.7027 - val_loss: 1.4675 - val_acc: 0.4955\n",
      "Epoch 374/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.5391 - acc: 0.6957 - val_loss: 1.4837 - val_acc: 0.4924\n",
      "Epoch 375/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.5003 - acc: 0.7193 - val_loss: 1.5266 - val_acc: 0.4804\n",
      "Epoch 376/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.5072 - acc: 0.7200 - val_loss: 1.5486 - val_acc: 0.4743\n",
      "Epoch 377/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4802 - acc: 0.7168 - val_loss: 1.5176 - val_acc: 0.4924\n",
      "Epoch 378/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.5055 - acc: 0.7140 - val_loss: 1.4851 - val_acc: 0.5196\n",
      "Epoch 379/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.5868 - acc: 0.6749 - val_loss: 1.4555 - val_acc: 0.5227\n",
      "Epoch 380/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.5317 - acc: 0.7059 - val_loss: 1.5344 - val_acc: 0.4743\n",
      "Epoch 381/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5082 - acc: 0.7203 - val_loss: 1.5190 - val_acc: 0.4924\n",
      "Epoch 382/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4949 - acc: 0.7161 - val_loss: 1.5602 - val_acc: 0.4955\n",
      "Epoch 383/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.4937 - acc: 0.7179 - val_loss: 1.5883 - val_acc: 0.4955\n",
      "Epoch 384/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.4969 - acc: 0.7200 - val_loss: 1.6737 - val_acc: 0.4532\n",
      "Epoch 385/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.5913 - acc: 0.6858 - val_loss: 1.5753 - val_acc: 0.4713\n",
      "Epoch 386/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.6055 - acc: 0.6883 - val_loss: 1.5055 - val_acc: 0.4985\n",
      "Epoch 387/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.5643 - acc: 0.6981 - val_loss: 1.4801 - val_acc: 0.5076\n",
      "Epoch 388/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.5201 - acc: 0.7083 - val_loss: 1.4856 - val_acc: 0.5076\n",
      "Epoch 389/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5228 - acc: 0.7154 - val_loss: 1.5488 - val_acc: 0.4592\n",
      "Epoch 390/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4902 - acc: 0.7235 - val_loss: 1.5630 - val_acc: 0.4773\n",
      "Epoch 391/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4809 - acc: 0.7175 - val_loss: 1.5597 - val_acc: 0.4834\n",
      "Epoch 392/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4839 - acc: 0.7164 - val_loss: 1.5616 - val_acc: 0.4804\n",
      "Epoch 393/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4823 - acc: 0.7344 - val_loss: 1.6066 - val_acc: 0.4773\n",
      "Epoch 394/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.5215 - acc: 0.7020 - val_loss: 1.6872 - val_acc: 0.4471\n",
      "Epoch 395/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.5523 - acc: 0.6738 - val_loss: 1.6421 - val_acc: 0.4592\n",
      "Epoch 396/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4919 - acc: 0.7087 - val_loss: 1.6556 - val_acc: 0.4381\n",
      "Epoch 397/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4906 - acc: 0.7147 - val_loss: 1.7267 - val_acc: 0.4320\n",
      "Epoch 398/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.5098 - acc: 0.6936 - val_loss: 1.8826 - val_acc: 0.3897\n",
      "Epoch 399/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.5769 - acc: 0.6784 - val_loss: 1.8149 - val_acc: 0.3958\n",
      "Epoch 400/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.5640 - acc: 0.6788 - val_loss: 1.6804 - val_acc: 0.4320\n",
      "Epoch 401/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.4818 - acc: 0.7263 - val_loss: 1.6563 - val_acc: 0.4411\n",
      "Epoch 402/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4769 - acc: 0.7217 - val_loss: 1.6745 - val_acc: 0.4622\n",
      "Epoch 403/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4745 - acc: 0.7337 - val_loss: 1.6801 - val_acc: 0.4441\n",
      "Epoch 404/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4689 - acc: 0.7179 - val_loss: 1.6493 - val_acc: 0.4864\n",
      "Epoch 405/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.4702 - acc: 0.7274 - val_loss: 1.8460 - val_acc: 0.3988\n",
      "Epoch 406/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5515 - acc: 0.7002 - val_loss: 1.8371 - val_acc: 0.4079\n",
      "Epoch 407/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5476 - acc: 0.7024 - val_loss: 1.6790 - val_acc: 0.4471\n",
      "Epoch 408/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.4774 - acc: 0.7238 - val_loss: 1.7994 - val_acc: 0.4199\n",
      "Epoch 409/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.5282 - acc: 0.6995 - val_loss: 1.8359 - val_acc: 0.4169\n",
      "Epoch 410/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.5607 - acc: 0.6830 - val_loss: 1.6969 - val_acc: 0.4350\n",
      "Epoch 411/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4951 - acc: 0.7094 - val_loss: 1.6990 - val_acc: 0.4320\n",
      "Epoch 412/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4746 - acc: 0.7182 - val_loss: 1.6980 - val_acc: 0.4411\n",
      "Epoch 413/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.4810 - acc: 0.7168 - val_loss: 1.7781 - val_acc: 0.4139\n",
      "Epoch 414/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.5003 - acc: 0.7024 - val_loss: 1.7711 - val_acc: 0.4260\n",
      "Epoch 415/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.5149 - acc: 0.6865 - val_loss: 1.7416 - val_acc: 0.4169\n",
      "Epoch 416/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.4907 - acc: 0.7041 - val_loss: 1.7953 - val_acc: 0.4169\n",
      "Epoch 417/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.5137 - acc: 0.7024 - val_loss: 1.8474 - val_acc: 0.3988\n",
      "Epoch 418/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.5050 - acc: 0.7041 - val_loss: 1.7735 - val_acc: 0.4381\n",
      "Epoch 419/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.4870 - acc: 0.7242 - val_loss: 1.7486 - val_acc: 0.4381\n",
      "Epoch 420/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4692 - acc: 0.7404 - val_loss: 1.7063 - val_acc: 0.4592\n",
      "Epoch 421/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4998 - acc: 0.7175 - val_loss: 1.6542 - val_acc: 0.4773\n",
      "Epoch 422/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.4987 - acc: 0.7263 - val_loss: 1.5867 - val_acc: 0.5136\n",
      "Epoch 423/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.5307 - acc: 0.7179 - val_loss: 1.5655 - val_acc: 0.5045\n",
      "Epoch 424/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.5223 - acc: 0.7249 - val_loss: 1.5852 - val_acc: 0.5076\n",
      "Epoch 425/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4771 - acc: 0.7263 - val_loss: 1.6607 - val_acc: 0.4804\n",
      "Epoch 426/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4378 - acc: 0.7383 - val_loss: 1.6649 - val_acc: 0.4924\n",
      "Epoch 427/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4647 - acc: 0.7397 - val_loss: 1.6349 - val_acc: 0.5076\n",
      "Epoch 428/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.5284 - acc: 0.7034 - val_loss: 1.6571 - val_acc: 0.4834\n",
      "Epoch 429/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4931 - acc: 0.7069 - val_loss: 1.6678 - val_acc: 0.4804\n",
      "Epoch 430/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4910 - acc: 0.7143 - val_loss: 1.6612 - val_acc: 0.4834\n",
      "Epoch 431/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4746 - acc: 0.7147 - val_loss: 1.6829 - val_acc: 0.4773\n",
      "Epoch 432/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4689 - acc: 0.7256 - val_loss: 1.6405 - val_acc: 0.4653\n",
      "Epoch 433/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4840 - acc: 0.7193 - val_loss: 1.6186 - val_acc: 0.4864\n",
      "Epoch 434/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.4991 - acc: 0.7133 - val_loss: 1.6019 - val_acc: 0.5015\n",
      "Epoch 435/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.6849 - acc: 0.6911 - val_loss: 1.6326 - val_acc: 0.4411\n",
      "Epoch 436/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4633 - acc: 0.7302 - val_loss: 1.6874 - val_acc: 0.4653\n",
      "Epoch 437/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.4495 - acc: 0.7415 - val_loss: 1.6978 - val_acc: 0.4471\n",
      "Epoch 438/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4597 - acc: 0.7478 - val_loss: 1.6806 - val_acc: 0.4562\n",
      "Epoch 439/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4392 - acc: 0.7489 - val_loss: 1.7078 - val_acc: 0.4653\n",
      "Epoch 440/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4499 - acc: 0.7474 - val_loss: 1.7075 - val_acc: 0.4622\n",
      "Epoch 441/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.4343 - acc: 0.7474 - val_loss: 1.6842 - val_acc: 0.4773\n",
      "Epoch 442/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.5127 - acc: 0.7059 - val_loss: 1.7228 - val_acc: 0.4743\n",
      "Epoch 443/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.5543 - acc: 0.6816 - val_loss: 1.6349 - val_acc: 0.4834\n",
      "Epoch 444/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4656 - acc: 0.7270 - val_loss: 1.6748 - val_acc: 0.4683\n",
      "Epoch 445/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.4451 - acc: 0.7358 - val_loss: 1.6735 - val_acc: 0.4773\n",
      "Epoch 446/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4675 - acc: 0.7217 - val_loss: 1.6012 - val_acc: 0.5227\n",
      "Epoch 447/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.5613 - acc: 0.7168 - val_loss: 1.6222 - val_acc: 0.4743\n",
      "Epoch 448/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4954 - acc: 0.7214 - val_loss: 1.6984 - val_acc: 0.4653\n",
      "Epoch 449/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.4480 - acc: 0.7408 - val_loss: 1.7293 - val_acc: 0.4592\n",
      "Epoch 450/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4373 - acc: 0.7408 - val_loss: 1.7358 - val_acc: 0.4502\n",
      "Epoch 451/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4316 - acc: 0.7566 - val_loss: 1.6786 - val_acc: 0.5076\n",
      "Epoch 452/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.4717 - acc: 0.7408 - val_loss: 1.6796 - val_acc: 0.4924\n",
      "Epoch 453/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.5492 - acc: 0.7150 - val_loss: 1.6565 - val_acc: 0.4955\n",
      "Epoch 454/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4824 - acc: 0.7355 - val_loss: 1.6611 - val_acc: 0.4683\n",
      "Epoch 455/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4399 - acc: 0.7534 - val_loss: 1.6904 - val_acc: 0.4592\n",
      "Epoch 456/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4308 - acc: 0.7411 - val_loss: 1.6745 - val_acc: 0.4894\n",
      "Epoch 457/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.4526 - acc: 0.7467 - val_loss: 1.6887 - val_acc: 0.5015\n",
      "Epoch 458/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.5061 - acc: 0.7105 - val_loss: 1.7094 - val_acc: 0.4894\n",
      "Epoch 459/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.4658 - acc: 0.7217 - val_loss: 1.6796 - val_acc: 0.4743\n",
      "Epoch 460/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4605 - acc: 0.7309 - val_loss: 1.6691 - val_acc: 0.4864\n",
      "Epoch 461/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4844 - acc: 0.7369 - val_loss: 1.6618 - val_acc: 0.4924\n",
      "Epoch 462/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.5023 - acc: 0.7231 - val_loss: 1.6982 - val_acc: 0.4441\n",
      "Epoch 463/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4480 - acc: 0.7482 - val_loss: 1.7496 - val_acc: 0.4562\n",
      "Epoch 464/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.4267 - acc: 0.7566 - val_loss: 1.7655 - val_acc: 0.4622\n",
      "Epoch 465/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.4404 - acc: 0.7545 - val_loss: 1.8875 - val_acc: 0.4290\n",
      "Epoch 466/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.4828 - acc: 0.7330 - val_loss: 1.9393 - val_acc: 0.4139\n",
      "Epoch 467/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.4815 - acc: 0.7337 - val_loss: 1.7861 - val_acc: 0.4622\n",
      "Epoch 468/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4389 - acc: 0.7689 - val_loss: 1.9028 - val_acc: 0.4230\n",
      "Epoch 469/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4714 - acc: 0.7291 - val_loss: 1.9614 - val_acc: 0.4109\n",
      "Epoch 470/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.5230 - acc: 0.7055 - val_loss: 1.9549 - val_acc: 0.3897\n",
      "Epoch 471/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.5088 - acc: 0.6939 - val_loss: 1.7591 - val_acc: 0.4622\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4416 - acc: 0.7418 - val_loss: 1.7174 - val_acc: 0.4653\n",
      "Epoch 473/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4312 - acc: 0.7425 - val_loss: 1.7454 - val_acc: 0.4532\n",
      "Epoch 474/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.4325 - acc: 0.7499 - val_loss: 1.7236 - val_acc: 0.4653\n",
      "Epoch 475/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.4178 - acc: 0.7499 - val_loss: 1.7112 - val_acc: 0.4834\n",
      "Epoch 476/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4745 - acc: 0.7291 - val_loss: 1.6899 - val_acc: 0.4894\n",
      "Epoch 477/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.5010 - acc: 0.7193 - val_loss: 1.6800 - val_acc: 0.4894\n",
      "Epoch 478/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.4831 - acc: 0.7386 - val_loss: 1.7012 - val_acc: 0.4864\n",
      "Epoch 479/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.4371 - acc: 0.7661 - val_loss: 1.7895 - val_acc: 0.4592\n",
      "Epoch 480/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4727 - acc: 0.7397 - val_loss: 1.8610 - val_acc: 0.4230\n",
      "Epoch 481/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4497 - acc: 0.7545 - val_loss: 1.8596 - val_acc: 0.4350\n",
      "Epoch 482/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.4481 - acc: 0.7408 - val_loss: 1.8936 - val_acc: 0.4290\n",
      "Epoch 483/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4690 - acc: 0.7383 - val_loss: 1.8762 - val_acc: 0.4350\n",
      "Epoch 484/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4389 - acc: 0.7538 - val_loss: 1.7989 - val_acc: 0.4622\n",
      "Epoch 485/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4150 - acc: 0.7629 - val_loss: 1.8018 - val_acc: 0.4622\n",
      "Epoch 486/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4223 - acc: 0.7577 - val_loss: 1.8473 - val_acc: 0.4441\n",
      "Epoch 487/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4316 - acc: 0.7334 - val_loss: 1.8522 - val_acc: 0.4471\n",
      "Epoch 488/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.4594 - acc: 0.7217 - val_loss: 1.9802 - val_acc: 0.4320\n",
      "Epoch 489/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.5309 - acc: 0.6981 - val_loss: 1.9785 - val_acc: 0.4018\n",
      "Epoch 490/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4917 - acc: 0.7214 - val_loss: 1.9515 - val_acc: 0.4018\n",
      "Epoch 491/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4553 - acc: 0.7344 - val_loss: 1.7986 - val_acc: 0.4532\n",
      "Epoch 492/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4044 - acc: 0.7608 - val_loss: 1.7859 - val_acc: 0.4713\n",
      "Epoch 493/1000\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.4171 - acc: 0.7622 - val_loss: 1.8175 - val_acc: 0.4743\n",
      "Epoch 494/1000\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4120 - acc: 0.7626 - val_loss: 1.8369 - val_acc: 0.4713\n",
      "Epoch 495/1000\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4332 - acc: 0.7608 - val_loss: 1.8395 - val_acc: 0.4502\n",
      "Epoch 496/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4439 - acc: 0.7605 - val_loss: 1.7821 - val_acc: 0.4743\n",
      "Epoch 497/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4249 - acc: 0.7665 - val_loss: 1.7582 - val_acc: 0.5015\n",
      "Epoch 498/1000\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4292 - acc: 0.7570 - val_loss: 1.7427 - val_acc: 0.4804\n",
      "Epoch 499/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.5184 - acc: 0.7316 - val_loss: 1.6883 - val_acc: 0.5045\n",
      "Epoch 500/1000\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4755 - acc: 0.7429 - val_loss: 1.7841 - val_acc: 0.4502\n",
      "Epoch 501/1000\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.4152 - acc: 0.7612 - val_loss: 1.8329 - val_acc: 0.4622\n",
      "Epoch 502/1000\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.4191 - acc: 0.7608 - val_loss: 1.8934 - val_acc: 0.4592\n",
      "Epoch 503/1000\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4151 - acc: 0.7601 - val_loss: 1.9829 - val_acc: 0.4139\n",
      "Epoch 504/1000\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4458 - acc: 0.7503 - val_loss: 2.0405 - val_acc: 0.3958\n",
      "Epoch 505/1000\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.4681 - acc: 0.7390 - val_loss: 1.8675 - val_acc: 0.4532\n",
      "Epoch 506/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.4138 - acc: 0.7651 - val_loss: 1.8457 - val_acc: 0.4743\n",
      "Epoch 507/1000\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.4247 - acc: 0.7696 - val_loss: 1.7643 - val_acc: 0.4955\n",
      "Epoch 508/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4951 - acc: 0.7397 - val_loss: 1.7204 - val_acc: 0.5045\n",
      "Epoch 509/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.5076 - acc: 0.7404 - val_loss: 1.7605 - val_acc: 0.4804\n",
      "Epoch 510/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4075 - acc: 0.7605 - val_loss: 1.7790 - val_acc: 0.4834\n",
      "Epoch 511/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.3973 - acc: 0.7777 - val_loss: 1.9300 - val_acc: 0.4350\n",
      "Epoch 512/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.4078 - acc: 0.7534 - val_loss: 1.8915 - val_acc: 0.4592\n",
      "Epoch 513/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.4277 - acc: 0.7524 - val_loss: 1.9780 - val_acc: 0.4169\n",
      "Epoch 514/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.4113 - acc: 0.7439 - val_loss: 1.9362 - val_acc: 0.4260\n",
      "Epoch 515/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4303 - acc: 0.7372 - val_loss: 1.8986 - val_acc: 0.4441\n",
      "Epoch 516/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.4215 - acc: 0.7386 - val_loss: 1.8952 - val_acc: 0.4502\n",
      "Epoch 517/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4402 - acc: 0.7485 - val_loss: 1.8529 - val_acc: 0.4532\n",
      "Epoch 518/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.3966 - acc: 0.7693 - val_loss: 1.9242 - val_acc: 0.4592\n",
      "Epoch 519/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.3874 - acc: 0.7725 - val_loss: 2.0173 - val_acc: 0.4441\n",
      "Epoch 520/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.4092 - acc: 0.7728 - val_loss: 2.0228 - val_acc: 0.4320\n",
      "Epoch 521/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4527 - acc: 0.7330 - val_loss: 2.0502 - val_acc: 0.4230\n",
      "Epoch 522/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4533 - acc: 0.7548 - val_loss: 2.0101 - val_acc: 0.4592\n",
      "Epoch 523/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.4220 - acc: 0.7601 - val_loss: 2.0222 - val_acc: 0.4290\n",
      "Epoch 524/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.4236 - acc: 0.7661 - val_loss: 2.0740 - val_acc: 0.4109\n",
      "Epoch 525/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.4567 - acc: 0.7298 - val_loss: 2.0177 - val_acc: 0.4441\n",
      "Epoch 526/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4215 - acc: 0.7534 - val_loss: 1.9444 - val_acc: 0.4471\n",
      "Epoch 527/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3964 - acc: 0.7615 - val_loss: 1.8860 - val_acc: 0.4683\n",
      "Epoch 528/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4264 - acc: 0.7587 - val_loss: 1.9425 - val_acc: 0.4532\n",
      "Epoch 529/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4340 - acc: 0.7446 - val_loss: 1.9821 - val_acc: 0.4502\n",
      "Epoch 530/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.4243 - acc: 0.7453 - val_loss: 2.0303 - val_acc: 0.4350\n",
      "Epoch 531/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4375 - acc: 0.7524 - val_loss: 2.0388 - val_acc: 0.4230\n",
      "Epoch 532/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4306 - acc: 0.7478 - val_loss: 1.9624 - val_acc: 0.4381\n",
      "Epoch 533/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4000 - acc: 0.7636 - val_loss: 1.9970 - val_acc: 0.4532\n",
      "Epoch 534/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3980 - acc: 0.7647 - val_loss: 2.0304 - val_acc: 0.4411\n",
      "Epoch 535/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4145 - acc: 0.7482 - val_loss: 2.0262 - val_acc: 0.4260\n",
      "Epoch 536/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3977 - acc: 0.7629 - val_loss: 1.9830 - val_acc: 0.4441\n",
      "Epoch 537/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4018 - acc: 0.7732 - val_loss: 2.1867 - val_acc: 0.3897\n",
      "Epoch 538/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4593 - acc: 0.7348 - val_loss: 2.0477 - val_acc: 0.4139\n",
      "Epoch 539/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.4511 - acc: 0.7489 - val_loss: 1.8819 - val_acc: 0.4622\n",
      "Epoch 540/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.4172 - acc: 0.7732 - val_loss: 1.8199 - val_acc: 0.4924\n",
      "Epoch 541/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.3990 - acc: 0.7777 - val_loss: 1.8397 - val_acc: 0.4743\n",
      "Epoch 542/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3843 - acc: 0.7851 - val_loss: 1.9349 - val_acc: 0.4622\n",
      "Epoch 543/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3993 - acc: 0.7710 - val_loss: 2.0127 - val_acc: 0.4290\n",
      "Epoch 544/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4474 - acc: 0.7566 - val_loss: 1.9222 - val_acc: 0.4502\n",
      "Epoch 545/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4110 - acc: 0.7756 - val_loss: 1.8929 - val_acc: 0.4713\n",
      "Epoch 546/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3863 - acc: 0.7788 - val_loss: 1.8525 - val_acc: 0.4834\n",
      "Epoch 547/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.4433 - acc: 0.7573 - val_loss: 1.8008 - val_acc: 0.5015\n",
      "Epoch 548/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4789 - acc: 0.7492 - val_loss: 1.7928 - val_acc: 0.4894\n",
      "Epoch 549/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.4110 - acc: 0.7781 - val_loss: 1.8889 - val_acc: 0.4592\n",
      "Epoch 550/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3992 - acc: 0.7718 - val_loss: 1.9343 - val_acc: 0.4532\n",
      "Epoch 551/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.3817 - acc: 0.7862 - val_loss: 1.9188 - val_acc: 0.4622\n",
      "Epoch 552/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3749 - acc: 0.7830 - val_loss: 1.9617 - val_acc: 0.4653\n",
      "Epoch 553/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3756 - acc: 0.7908 - val_loss: 2.0852 - val_acc: 0.4109\n",
      "Epoch 554/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4251 - acc: 0.7598 - val_loss: 2.0016 - val_acc: 0.4230\n",
      "Epoch 555/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.4070 - acc: 0.7718 - val_loss: 1.8408 - val_acc: 0.5015\n",
      "Epoch 556/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4088 - acc: 0.7756 - val_loss: 1.8639 - val_acc: 0.5045\n",
      "Epoch 557/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.4134 - acc: 0.7816 - val_loss: 1.8835 - val_acc: 0.4864\n",
      "Epoch 558/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3878 - acc: 0.7865 - val_loss: 1.8859 - val_acc: 0.4894\n",
      "Epoch 559/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3879 - acc: 0.7844 - val_loss: 1.9114 - val_acc: 0.4773\n",
      "Epoch 560/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3945 - acc: 0.7703 - val_loss: 1.9636 - val_acc: 0.4592\n",
      "Epoch 561/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3912 - acc: 0.7795 - val_loss: 2.0204 - val_acc: 0.4532\n",
      "Epoch 562/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3978 - acc: 0.7777 - val_loss: 2.0768 - val_acc: 0.4350\n",
      "Epoch 563/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.4277 - acc: 0.7661 - val_loss: 2.0169 - val_acc: 0.4713\n",
      "Epoch 564/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.3852 - acc: 0.7922 - val_loss: 2.0554 - val_acc: 0.4411\n",
      "Epoch 565/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3933 - acc: 0.7809 - val_loss: 2.0345 - val_acc: 0.4320\n",
      "Epoch 566/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3825 - acc: 0.7763 - val_loss: 2.0450 - val_acc: 0.4653\n",
      "Epoch 567/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3791 - acc: 0.7872 - val_loss: 2.0480 - val_acc: 0.4592\n",
      "Epoch 568/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.3976 - acc: 0.7746 - val_loss: 2.0673 - val_acc: 0.4743\n",
      "Epoch 569/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3874 - acc: 0.7795 - val_loss: 2.0702 - val_acc: 0.4471\n",
      "Epoch 570/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3740 - acc: 0.7770 - val_loss: 1.9574 - val_acc: 0.4924\n",
      "Epoch 571/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.4149 - acc: 0.7732 - val_loss: 1.8780 - val_acc: 0.5136\n",
      "Epoch 572/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.6007 - acc: 0.7429 - val_loss: 1.8506 - val_acc: 0.4894\n",
      "Epoch 573/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3898 - acc: 0.7858 - val_loss: 1.9338 - val_acc: 0.4773\n",
      "Epoch 574/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.3581 - acc: 0.7901 - val_loss: 2.0054 - val_acc: 0.4653\n",
      "Epoch 575/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.3668 - acc: 0.7887 - val_loss: 1.9718 - val_acc: 0.4804\n",
      "Epoch 576/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3642 - acc: 0.7911 - val_loss: 1.9484 - val_acc: 0.4562\n",
      "Epoch 577/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3766 - acc: 0.7946 - val_loss: 1.9252 - val_acc: 0.4804\n",
      "Epoch 578/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3808 - acc: 0.7865 - val_loss: 1.9934 - val_acc: 0.4622\n",
      "Epoch 579/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3735 - acc: 0.7876 - val_loss: 2.0507 - val_acc: 0.4834\n",
      "Epoch 580/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3682 - acc: 0.7971 - val_loss: 2.1905 - val_acc: 0.4411\n",
      "Epoch 581/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4249 - acc: 0.7686 - val_loss: 2.0145 - val_acc: 0.4653\n",
      "Epoch 582/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.3920 - acc: 0.7813 - val_loss: 2.0060 - val_acc: 0.4622\n",
      "Epoch 583/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.4143 - acc: 0.7788 - val_loss: 1.9546 - val_acc: 0.4622\n",
      "Epoch 584/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.4127 - acc: 0.7742 - val_loss: 1.9146 - val_acc: 0.4864\n",
      "Epoch 585/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4330 - acc: 0.7841 - val_loss: 1.9029 - val_acc: 0.4864\n",
      "Epoch 586/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.3880 - acc: 0.7936 - val_loss: 2.0322 - val_acc: 0.4653\n",
      "Epoch 587/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.3764 - acc: 0.7890 - val_loss: 1.9473 - val_acc: 0.4985\n",
      "Epoch 588/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3665 - acc: 0.7957 - val_loss: 1.9798 - val_acc: 0.4955\n",
      "Epoch 589/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4144 - acc: 0.7689 - val_loss: 1.9325 - val_acc: 0.5106\n",
      "Epoch 590/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.4204 - acc: 0.7858 - val_loss: 1.9135 - val_acc: 0.4834\n",
      "Epoch 591/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.3962 - acc: 0.7728 - val_loss: 1.9580 - val_acc: 0.4713\n",
      "Epoch 592/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3604 - acc: 0.7806 - val_loss: 2.0111 - val_acc: 0.4622\n",
      "Epoch 593/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3846 - acc: 0.7658 - val_loss: 2.0315 - val_acc: 0.4683\n",
      "Epoch 594/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3889 - acc: 0.7679 - val_loss: 1.9708 - val_acc: 0.4743\n",
      "Epoch 595/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3849 - acc: 0.7661 - val_loss: 1.9576 - val_acc: 0.4955\n",
      "Epoch 596/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4015 - acc: 0.7651 - val_loss: 1.9733 - val_acc: 0.4894\n",
      "Epoch 597/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3662 - acc: 0.7725 - val_loss: 2.1749 - val_acc: 0.4411\n",
      "Epoch 598/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.3873 - acc: 0.7707 - val_loss: 2.1268 - val_acc: 0.4502\n",
      "Epoch 599/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4239 - acc: 0.7594 - val_loss: 2.2441 - val_acc: 0.3897\n",
      "Epoch 600/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.4406 - acc: 0.7482 - val_loss: 2.1320 - val_acc: 0.4350\n",
      "Epoch 601/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4001 - acc: 0.7619 - val_loss: 1.9482 - val_acc: 0.4804\n",
      "Epoch 602/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3651 - acc: 0.7830 - val_loss: 1.9976 - val_acc: 0.4653\n",
      "Epoch 603/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.3605 - acc: 0.7869 - val_loss: 1.9993 - val_acc: 0.4532\n",
      "Epoch 604/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.3657 - acc: 0.7823 - val_loss: 2.0253 - val_acc: 0.4622\n",
      "Epoch 605/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3738 - acc: 0.7788 - val_loss: 1.9899 - val_acc: 0.4924\n",
      "Epoch 606/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3651 - acc: 0.7806 - val_loss: 1.9983 - val_acc: 0.4804\n",
      "Epoch 607/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3518 - acc: 0.8017 - val_loss: 1.9993 - val_acc: 0.4834\n",
      "Epoch 608/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3729 - acc: 0.7851 - val_loss: 2.0249 - val_acc: 0.4834\n",
      "Epoch 609/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3576 - acc: 0.7954 - val_loss: 2.0528 - val_acc: 0.4653\n",
      "Epoch 610/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3660 - acc: 0.7799 - val_loss: 2.1385 - val_acc: 0.4653\n",
      "Epoch 611/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.4123 - acc: 0.7513 - val_loss: 2.0621 - val_acc: 0.4683\n",
      "Epoch 612/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.3935 - acc: 0.7580 - val_loss: 1.9897 - val_acc: 0.5015\n",
      "Epoch 613/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3626 - acc: 0.7795 - val_loss: 2.0049 - val_acc: 0.4864\n",
      "Epoch 614/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3542 - acc: 0.7890 - val_loss: 2.0253 - val_acc: 0.4955\n",
      "Epoch 615/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3926 - acc: 0.7901 - val_loss: 1.9855 - val_acc: 0.5015\n",
      "Epoch 616/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4741 - acc: 0.7626 - val_loss: 1.9999 - val_acc: 0.4864\n",
      "Epoch 617/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4008 - acc: 0.7964 - val_loss: 1.9984 - val_acc: 0.4804\n",
      "Epoch 618/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3454 - acc: 0.7992 - val_loss: 2.0871 - val_acc: 0.4683\n",
      "Epoch 619/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.3248 - acc: 0.8070 - val_loss: 2.0630 - val_acc: 0.4743\n",
      "Epoch 620/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3506 - acc: 0.7957 - val_loss: 2.1369 - val_acc: 0.4502\n",
      "Epoch 621/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3317 - acc: 0.8073 - val_loss: 2.1018 - val_acc: 0.4653\n",
      "Epoch 622/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3473 - acc: 0.7894 - val_loss: 2.1083 - val_acc: 0.4653\n",
      "Epoch 623/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3440 - acc: 0.7971 - val_loss: 2.0938 - val_acc: 0.5045\n",
      "Epoch 624/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.4143 - acc: 0.7668 - val_loss: 2.1019 - val_acc: 0.4834\n",
      "Epoch 625/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.4319 - acc: 0.7386 - val_loss: 2.1092 - val_acc: 0.4532\n",
      "Epoch 626/1000\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.3626 - acc: 0.7791 - val_loss: 2.2025 - val_acc: 0.4441\n",
      "Epoch 627/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3798 - acc: 0.7816 - val_loss: 2.3480 - val_acc: 0.3958\n",
      "Epoch 628/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4106 - acc: 0.7679 - val_loss: 2.4255 - val_acc: 0.4048\n",
      "Epoch 629/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4089 - acc: 0.7703 - val_loss: 2.1060 - val_acc: 0.4592\n",
      "Epoch 630/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3255 - acc: 0.8172 - val_loss: 2.1161 - val_acc: 0.4562\n",
      "Epoch 631/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3298 - acc: 0.8094 - val_loss: 2.1981 - val_acc: 0.4471\n",
      "Epoch 632/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3483 - acc: 0.7989 - val_loss: 2.2408 - val_acc: 0.4562\n",
      "Epoch 633/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3485 - acc: 0.7950 - val_loss: 2.2672 - val_acc: 0.4441\n",
      "Epoch 634/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3735 - acc: 0.7802 - val_loss: 2.2980 - val_acc: 0.4320\n",
      "Epoch 635/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3856 - acc: 0.7813 - val_loss: 2.2062 - val_acc: 0.4502\n",
      "Epoch 636/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3752 - acc: 0.7848 - val_loss: 2.0858 - val_acc: 0.4743\n",
      "Epoch 637/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3549 - acc: 0.7925 - val_loss: 2.1282 - val_acc: 0.4924\n",
      "Epoch 638/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.4003 - acc: 0.7552 - val_loss: 2.0748 - val_acc: 0.4804\n",
      "Epoch 639/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3359 - acc: 0.7954 - val_loss: 2.0605 - val_acc: 0.4985\n",
      "Epoch 640/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.3440 - acc: 0.7957 - val_loss: 2.0731 - val_acc: 0.4683\n",
      "Epoch 641/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3307 - acc: 0.8137 - val_loss: 2.0788 - val_acc: 0.4804\n",
      "Epoch 642/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3499 - acc: 0.8006 - val_loss: 2.0731 - val_acc: 0.4924\n",
      "Epoch 643/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.3282 - acc: 0.8094 - val_loss: 2.0997 - val_acc: 0.4894\n",
      "Epoch 644/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3374 - acc: 0.8077 - val_loss: 2.1578 - val_acc: 0.4864\n",
      "Epoch 645/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4141 - acc: 0.7844 - val_loss: 2.1037 - val_acc: 0.4562\n",
      "Epoch 646/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4245 - acc: 0.7809 - val_loss: 2.1250 - val_acc: 0.4653\n",
      "Epoch 647/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.3316 - acc: 0.8123 - val_loss: 2.0971 - val_acc: 0.4683\n",
      "Epoch 648/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.3367 - acc: 0.8133 - val_loss: 2.0881 - val_acc: 0.4894\n",
      "Epoch 649/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3301 - acc: 0.8108 - val_loss: 2.1175 - val_acc: 0.5015\n",
      "Epoch 650/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.3843 - acc: 0.7770 - val_loss: 2.2117 - val_acc: 0.4562\n",
      "Epoch 651/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3664 - acc: 0.7728 - val_loss: 2.2064 - val_acc: 0.4411\n",
      "Epoch 652/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3268 - acc: 0.8027 - val_loss: 2.2466 - val_acc: 0.4350\n",
      "Epoch 653/1000\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.3245 - acc: 0.8130 - val_loss: 2.3132 - val_acc: 0.4199\n",
      "Epoch 654/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3537 - acc: 0.7985 - val_loss: 2.2104 - val_acc: 0.4713\n",
      "Epoch 655/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.3351 - acc: 0.8144 - val_loss: 2.2345 - val_acc: 0.4502\n",
      "Epoch 656/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3310 - acc: 0.8119 - val_loss: 2.2943 - val_acc: 0.4350\n",
      "Epoch 657/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3509 - acc: 0.8066 - val_loss: 2.4301 - val_acc: 0.4048\n",
      "Epoch 658/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.3892 - acc: 0.7876 - val_loss: 2.3141 - val_acc: 0.4411\n",
      "Epoch 659/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3668 - acc: 0.7978 - val_loss: 2.2762 - val_acc: 0.4743\n",
      "Epoch 660/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.4017 - acc: 0.7619 - val_loss: 2.1519 - val_acc: 0.4743\n",
      "Epoch 661/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3384 - acc: 0.8003 - val_loss: 2.1611 - val_acc: 0.4683\n",
      "Epoch 662/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.3338 - acc: 0.7943 - val_loss: 2.1935 - val_acc: 0.4562\n",
      "Epoch 663/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3081 - acc: 0.8197 - val_loss: 2.1959 - val_acc: 0.4592\n",
      "Epoch 664/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3278 - acc: 0.8045 - val_loss: 2.2444 - val_acc: 0.4592\n",
      "Epoch 665/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.3355 - acc: 0.8042 - val_loss: 2.2761 - val_acc: 0.4441\n",
      "Epoch 666/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3513 - acc: 0.7827 - val_loss: 2.1420 - val_acc: 0.4743\n",
      "Epoch 667/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3505 - acc: 0.7943 - val_loss: 2.0941 - val_acc: 0.4743\n",
      "Epoch 668/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3634 - acc: 0.7841 - val_loss: 2.0889 - val_acc: 0.4924\n",
      "Epoch 669/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3557 - acc: 0.7894 - val_loss: 2.1261 - val_acc: 0.4894\n",
      "Epoch 670/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.3138 - acc: 0.8211 - val_loss: 2.1527 - val_acc: 0.4743\n",
      "Epoch 671/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.3270 - acc: 0.8151 - val_loss: 2.1260 - val_acc: 0.4985\n",
      "Epoch 672/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3496 - acc: 0.7999 - val_loss: 2.1542 - val_acc: 0.4622\n",
      "Epoch 673/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3947 - acc: 0.7957 - val_loss: 2.1628 - val_acc: 0.4562\n",
      "Epoch 674/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.3576 - acc: 0.8179 - val_loss: 2.1406 - val_acc: 0.4683\n",
      "Epoch 675/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3254 - acc: 0.8274 - val_loss: 2.1116 - val_acc: 0.4713\n",
      "Epoch 676/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3123 - acc: 0.8175 - val_loss: 2.1632 - val_acc: 0.4713\n",
      "Epoch 677/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3195 - acc: 0.8073 - val_loss: 2.2109 - val_acc: 0.4834\n",
      "Epoch 678/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3456 - acc: 0.7897 - val_loss: 2.2781 - val_acc: 0.4532\n",
      "Epoch 679/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.3447 - acc: 0.7872 - val_loss: 2.3343 - val_acc: 0.4562\n",
      "Epoch 680/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3530 - acc: 0.7865 - val_loss: 2.2614 - val_acc: 0.4713\n",
      "Epoch 681/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3131 - acc: 0.8158 - val_loss: 2.2242 - val_acc: 0.4713\n",
      "Epoch 682/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3138 - acc: 0.8179 - val_loss: 2.2845 - val_acc: 0.4592\n",
      "Epoch 683/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3115 - acc: 0.8190 - val_loss: 2.3046 - val_acc: 0.4622\n",
      "Epoch 684/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3514 - acc: 0.8035 - val_loss: 2.4332 - val_acc: 0.4230\n",
      "Epoch 685/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3983 - acc: 0.7802 - val_loss: 2.3205 - val_acc: 0.4532\n",
      "Epoch 686/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3588 - acc: 0.8038 - val_loss: 2.2331 - val_acc: 0.4834\n",
      "Epoch 687/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3429 - acc: 0.8073 - val_loss: 2.1901 - val_acc: 0.4773\n",
      "Epoch 688/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3568 - acc: 0.8186 - val_loss: 2.1713 - val_acc: 0.4834\n",
      "Epoch 689/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3858 - acc: 0.8123 - val_loss: 2.1156 - val_acc: 0.4955\n",
      "Epoch 690/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.4031 - acc: 0.8031 - val_loss: 2.1676 - val_acc: 0.4773\n",
      "Epoch 691/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3108 - acc: 0.8302 - val_loss: 2.2363 - val_acc: 0.4592\n",
      "Epoch 692/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.3039 - acc: 0.8274 - val_loss: 2.2286 - val_acc: 0.4713\n",
      "Epoch 693/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3029 - acc: 0.8313 - val_loss: 2.2475 - val_acc: 0.4562\n",
      "Epoch 694/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.3109 - acc: 0.8228 - val_loss: 2.3379 - val_acc: 0.4592\n",
      "Epoch 695/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3351 - acc: 0.8137 - val_loss: 2.3519 - val_acc: 0.4502\n",
      "Epoch 696/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.3333 - acc: 0.8112 - val_loss: 2.3005 - val_acc: 0.4653\n",
      "Epoch 697/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.3448 - acc: 0.8049 - val_loss: 2.2910 - val_acc: 0.4592\n",
      "Epoch 698/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.3149 - acc: 0.8214 - val_loss: 2.2585 - val_acc: 0.4532\n",
      "Epoch 699/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.3110 - acc: 0.8267 - val_loss: 2.3061 - val_acc: 0.4592\n",
      "Epoch 700/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2859 - acc: 0.8440 - val_loss: 2.3082 - val_acc: 0.4592\n",
      "Epoch 701/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3092 - acc: 0.8267 - val_loss: 2.3326 - val_acc: 0.4502\n",
      "Epoch 702/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3242 - acc: 0.8144 - val_loss: 2.4027 - val_acc: 0.4683\n",
      "Epoch 703/1000\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.3823 - acc: 0.7777 - val_loss: 2.6758 - val_acc: 0.4169\n",
      "Epoch 704/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.4997 - acc: 0.7274 - val_loss: 2.5832 - val_acc: 0.3837\n",
      "Epoch 705/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3607 - acc: 0.7936 - val_loss: 2.2523 - val_acc: 0.4532\n",
      "Epoch 706/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.2955 - acc: 0.8394 - val_loss: 2.2460 - val_acc: 0.4653\n",
      "Epoch 707/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.2987 - acc: 0.8288 - val_loss: 2.2247 - val_acc: 0.4713\n",
      "Epoch 708/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.2878 - acc: 0.8344 - val_loss: 2.2447 - val_acc: 0.4683\n",
      "Epoch 709/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3076 - acc: 0.8221 - val_loss: 2.2750 - val_acc: 0.4773\n",
      "Epoch 710/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3289 - acc: 0.8080 - val_loss: 2.2463 - val_acc: 0.4955\n",
      "Epoch 711/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.3446 - acc: 0.7943 - val_loss: 2.2648 - val_acc: 0.4743\n",
      "Epoch 712/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3145 - acc: 0.8052 - val_loss: 2.3088 - val_acc: 0.4743\n",
      "Epoch 713/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3172 - acc: 0.8108 - val_loss: 2.2718 - val_acc: 0.4804\n",
      "Epoch 714/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3144 - acc: 0.8126 - val_loss: 2.3085 - val_acc: 0.4834\n",
      "Epoch 715/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3390 - acc: 0.8049 - val_loss: 2.4572 - val_acc: 0.4471\n",
      "Epoch 716/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.3654 - acc: 0.7887 - val_loss: 2.3138 - val_acc: 0.4653\n",
      "Epoch 717/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.3265 - acc: 0.8193 - val_loss: 2.4080 - val_acc: 0.4532\n",
      "Epoch 718/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3178 - acc: 0.8186 - val_loss: 2.4213 - val_acc: 0.4350\n",
      "Epoch 719/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.3149 - acc: 0.8285 - val_loss: 2.3666 - val_acc: 0.4441\n",
      "Epoch 720/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3306 - acc: 0.8154 - val_loss: 2.3143 - val_acc: 0.4773\n",
      "Epoch 721/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3441 - acc: 0.8087 - val_loss: 2.2784 - val_acc: 0.4773\n",
      "Epoch 722/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3244 - acc: 0.8323 - val_loss: 2.2966 - val_acc: 0.4713\n",
      "Epoch 723/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2965 - acc: 0.8334 - val_loss: 2.2858 - val_acc: 0.4622\n",
      "Epoch 724/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2994 - acc: 0.8232 - val_loss: 2.1917 - val_acc: 0.5076\n",
      "Epoch 725/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3600 - acc: 0.8073 - val_loss: 2.1754 - val_acc: 0.5287\n",
      "Epoch 726/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3905 - acc: 0.7872 - val_loss: 2.1950 - val_acc: 0.4955\n",
      "Epoch 727/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.3056 - acc: 0.8225 - val_loss: 2.2722 - val_acc: 0.4653\n",
      "Epoch 728/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2918 - acc: 0.8302 - val_loss: 2.2876 - val_acc: 0.4683\n",
      "Epoch 729/1000\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.3081 - acc: 0.8299 - val_loss: 2.3139 - val_acc: 0.4773\n",
      "Epoch 730/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3003 - acc: 0.8330 - val_loss: 2.3570 - val_acc: 0.4773\n",
      "Epoch 731/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2938 - acc: 0.8316 - val_loss: 2.3813 - val_acc: 0.4592\n",
      "Epoch 732/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.3120 - acc: 0.8295 - val_loss: 2.3444 - val_acc: 0.4622\n",
      "Epoch 733/1000\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.3794 - acc: 0.8056 - val_loss: 2.2344 - val_acc: 0.4804\n",
      "Epoch 734/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.4132 - acc: 0.8052 - val_loss: 2.2716 - val_acc: 0.4622\n",
      "Epoch 735/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.2965 - acc: 0.8302 - val_loss: 2.2779 - val_acc: 0.4622\n",
      "Epoch 736/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.3002 - acc: 0.8267 - val_loss: 2.3451 - val_acc: 0.4683\n",
      "Epoch 737/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.2841 - acc: 0.8359 - val_loss: 2.4039 - val_acc: 0.4653\n",
      "Epoch 738/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.2926 - acc: 0.8359 - val_loss: 2.4057 - val_acc: 0.4562\n",
      "Epoch 739/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2881 - acc: 0.8366 - val_loss: 2.4245 - val_acc: 0.4653\n",
      "Epoch 740/1000\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.3186 - acc: 0.8256 - val_loss: 2.4922 - val_acc: 0.4381\n",
      "Epoch 741/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3266 - acc: 0.8242 - val_loss: 2.5138 - val_acc: 0.4350\n",
      "Epoch 742/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.3141 - acc: 0.8242 - val_loss: 2.4399 - val_acc: 0.4381\n",
      "Epoch 743/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.2939 - acc: 0.8306 - val_loss: 2.3838 - val_acc: 0.4532\n",
      "Epoch 744/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.3157 - acc: 0.8154 - val_loss: 2.5087 - val_acc: 0.4471\n",
      "Epoch 745/1000\n",
      "2839/2839 [==============================] - 0s 90us/step - loss: 0.4163 - acc: 0.7665 - val_loss: 2.4623 - val_acc: 0.4502\n",
      "Epoch 746/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3744 - acc: 0.8080 - val_loss: 2.2664 - val_acc: 0.4653\n",
      "Epoch 747/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.2788 - acc: 0.8443 - val_loss: 2.2912 - val_acc: 0.4653\n",
      "Epoch 748/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2660 - acc: 0.8429 - val_loss: 2.3744 - val_acc: 0.4592\n",
      "Epoch 749/1000\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.2853 - acc: 0.8443 - val_loss: 2.4146 - val_acc: 0.4532\n",
      "Epoch 750/1000\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.2845 - acc: 0.8380 - val_loss: 2.4064 - val_acc: 0.4683\n",
      "Epoch 751/1000\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 0.2820 - acc: 0.8436 - val_loss: 2.5394 - val_acc: 0.4441\n",
      "Epoch 752/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3436 - acc: 0.8144 - val_loss: 2.8274 - val_acc: 0.3958\n",
      "Epoch 753/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.4073 - acc: 0.7827 - val_loss: 2.4631 - val_acc: 0.4622\n",
      "Epoch 754/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.3116 - acc: 0.8309 - val_loss: 2.3499 - val_acc: 0.4532\n",
      "Epoch 755/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.2825 - acc: 0.8359 - val_loss: 2.3924 - val_acc: 0.4683\n",
      "Epoch 756/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2939 - acc: 0.8292 - val_loss: 2.3818 - val_acc: 0.4683\n",
      "Epoch 757/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.3174 - acc: 0.7968 - val_loss: 2.3593 - val_acc: 0.4683\n",
      "Epoch 758/1000\n",
      "2839/2839 [==============================] - 0s 87us/step - loss: 0.2775 - acc: 0.8179 - val_loss: 2.3386 - val_acc: 0.4834\n",
      "Epoch 759/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.2804 - acc: 0.8362 - val_loss: 2.3567 - val_acc: 0.4773\n",
      "Epoch 760/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.2933 - acc: 0.8278 - val_loss: 2.3204 - val_acc: 0.5015\n",
      "Epoch 761/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3669 - acc: 0.7901 - val_loss: 2.3138 - val_acc: 0.4924\n",
      "Epoch 762/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3226 - acc: 0.8084 - val_loss: 2.3095 - val_acc: 0.5196\n",
      "Epoch 763/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2853 - acc: 0.8281 - val_loss: 2.4179 - val_acc: 0.4864\n",
      "Epoch 764/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.2918 - acc: 0.8263 - val_loss: 2.4206 - val_acc: 0.4653\n",
      "Epoch 765/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2764 - acc: 0.8341 - val_loss: 2.4552 - val_acc: 0.4955\n",
      "Epoch 766/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.3010 - acc: 0.8373 - val_loss: 2.4548 - val_acc: 0.4713\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2841 - acc: 0.8429 - val_loss: 2.4318 - val_acc: 0.4532\n",
      "Epoch 768/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.2864 - acc: 0.8369 - val_loss: 2.5240 - val_acc: 0.4350\n",
      "Epoch 769/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.3259 - acc: 0.8204 - val_loss: 2.7712 - val_acc: 0.3837\n",
      "Epoch 770/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.4380 - acc: 0.7485 - val_loss: 2.6510 - val_acc: 0.4471\n",
      "Epoch 771/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3576 - acc: 0.8027 - val_loss: 2.3611 - val_acc: 0.4713\n",
      "Epoch 772/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.2785 - acc: 0.8454 - val_loss: 2.2588 - val_acc: 0.4894\n",
      "Epoch 773/1000\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.2995 - acc: 0.8302 - val_loss: 2.2783 - val_acc: 0.4834\n",
      "Epoch 774/1000\n",
      "2839/2839 [==============================] - 0s 86us/step - loss: 0.2950 - acc: 0.8232 - val_loss: 2.3864 - val_acc: 0.4864\n",
      "Epoch 775/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.2735 - acc: 0.8376 - val_loss: 2.3763 - val_acc: 0.4924\n",
      "Epoch 776/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.2719 - acc: 0.8383 - val_loss: 2.3810 - val_acc: 0.4773\n",
      "Epoch 777/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.2697 - acc: 0.8433 - val_loss: 2.3881 - val_acc: 0.4894\n",
      "Epoch 778/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2991 - acc: 0.8313 - val_loss: 2.2942 - val_acc: 0.5227\n",
      "Epoch 779/1000\n",
      "2839/2839 [==============================] - 0s 109us/step - loss: 0.3737 - acc: 0.8027 - val_loss: 2.3911 - val_acc: 0.4864\n",
      "Epoch 780/1000\n",
      "2839/2839 [==============================] - 0s 94us/step - loss: 0.3092 - acc: 0.8263 - val_loss: 2.4347 - val_acc: 0.4592\n",
      "Epoch 781/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.3062 - acc: 0.8119 - val_loss: 2.4693 - val_acc: 0.4471\n",
      "Epoch 782/1000\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.3042 - acc: 0.8140 - val_loss: 2.4962 - val_acc: 0.4471\n",
      "Epoch 783/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.3160 - acc: 0.8063 - val_loss: 2.4206 - val_acc: 0.4864\n",
      "Epoch 784/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2861 - acc: 0.8232 - val_loss: 2.4046 - val_acc: 0.4864\n",
      "Epoch 785/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.2955 - acc: 0.8253 - val_loss: 2.3542 - val_acc: 0.4834\n",
      "Epoch 786/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.2783 - acc: 0.8436 - val_loss: 2.3834 - val_acc: 0.4773\n",
      "Epoch 787/1000\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.2839 - acc: 0.8376 - val_loss: 2.3673 - val_acc: 0.4834\n",
      "Epoch 788/1000\n",
      "2839/2839 [==============================] - 0s 107us/step - loss: 0.3223 - acc: 0.8278 - val_loss: 2.3594 - val_acc: 0.4985\n",
      "Epoch 789/1000\n",
      "2839/2839 [==============================] - 0s 124us/step - loss: 0.3221 - acc: 0.8123 - val_loss: 2.3604 - val_acc: 0.4864\n",
      "Epoch 790/1000\n",
      "2839/2839 [==============================] - 0s 101us/step - loss: 0.3051 - acc: 0.8179 - val_loss: 2.3840 - val_acc: 0.4773\n",
      "Epoch 791/1000\n",
      "2839/2839 [==============================] - 0s 103us/step - loss: 0.2836 - acc: 0.8359 - val_loss: 2.4414 - val_acc: 0.4713\n",
      "Epoch 792/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2851 - acc: 0.8316 - val_loss: 2.5644 - val_acc: 0.4532\n",
      "Epoch 793/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.3294 - acc: 0.8101 - val_loss: 2.7023 - val_acc: 0.4290\n",
      "Epoch 794/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.3951 - acc: 0.7781 - val_loss: 2.6292 - val_acc: 0.4411\n",
      "Epoch 795/1000\n",
      "2839/2839 [==============================] - 0s 93us/step - loss: 0.3152 - acc: 0.8285 - val_loss: 2.3973 - val_acc: 0.4773\n",
      "Epoch 796/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2868 - acc: 0.8316 - val_loss: 2.4168 - val_acc: 0.4864\n",
      "Epoch 797/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.2557 - acc: 0.8535 - val_loss: 2.3593 - val_acc: 0.4894\n",
      "Epoch 798/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2837 - acc: 0.8580 - val_loss: 2.4176 - val_acc: 0.4834\n",
      "Epoch 799/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.2610 - acc: 0.8489 - val_loss: 2.4392 - val_acc: 0.4622\n",
      "Epoch 800/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.2512 - acc: 0.8651 - val_loss: 2.5434 - val_acc: 0.4592\n",
      "Epoch 801/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.2623 - acc: 0.8485 - val_loss: 2.6216 - val_acc: 0.4532\n",
      "Epoch 802/1000\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 0.2891 - acc: 0.8369 - val_loss: 2.7703 - val_acc: 0.4350\n",
      "Epoch 803/1000\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.4033 - acc: 0.7837 - val_loss: 2.5337 - val_acc: 0.4320\n",
      "Epoch 804/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.2948 - acc: 0.8468 - val_loss: 2.5201 - val_acc: 0.4562\n",
      "Epoch 805/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.2588 - acc: 0.8566 - val_loss: 2.4882 - val_acc: 0.4804\n",
      "Epoch 806/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2562 - acc: 0.8503 - val_loss: 2.4381 - val_acc: 0.4773\n",
      "Epoch 807/1000\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.2470 - acc: 0.8605 - val_loss: 2.5133 - val_acc: 0.4532\n",
      "Epoch 808/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.2890 - acc: 0.8426 - val_loss: 2.4175 - val_acc: 0.4924\n",
      "Epoch 809/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3583 - acc: 0.8274 - val_loss: 2.3718 - val_acc: 0.4985\n",
      "Epoch 810/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.2936 - acc: 0.8461 - val_loss: 2.5121 - val_acc: 0.4562\n",
      "Epoch 811/1000\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 0.2583 - acc: 0.8545 - val_loss: 2.4856 - val_acc: 0.4804\n",
      "Epoch 812/1000\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 0.2784 - acc: 0.8422 - val_loss: 2.5322 - val_acc: 0.4773\n",
      "Epoch 813/1000\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.2725 - acc: 0.8475 - val_loss: 2.6013 - val_acc: 0.4502\n",
      "Epoch 814/1000\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 0.3127 - acc: 0.8352 - val_loss: 2.6975 - val_acc: 0.4260\n",
      "Epoch 815/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.3156 - acc: 0.8228 - val_loss: 2.5725 - val_acc: 0.4562\n",
      "Epoch 816/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2742 - acc: 0.8552 - val_loss: 2.5417 - val_acc: 0.4924\n",
      "Epoch 817/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2466 - acc: 0.8538 - val_loss: 2.5701 - val_acc: 0.4592\n",
      "Epoch 818/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2520 - acc: 0.8612 - val_loss: 2.5853 - val_acc: 0.4622\n",
      "Epoch 819/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2695 - acc: 0.8573 - val_loss: 2.6102 - val_acc: 0.4320\n",
      "Epoch 820/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2754 - acc: 0.8559 - val_loss: 2.5998 - val_acc: 0.4622\n",
      "Epoch 821/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.2696 - acc: 0.8503 - val_loss: 2.7932 - val_acc: 0.4350\n",
      "Epoch 822/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3295 - acc: 0.8292 - val_loss: 2.6811 - val_acc: 0.4441\n",
      "Epoch 823/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.2968 - acc: 0.8355 - val_loss: 2.6309 - val_acc: 0.4713\n",
      "Epoch 824/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.3112 - acc: 0.8165 - val_loss: 2.5946 - val_acc: 0.4562\n",
      "Epoch 825/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.2899 - acc: 0.8165 - val_loss: 2.5188 - val_acc: 0.4713\n",
      "Epoch 826/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.2793 - acc: 0.8369 - val_loss: 2.5447 - val_acc: 0.4411\n",
      "Epoch 827/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2855 - acc: 0.8158 - val_loss: 2.5001 - val_acc: 0.4622\n",
      "Epoch 828/1000\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 0.2792 - acc: 0.8260 - val_loss: 2.5304 - val_acc: 0.4834\n",
      "Epoch 829/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.2628 - acc: 0.8359 - val_loss: 2.6068 - val_acc: 0.4502\n",
      "Epoch 830/1000\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.3238 - acc: 0.7918 - val_loss: 2.5490 - val_acc: 0.4683\n",
      "Epoch 831/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.2744 - acc: 0.8404 - val_loss: 2.4656 - val_acc: 0.4804\n",
      "Epoch 832/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2809 - acc: 0.8316 - val_loss: 2.4608 - val_acc: 0.4924\n",
      "Epoch 833/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.2539 - acc: 0.8591 - val_loss: 2.4732 - val_acc: 0.4743\n",
      "Epoch 834/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.2687 - acc: 0.8499 - val_loss: 2.4341 - val_acc: 0.4955\n",
      "Epoch 835/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3326 - acc: 0.8292 - val_loss: 2.4385 - val_acc: 0.5015\n",
      "Epoch 836/1000\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 0.3046 - acc: 0.8475 - val_loss: 2.4806 - val_acc: 0.4562\n",
      "Epoch 837/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.2489 - acc: 0.8654 - val_loss: 2.4975 - val_acc: 0.4804\n",
      "Epoch 838/1000\n",
      "2839/2839 [==============================] - 0s 107us/step - loss: 0.2575 - acc: 0.8580 - val_loss: 2.5860 - val_acc: 0.4562\n",
      "Epoch 839/1000\n",
      "2839/2839 [==============================] - 0s 98us/step - loss: 0.2505 - acc: 0.8602 - val_loss: 2.5414 - val_acc: 0.4683\n",
      "Epoch 840/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2437 - acc: 0.8669 - val_loss: 2.6274 - val_acc: 0.4743\n",
      "Epoch 841/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3080 - acc: 0.8355 - val_loss: 2.7595 - val_acc: 0.4260\n",
      "Epoch 842/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3661 - acc: 0.8133 - val_loss: 2.6615 - val_acc: 0.4230\n",
      "Epoch 843/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2890 - acc: 0.8517 - val_loss: 2.5879 - val_acc: 0.4562\n",
      "Epoch 844/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.2550 - acc: 0.8658 - val_loss: 2.5436 - val_acc: 0.4471\n",
      "Epoch 845/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.2552 - acc: 0.8528 - val_loss: 2.5970 - val_acc: 0.4502\n",
      "Epoch 846/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2711 - acc: 0.8524 - val_loss: 2.6252 - val_acc: 0.4471\n",
      "Epoch 847/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.2700 - acc: 0.8517 - val_loss: 2.5472 - val_acc: 0.4592\n",
      "Epoch 848/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2463 - acc: 0.8538 - val_loss: 2.5793 - val_acc: 0.4502\n",
      "Epoch 849/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2628 - acc: 0.8528 - val_loss: 2.6631 - val_acc: 0.4502\n",
      "Epoch 850/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3172 - acc: 0.8116 - val_loss: 2.5067 - val_acc: 0.4834\n",
      "Epoch 851/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3141 - acc: 0.8190 - val_loss: 2.5746 - val_acc: 0.4592\n",
      "Epoch 852/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.2838 - acc: 0.8316 - val_loss: 2.4521 - val_acc: 0.4894\n",
      "Epoch 853/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.2792 - acc: 0.8411 - val_loss: 2.4213 - val_acc: 0.5045\n",
      "Epoch 854/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.3048 - acc: 0.8341 - val_loss: 2.4867 - val_acc: 0.4894\n",
      "Epoch 855/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2819 - acc: 0.8499 - val_loss: 2.5200 - val_acc: 0.4592\n",
      "Epoch 856/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2653 - acc: 0.8584 - val_loss: 2.5439 - val_acc: 0.4471\n",
      "Epoch 857/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2307 - acc: 0.8623 - val_loss: 2.5790 - val_acc: 0.4592\n",
      "Epoch 858/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2341 - acc: 0.8721 - val_loss: 2.5919 - val_acc: 0.4683\n",
      "Epoch 859/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2792 - acc: 0.8549 - val_loss: 2.5722 - val_acc: 0.4683\n",
      "Epoch 860/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.2498 - acc: 0.8580 - val_loss: 2.7058 - val_acc: 0.4502\n",
      "Epoch 861/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.3966 - acc: 0.8049 - val_loss: 2.5623 - val_acc: 0.4502\n",
      "Epoch 862/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2956 - acc: 0.8549 - val_loss: 2.4871 - val_acc: 0.4592\n",
      "Epoch 863/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.2387 - acc: 0.8700 - val_loss: 2.5487 - val_acc: 0.4773\n",
      "Epoch 864/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2466 - acc: 0.8637 - val_loss: 2.5733 - val_acc: 0.4834\n",
      "Epoch 865/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.2496 - acc: 0.8669 - val_loss: 2.6001 - val_acc: 0.4834\n",
      "Epoch 866/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.2340 - acc: 0.8764 - val_loss: 2.6323 - val_acc: 0.4834\n",
      "Epoch 867/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.2521 - acc: 0.8598 - val_loss: 2.6214 - val_acc: 0.4834\n",
      "Epoch 868/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2566 - acc: 0.8591 - val_loss: 2.7478 - val_acc: 0.4562\n",
      "Epoch 869/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.3127 - acc: 0.8327 - val_loss: 2.7160 - val_acc: 0.4139\n",
      "Epoch 870/1000\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.3032 - acc: 0.8404 - val_loss: 2.6309 - val_acc: 0.4532\n",
      "Epoch 871/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.2453 - acc: 0.8686 - val_loss: 2.6332 - val_acc: 0.4864\n",
      "Epoch 872/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2415 - acc: 0.8623 - val_loss: 2.6651 - val_acc: 0.4653\n",
      "Epoch 873/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.2458 - acc: 0.8640 - val_loss: 2.7455 - val_acc: 0.4502\n",
      "Epoch 874/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2513 - acc: 0.8672 - val_loss: 2.6703 - val_acc: 0.4592\n",
      "Epoch 875/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2566 - acc: 0.8531 - val_loss: 2.8377 - val_acc: 0.4350\n",
      "Epoch 876/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.3009 - acc: 0.8249 - val_loss: 2.7432 - val_acc: 0.4653\n",
      "Epoch 877/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.2852 - acc: 0.8461 - val_loss: 2.7739 - val_acc: 0.4411\n",
      "Epoch 878/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.2525 - acc: 0.8559 - val_loss: 2.6662 - val_acc: 0.4713\n",
      "Epoch 879/1000\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.2483 - acc: 0.8602 - val_loss: 2.5594 - val_acc: 0.4985\n",
      "Epoch 880/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.2525 - acc: 0.8577 - val_loss: 2.6037 - val_acc: 0.4713\n",
      "Epoch 881/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.2530 - acc: 0.8556 - val_loss: 2.5904 - val_acc: 0.4834\n",
      "Epoch 882/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2755 - acc: 0.8397 - val_loss: 2.6773 - val_acc: 0.4471\n",
      "Epoch 883/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.3165 - acc: 0.8066 - val_loss: 2.6550 - val_acc: 0.4622\n",
      "Epoch 884/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.2806 - acc: 0.8341 - val_loss: 2.6039 - val_acc: 0.4924\n",
      "Epoch 885/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.2369 - acc: 0.8464 - val_loss: 2.6145 - val_acc: 0.4743\n",
      "Epoch 886/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.2422 - acc: 0.8693 - val_loss: 2.5842 - val_acc: 0.4834\n",
      "Epoch 887/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.2911 - acc: 0.8542 - val_loss: 2.4856 - val_acc: 0.5136\n",
      "Epoch 888/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.3224 - acc: 0.8415 - val_loss: 2.5095 - val_acc: 0.4985\n",
      "Epoch 889/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.2543 - acc: 0.8580 - val_loss: 2.6045 - val_acc: 0.5015\n",
      "Epoch 890/1000\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 0.2390 - acc: 0.8619 - val_loss: 2.5989 - val_acc: 0.4924\n",
      "Epoch 891/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2415 - acc: 0.8633 - val_loss: 2.6559 - val_acc: 0.4532\n",
      "Epoch 892/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.2532 - acc: 0.824 - 0s 70us/step - loss: 0.2463 - acc: 0.8415 - val_loss: 2.6472 - val_acc: 0.4773\n",
      "Epoch 893/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.2452 - acc: 0.8566 - val_loss: 2.6174 - val_acc: 0.5015\n",
      "Epoch 894/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2692 - acc: 0.8373 - val_loss: 2.6755 - val_acc: 0.5106\n",
      "Epoch 895/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2599 - acc: 0.8355 - val_loss: 2.6587 - val_acc: 0.4834\n",
      "Epoch 896/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2493 - acc: 0.8542 - val_loss: 2.6342 - val_acc: 0.4955\n",
      "Epoch 897/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.2338 - acc: 0.8669 - val_loss: 2.7323 - val_acc: 0.4622\n",
      "Epoch 898/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2362 - acc: 0.8630 - val_loss: 2.6829 - val_acc: 0.4713\n",
      "Epoch 899/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.2461 - acc: 0.8552 - val_loss: 2.7567 - val_acc: 0.4773\n",
      "Epoch 900/1000\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 0.2654 - acc: 0.8443 - val_loss: 2.7919 - val_acc: 0.4683\n",
      "Epoch 901/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.3121 - acc: 0.8267 - val_loss: 2.6739 - val_acc: 0.4502\n",
      "Epoch 902/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.3103 - acc: 0.8168 - val_loss: 2.5549 - val_acc: 0.4864\n",
      "Epoch 903/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.2591 - acc: 0.847 - 0s 65us/step - loss: 0.2538 - acc: 0.8517 - val_loss: 2.5690 - val_acc: 0.5076\n",
      "Epoch 904/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.2631 - acc: 0.8647 - val_loss: 2.6016 - val_acc: 0.5106\n",
      "Epoch 905/1000\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.2641 - acc: 0.8644 - val_loss: 2.6982 - val_acc: 0.4743\n",
      "Epoch 906/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2310 - acc: 0.8767 - val_loss: 2.7398 - val_acc: 0.4864\n",
      "Epoch 907/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2401 - acc: 0.8647 - val_loss: 2.7360 - val_acc: 0.4683\n",
      "Epoch 908/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2345 - acc: 0.8690 - val_loss: 2.7224 - val_acc: 0.4743\n",
      "Epoch 909/1000\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.2837 - acc: 0.8521 - val_loss: 2.7812 - val_acc: 0.4562\n",
      "Epoch 910/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.2769 - acc: 0.8549 - val_loss: 2.7219 - val_acc: 0.4532\n",
      "Epoch 911/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2365 - acc: 0.8707 - val_loss: 2.8044 - val_acc: 0.4683\n",
      "Epoch 912/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.2587 - acc: 0.8619 - val_loss: 2.7889 - val_acc: 0.4622\n",
      "Epoch 913/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.2333 - acc: 0.8809 - val_loss: 2.8129 - val_acc: 0.4743\n",
      "Epoch 914/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2678 - acc: 0.8552 - val_loss: 2.8964 - val_acc: 0.4622\n",
      "Epoch 915/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2647 - acc: 0.8584 - val_loss: 2.9331 - val_acc: 0.4350\n",
      "Epoch 916/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2994 - acc: 0.8214 - val_loss: 2.8049 - val_acc: 0.4562\n",
      "Epoch 917/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2776 - acc: 0.8316 - val_loss: 2.7197 - val_acc: 0.4622\n",
      "Epoch 918/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2283 - acc: 0.8591 - val_loss: 2.7111 - val_acc: 0.4683\n",
      "Epoch 919/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2351 - acc: 0.8535 - val_loss: 2.7550 - val_acc: 0.4562\n",
      "Epoch 920/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.2329 - acc: 0.8662 - val_loss: 2.6907 - val_acc: 0.4804\n",
      "Epoch 921/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2518 - acc: 0.8598 - val_loss: 2.6556 - val_acc: 0.5045\n",
      "Epoch 922/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2539 - acc: 0.8475 - val_loss: 2.6276 - val_acc: 0.4955\n",
      "Epoch 923/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2316 - acc: 0.8658 - val_loss: 2.6903 - val_acc: 0.4985\n",
      "Epoch 924/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2546 - acc: 0.8577 - val_loss: 2.6695 - val_acc: 0.4955\n",
      "Epoch 925/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2546 - acc: 0.8623 - val_loss: 2.7126 - val_acc: 0.4834\n",
      "Epoch 926/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.2505 - acc: 0.845 - 0s 69us/step - loss: 0.2661 - acc: 0.8492 - val_loss: 2.9092 - val_acc: 0.4411\n",
      "Epoch 927/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.4007 - acc: 0.7742 - val_loss: 2.7286 - val_acc: 0.4653\n",
      "Epoch 928/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2528 - acc: 0.8499 - val_loss: 2.7457 - val_acc: 0.4743\n",
      "Epoch 929/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2303 - acc: 0.8728 - val_loss: 2.7064 - val_acc: 0.4864\n",
      "Epoch 930/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.2136 - acc: 0.8806 - val_loss: 2.7748 - val_acc: 0.4834\n",
      "Epoch 931/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2350 - acc: 0.8809 - val_loss: 2.8454 - val_acc: 0.4804\n",
      "Epoch 932/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.2165 - acc: 0.8824 - val_loss: 2.9232 - val_acc: 0.4592\n",
      "Epoch 933/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2502 - acc: 0.8647 - val_loss: 3.0879 - val_acc: 0.4320\n",
      "Epoch 934/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2886 - acc: 0.8450 - val_loss: 2.8793 - val_acc: 0.4653\n",
      "Epoch 935/1000\n",
      "2839/2839 [==============================] - 0s 53us/step - loss: 0.2414 - acc: 0.8714 - val_loss: 2.7361 - val_acc: 0.4773\n",
      "Epoch 936/1000\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.2573 - acc: 0.8676 - val_loss: 2.7728 - val_acc: 0.4653\n",
      "Epoch 937/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.3017 - acc: 0.8443 - val_loss: 2.7404 - val_acc: 0.4471\n",
      "Epoch 938/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2353 - acc: 0.8767 - val_loss: 2.8060 - val_acc: 0.4653\n",
      "Epoch 939/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2204 - acc: 0.8739 - val_loss: 2.7198 - val_acc: 0.4683\n",
      "Epoch 940/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2850 - acc: 0.8528 - val_loss: 2.6413 - val_acc: 0.5015\n",
      "Epoch 941/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.2592 - acc: 0.8630 - val_loss: 2.6818 - val_acc: 0.5015\n",
      "Epoch 942/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2312 - acc: 0.8707 - val_loss: 2.7452 - val_acc: 0.4924\n",
      "Epoch 943/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.2346 - acc: 0.8735 - val_loss: 2.7817 - val_acc: 0.4955\n",
      "Epoch 944/1000\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.2399 - acc: 0.8735 - val_loss: 2.8424 - val_acc: 0.4592\n",
      "Epoch 945/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2589 - acc: 0.8665 - val_loss: 2.8685 - val_acc: 0.4562\n",
      "Epoch 946/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.2371 - acc: 0.8732 - val_loss: 2.8213 - val_acc: 0.4653\n",
      "Epoch 947/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2492 - acc: 0.8602 - val_loss: 2.8355 - val_acc: 0.4864\n",
      "Epoch 948/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2571 - acc: 0.8707 - val_loss: 2.7430 - val_acc: 0.4894\n",
      "Epoch 949/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.2386 - acc: 0.8788 - val_loss: 2.7558 - val_acc: 0.4804\n",
      "Epoch 950/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2263 - acc: 0.8876 - val_loss: 2.7551 - val_acc: 0.4955\n",
      "Epoch 951/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.2164 - acc: 0.8880 - val_loss: 2.8857 - val_acc: 0.4502\n",
      "Epoch 952/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.2962 - acc: 0.8517 - val_loss: 2.9356 - val_acc: 0.4562\n",
      "Epoch 953/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2846 - acc: 0.8496 - val_loss: 2.8380 - val_acc: 0.4683\n",
      "Epoch 954/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.2239 - acc: 0.8771 - val_loss: 2.8016 - val_acc: 0.4864\n",
      "Epoch 955/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2013 - acc: 0.8890 - val_loss: 2.7580 - val_acc: 0.5076\n",
      "Epoch 956/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.2100 - acc: 0.8788 - val_loss: 2.7190 - val_acc: 0.5136\n",
      "Epoch 957/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.2459 - acc: 0.8721 - val_loss: 2.6841 - val_acc: 0.5287\n",
      "Epoch 958/1000\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.2980 - acc: 0.8556 - val_loss: 2.7061 - val_acc: 0.5015\n",
      "Epoch 959/1000\n",
      "2839/2839 [==============================] - 0s 99us/step - loss: 0.2371 - acc: 0.8725 - val_loss: 2.7599 - val_acc: 0.4864\n",
      "Epoch 960/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.2155 - acc: 0.8873 - val_loss: 2.8185 - val_acc: 0.4592\n",
      "Epoch 961/1000\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.2157 - acc: 0.8862 - val_loss: 2.8742 - val_acc: 0.4713\n",
      "Epoch 962/1000\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.2093 - acc: 0.8795 - val_loss: 2.8201 - val_acc: 0.4683\n",
      "Epoch 963/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.2218 - acc: 0.871 - 0s 69us/step - loss: 0.2193 - acc: 0.8792 - val_loss: 2.7571 - val_acc: 0.4955\n",
      "Epoch 964/1000\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.2660 - acc: 0.8454 - val_loss: 2.8414 - val_acc: 0.4622\n",
      "Epoch 965/1000\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.3015 - acc: 0.8126 - val_loss: 2.7655 - val_acc: 0.4834\n",
      "Epoch 966/1000\n",
      "2839/2839 [==============================] - 0s 54us/step - loss: 0.2266 - acc: 0.8704 - val_loss: 2.8669 - val_acc: 0.4743\n",
      "Epoch 967/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.2219 - acc: 0.8718 - val_loss: 2.8515 - val_acc: 0.4713\n",
      "Epoch 968/1000\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 0.2227 - acc: 0.8697 - val_loss: 3.0922 - val_acc: 0.4683\n",
      "Epoch 969/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.3102 - acc: 0.8542 - val_loss: 3.2669 - val_acc: 0.4471\n",
      "Epoch 970/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.3490 - acc: 0.8281 - val_loss: 2.8210 - val_acc: 0.4562\n",
      "Epoch 971/1000\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.2278 - acc: 0.8679 - val_loss: 2.8246 - val_acc: 0.4864\n",
      "Epoch 972/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.2050 - acc: 0.8880 - val_loss: 2.8521 - val_acc: 0.4804\n",
      "Epoch 973/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2290 - acc: 0.8732 - val_loss: 2.8145 - val_acc: 0.4804\n",
      "Epoch 974/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2051 - acc: 0.8905 - val_loss: 2.9259 - val_acc: 0.4683\n",
      "Epoch 975/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.2190 - acc: 0.8802 - val_loss: 2.9449 - val_acc: 0.4653\n",
      "Epoch 976/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2096 - acc: 0.8876 - val_loss: 2.9840 - val_acc: 0.4622\n",
      "Epoch 977/1000\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 0.2863 - acc: 0.8482 - val_loss: 3.0287 - val_acc: 0.4411\n",
      "Epoch 978/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2733 - acc: 0.8577 - val_loss: 2.8190 - val_acc: 0.4743\n",
      "Epoch 979/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.2143 - acc: 0.8855 - val_loss: 2.8271 - val_acc: 0.4653\n",
      "Epoch 980/1000\n",
      "2839/2839 [==============================] - 0s 58us/step - loss: 0.1980 - acc: 0.8961 - val_loss: 2.8919 - val_acc: 0.4562\n",
      "Epoch 981/1000\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.2054 - acc: 0.8862 - val_loss: 2.9215 - val_acc: 0.4653\n",
      "Epoch 982/1000\n",
      "2839/2839 [==============================] - 0s 56us/step - loss: 0.2007 - acc: 0.8894 - val_loss: 2.8738 - val_acc: 0.5106\n",
      "Epoch 983/1000\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.2224 - acc: 0.8802 - val_loss: 2.9831 - val_acc: 0.4713\n",
      "Epoch 984/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.2288 - acc: 0.8873 - val_loss: 2.9297 - val_acc: 0.4532\n",
      "Epoch 985/1000\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 0.2126 - acc: 0.8894 - val_loss: 2.9226 - val_acc: 0.4834\n",
      "Epoch 986/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.3519 - acc: 0.8373 - val_loss: 2.8403 - val_acc: 0.4713\n",
      "Epoch 987/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.3638 - acc: 0.8443 - val_loss: 2.7907 - val_acc: 0.4773\n",
      "Epoch 988/1000\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.2237 - acc: 0.8771 - val_loss: 2.7815 - val_acc: 0.4743\n",
      "Epoch 989/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.2351 - acc: 0.8686 - val_loss: 2.7738 - val_acc: 0.4804\n",
      "Epoch 990/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.2119 - acc: 0.8746 - val_loss: 2.8172 - val_acc: 0.5076\n",
      "Epoch 991/1000\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.2109 - acc: 0.8799 - val_loss: 2.8111 - val_acc: 0.4683\n",
      "Epoch 992/1000\n",
      "2839/2839 [==============================] - 0s 52us/step - loss: 0.2129 - acc: 0.8785 - val_loss: 2.8763 - val_acc: 0.4924\n",
      "Epoch 993/1000\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 0.2074 - acc: 0.8820 - val_loss: 2.8362 - val_acc: 0.4834\n",
      "Epoch 994/1000\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.2141 - acc: 0.8831 - val_loss: 2.9563 - val_acc: 0.4653\n",
      "Epoch 995/1000\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.2100 - acc: 0.8757 - val_loss: 2.9207 - val_acc: 0.4955\n",
      "Epoch 996/1000\n",
      "2839/2839 [==============================] - 0s 55us/step - loss: 0.2093 - acc: 0.8862 - val_loss: 2.8472 - val_acc: 0.4894\n",
      "Epoch 997/1000\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 0.2556 - acc: 0.8454 - val_loss: 2.9020 - val_acc: 0.4713\n",
      "Epoch 998/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.3070 - acc: 0.8098 - val_loss: 2.8452 - val_acc: 0.4773\n",
      "Epoch 999/1000\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.2220 - acc: 0.8651 - val_loss: 2.8082 - val_acc: 0.4894\n",
      "Epoch 1000/1000\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.2103 - acc: 0.8852 - val_loss: 2.8317 - val_acc: 0.4924\n"
     ]
    }
   ],
   "source": [
    "zillow_model_2 = model.fit(x=X_train, y=y_cat_train, \n",
    "          batch_size=2000, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_cat_test),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/150\n",
      "2839/2839 [==============================] - 25s 9ms/step - loss: 1.4665 - acc: 0.2335 - val_loss: 1.5538 - val_acc: 0.3323\n",
      "Epoch 2/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.3345 - acc: 0.3353 - val_loss: 1.5288 - val_acc: 0.3293\n",
      "Epoch 3/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3296 - acc: 0.3241 - val_loss: 1.4801 - val_acc: 0.3263\n",
      "Epoch 4/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2943 - acc: 0.3575 - val_loss: 1.4591 - val_acc: 0.3293\n",
      "Epoch 5/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.2981 - acc: 0.3575 - val_loss: 1.4667 - val_acc: 0.3142\n",
      "Epoch 6/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.2871 - acc: 0.3568 - val_loss: 1.4313 - val_acc: 0.3293\n",
      "Epoch 7/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.2726 - acc: 0.3783 - val_loss: 1.4390 - val_acc: 0.3293\n",
      "Epoch 8/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.2748 - acc: 0.3621 - val_loss: 1.4171 - val_acc: 0.3263\n",
      "Epoch 9/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2723 - acc: 0.3663 - val_loss: 1.3806 - val_acc: 0.3293\n",
      "Epoch 10/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.2534 - acc: 0.3836 - val_loss: 1.4212 - val_acc: 0.3293\n",
      "Epoch 11/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.2485 - acc: 0.3825 - val_loss: 1.4331 - val_acc: 0.3263\n",
      "Epoch 12/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.2357 - acc: 0.3730 - val_loss: 1.3884 - val_acc: 0.3263\n",
      "Epoch 13/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2264 - acc: 0.4037 - val_loss: 1.4283 - val_acc: 0.3263\n",
      "Epoch 14/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2301 - acc: 0.3853 - val_loss: 1.4469 - val_acc: 0.3202\n",
      "Epoch 15/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.2209 - acc: 0.3878 - val_loss: 1.4669 - val_acc: 0.2961\n",
      "Epoch 16/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2204 - acc: 0.3734 - val_loss: 1.3843 - val_acc: 0.3263\n",
      "Epoch 17/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2042 - acc: 0.3942 - val_loss: 1.3568 - val_acc: 0.3233\n",
      "Epoch 18/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.2125 - acc: 0.3973 - val_loss: 1.3431 - val_acc: 0.3323\n",
      "Epoch 19/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2002 - acc: 0.4001 - val_loss: 1.3890 - val_acc: 0.3293\n",
      "Epoch 20/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1904 - acc: 0.3998 - val_loss: 1.3569 - val_acc: 0.3263\n",
      "Epoch 21/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1802 - acc: 0.4223 - val_loss: 1.4495 - val_acc: 0.3112\n",
      "Epoch 22/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1607 - acc: 0.4132 - val_loss: 1.3972 - val_acc: 0.3414\n",
      "Epoch 23/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1652 - acc: 0.4012 - val_loss: 1.4531 - val_acc: 0.3353\n",
      "Epoch 24/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1671 - acc: 0.3927 - val_loss: 1.4824 - val_acc: 0.3263\n",
      "Epoch 25/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.1679 - acc: 0.4079 - val_loss: 1.4450 - val_acc: 0.3353\n",
      "Epoch 26/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.1577 - acc: 0.4220 - val_loss: 1.4237 - val_acc: 0.3444\n",
      "Epoch 27/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1570 - acc: 0.4287 - val_loss: 1.4322 - val_acc: 0.3444\n",
      "Epoch 28/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1401 - acc: 0.4244 - val_loss: 1.3937 - val_acc: 0.3565\n",
      "Epoch 29/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1464 - acc: 0.4311 - val_loss: 1.4108 - val_acc: 0.3444\n",
      "Epoch 30/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1397 - acc: 0.4364 - val_loss: 1.3973 - val_acc: 0.3686\n",
      "Epoch 31/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1348 - acc: 0.4329 - val_loss: 1.3769 - val_acc: 0.3927\n",
      "Epoch 32/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1144 - acc: 0.4322 - val_loss: 1.3170 - val_acc: 0.4199\n",
      "Epoch 33/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1302 - acc: 0.4410 - val_loss: 1.4015 - val_acc: 0.3535\n",
      "Epoch 34/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1490 - acc: 0.4283 - val_loss: 1.3924 - val_acc: 0.3716\n",
      "Epoch 35/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1186 - acc: 0.4435 - val_loss: 1.4125 - val_acc: 0.3746\n",
      "Epoch 36/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1231 - acc: 0.4280 - val_loss: 1.4117 - val_acc: 0.3565\n",
      "Epoch 37/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1081 - acc: 0.4325 - val_loss: 1.2999 - val_acc: 0.4260\n",
      "Epoch 38/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1065 - acc: 0.4547 - val_loss: 1.2733 - val_acc: 0.4381\n",
      "Epoch 39/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0958 - acc: 0.4484 - val_loss: 1.2202 - val_acc: 0.4350\n",
      "Epoch 40/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1044 - acc: 0.4685 - val_loss: 1.2879 - val_acc: 0.4260\n",
      "Epoch 41/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0990 - acc: 0.4597 - val_loss: 1.2391 - val_acc: 0.4471\n",
      "Epoch 42/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0785 - acc: 0.4625 - val_loss: 1.2777 - val_acc: 0.4350\n",
      "Epoch 43/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0842 - acc: 0.4628 - val_loss: 1.2284 - val_acc: 0.4471\n",
      "Epoch 44/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0740 - acc: 0.4716 - val_loss: 1.2339 - val_acc: 0.4350\n",
      "Epoch 45/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0787 - acc: 0.4671 - val_loss: 1.2030 - val_acc: 0.4743\n",
      "Epoch 46/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0863 - acc: 0.4572 - val_loss: 1.2132 - val_acc: 0.4653\n",
      "Epoch 47/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0657 - acc: 0.4819 - val_loss: 1.2707 - val_acc: 0.4230\n",
      "Epoch 48/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0663 - acc: 0.4660 - val_loss: 1.1956 - val_acc: 0.4924\n",
      "Epoch 49/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0608 - acc: 0.4773 - val_loss: 1.2084 - val_acc: 0.4622\n",
      "Epoch 50/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0639 - acc: 0.4935 - val_loss: 1.2505 - val_acc: 0.4471\n",
      "Epoch 51/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0587 - acc: 0.4639 - val_loss: 1.2779 - val_acc: 0.4441\n",
      "Epoch 52/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0742 - acc: 0.4621 - val_loss: 1.2937 - val_acc: 0.4320\n",
      "Epoch 53/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0661 - acc: 0.4702 - val_loss: 1.4799 - val_acc: 0.3565\n",
      "Epoch 54/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0906 - acc: 0.4086 - val_loss: 1.2586 - val_acc: 0.4411\n",
      "Epoch 55/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0289 - acc: 0.4731 - val_loss: 1.2062 - val_acc: 0.5076\n",
      "Epoch 56/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0355 - acc: 0.4805 - val_loss: 1.2794 - val_acc: 0.4230\n",
      "Epoch 57/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0409 - acc: 0.4815 - val_loss: 1.3031 - val_acc: 0.4079\n",
      "Epoch 58/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0211 - acc: 0.4773 - val_loss: 1.2633 - val_acc: 0.4350\n",
      "Epoch 59/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0164 - acc: 0.4826 - val_loss: 1.3325 - val_acc: 0.3927\n",
      "Epoch 60/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0197 - acc: 0.4766 - val_loss: 1.2945 - val_acc: 0.4169\n",
      "Epoch 61/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0305 - acc: 0.4812 - val_loss: 1.2885 - val_acc: 0.4441\n",
      "Epoch 62/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0274 - acc: 0.4801 - val_loss: 1.2532 - val_acc: 0.4381\n",
      "Epoch 63/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0152 - acc: 0.4935 - val_loss: 1.3054 - val_acc: 0.4109\n",
      "Epoch 64/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0206 - acc: 0.4713 - val_loss: 1.2360 - val_acc: 0.4471\n",
      "Epoch 65/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9998 - acc: 0.4805 - val_loss: 1.2704 - val_acc: 0.4230\n",
      "Epoch 66/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0064 - acc: 0.4861 - val_loss: 1.2869 - val_acc: 0.4230\n",
      "Epoch 67/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0027 - acc: 0.4805 - val_loss: 1.2618 - val_acc: 0.4350\n",
      "Epoch 68/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0031 - acc: 0.4776 - val_loss: 1.2015 - val_acc: 0.4683\n",
      "Epoch 69/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9990 - acc: 0.4935 - val_loss: 1.1374 - val_acc: 0.5408\n",
      "Epoch 70/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0123 - acc: 0.4903 - val_loss: 1.1838 - val_acc: 0.5076\n",
      "Epoch 71/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9924 - acc: 0.5069 - val_loss: 1.2025 - val_acc: 0.4955\n",
      "Epoch 72/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9762 - acc: 0.4970 - val_loss: 1.2541 - val_acc: 0.4592\n",
      "Epoch 73/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9918 - acc: 0.4928 - val_loss: 1.3333 - val_acc: 0.4079\n",
      "Epoch 74/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.0073 - acc: 0.4607 - val_loss: 1.2759 - val_acc: 0.4350\n",
      "Epoch 75/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9959 - acc: 0.4773 - val_loss: 1.2823 - val_acc: 0.4441\n",
      "Epoch 76/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0039 - acc: 0.4794 - val_loss: 1.3169 - val_acc: 0.4230\n",
      "Epoch 77/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9845 - acc: 0.4850 - val_loss: 1.2110 - val_acc: 0.4653\n",
      "Epoch 78/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9757 - acc: 0.4921 - val_loss: 1.2158 - val_acc: 0.4683\n",
      "Epoch 79/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9656 - acc: 0.5072 - val_loss: 1.2513 - val_acc: 0.4532\n",
      "Epoch 80/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9825 - acc: 0.4928 - val_loss: 1.2269 - val_acc: 0.4683\n",
      "Epoch 81/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9581 - acc: 0.5033 - val_loss: 1.1857 - val_acc: 0.4955\n",
      "Epoch 82/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9388 - acc: 0.5069 - val_loss: 1.2463 - val_acc: 0.4622\n",
      "Epoch 83/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9592 - acc: 0.4945 - val_loss: 1.2417 - val_acc: 0.4622\n",
      "Epoch 84/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9596 - acc: 0.5023 - val_loss: 1.1961 - val_acc: 0.4743\n",
      "Epoch 85/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9339 - acc: 0.5076 - val_loss: 1.2415 - val_acc: 0.4592\n",
      "Epoch 86/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9559 - acc: 0.4896 - val_loss: 1.2183 - val_acc: 0.4441\n",
      "Epoch 87/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9516 - acc: 0.4949 - val_loss: 1.1110 - val_acc: 0.5529\n",
      "Epoch 88/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9588 - acc: 0.4917 - val_loss: 1.0781 - val_acc: 0.5740\n",
      "Epoch 89/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9698 - acc: 0.5097 - val_loss: 1.1429 - val_acc: 0.5347\n",
      "Epoch 90/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9202 - acc: 0.5298 - val_loss: 1.1689 - val_acc: 0.5136\n",
      "Epoch 91/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9123 - acc: 0.5361 - val_loss: 1.1604 - val_acc: 0.5227\n",
      "Epoch 92/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9169 - acc: 0.5322 - val_loss: 1.1761 - val_acc: 0.4985\n",
      "Epoch 93/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9181 - acc: 0.5118 - val_loss: 1.2177 - val_acc: 0.4562\n",
      "Epoch 94/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9438 - acc: 0.5069 - val_loss: 1.2754 - val_acc: 0.4290\n",
      "Epoch 95/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9336 - acc: 0.5083 - val_loss: 1.2792 - val_acc: 0.4260\n",
      "Epoch 96/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9399 - acc: 0.5104 - val_loss: 1.3009 - val_acc: 0.4320\n",
      "Epoch 97/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9434 - acc: 0.4981 - val_loss: 1.2584 - val_acc: 0.4411\n",
      "Epoch 98/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9326 - acc: 0.5023 - val_loss: 1.2646 - val_acc: 0.4381\n",
      "Epoch 99/150\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.9166 - acc: 0.5069 - val_loss: 1.1550 - val_acc: 0.5166\n",
      "Epoch 100/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8951 - acc: 0.5231 - val_loss: 1.1497 - val_acc: 0.5196\n",
      "Epoch 101/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8783 - acc: 0.5428 - val_loss: 1.1583 - val_acc: 0.5257\n",
      "Epoch 102/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8960 - acc: 0.5277 - val_loss: 1.2125 - val_acc: 0.4592\n",
      "Epoch 103/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8975 - acc: 0.5284 - val_loss: 1.3264 - val_acc: 0.4230\n",
      "Epoch 104/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9433 - acc: 0.4938 - val_loss: 1.2993 - val_acc: 0.4048\n",
      "Epoch 105/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9206 - acc: 0.5051 - val_loss: 1.2500 - val_acc: 0.4502\n",
      "Epoch 106/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9072 - acc: 0.5030 - val_loss: 1.2306 - val_acc: 0.4683\n",
      "Epoch 107/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8899 - acc: 0.5259 - val_loss: 1.2462 - val_acc: 0.4713\n",
      "Epoch 108/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8834 - acc: 0.5252 - val_loss: 1.2055 - val_acc: 0.4864\n",
      "Epoch 109/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.8788 - acc: 0.5287 - val_loss: 1.2673 - val_acc: 0.4411\n",
      "Epoch 110/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9023 - acc: 0.5150 - val_loss: 1.2945 - val_acc: 0.4350\n",
      "Epoch 111/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9004 - acc: 0.5129 - val_loss: 1.2783 - val_acc: 0.4381\n",
      "Epoch 112/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9020 - acc: 0.5181 - val_loss: 1.2373 - val_acc: 0.4653\n",
      "Epoch 113/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8982 - acc: 0.5227 - val_loss: 1.2409 - val_acc: 0.4441\n",
      "Epoch 114/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8817 - acc: 0.5178 - val_loss: 1.2371 - val_acc: 0.4592\n",
      "Epoch 115/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8718 - acc: 0.5431 - val_loss: 1.2235 - val_acc: 0.4622\n",
      "Epoch 116/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8481 - acc: 0.5407 - val_loss: 1.1942 - val_acc: 0.4924\n",
      "Epoch 117/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8562 - acc: 0.5544 - val_loss: 1.3163 - val_acc: 0.4320\n",
      "Epoch 118/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9069 - acc: 0.5104 - val_loss: 1.2729 - val_acc: 0.4411\n",
      "Epoch 119/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8991 - acc: 0.5026 - val_loss: 1.2137 - val_acc: 0.4834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.8482 - acc: 0.5414 - val_loss: 1.1879 - val_acc: 0.4864\n",
      "Epoch 121/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8438 - acc: 0.5516 - val_loss: 1.1806 - val_acc: 0.5106\n",
      "Epoch 122/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8472 - acc: 0.5456 - val_loss: 1.2722 - val_acc: 0.4592\n",
      "Epoch 123/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8467 - acc: 0.5442 - val_loss: 1.2533 - val_acc: 0.4592\n",
      "Epoch 124/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8436 - acc: 0.5446 - val_loss: 1.3276 - val_acc: 0.4199\n",
      "Epoch 125/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8720 - acc: 0.5294 - val_loss: 1.2864 - val_acc: 0.4320\n",
      "Epoch 126/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8558 - acc: 0.5368 - val_loss: 1.2677 - val_acc: 0.4350\n",
      "Epoch 127/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8509 - acc: 0.5379 - val_loss: 1.2681 - val_acc: 0.4441\n",
      "Epoch 128/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8531 - acc: 0.5463 - val_loss: 1.3025 - val_acc: 0.4290\n",
      "Epoch 129/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8667 - acc: 0.5217 - val_loss: 1.2876 - val_acc: 0.4411\n",
      "Epoch 130/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8561 - acc: 0.5396 - val_loss: 1.2776 - val_acc: 0.4260\n",
      "Epoch 131/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8514 - acc: 0.5368 - val_loss: 1.2163 - val_acc: 0.4804\n",
      "Epoch 132/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8283 - acc: 0.5484 - val_loss: 1.2132 - val_acc: 0.4683\n",
      "Epoch 133/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.8398 - acc: 0.5463 - val_loss: 1.2520 - val_acc: 0.4653\n",
      "Epoch 134/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8412 - acc: 0.5414 - val_loss: 1.3066 - val_acc: 0.4320\n",
      "Epoch 135/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8522 - acc: 0.5424 - val_loss: 1.3692 - val_acc: 0.4109\n",
      "Epoch 136/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8463 - acc: 0.5199 - val_loss: 1.2481 - val_acc: 0.4532\n",
      "Epoch 137/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8258 - acc: 0.5558 - val_loss: 1.2658 - val_acc: 0.4502\n",
      "Epoch 138/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8196 - acc: 0.5555 - val_loss: 1.1625 - val_acc: 0.5227\n",
      "Epoch 139/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8106 - acc: 0.5759 - val_loss: 1.2445 - val_acc: 0.4653\n",
      "Epoch 140/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8200 - acc: 0.5484 - val_loss: 1.3498 - val_acc: 0.4411\n",
      "Epoch 141/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8788 - acc: 0.5375 - val_loss: 1.3333 - val_acc: 0.4290\n",
      "Epoch 142/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8455 - acc: 0.5312 - val_loss: 1.2665 - val_acc: 0.4502\n",
      "Epoch 143/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8102 - acc: 0.5667 - val_loss: 1.2623 - val_acc: 0.4532\n",
      "Epoch 144/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8131 - acc: 0.5618 - val_loss: 1.3042 - val_acc: 0.4471\n",
      "Epoch 145/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8233 - acc: 0.5572 - val_loss: 1.2262 - val_acc: 0.4653\n",
      "Epoch 146/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.7999 - acc: 0.5667 - val_loss: 1.1733 - val_acc: 0.4955\n",
      "Epoch 147/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7813 - acc: 0.5819 - val_loss: 1.1655 - val_acc: 0.5136\n",
      "Epoch 148/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7904 - acc: 0.5893 - val_loss: 1.1218 - val_acc: 0.5438\n",
      "Epoch 149/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8483 - acc: 0.5660 - val_loss: 1.1338 - val_acc: 0.5227\n",
      "Epoch 150/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8586 - acc: 0.5667 - val_loss: 1.1846 - val_acc: 0.4834\n",
      "0\n",
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/150\n",
      "2839/2839 [==============================] - 24s 8ms/step - loss: 1.4540 - acc: 0.2226 - val_loss: 1.4981 - val_acc: 0.3323\n",
      "Epoch 2/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3247 - acc: 0.3515 - val_loss: 1.4559 - val_acc: 0.3323\n",
      "Epoch 3/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.3061 - acc: 0.3762 - val_loss: 1.4501 - val_acc: 0.3323\n",
      "Epoch 4/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2975 - acc: 0.3843 - val_loss: 1.4402 - val_acc: 0.3323\n",
      "Epoch 5/150\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 1.2753 - acc: 0.3822 - val_loss: 1.4366 - val_acc: 0.3323\n",
      "Epoch 6/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.2666 - acc: 0.3882 - val_loss: 1.4583 - val_acc: 0.3263\n",
      "Epoch 7/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.2576 - acc: 0.3734 - val_loss: 1.4321 - val_acc: 0.3293\n",
      "Epoch 8/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.2407 - acc: 0.3889 - val_loss: 1.4520 - val_acc: 0.3263\n",
      "Epoch 9/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.2558 - acc: 0.3903 - val_loss: 1.4180 - val_acc: 0.3263\n",
      "Epoch 10/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.2428 - acc: 0.3846 - val_loss: 1.4336 - val_acc: 0.3233\n",
      "Epoch 11/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.2350 - acc: 0.3896 - val_loss: 1.4472 - val_acc: 0.3233\n",
      "Epoch 12/150\n",
      "2839/2839 [==============================] - 0s 90us/step - loss: 1.2194 - acc: 0.3896 - val_loss: 1.4177 - val_acc: 0.3202\n",
      "Epoch 13/150\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.2219 - acc: 0.3959 - val_loss: 1.4082 - val_acc: 0.3233\n",
      "Epoch 14/150\n",
      "2839/2839 [==============================] - 0s 57us/step - loss: 1.2166 - acc: 0.3903 - val_loss: 1.3777 - val_acc: 0.3202\n",
      "Epoch 15/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.2039 - acc: 0.3991 - val_loss: 1.3361 - val_acc: 0.3233\n",
      "Epoch 16/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.2007 - acc: 0.4114 - val_loss: 1.4271 - val_acc: 0.2931\n",
      "Epoch 17/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.1901 - acc: 0.4008 - val_loss: 1.3928 - val_acc: 0.2991\n",
      "Epoch 18/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1820 - acc: 0.4023 - val_loss: 1.4216 - val_acc: 0.3142\n",
      "Epoch 19/150\n",
      "2839/2839 [==============================] - 0s 51us/step - loss: 1.1790 - acc: 0.3942 - val_loss: 1.4012 - val_acc: 0.3233\n",
      "Epoch 20/150\n",
      "2839/2839 [==============================] - 0s 59us/step - loss: 1.1822 - acc: 0.4089 - val_loss: 1.4531 - val_acc: 0.3021\n",
      "Epoch 21/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.1781 - acc: 0.4001 - val_loss: 1.4358 - val_acc: 0.2961\n",
      "Epoch 22/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1739 - acc: 0.4001 - val_loss: 1.3942 - val_acc: 0.3172\n",
      "Epoch 23/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1532 - acc: 0.4072 - val_loss: 1.3731 - val_acc: 0.3263\n",
      "Epoch 24/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.1491 - acc: 0.4209 - val_loss: 1.3666 - val_acc: 0.3474\n",
      "Epoch 25/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.1530 - acc: 0.4266 - val_loss: 1.3700 - val_acc: 0.3535\n",
      "Epoch 26/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.1459 - acc: 0.4213 - val_loss: 1.3594 - val_acc: 0.3293\n",
      "Epoch 27/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1347 - acc: 0.4192 - val_loss: 1.3526 - val_acc: 0.3505\n",
      "Epoch 28/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.1323 - acc: 0.4241 - val_loss: 1.3155 - val_acc: 0.3414\n",
      "Epoch 29/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.1476 - acc: 0.4230 - val_loss: 1.3143 - val_acc: 0.3384\n",
      "Epoch 30/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.1260 - acc: 0.4403 - val_loss: 1.3415 - val_acc: 0.3595\n",
      "Epoch 31/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.1277 - acc: 0.4354 - val_loss: 1.3258 - val_acc: 0.3716\n",
      "Epoch 32/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.1147 - acc: 0.4329 - val_loss: 1.3082 - val_acc: 0.3807\n",
      "Epoch 33/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1029 - acc: 0.4340 - val_loss: 1.2496 - val_acc: 0.3927\n",
      "Epoch 34/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.1204 - acc: 0.4428 - val_loss: 1.2514 - val_acc: 0.3807\n",
      "Epoch 35/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.1111 - acc: 0.4477 - val_loss: 1.2875 - val_acc: 0.3746\n",
      "Epoch 36/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1033 - acc: 0.4491 - val_loss: 1.3105 - val_acc: 0.3837\n",
      "Epoch 37/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.1052 - acc: 0.4452 - val_loss: 1.3182 - val_acc: 0.3837\n",
      "Epoch 38/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0866 - acc: 0.4452 - val_loss: 1.3655 - val_acc: 0.3505\n",
      "Epoch 39/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0967 - acc: 0.4403 - val_loss: 1.3995 - val_acc: 0.3384\n",
      "Epoch 40/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.0973 - acc: 0.4414 - val_loss: 1.3481 - val_acc: 0.3656\n",
      "Epoch 41/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.0740 - acc: 0.4449 - val_loss: 1.3177 - val_acc: 0.3686\n",
      "Epoch 42/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.0845 - acc: 0.4572 - val_loss: 1.2449 - val_acc: 0.3958\n",
      "Epoch 43/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0835 - acc: 0.4745 - val_loss: 1.2734 - val_acc: 0.4320\n",
      "Epoch 44/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0675 - acc: 0.4512 - val_loss: 1.1568 - val_acc: 0.4683\n",
      "Epoch 45/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.0810 - acc: 0.4783 - val_loss: 1.2225 - val_acc: 0.4230\n",
      "Epoch 46/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0639 - acc: 0.4695 - val_loss: 1.2729 - val_acc: 0.4230\n",
      "Epoch 47/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0554 - acc: 0.4519 - val_loss: 1.2201 - val_acc: 0.4290\n",
      "Epoch 48/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0552 - acc: 0.4590 - val_loss: 1.1901 - val_acc: 0.4683\n",
      "Epoch 49/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0529 - acc: 0.4780 - val_loss: 1.2223 - val_acc: 0.4622\n",
      "Epoch 50/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0441 - acc: 0.4720 - val_loss: 1.2668 - val_acc: 0.4350\n",
      "Epoch 51/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.0525 - acc: 0.4720 - val_loss: 1.1995 - val_acc: 0.4713\n",
      "Epoch 52/150\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 1.0537 - acc: 0.4857 - val_loss: 1.1317 - val_acc: 0.5106\n",
      "Epoch 53/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.0673 - acc: 0.4840 - val_loss: 1.2665 - val_acc: 0.4169\n",
      "Epoch 54/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0291 - acc: 0.4808 - val_loss: 1.2633 - val_acc: 0.4199\n",
      "Epoch 55/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0189 - acc: 0.4706 - val_loss: 1.2132 - val_acc: 0.4381\n",
      "Epoch 56/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0461 - acc: 0.4797 - val_loss: 1.1385 - val_acc: 0.4985\n",
      "Epoch 57/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.0464 - acc: 0.4907 - val_loss: 1.2164 - val_acc: 0.4955\n",
      "Epoch 58/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0311 - acc: 0.5030 - val_loss: 1.2697 - val_acc: 0.4350\n",
      "Epoch 59/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0236 - acc: 0.4752 - val_loss: 1.2597 - val_acc: 0.4502\n",
      "Epoch 60/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.0176 - acc: 0.4826 - val_loss: 1.2524 - val_acc: 0.4532\n",
      "Epoch 61/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0176 - acc: 0.4938 - val_loss: 1.3156 - val_acc: 0.4048\n",
      "Epoch 62/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0331 - acc: 0.4685 - val_loss: 1.4344 - val_acc: 0.3595\n",
      "Epoch 63/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0500 - acc: 0.4470 - val_loss: 1.2399 - val_acc: 0.4381\n",
      "Epoch 64/150\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 1.0061 - acc: 0.4942 - val_loss: 1.1646 - val_acc: 0.4804\n",
      "Epoch 65/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0240 - acc: 0.4914 - val_loss: 1.1545 - val_acc: 0.5166\n",
      "Epoch 66/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9998 - acc: 0.5019 - val_loss: 1.2021 - val_acc: 0.5076\n",
      "Epoch 67/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0010 - acc: 0.4847 - val_loss: 1.2515 - val_acc: 0.4471\n",
      "Epoch 68/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.0060 - acc: 0.4857 - val_loss: 1.2554 - val_acc: 0.4502\n",
      "Epoch 69/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9865 - acc: 0.4988 - val_loss: 1.2731 - val_acc: 0.4320\n",
      "Epoch 70/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9808 - acc: 0.4952 - val_loss: 1.2650 - val_acc: 0.4411\n",
      "Epoch 71/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9931 - acc: 0.4882 - val_loss: 1.3166 - val_acc: 0.4079\n",
      "Epoch 72/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0130 - acc: 0.4748 - val_loss: 1.3707 - val_acc: 0.3776\n",
      "Epoch 73/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9853 - acc: 0.4801 - val_loss: 1.1973 - val_acc: 0.4985\n",
      "Epoch 74/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9758 - acc: 0.4995 - val_loss: 1.1955 - val_acc: 0.4985\n",
      "Epoch 75/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9821 - acc: 0.4963 - val_loss: 1.1532 - val_acc: 0.5378\n",
      "Epoch 76/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9591 - acc: 0.5171 - val_loss: 1.1707 - val_acc: 0.5317\n",
      "Epoch 77/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9560 - acc: 0.4991 - val_loss: 1.1345 - val_acc: 0.5196\n",
      "Epoch 78/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9669 - acc: 0.5037 - val_loss: 1.0949 - val_acc: 0.5287\n",
      "Epoch 79/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0117 - acc: 0.5005 - val_loss: 1.1008 - val_acc: 0.5891\n",
      "Epoch 80/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9754 - acc: 0.5016 - val_loss: 1.1254 - val_acc: 0.5559\n",
      "Epoch 81/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9701 - acc: 0.4998 - val_loss: 1.1013 - val_acc: 0.5770\n",
      "Epoch 82/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9700 - acc: 0.5160 - val_loss: 1.1218 - val_acc: 0.5710\n",
      "Epoch 83/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9611 - acc: 0.5195 - val_loss: 1.1360 - val_acc: 0.5378\n",
      "Epoch 84/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9740 - acc: 0.5076 - val_loss: 1.1755 - val_acc: 0.5045\n",
      "Epoch 85/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9586 - acc: 0.5146 - val_loss: 1.1638 - val_acc: 0.5227\n",
      "Epoch 86/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9444 - acc: 0.5181 - val_loss: 1.1618 - val_acc: 0.5196\n",
      "Epoch 87/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9471 - acc: 0.5122 - val_loss: 1.2366 - val_acc: 0.4653\n",
      "Epoch 88/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9500 - acc: 0.5062 - val_loss: 1.2434 - val_acc: 0.4653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9628 - acc: 0.5093 - val_loss: 1.3351 - val_acc: 0.4109\n",
      "Epoch 90/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9738 - acc: 0.4794 - val_loss: 1.1932 - val_acc: 0.4894\n",
      "Epoch 91/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9238 - acc: 0.5114 - val_loss: 1.2073 - val_acc: 0.4894\n",
      "Epoch 92/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9475 - acc: 0.5069 - val_loss: 1.1300 - val_acc: 0.5408\n",
      "Epoch 93/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9445 - acc: 0.5146 - val_loss: 1.1023 - val_acc: 0.5801\n",
      "Epoch 94/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9228 - acc: 0.5305 - val_loss: 1.0843 - val_acc: 0.5589\n",
      "Epoch 95/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9404 - acc: 0.5238 - val_loss: 1.0955 - val_acc: 0.5317\n",
      "Epoch 96/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9527 - acc: 0.5227 - val_loss: 1.0797 - val_acc: 0.5650\n",
      "Epoch 97/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.5340 - val_loss: 1.1420 - val_acc: 0.5257\n",
      "Epoch 98/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9299 - acc: 0.5185 - val_loss: 1.1094 - val_acc: 0.5498\n",
      "Epoch 99/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9170 - acc: 0.5241 - val_loss: 1.1280 - val_acc: 0.5408\n",
      "Epoch 100/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9071 - acc: 0.5291 - val_loss: 1.0890 - val_acc: 0.5589\n",
      "Epoch 101/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9099 - acc: 0.5329 - val_loss: 1.0940 - val_acc: 0.5166\n",
      "Epoch 102/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9187 - acc: 0.5266 - val_loss: 1.0925 - val_acc: 0.5287\n",
      "Epoch 103/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9119 - acc: 0.5347 - val_loss: 1.1048 - val_acc: 0.5015\n",
      "Epoch 104/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9061 - acc: 0.5386 - val_loss: 1.1245 - val_acc: 0.5015\n",
      "Epoch 105/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9000 - acc: 0.5340 - val_loss: 1.0994 - val_acc: 0.5408\n",
      "Epoch 106/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9023 - acc: 0.5386 - val_loss: 1.0771 - val_acc: 0.5468\n",
      "Epoch 107/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9147 - acc: 0.5269 - val_loss: 1.0683 - val_acc: 0.5801\n",
      "Epoch 108/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9122 - acc: 0.5312 - val_loss: 1.1763 - val_acc: 0.4955\n",
      "Epoch 109/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8907 - acc: 0.5203 - val_loss: 1.1861 - val_acc: 0.4834\n",
      "Epoch 110/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8975 - acc: 0.5227 - val_loss: 1.2443 - val_acc: 0.4350\n",
      "Epoch 111/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9048 - acc: 0.5132 - val_loss: 1.2214 - val_acc: 0.4562\n",
      "Epoch 112/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8959 - acc: 0.5238 - val_loss: 1.1809 - val_acc: 0.4834\n",
      "Epoch 113/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8787 - acc: 0.5403 - val_loss: 1.2676 - val_acc: 0.4441\n",
      "Epoch 114/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9239 - acc: 0.5111 - val_loss: 1.3309 - val_acc: 0.4048\n",
      "Epoch 115/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9224 - acc: 0.4910 - val_loss: 1.2017 - val_acc: 0.4683\n",
      "Epoch 116/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8875 - acc: 0.5287 - val_loss: 1.1838 - val_acc: 0.4683\n",
      "Epoch 117/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8521 - acc: 0.5368 - val_loss: 1.1734 - val_acc: 0.4894\n",
      "Epoch 118/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8628 - acc: 0.5417 - val_loss: 1.3054 - val_acc: 0.4411\n",
      "Epoch 119/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9193 - acc: 0.5055 - val_loss: 1.2825 - val_acc: 0.4260\n",
      "Epoch 120/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8924 - acc: 0.5118 - val_loss: 1.2139 - val_acc: 0.4562\n",
      "Epoch 121/150\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.8657 - acc: 0.5410 - val_loss: 1.2204 - val_acc: 0.4683\n",
      "Epoch 122/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8779 - acc: 0.5262 - val_loss: 1.1720 - val_acc: 0.5045\n",
      "Epoch 123/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8680 - acc: 0.5393 - val_loss: 1.1365 - val_acc: 0.5287\n",
      "Epoch 124/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8590 - acc: 0.5396 - val_loss: 1.0926 - val_acc: 0.5529\n",
      "Epoch 125/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8493 - acc: 0.5449 - val_loss: 1.1158 - val_acc: 0.5196\n",
      "Epoch 126/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8521 - acc: 0.5597 - val_loss: 1.0677 - val_acc: 0.5287\n",
      "Epoch 127/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.8916 - acc: 0.5442 - val_loss: 1.0764 - val_acc: 0.5529\n",
      "Epoch 128/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8779 - acc: 0.5625 - val_loss: 1.1238 - val_acc: 0.5106\n",
      "Epoch 129/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8366 - acc: 0.5586 - val_loss: 1.1271 - val_acc: 0.5166\n",
      "Epoch 130/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8273 - acc: 0.5727 - val_loss: 1.1625 - val_acc: 0.5015\n",
      "Epoch 131/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8599 - acc: 0.5446 - val_loss: 1.2130 - val_acc: 0.4924\n",
      "Epoch 132/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8530 - acc: 0.5495 - val_loss: 1.2516 - val_acc: 0.4502\n",
      "Epoch 133/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8575 - acc: 0.5498 - val_loss: 1.2966 - val_acc: 0.4260\n",
      "Epoch 134/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8711 - acc: 0.5167 - val_loss: 1.2507 - val_acc: 0.4411\n",
      "Epoch 135/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8475 - acc: 0.5421 - val_loss: 1.1894 - val_acc: 0.4894\n",
      "Epoch 136/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8359 - acc: 0.5520 - val_loss: 1.2201 - val_acc: 0.4622\n",
      "Epoch 137/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8335 - acc: 0.5576 - val_loss: 1.2305 - val_acc: 0.4532\n",
      "Epoch 138/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8530 - acc: 0.5502 - val_loss: 1.1949 - val_acc: 0.4804\n",
      "Epoch 139/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8365 - acc: 0.5601 - val_loss: 1.2234 - val_acc: 0.4713\n",
      "Epoch 140/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8276 - acc: 0.5544 - val_loss: 1.1492 - val_acc: 0.5106\n",
      "Epoch 141/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8108 - acc: 0.5756 - val_loss: 1.1191 - val_acc: 0.5257\n",
      "Epoch 142/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8506 - acc: 0.5562 - val_loss: 1.1094 - val_acc: 0.5166\n",
      "Epoch 143/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8170 - acc: 0.5713 - val_loss: 1.1728 - val_acc: 0.4985\n",
      "Epoch 144/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8051 - acc: 0.5615 - val_loss: 1.1515 - val_acc: 0.5106\n",
      "Epoch 145/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8177 - acc: 0.5601 - val_loss: 1.0744 - val_acc: 0.5650\n",
      "Epoch 146/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8728 - acc: 0.5424 - val_loss: 1.0902 - val_acc: 0.5438\n",
      "Epoch 147/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8328 - acc: 0.5745 - val_loss: 1.1327 - val_acc: 0.5287\n",
      "Epoch 148/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8146 - acc: 0.5713 - val_loss: 1.1130 - val_acc: 0.5166\n",
      "Epoch 149/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8319 - acc: 0.5724 - val_loss: 1.1119 - val_acc: 0.5257\n",
      "Epoch 150/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8099 - acc: 0.5773 - val_loss: 1.1758 - val_acc: 0.4804\n",
      "1\n",
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/150\n",
      "2839/2839 [==============================] - 24s 8ms/step - loss: 1.4421 - acc: 0.2046 - val_loss: 1.4970 - val_acc: 0.3323\n",
      "Epoch 2/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.3371 - acc: 0.3198 - val_loss: 1.4269 - val_acc: 0.3323\n",
      "Epoch 3/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.3173 - acc: 0.3417 - val_loss: 1.4476 - val_acc: 0.3323\n",
      "Epoch 4/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.3005 - acc: 0.3670 - val_loss: 1.4595 - val_acc: 0.3353\n",
      "Epoch 5/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2814 - acc: 0.3632 - val_loss: 1.4266 - val_acc: 0.3323\n",
      "Epoch 6/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2865 - acc: 0.3639 - val_loss: 1.4471 - val_acc: 0.3323\n",
      "Epoch 7/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2692 - acc: 0.3691 - val_loss: 1.4264 - val_acc: 0.3323\n",
      "Epoch 8/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2475 - acc: 0.3818 - val_loss: 1.4049 - val_acc: 0.3353\n",
      "Epoch 9/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.2482 - acc: 0.3734 - val_loss: 1.4236 - val_acc: 0.3263\n",
      "Epoch 10/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.2357 - acc: 0.3762 - val_loss: 1.4725 - val_acc: 0.3112\n",
      "Epoch 11/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2208 - acc: 0.3681 - val_loss: 1.4497 - val_acc: 0.3323\n",
      "Epoch 12/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2265 - acc: 0.3776 - val_loss: 1.3621 - val_acc: 0.3323\n",
      "Epoch 13/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2230 - acc: 0.3906 - val_loss: 1.3737 - val_acc: 0.3263\n",
      "Epoch 14/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2090 - acc: 0.3857 - val_loss: 1.3403 - val_acc: 0.3293\n",
      "Epoch 15/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2022 - acc: 0.4068 - val_loss: 1.3757 - val_acc: 0.3293\n",
      "Epoch 16/150\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 1.1839 - acc: 0.385 - 0s 35us/step - loss: 1.1925 - acc: 0.3913 - val_loss: 1.4581 - val_acc: 0.3172\n",
      "Epoch 17/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1977 - acc: 0.3846 - val_loss: 1.4631 - val_acc: 0.3112\n",
      "Epoch 18/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1878 - acc: 0.3875 - val_loss: 1.4126 - val_acc: 0.3051\n",
      "Epoch 19/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1792 - acc: 0.3829 - val_loss: 1.4039 - val_acc: 0.3051\n",
      "Epoch 20/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1636 - acc: 0.3945 - val_loss: 1.3196 - val_acc: 0.3293\n",
      "Epoch 21/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1521 - acc: 0.3956 - val_loss: 1.3525 - val_acc: 0.3263\n",
      "Epoch 22/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1559 - acc: 0.4093 - val_loss: 1.2868 - val_acc: 0.3505\n",
      "Epoch 23/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1667 - acc: 0.3987 - val_loss: 1.3308 - val_acc: 0.3474\n",
      "Epoch 24/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1443 - acc: 0.4012 - val_loss: 1.2939 - val_acc: 0.3474\n",
      "Epoch 25/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1515 - acc: 0.4149 - val_loss: 1.3862 - val_acc: 0.3353\n",
      "Epoch 26/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1359 - acc: 0.4058 - val_loss: 1.4910 - val_acc: 0.3233\n",
      "Epoch 27/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1514 - acc: 0.3945 - val_loss: 1.3792 - val_acc: 0.3323\n",
      "Epoch 28/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1224 - acc: 0.4047 - val_loss: 1.3218 - val_acc: 0.3505\n",
      "Epoch 29/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1135 - acc: 0.4104 - val_loss: 1.3441 - val_acc: 0.3505\n",
      "Epoch 30/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1258 - acc: 0.4121 - val_loss: 1.3735 - val_acc: 0.3112\n",
      "Epoch 31/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1255 - acc: 0.4146 - val_loss: 1.3977 - val_acc: 0.3202\n",
      "Epoch 32/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1189 - acc: 0.4097 - val_loss: 1.3470 - val_acc: 0.3353\n",
      "Epoch 33/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1102 - acc: 0.4139 - val_loss: 1.4432 - val_acc: 0.3384\n",
      "Epoch 34/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1150 - acc: 0.4047 - val_loss: 1.3848 - val_acc: 0.3444\n",
      "Epoch 35/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1025 - acc: 0.4174 - val_loss: 1.3537 - val_acc: 0.3474\n",
      "Epoch 36/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0962 - acc: 0.4206 - val_loss: 1.3305 - val_acc: 0.3505\n",
      "Epoch 37/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0862 - acc: 0.4202 - val_loss: 1.3068 - val_acc: 0.3565\n",
      "Epoch 38/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0809 - acc: 0.4361 - val_loss: 1.3417 - val_acc: 0.3414\n",
      "Epoch 39/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0983 - acc: 0.4132 - val_loss: 1.4323 - val_acc: 0.3202\n",
      "Epoch 40/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0851 - acc: 0.3991 - val_loss: 1.3522 - val_acc: 0.3565\n",
      "Epoch 41/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0758 - acc: 0.4283 - val_loss: 1.2987 - val_acc: 0.3746\n",
      "Epoch 42/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0652 - acc: 0.4477 - val_loss: 1.3284 - val_acc: 0.3807\n",
      "Epoch 43/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0736 - acc: 0.4287 - val_loss: 1.2753 - val_acc: 0.4079\n",
      "Epoch 44/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0530 - acc: 0.4336 - val_loss: 1.3583 - val_acc: 0.3746\n",
      "Epoch 45/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0750 - acc: 0.4248 - val_loss: 1.3577 - val_acc: 0.3625\n",
      "Epoch 46/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0658 - acc: 0.4241 - val_loss: 1.3213 - val_acc: 0.3716\n",
      "Epoch 47/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0525 - acc: 0.4297 - val_loss: 1.3352 - val_acc: 0.3716\n",
      "Epoch 48/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0545 - acc: 0.4322 - val_loss: 1.3151 - val_acc: 0.3837\n",
      "Epoch 49/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0391 - acc: 0.4519 - val_loss: 1.2866 - val_acc: 0.4109\n",
      "Epoch 50/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.0362 - acc: 0.4628 - val_loss: 1.3049 - val_acc: 0.3897\n",
      "Epoch 51/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0225 - acc: 0.4569 - val_loss: 1.2858 - val_acc: 0.4079\n",
      "Epoch 52/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0404 - acc: 0.4583 - val_loss: 1.3094 - val_acc: 0.3897\n",
      "Epoch 53/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0384 - acc: 0.4442 - val_loss: 1.3963 - val_acc: 0.3565\n",
      "Epoch 54/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0507 - acc: 0.4290 - val_loss: 1.2877 - val_acc: 0.3927\n",
      "Epoch 55/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0269 - acc: 0.4526 - val_loss: 1.3037 - val_acc: 0.4139\n",
      "Epoch 56/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0216 - acc: 0.4417 - val_loss: 1.1944 - val_acc: 0.4713\n",
      "Epoch 57/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0056 - acc: 0.4748 - val_loss: 1.1469 - val_acc: 0.5076\n",
      "Epoch 58/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0387 - acc: 0.4720 - val_loss: 1.1898 - val_acc: 0.4924\n",
      "Epoch 59/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0005 - acc: 0.4822 - val_loss: 1.2122 - val_acc: 0.4653\n",
      "Epoch 60/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9990 - acc: 0.4699 - val_loss: 1.1960 - val_acc: 0.4864\n",
      "Epoch 61/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9954 - acc: 0.4738 - val_loss: 1.1780 - val_acc: 0.4532\n",
      "Epoch 62/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0150 - acc: 0.4773 - val_loss: 1.1675 - val_acc: 0.4622\n",
      "Epoch 63/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0062 - acc: 0.4635 - val_loss: 1.1710 - val_acc: 0.4955\n",
      "Epoch 64/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9875 - acc: 0.4935 - val_loss: 1.2059 - val_acc: 0.4773\n",
      "Epoch 65/150\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 0.9847 - acc: 0.4702 - val_loss: 1.1943 - val_acc: 0.5106\n",
      "Epoch 66/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9872 - acc: 0.4836 - val_loss: 1.1456 - val_acc: 0.5559\n",
      "Epoch 67/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0155 - acc: 0.4833 - val_loss: 1.1698 - val_acc: 0.5378\n",
      "Epoch 68/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9899 - acc: 0.4952 - val_loss: 1.1946 - val_acc: 0.5257\n",
      "Epoch 69/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9647 - acc: 0.4984 - val_loss: 1.2485 - val_acc: 0.5015\n",
      "Epoch 70/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9692 - acc: 0.4875 - val_loss: 1.2912 - val_acc: 0.4653\n",
      "Epoch 71/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9828 - acc: 0.4808 - val_loss: 1.2987 - val_acc: 0.4230\n",
      "Epoch 72/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9878 - acc: 0.4843 - val_loss: 1.3051 - val_acc: 0.4290\n",
      "Epoch 73/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9969 - acc: 0.4674 - val_loss: 1.2803 - val_acc: 0.4622\n",
      "Epoch 74/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9601 - acc: 0.4766 - val_loss: 1.2737 - val_acc: 0.4622\n",
      "Epoch 75/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.9680 - acc: 0.4822 - val_loss: 1.2799 - val_acc: 0.4653\n",
      "Epoch 76/150\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.9518 - acc: 0.4935 - val_loss: 1.2151 - val_acc: 0.5106\n",
      "Epoch 77/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.9466 - acc: 0.4995 - val_loss: 1.2439 - val_acc: 0.4985\n",
      "Epoch 78/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9476 - acc: 0.4967 - val_loss: 1.2017 - val_acc: 0.4804\n",
      "Epoch 79/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9552 - acc: 0.4896 - val_loss: 1.1609 - val_acc: 0.5196\n",
      "Epoch 80/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9598 - acc: 0.4998 - val_loss: 1.1321 - val_acc: 0.5498\n",
      "Epoch 81/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9495 - acc: 0.5104 - val_loss: 1.1368 - val_acc: 0.5498\n",
      "Epoch 82/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9604 - acc: 0.4981 - val_loss: 1.1152 - val_acc: 0.5589\n",
      "Epoch 83/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9894 - acc: 0.5016 - val_loss: 1.1442 - val_acc: 0.5196\n",
      "Epoch 84/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9408 - acc: 0.5238 - val_loss: 1.2351 - val_acc: 0.4804\n",
      "Epoch 85/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9388 - acc: 0.5023 - val_loss: 1.2303 - val_acc: 0.4864\n",
      "Epoch 86/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9328 - acc: 0.5097 - val_loss: 1.2003 - val_acc: 0.4834\n",
      "Epoch 87/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9218 - acc: 0.5044 - val_loss: 1.2033 - val_acc: 0.4894\n",
      "Epoch 88/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9169 - acc: 0.5090 - val_loss: 1.1615 - val_acc: 0.5045\n",
      "Epoch 89/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9233 - acc: 0.5192 - val_loss: 1.1162 - val_acc: 0.5378\n",
      "Epoch 90/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9556 - acc: 0.4931 - val_loss: 1.1354 - val_acc: 0.5468\n",
      "Epoch 91/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9436 - acc: 0.5188 - val_loss: 1.1906 - val_acc: 0.4804\n",
      "Epoch 92/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9006 - acc: 0.5277 - val_loss: 1.1886 - val_acc: 0.4773\n",
      "Epoch 93/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8991 - acc: 0.5178 - val_loss: 1.1617 - val_acc: 0.5045\n",
      "Epoch 94/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9213 - acc: 0.5002 - val_loss: 1.1019 - val_acc: 0.5529\n",
      "Epoch 95/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9690 - acc: 0.5002 - val_loss: 1.1244 - val_acc: 0.5378\n",
      "Epoch 96/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.9371 - acc: 0.5213 - val_loss: 1.1831 - val_acc: 0.4834\n",
      "Epoch 97/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8979 - acc: 0.5234 - val_loss: 1.1577 - val_acc: 0.5106\n",
      "Epoch 98/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9137 - acc: 0.5227 - val_loss: 1.1959 - val_acc: 0.4924\n",
      "Epoch 99/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9123 - acc: 0.5234 - val_loss: 1.2070 - val_acc: 0.4985\n",
      "Epoch 100/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8922 - acc: 0.5343 - val_loss: 1.2515 - val_acc: 0.4743\n",
      "Epoch 101/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8842 - acc: 0.5178 - val_loss: 1.1977 - val_acc: 0.4985\n",
      "Epoch 102/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8828 - acc: 0.5248 - val_loss: 1.2608 - val_acc: 0.4683\n",
      "Epoch 103/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8956 - acc: 0.5150 - val_loss: 1.2568 - val_acc: 0.4592\n",
      "Epoch 104/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9002 - acc: 0.5203 - val_loss: 1.3303 - val_acc: 0.4471\n",
      "Epoch 105/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9187 - acc: 0.4826 - val_loss: 1.2357 - val_acc: 0.4743\n",
      "Epoch 106/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9079 - acc: 0.5111 - val_loss: 1.2610 - val_acc: 0.4743\n",
      "Epoch 107/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8799 - acc: 0.5153 - val_loss: 1.2307 - val_acc: 0.4834\n",
      "Epoch 108/150\n",
      "2839/2839 [==============================] - 0s 49us/step - loss: 0.8734 - acc: 0.5245 - val_loss: 1.2221 - val_acc: 0.4804\n",
      "Epoch 109/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8527 - acc: 0.5350 - val_loss: 1.2434 - val_acc: 0.4773\n",
      "Epoch 110/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8692 - acc: 0.5333 - val_loss: 1.3301 - val_acc: 0.4441\n",
      "Epoch 111/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9163 - acc: 0.5171 - val_loss: 1.3428 - val_acc: 0.4381\n",
      "Epoch 112/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8912 - acc: 0.5143 - val_loss: 1.2308 - val_acc: 0.4864\n",
      "Epoch 113/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8637 - acc: 0.5319 - val_loss: 1.3247 - val_acc: 0.4532\n",
      "Epoch 114/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8758 - acc: 0.5181 - val_loss: 1.2917 - val_acc: 0.4683\n",
      "Epoch 115/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8754 - acc: 0.5195 - val_loss: 1.2704 - val_acc: 0.4653\n",
      "Epoch 116/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8724 - acc: 0.5174 - val_loss: 1.2713 - val_acc: 0.4713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8667 - acc: 0.5224 - val_loss: 1.2394 - val_acc: 0.4924\n",
      "Epoch 118/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8527 - acc: 0.5347 - val_loss: 1.2662 - val_acc: 0.4743\n",
      "Epoch 119/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8431 - acc: 0.5350 - val_loss: 1.2721 - val_acc: 0.4713\n",
      "Epoch 120/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8523 - acc: 0.5403 - val_loss: 1.3427 - val_acc: 0.4471\n",
      "Epoch 121/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8689 - acc: 0.5188 - val_loss: 1.2446 - val_acc: 0.4743\n",
      "Epoch 122/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8353 - acc: 0.5474 - val_loss: 1.2407 - val_acc: 0.4834\n",
      "Epoch 123/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.5358 - val_loss: 1.3209 - val_acc: 0.4562\n",
      "Epoch 124/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8878 - acc: 0.5181 - val_loss: 1.3477 - val_acc: 0.4471\n",
      "Epoch 125/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8655 - acc: 0.5174 - val_loss: 1.2287 - val_acc: 0.4653\n",
      "Epoch 126/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8403 - acc: 0.5435 - val_loss: 1.2119 - val_acc: 0.4683\n",
      "Epoch 127/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8275 - acc: 0.5456 - val_loss: 1.2558 - val_acc: 0.4834\n",
      "Epoch 128/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8488 - acc: 0.5400 - val_loss: 1.2326 - val_acc: 0.4864\n",
      "Epoch 129/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 0.8225 - acc: 0.5495 - val_loss: 1.3310 - val_acc: 0.4532\n",
      "Epoch 130/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8569 - acc: 0.5431 - val_loss: 1.3968 - val_acc: 0.4230\n",
      "Epoch 131/150\n",
      "2839/2839 [==============================] - 0s 47us/step - loss: 0.8801 - acc: 0.5129 - val_loss: 1.2566 - val_acc: 0.4713\n",
      "Epoch 132/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8316 - acc: 0.5389 - val_loss: 1.2178 - val_acc: 0.4743\n",
      "Epoch 133/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8137 - acc: 0.5530 - val_loss: 1.2376 - val_acc: 0.4743\n",
      "Epoch 134/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8179 - acc: 0.5453 - val_loss: 1.3109 - val_acc: 0.4592\n",
      "Epoch 135/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8362 - acc: 0.5358 - val_loss: 1.3692 - val_acc: 0.4260\n",
      "Epoch 136/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8649 - acc: 0.5227 - val_loss: 1.2611 - val_acc: 0.4743\n",
      "Epoch 137/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8081 - acc: 0.5498 - val_loss: 1.2212 - val_acc: 0.4864\n",
      "Epoch 138/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8106 - acc: 0.5643 - val_loss: 1.2722 - val_acc: 0.4683\n",
      "Epoch 139/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.8095 - acc: 0.5646 - val_loss: 1.2543 - val_acc: 0.4743\n",
      "Epoch 140/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.8146 - acc: 0.5562 - val_loss: 1.3014 - val_acc: 0.4562\n",
      "Epoch 141/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 0.8259 - acc: 0.5477 - val_loss: 1.3014 - val_acc: 0.4683\n",
      "Epoch 142/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8190 - acc: 0.5442 - val_loss: 1.2847 - val_acc: 0.4622\n",
      "Epoch 143/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8315 - acc: 0.5396 - val_loss: 1.2812 - val_acc: 0.4562\n",
      "Epoch 144/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.7939 - acc: 0.5555 - val_loss: 1.2588 - val_acc: 0.4955\n",
      "Epoch 145/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7995 - acc: 0.5495 - val_loss: 1.2901 - val_acc: 0.4743\n",
      "Epoch 146/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8012 - acc: 0.5495 - val_loss: 1.3711 - val_acc: 0.4471\n",
      "Epoch 147/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8430 - acc: 0.5266 - val_loss: 1.4125 - val_acc: 0.4260\n",
      "Epoch 148/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.8405 - acc: 0.5291 - val_loss: 1.2458 - val_acc: 0.4985\n",
      "Epoch 149/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.7873 - acc: 0.5569 - val_loss: 1.2206 - val_acc: 0.5076\n",
      "Epoch 150/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7673 - acc: 0.5812 - val_loss: 1.2268 - val_acc: 0.4743\n",
      "2\n",
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/150\n",
      "2839/2839 [==============================] - 23s 8ms/step - loss: 1.5810 - acc: 0.1772 - val_loss: 1.5155 - val_acc: 0.3414\n",
      "Epoch 2/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.4456 - acc: 0.3082 - val_loss: 1.4458 - val_acc: 0.3353\n",
      "Epoch 3/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.3948 - acc: 0.3251 - val_loss: 1.4168 - val_acc: 0.3293\n",
      "Epoch 4/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.3846 - acc: 0.3286 - val_loss: 1.4570 - val_acc: 0.3202\n",
      "Epoch 5/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.3566 - acc: 0.3191 - val_loss: 1.4014 - val_acc: 0.3293\n",
      "Epoch 6/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.3542 - acc: 0.3554 - val_loss: 1.4068 - val_acc: 0.3293\n",
      "Epoch 7/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.3300 - acc: 0.3470 - val_loss: 1.4225 - val_acc: 0.3263\n",
      "Epoch 8/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.3172 - acc: 0.3519 - val_loss: 1.4360 - val_acc: 0.3112\n",
      "Epoch 9/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.3027 - acc: 0.3575 - val_loss: 1.4440 - val_acc: 0.3172\n",
      "Epoch 10/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.3050 - acc: 0.3522 - val_loss: 1.4396 - val_acc: 0.3172\n",
      "Epoch 11/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2897 - acc: 0.3568 - val_loss: 1.4083 - val_acc: 0.3233\n",
      "Epoch 12/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2813 - acc: 0.3663 - val_loss: 1.4156 - val_acc: 0.3202\n",
      "Epoch 13/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2757 - acc: 0.3822 - val_loss: 1.4495 - val_acc: 0.3051\n",
      "Epoch 14/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2681 - acc: 0.3713 - val_loss: 1.4191 - val_acc: 0.3142\n",
      "Epoch 15/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2640 - acc: 0.3702 - val_loss: 1.4142 - val_acc: 0.3112\n",
      "Epoch 16/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2603 - acc: 0.3853 - val_loss: 1.4408 - val_acc: 0.3112\n",
      "Epoch 17/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2467 - acc: 0.3787 - val_loss: 1.3975 - val_acc: 0.3172\n",
      "Epoch 18/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2436 - acc: 0.3861 - val_loss: 1.3865 - val_acc: 0.3142\n",
      "Epoch 19/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2446 - acc: 0.3917 - val_loss: 1.3973 - val_acc: 0.3112\n",
      "Epoch 20/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2341 - acc: 0.3945 - val_loss: 1.3789 - val_acc: 0.3172\n",
      "Epoch 21/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2080 - acc: 0.4001 - val_loss: 1.4519 - val_acc: 0.2961\n",
      "Epoch 22/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2267 - acc: 0.3769 - val_loss: 1.4327 - val_acc: 0.3142\n",
      "Epoch 23/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2123 - acc: 0.3998 - val_loss: 1.3994 - val_acc: 0.3172\n",
      "Epoch 24/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1999 - acc: 0.3980 - val_loss: 1.4137 - val_acc: 0.3142\n",
      "Epoch 25/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2027 - acc: 0.3913 - val_loss: 1.4547 - val_acc: 0.2810\n",
      "Epoch 26/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1980 - acc: 0.3956 - val_loss: 1.4376 - val_acc: 0.2840\n",
      "Epoch 27/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1909 - acc: 0.3984 - val_loss: 1.4506 - val_acc: 0.2810\n",
      "Epoch 28/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1838 - acc: 0.3991 - val_loss: 1.3903 - val_acc: 0.3172\n",
      "Epoch 29/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1651 - acc: 0.4089 - val_loss: 1.4634 - val_acc: 0.2810\n",
      "Epoch 30/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1885 - acc: 0.4012 - val_loss: 1.4095 - val_acc: 0.2991\n",
      "Epoch 31/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1778 - acc: 0.4037 - val_loss: 1.3668 - val_acc: 0.3112\n",
      "Epoch 32/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1761 - acc: 0.4142 - val_loss: 1.4048 - val_acc: 0.3172\n",
      "Epoch 33/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1656 - acc: 0.4139 - val_loss: 1.4505 - val_acc: 0.2810\n",
      "Epoch 34/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.1555 - acc: 0.3991 - val_loss: 1.4345 - val_acc: 0.2870\n",
      "Epoch 35/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1448 - acc: 0.4153 - val_loss: 1.3288 - val_acc: 0.3323\n",
      "Epoch 36/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1400 - acc: 0.4202 - val_loss: 1.3637 - val_acc: 0.3323\n",
      "Epoch 37/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1532 - acc: 0.4153 - val_loss: 1.2952 - val_acc: 0.3474\n",
      "Epoch 38/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1433 - acc: 0.4301 - val_loss: 1.3177 - val_acc: 0.3233\n",
      "Epoch 39/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1476 - acc: 0.4259 - val_loss: 1.3501 - val_acc: 0.3172\n",
      "Epoch 40/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1338 - acc: 0.4202 - val_loss: 1.3582 - val_acc: 0.3051\n",
      "Epoch 41/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1261 - acc: 0.4290 - val_loss: 1.3576 - val_acc: 0.3112\n",
      "Epoch 42/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1180 - acc: 0.4311 - val_loss: 1.4706 - val_acc: 0.2810\n",
      "Epoch 43/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1351 - acc: 0.4089 - val_loss: 1.3997 - val_acc: 0.3112\n",
      "Epoch 44/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.1306 - acc: 0.4206 - val_loss: 1.3344 - val_acc: 0.3353\n",
      "Epoch 45/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1026 - acc: 0.4417 - val_loss: 1.3906 - val_acc: 0.3172\n",
      "Epoch 46/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1050 - acc: 0.4273 - val_loss: 1.4047 - val_acc: 0.3233\n",
      "Epoch 47/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1070 - acc: 0.4403 - val_loss: 1.2894 - val_acc: 0.3565\n",
      "Epoch 48/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1050 - acc: 0.4382 - val_loss: 1.2895 - val_acc: 0.3535\n",
      "Epoch 49/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0942 - acc: 0.4421 - val_loss: 1.2991 - val_acc: 0.3474\n",
      "Epoch 50/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0877 - acc: 0.4336 - val_loss: 1.3764 - val_acc: 0.3202\n",
      "Epoch 51/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0747 - acc: 0.4523 - val_loss: 1.3735 - val_acc: 0.3293\n",
      "Epoch 52/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1012 - acc: 0.4280 - val_loss: 1.3718 - val_acc: 0.3353\n",
      "Epoch 53/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0848 - acc: 0.4318 - val_loss: 1.4061 - val_acc: 0.2991\n",
      "Epoch 54/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0822 - acc: 0.4347 - val_loss: 1.3439 - val_acc: 0.3323\n",
      "Epoch 55/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0716 - acc: 0.4530 - val_loss: 1.3213 - val_acc: 0.3505\n",
      "Epoch 56/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0716 - acc: 0.4414 - val_loss: 1.4186 - val_acc: 0.3142\n",
      "Epoch 57/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0895 - acc: 0.4297 - val_loss: 1.3101 - val_acc: 0.3565\n",
      "Epoch 58/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0508 - acc: 0.4586 - val_loss: 1.2560 - val_acc: 0.3686\n",
      "Epoch 59/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0536 - acc: 0.4600 - val_loss: 1.2021 - val_acc: 0.3837\n",
      "Epoch 60/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0570 - acc: 0.4628 - val_loss: 1.2524 - val_acc: 0.3686\n",
      "Epoch 61/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0457 - acc: 0.4551 - val_loss: 1.2577 - val_acc: 0.3686\n",
      "Epoch 62/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0380 - acc: 0.4590 - val_loss: 1.2461 - val_acc: 0.3746\n",
      "Epoch 63/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0500 - acc: 0.4653 - val_loss: 1.2554 - val_acc: 0.4048\n",
      "Epoch 64/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0633 - acc: 0.4618 - val_loss: 1.3402 - val_acc: 0.3474\n",
      "Epoch 65/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0594 - acc: 0.4428 - val_loss: 1.3834 - val_acc: 0.3323\n",
      "Epoch 66/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0498 - acc: 0.4551 - val_loss: 1.2945 - val_acc: 0.3807\n",
      "Epoch 67/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0324 - acc: 0.4741 - val_loss: 1.3062 - val_acc: 0.3867\n",
      "Epoch 68/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0361 - acc: 0.4604 - val_loss: 1.3130 - val_acc: 0.3776\n",
      "Epoch 69/150\n",
      "2839/2839 [==============================] - 0s 44us/step - loss: 1.0225 - acc: 0.4815 - val_loss: 1.2985 - val_acc: 0.3867\n",
      "Epoch 70/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.0154 - acc: 0.4635 - val_loss: 1.3245 - val_acc: 0.3807\n",
      "Epoch 71/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0249 - acc: 0.4576 - val_loss: 1.3372 - val_acc: 0.4018\n",
      "Epoch 72/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0091 - acc: 0.4709 - val_loss: 1.2469 - val_acc: 0.4139\n",
      "Epoch 73/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0039 - acc: 0.4797 - val_loss: 1.3373 - val_acc: 0.3776\n",
      "Epoch 74/150\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 1.0479 - acc: 0.428 - 0s 35us/step - loss: 1.0357 - acc: 0.4495 - val_loss: 1.2997 - val_acc: 0.3716\n",
      "Epoch 75/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9979 - acc: 0.4713 - val_loss: 1.3338 - val_acc: 0.3656\n",
      "Epoch 76/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0005 - acc: 0.4706 - val_loss: 1.2870 - val_acc: 0.3927\n",
      "Epoch 77/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0014 - acc: 0.4829 - val_loss: 1.3821 - val_acc: 0.3474\n",
      "Epoch 78/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0320 - acc: 0.4544 - val_loss: 1.3446 - val_acc: 0.3565\n",
      "Epoch 79/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9941 - acc: 0.4797 - val_loss: 1.2427 - val_acc: 0.4441\n",
      "Epoch 80/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9841 - acc: 0.4829 - val_loss: 1.2885 - val_acc: 0.3897\n",
      "Epoch 81/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9930 - acc: 0.4776 - val_loss: 1.2653 - val_acc: 0.4381\n",
      "Epoch 82/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9776 - acc: 0.4787 - val_loss: 1.1834 - val_acc: 0.5287\n",
      "Epoch 83/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9810 - acc: 0.4981 - val_loss: 1.1321 - val_acc: 0.5740\n",
      "Epoch 84/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9998 - acc: 0.5058 - val_loss: 1.1425 - val_acc: 0.5257\n",
      "Epoch 85/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9828 - acc: 0.4998 - val_loss: 1.1508 - val_acc: 0.5287\n",
      "Epoch 86/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9686 - acc: 0.4967 - val_loss: 1.2110 - val_acc: 0.4592\n",
      "Epoch 87/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9608 - acc: 0.4900 - val_loss: 1.1859 - val_acc: 0.5166\n",
      "Epoch 88/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9658 - acc: 0.5055 - val_loss: 1.1446 - val_acc: 0.5287\n",
      "Epoch 89/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.9625 - acc: 0.5114 - val_loss: 1.1543 - val_acc: 0.5347\n",
      "Epoch 90/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9616 - acc: 0.5062 - val_loss: 1.1489 - val_acc: 0.5589\n",
      "Epoch 91/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.9866 - acc: 0.4974 - val_loss: 1.1182 - val_acc: 0.5680\n",
      "Epoch 92/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9961 - acc: 0.4914 - val_loss: 1.1566 - val_acc: 0.5076\n",
      "Epoch 93/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9606 - acc: 0.5016 - val_loss: 1.1760 - val_acc: 0.5106\n",
      "Epoch 94/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9399 - acc: 0.5037 - val_loss: 1.2107 - val_acc: 0.4894\n",
      "Epoch 95/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9569 - acc: 0.4977 - val_loss: 1.2173 - val_acc: 0.4834\n",
      "Epoch 96/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9654 - acc: 0.5076 - val_loss: 1.1920 - val_acc: 0.5196\n",
      "Epoch 97/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9266 - acc: 0.5041 - val_loss: 1.1676 - val_acc: 0.5287\n",
      "Epoch 98/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9408 - acc: 0.5125 - val_loss: 1.1581 - val_acc: 0.5227\n",
      "Epoch 99/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9211 - acc: 0.5104 - val_loss: 1.1809 - val_acc: 0.4955\n",
      "Epoch 100/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.9331 - acc: 0.5062 - val_loss: 1.1407 - val_acc: 0.5166\n",
      "Epoch 101/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9354 - acc: 0.5122 - val_loss: 1.1484 - val_acc: 0.5378\n",
      "Epoch 102/150\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.9223 - acc: 0.5188 - val_loss: 1.1163 - val_acc: 0.5619\n",
      "Epoch 103/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.9474 - acc: 0.5143 - val_loss: 1.1316 - val_acc: 0.5619\n",
      "Epoch 104/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9219 - acc: 0.5312 - val_loss: 1.1718 - val_acc: 0.5196\n",
      "Epoch 105/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9082 - acc: 0.5217 - val_loss: 1.1405 - val_acc: 0.5498\n",
      "Epoch 106/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9295 - acc: 0.5188 - val_loss: 1.1593 - val_acc: 0.5196\n",
      "Epoch 107/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9119 - acc: 0.5259 - val_loss: 1.2211 - val_acc: 0.4411\n",
      "Epoch 108/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9203 - acc: 0.5217 - val_loss: 1.1569 - val_acc: 0.5166\n",
      "Epoch 109/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.9146 - acc: 0.5132 - val_loss: 1.1091 - val_acc: 0.5680\n",
      "Epoch 110/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9449 - acc: 0.5210 - val_loss: 1.1266 - val_acc: 0.5529\n",
      "Epoch 111/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9250 - acc: 0.5312 - val_loss: 1.1907 - val_acc: 0.4985\n",
      "Epoch 112/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8841 - acc: 0.5347 - val_loss: 1.1859 - val_acc: 0.5166\n",
      "Epoch 113/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8908 - acc: 0.5428 - val_loss: 1.1794 - val_acc: 0.5136\n",
      "Epoch 114/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8802 - acc: 0.5262 - val_loss: 1.1299 - val_acc: 0.5438\n",
      "Epoch 115/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.9268 - acc: 0.5097 - val_loss: 1.0902 - val_acc: 0.5559\n",
      "Epoch 116/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9638 - acc: 0.5160 - val_loss: 1.1445 - val_acc: 0.5317\n",
      "Epoch 117/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.8795 - acc: 0.5488 - val_loss: 1.2753 - val_acc: 0.4562\n",
      "Epoch 118/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8875 - acc: 0.5199 - val_loss: 1.2031 - val_acc: 0.4864\n",
      "Epoch 119/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8801 - acc: 0.5389 - val_loss: 1.1988 - val_acc: 0.5106\n",
      "Epoch 120/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8613 - acc: 0.5379 - val_loss: 1.2302 - val_acc: 0.4773\n",
      "Epoch 121/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8621 - acc: 0.5477 - val_loss: 1.1953 - val_acc: 0.4743\n",
      "Epoch 122/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8967 - acc: 0.5329 - val_loss: 1.1529 - val_acc: 0.5257\n",
      "Epoch 123/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8846 - acc: 0.5414 - val_loss: 1.1216 - val_acc: 0.5347\n",
      "Epoch 124/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8853 - acc: 0.5414 - val_loss: 1.1058 - val_acc: 0.5559\n",
      "Epoch 125/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8892 - acc: 0.5562 - val_loss: 1.1479 - val_acc: 0.5378\n",
      "Epoch 126/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8724 - acc: 0.5477 - val_loss: 1.1784 - val_acc: 0.5106\n",
      "Epoch 127/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8387 - acc: 0.5495 - val_loss: 1.1702 - val_acc: 0.5076\n",
      "Epoch 128/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8552 - acc: 0.5336 - val_loss: 1.1141 - val_acc: 0.5498\n",
      "Epoch 129/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8858 - acc: 0.5544 - val_loss: 1.0928 - val_acc: 0.5529\n",
      "Epoch 130/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9087 - acc: 0.5467 - val_loss: 1.1318 - val_acc: 0.5317\n",
      "Epoch 131/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8693 - acc: 0.5477 - val_loss: 1.1452 - val_acc: 0.5287\n",
      "Epoch 132/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8518 - acc: 0.5463 - val_loss: 1.1550 - val_acc: 0.5257\n",
      "Epoch 133/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8438 - acc: 0.5442 - val_loss: 1.1321 - val_acc: 0.5408\n",
      "Epoch 134/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8690 - acc: 0.5393 - val_loss: 1.1376 - val_acc: 0.5468\n",
      "Epoch 135/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8824 - acc: 0.5343 - val_loss: 1.1634 - val_acc: 0.5257\n",
      "Epoch 136/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8454 - acc: 0.5541 - val_loss: 1.1400 - val_acc: 0.5287\n",
      "Epoch 137/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8486 - acc: 0.5604 - val_loss: 1.1602 - val_acc: 0.5196\n",
      "Epoch 138/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8376 - acc: 0.5513 - val_loss: 1.1811 - val_acc: 0.5045\n",
      "Epoch 139/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8203 - acc: 0.5608 - val_loss: 1.2362 - val_acc: 0.4743\n",
      "Epoch 140/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8194 - acc: 0.5657 - val_loss: 1.2865 - val_acc: 0.4411\n",
      "Epoch 141/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8239 - acc: 0.5558 - val_loss: 1.3611 - val_acc: 0.4079\n",
      "Epoch 142/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8720 - acc: 0.5343 - val_loss: 1.4024 - val_acc: 0.3927\n",
      "Epoch 143/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8641 - acc: 0.5248 - val_loss: 1.2430 - val_acc: 0.4653\n",
      "Epoch 144/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8216 - acc: 0.5548 - val_loss: 1.2348 - val_acc: 0.4713\n",
      "Epoch 145/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8239 - acc: 0.5523 - val_loss: 1.2636 - val_acc: 0.4532\n",
      "Epoch 146/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8201 - acc: 0.5618 - val_loss: 1.3718 - val_acc: 0.3988\n",
      "Epoch 147/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8569 - acc: 0.5379 - val_loss: 1.3295 - val_acc: 0.4079\n",
      "Epoch 148/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8281 - acc: 0.5463 - val_loss: 1.3043 - val_acc: 0.4260\n",
      "Epoch 149/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8290 - acc: 0.5424 - val_loss: 1.2842 - val_acc: 0.4260\n",
      "Epoch 150/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8043 - acc: 0.5428 - val_loss: 1.2469 - val_acc: 0.4592\n",
      "3\n",
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/150\n",
      "2839/2839 [==============================] - 24s 8ms/step - loss: 1.4674 - acc: 0.3015 - val_loss: 1.4507 - val_acc: 0.3323\n",
      "Epoch 2/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.3442 - acc: 0.3850 - val_loss: 1.5358 - val_acc: 0.3293\n",
      "Epoch 3/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.3317 - acc: 0.3677 - val_loss: 1.4675 - val_acc: 0.3233\n",
      "Epoch 4/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.3183 - acc: 0.3801 - val_loss: 1.3974 - val_acc: 0.3233\n",
      "Epoch 5/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2947 - acc: 0.3917 - val_loss: 1.3396 - val_acc: 0.3414\n",
      "Epoch 6/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2850 - acc: 0.4114 - val_loss: 1.4049 - val_acc: 0.3293\n",
      "Epoch 7/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.2748 - acc: 0.3906 - val_loss: 1.3834 - val_acc: 0.3323\n",
      "Epoch 8/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2650 - acc: 0.4054 - val_loss: 1.3628 - val_acc: 0.3323\n",
      "Epoch 9/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2385 - acc: 0.4047 - val_loss: 1.3527 - val_acc: 0.3293\n",
      "Epoch 10/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2562 - acc: 0.4019 - val_loss: 1.2874 - val_acc: 0.3535\n",
      "Epoch 11/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2383 - acc: 0.4202 - val_loss: 1.4112 - val_acc: 0.3323\n",
      "Epoch 12/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2252 - acc: 0.3998 - val_loss: 1.3632 - val_acc: 0.3293\n",
      "Epoch 13/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2021 - acc: 0.4125 - val_loss: 1.3800 - val_acc: 0.3414\n",
      "Epoch 14/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2044 - acc: 0.4251 - val_loss: 1.3627 - val_acc: 0.3384\n",
      "Epoch 15/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.2024 - acc: 0.4160 - val_loss: 1.3491 - val_acc: 0.3353\n",
      "Epoch 16/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.2073 - acc: 0.4082 - val_loss: 1.4252 - val_acc: 0.3293\n",
      "Epoch 17/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.2015 - acc: 0.4114 - val_loss: 1.5321 - val_acc: 0.3172\n",
      "Epoch 18/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1911 - acc: 0.4135 - val_loss: 1.3687 - val_acc: 0.3233\n",
      "Epoch 19/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1886 - acc: 0.4149 - val_loss: 1.3009 - val_acc: 0.3444\n",
      "Epoch 20/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.1774 - acc: 0.4421 - val_loss: 1.3135 - val_acc: 0.3474\n",
      "Epoch 21/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1801 - acc: 0.4273 - val_loss: 1.3636 - val_acc: 0.3474\n",
      "Epoch 22/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1521 - acc: 0.4333 - val_loss: 1.3840 - val_acc: 0.3414\n",
      "Epoch 23/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1698 - acc: 0.4195 - val_loss: 1.4623 - val_acc: 0.3172\n",
      "Epoch 24/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1501 - acc: 0.4058 - val_loss: 1.3534 - val_acc: 0.3505\n",
      "Epoch 25/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1435 - acc: 0.4318 - val_loss: 1.3278 - val_acc: 0.3565\n",
      "Epoch 26/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1426 - acc: 0.4329 - val_loss: 1.2343 - val_acc: 0.3746\n",
      "Epoch 27/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.1536 - acc: 0.4333 - val_loss: 1.2463 - val_acc: 0.3716\n",
      "Epoch 28/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1416 - acc: 0.4371 - val_loss: 1.2917 - val_acc: 0.3535\n",
      "Epoch 29/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1298 - acc: 0.4364 - val_loss: 1.3036 - val_acc: 0.3384\n",
      "Epoch 30/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.1217 - acc: 0.4414 - val_loss: 1.2565 - val_acc: 0.3595\n",
      "Epoch 31/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1162 - acc: 0.4491 - val_loss: 1.3149 - val_acc: 0.3535\n",
      "Epoch 32/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1068 - acc: 0.4354 - val_loss: 1.2887 - val_acc: 0.3565\n",
      "Epoch 33/150\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.0980 - acc: 0.4375 - val_loss: 1.2832 - val_acc: 0.3474\n",
      "Epoch 34/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.1049 - acc: 0.4442 - val_loss: 1.2412 - val_acc: 0.3656\n",
      "Epoch 35/150\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0840 - acc: 0.4579 - val_loss: 1.2285 - val_acc: 0.3746\n",
      "Epoch 36/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 1.1031 - acc: 0.4590 - val_loss: 1.1778 - val_acc: 0.3988\n",
      "Epoch 37/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.1087 - acc: 0.4639 - val_loss: 1.2388 - val_acc: 0.3656\n",
      "Epoch 38/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0847 - acc: 0.4516 - val_loss: 1.2221 - val_acc: 0.3595\n",
      "Epoch 39/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0985 - acc: 0.4477 - val_loss: 1.2213 - val_acc: 0.3716\n",
      "Epoch 40/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0836 - acc: 0.4477 - val_loss: 1.2576 - val_acc: 0.3625\n",
      "Epoch 41/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0677 - acc: 0.4456 - val_loss: 1.3840 - val_acc: 0.3172\n",
      "Epoch 42/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0757 - acc: 0.4382 - val_loss: 1.4240 - val_acc: 0.3172\n",
      "Epoch 43/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1202 - acc: 0.4220 - val_loss: 1.3930 - val_acc: 0.3233\n",
      "Epoch 44/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0807 - acc: 0.4304 - val_loss: 1.2748 - val_acc: 0.3414\n",
      "Epoch 45/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0587 - acc: 0.4502 - val_loss: 1.2788 - val_acc: 0.3505\n",
      "Epoch 46/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.0613 - acc: 0.4650 - val_loss: 1.3001 - val_acc: 0.3535\n",
      "Epoch 47/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0456 - acc: 0.4498 - val_loss: 1.2930 - val_acc: 0.3474\n",
      "Epoch 48/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0397 - acc: 0.4530 - val_loss: 1.2754 - val_acc: 0.3444\n",
      "Epoch 49/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0465 - acc: 0.4745 - val_loss: 1.3836 - val_acc: 0.3263\n",
      "Epoch 50/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 1.0598 - acc: 0.4509 - val_loss: 1.2952 - val_acc: 0.3565\n",
      "Epoch 51/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0507 - acc: 0.4572 - val_loss: 1.3372 - val_acc: 0.3353\n",
      "Epoch 52/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 1.0551 - acc: 0.4495 - val_loss: 1.3438 - val_acc: 0.3444\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.0585 - acc: 0.4477 - val_loss: 1.3605 - val_acc: 0.3414\n",
      "Epoch 54/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0592 - acc: 0.4449 - val_loss: 1.2749 - val_acc: 0.3807\n",
      "Epoch 55/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0247 - acc: 0.4639 - val_loss: 1.2631 - val_acc: 0.4139\n",
      "Epoch 56/150\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0355 - acc: 0.4607 - val_loss: 1.2453 - val_acc: 0.4230\n",
      "Epoch 57/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0237 - acc: 0.4674 - val_loss: 1.2444 - val_acc: 0.3958\n",
      "Epoch 58/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0107 - acc: 0.4741 - val_loss: 1.2813 - val_acc: 0.3746\n",
      "Epoch 59/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0310 - acc: 0.4628 - val_loss: 1.3442 - val_acc: 0.3505\n",
      "Epoch 60/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 1.0430 - acc: 0.4523 - val_loss: 1.3076 - val_acc: 0.3807\n",
      "Epoch 61/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0283 - acc: 0.4657 - val_loss: 1.2712 - val_acc: 0.3988\n",
      "Epoch 62/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9978 - acc: 0.4688 - val_loss: 1.2185 - val_acc: 0.4411\n",
      "Epoch 63/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0067 - acc: 0.4875 - val_loss: 1.2866 - val_acc: 0.3897\n",
      "Epoch 64/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9962 - acc: 0.4829 - val_loss: 1.2326 - val_acc: 0.4260\n",
      "Epoch 65/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9983 - acc: 0.4773 - val_loss: 1.1405 - val_acc: 0.4864\n",
      "Epoch 66/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9990 - acc: 0.4826 - val_loss: 1.0950 - val_acc: 0.5438\n",
      "Epoch 67/150\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.0063 - acc: 0.4981 - val_loss: 1.1102 - val_acc: 0.5196\n",
      "Epoch 68/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9999 - acc: 0.5009 - val_loss: 1.1340 - val_acc: 0.4985\n",
      "Epoch 69/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9748 - acc: 0.5072 - val_loss: 1.1020 - val_acc: 0.5257\n",
      "Epoch 70/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9808 - acc: 0.5090 - val_loss: 1.1265 - val_acc: 0.5287\n",
      "Epoch 71/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9864 - acc: 0.5086 - val_loss: 1.1353 - val_acc: 0.5015\n",
      "Epoch 72/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9830 - acc: 0.4998 - val_loss: 1.1450 - val_acc: 0.4985\n",
      "Epoch 73/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9714 - acc: 0.5111 - val_loss: 1.1523 - val_acc: 0.5045\n",
      "Epoch 74/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.9610 - acc: 0.5065 - val_loss: 1.0986 - val_acc: 0.5378\n",
      "Epoch 75/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9740 - acc: 0.5005 - val_loss: 1.0764 - val_acc: 0.5710\n",
      "Epoch 76/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0051 - acc: 0.5178 - val_loss: 1.1281 - val_acc: 0.4955\n",
      "Epoch 77/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9608 - acc: 0.5093 - val_loss: 1.1815 - val_acc: 0.4864\n",
      "Epoch 78/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9439 - acc: 0.5114 - val_loss: 1.0932 - val_acc: 0.5468\n",
      "Epoch 79/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9555 - acc: 0.5026 - val_loss: 1.0879 - val_acc: 0.5529\n",
      "Epoch 80/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9515 - acc: 0.5298 - val_loss: 1.1339 - val_acc: 0.5801\n",
      "Epoch 81/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9671 - acc: 0.5203 - val_loss: 1.0957 - val_acc: 0.5680\n",
      "Epoch 82/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9547 - acc: 0.5354 - val_loss: 1.0804 - val_acc: 0.5468\n",
      "Epoch 83/150\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.9661 - acc: 0.5213 - val_loss: 1.1083 - val_acc: 0.5529\n",
      "Epoch 84/150\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.9610 - acc: 0.5083 - val_loss: 1.0853 - val_acc: 0.5438\n",
      "Epoch 85/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9461 - acc: 0.5234 - val_loss: 1.1000 - val_acc: 0.5559\n",
      "Epoch 86/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9422 - acc: 0.5157 - val_loss: 1.1109 - val_acc: 0.5529\n",
      "Epoch 87/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9262 - acc: 0.5294 - val_loss: 1.1334 - val_acc: 0.5529\n",
      "Epoch 88/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9104 - acc: 0.5269 - val_loss: 1.1474 - val_acc: 0.5680\n",
      "Epoch 89/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9160 - acc: 0.5354 - val_loss: 1.1012 - val_acc: 0.5801\n",
      "Epoch 90/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9139 - acc: 0.5280 - val_loss: 1.0630 - val_acc: 0.5801\n",
      "Epoch 91/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9436 - acc: 0.5238 - val_loss: 1.0584 - val_acc: 0.5982\n",
      "Epoch 92/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9681 - acc: 0.5129 - val_loss: 1.0756 - val_acc: 0.5801\n",
      "Epoch 93/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9224 - acc: 0.5414 - val_loss: 1.1235 - val_acc: 0.5498\n",
      "Epoch 94/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8960 - acc: 0.5389 - val_loss: 1.1219 - val_acc: 0.5650\n",
      "Epoch 95/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8903 - acc: 0.5322 - val_loss: 1.1013 - val_acc: 0.5559\n",
      "Epoch 96/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9022 - acc: 0.5446 - val_loss: 1.0891 - val_acc: 0.5680\n",
      "Epoch 97/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9081 - acc: 0.5442 - val_loss: 1.0640 - val_acc: 0.6012\n",
      "Epoch 98/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9158 - acc: 0.5280 - val_loss: 1.0848 - val_acc: 0.6012\n",
      "Epoch 99/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9071 - acc: 0.5386 - val_loss: 1.0646 - val_acc: 0.5982\n",
      "Epoch 100/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9149 - acc: 0.5329 - val_loss: 1.0812 - val_acc: 0.5861\n",
      "Epoch 101/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8868 - acc: 0.5414 - val_loss: 1.0755 - val_acc: 0.5770\n",
      "Epoch 102/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9057 - acc: 0.5308 - val_loss: 1.0852 - val_acc: 0.5891\n",
      "Epoch 103/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8969 - acc: 0.5400 - val_loss: 1.1056 - val_acc: 0.6012\n",
      "Epoch 104/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8834 - acc: 0.5379 - val_loss: 1.0941 - val_acc: 0.5891\n",
      "Epoch 105/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9024 - acc: 0.5375 - val_loss: 1.0948 - val_acc: 0.6012\n",
      "Epoch 106/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8911 - acc: 0.5484 - val_loss: 1.0746 - val_acc: 0.6012\n",
      "Epoch 107/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.9013 - acc: 0.5262 - val_loss: 1.0672 - val_acc: 0.5891\n",
      "Epoch 108/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.8972 - acc: 0.5396 - val_loss: 1.1129 - val_acc: 0.5710\n",
      "Epoch 109/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8664 - acc: 0.5516 - val_loss: 1.1444 - val_acc: 0.5619\n",
      "Epoch 110/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8578 - acc: 0.5329 - val_loss: 1.1613 - val_acc: 0.5559\n",
      "Epoch 111/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8628 - acc: 0.5379 - val_loss: 1.1885 - val_acc: 0.5166\n",
      "Epoch 112/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8713 - acc: 0.5424 - val_loss: 1.2008 - val_acc: 0.5378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8850 - acc: 0.5400 - val_loss: 1.2989 - val_acc: 0.4743\n",
      "Epoch 114/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9132 - acc: 0.5100 - val_loss: 1.2103 - val_acc: 0.5015\n",
      "Epoch 115/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8530 - acc: 0.5435 - val_loss: 1.1983 - val_acc: 0.5196\n",
      "Epoch 116/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8700 - acc: 0.5181 - val_loss: 1.1880 - val_acc: 0.5166\n",
      "Epoch 117/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8778 - acc: 0.5393 - val_loss: 1.1756 - val_acc: 0.5438\n",
      "Epoch 118/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8515 - acc: 0.5389 - val_loss: 1.1821 - val_acc: 0.5347\n",
      "Epoch 119/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8463 - acc: 0.5435 - val_loss: 1.1915 - val_acc: 0.5136\n",
      "Epoch 120/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8597 - acc: 0.5347 - val_loss: 1.2803 - val_acc: 0.4773\n",
      "Epoch 121/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8818 - acc: 0.5107 - val_loss: 1.2330 - val_acc: 0.4743\n",
      "Epoch 122/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8724 - acc: 0.5389 - val_loss: 1.1771 - val_acc: 0.5257\n",
      "Epoch 123/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8485 - acc: 0.5555 - val_loss: 1.1514 - val_acc: 0.5347\n",
      "Epoch 124/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8449 - acc: 0.5576 - val_loss: 1.1100 - val_acc: 0.5650\n",
      "Epoch 125/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8707 - acc: 0.5576 - val_loss: 1.0978 - val_acc: 0.5710\n",
      "Epoch 126/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8269 - acc: 0.5759 - val_loss: 1.1176 - val_acc: 0.5559\n",
      "Epoch 127/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8237 - acc: 0.5696 - val_loss: 1.0903 - val_acc: 0.5891\n",
      "Epoch 128/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8380 - acc: 0.5601 - val_loss: 1.0622 - val_acc: 0.5952\n",
      "Epoch 129/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8863 - acc: 0.5555 - val_loss: 1.0669 - val_acc: 0.5831\n",
      "Epoch 130/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8566 - acc: 0.5643 - val_loss: 1.0987 - val_acc: 0.5589\n",
      "Epoch 131/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8076 - acc: 0.5752 - val_loss: 1.1171 - val_acc: 0.5619\n",
      "Epoch 132/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8100 - acc: 0.5685 - val_loss: 1.1064 - val_acc: 0.5650\n",
      "Epoch 133/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8161 - acc: 0.5734 - val_loss: 1.1787 - val_acc: 0.5045\n",
      "Epoch 134/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8276 - acc: 0.5601 - val_loss: 1.1939 - val_acc: 0.5076\n",
      "Epoch 135/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8441 - acc: 0.5734 - val_loss: 1.2155 - val_acc: 0.4834\n",
      "Epoch 136/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8521 - acc: 0.5488 - val_loss: 1.3450 - val_acc: 0.4471\n",
      "Epoch 137/150\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.8875 - acc: 0.5291 - val_loss: 1.2333 - val_acc: 0.4894\n",
      "Epoch 138/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8227 - acc: 0.5527 - val_loss: 1.1757 - val_acc: 0.5166\n",
      "Epoch 139/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.7981 - acc: 0.5615 - val_loss: 1.1236 - val_acc: 0.5378\n",
      "Epoch 140/150\n",
      "2839/2839 [==============================] - 0s 46us/step - loss: 0.8021 - acc: 0.5685 - val_loss: 1.1517 - val_acc: 0.5106\n",
      "Epoch 141/150\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.8037 - acc: 0.5731 - val_loss: 1.1425 - val_acc: 0.5317\n",
      "Epoch 142/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7974 - acc: 0.5875 - val_loss: 1.1665 - val_acc: 0.5136\n",
      "Epoch 143/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8108 - acc: 0.5787 - val_loss: 1.1822 - val_acc: 0.5196\n",
      "Epoch 144/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8114 - acc: 0.5777 - val_loss: 1.2271 - val_acc: 0.4864\n",
      "Epoch 145/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8093 - acc: 0.5703 - val_loss: 1.2101 - val_acc: 0.4955\n",
      "Epoch 146/150\n",
      "2839/2839 [==============================] - 0s 38us/step - loss: 0.7868 - acc: 0.5851 - val_loss: 1.2172 - val_acc: 0.5015\n",
      "Epoch 147/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8085 - acc: 0.5664 - val_loss: 1.2782 - val_acc: 0.4713\n",
      "Epoch 148/150\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.8294 - acc: 0.5421 - val_loss: 1.2349 - val_acc: 0.4864\n",
      "Epoch 149/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.8089 - acc: 0.5586 - val_loss: 1.2047 - val_acc: 0.5106\n",
      "Epoch 150/150\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.7880 - acc: 0.5731 - val_loss: 1.1792 - val_acc: 0.5136\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.184601</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.175836</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.953947</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>1.226775</td>\n",
       "      <td>0.474320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.950704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.506250</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>1.246916</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.475862</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>1.179244</td>\n",
       "      <td>0.513595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       106.0  0.094972  0.754717   0.952830           0.272727   \n",
       "1       114.0  0.094972  0.745614   0.956140           0.303030   \n",
       "2       152.0  0.094972  0.651316   0.953947           0.303030   \n",
       "3       142.0  0.094972  0.676056   0.950704           0.333333   \n",
       "4       113.0  0.094972  0.734513   0.938053           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.787879            0.939394             0.081481   \n",
       "1          0.757576            0.878788             0.070312   \n",
       "2          0.757576            0.909091             0.084746   \n",
       "3          0.818182            0.939394             0.093750   \n",
       "4          0.787879            0.939394             0.110345   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.525926              0.851852  1.184601  0.483384  \n",
       "1            0.507812              0.843750  1.175836  0.480363  \n",
       "2            0.525424              0.847458  1.226775  0.474320  \n",
       "3            0.506250              0.862500  1.246916  0.459215  \n",
       "4            0.475862              0.882759  1.179244  0.513595  "
      ]
     },
     "execution_count": 1187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zillow_analysis = pd.DataFrame()\n",
    "#, kernel_regularizer=regularizers.l1(0.0001)\n",
    "for num in range(5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_shape=(120,), activation='relu'))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    ada = keras.optimizers.Adagrad()\n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "    sgd = keras.optimizers.SGD(lr=0.75)\n",
    "    model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    zillow_model_2 = model.fit(x=X_train, y=y_cat_train, \n",
    "          batch_size=2000, \n",
    "          epochs=150, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_cat_test),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    loss = zillow_model_2.history['val_loss'][-1]\n",
    "    acc = zillow_model_2.history['val_acc'][-1]\n",
    "    gain_pred, gain_true_large, gain_true, gain_stay_true, four_five_split_large, four_five_split, four_five_split_mean, predicted_classes_gain_true, predicted_classes_gain_large_true, predicted_classes_gain_mean = model_metrics(predictions, y_cat_test)\n",
    "    df_zillow_analysis.loc[num,'pred_count'] = gain_pred\n",
    "    df_zillow_analysis.loc[num,'gain_10%'] = gain_large_true_large\n",
    "    df_zillow_analysis.loc[num,'gain_3%'] = gain_true\n",
    "    df_zillow_analysis.loc[num,'gain_mean'] = gain_stay_true\n",
    "    df_zillow_analysis.loc[num,'top_pred_prob_10%'] = four_five_split_large\n",
    "    df_zillow_analysis.loc[num,'top_pred_prob_3%'] = four_five_split\n",
    "    df_zillow_analysis.loc[num,'top_pred_prob_mean'] = four_five_split_mean\n",
    "    df_zillow_analysis.loc[num,'model_pred_prob_10%'] = predicted_classes_gain_large_true\n",
    "    df_zillow_analysis.loc[num,'model_pred_prob_3%'] = predicted_classes_gain_true\n",
    "    df_zillow_analysis.loc[num,'model_pred_prob_mean'] = predicted_classes_gain_mean\n",
    "    df_zillow_analysis.loc[num,'loss'] = loss\n",
    "    df_zillow_analysis.loc[num,'acc'] = acc\n",
    "    print(num)\n",
    "df_150_z = df_zillow_analysis\n",
    "df_zillow_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.491803</td>\n",
       "      <td>0.498489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.843023</td>\n",
       "      <td>1.549860</td>\n",
       "      <td>0.453172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.609467</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.075377</td>\n",
       "      <td>0.482412</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>1.910163</td>\n",
       "      <td>0.365559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.510345</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>1.516773</td>\n",
       "      <td>0.492447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.860606</td>\n",
       "      <td>1.829167</td>\n",
       "      <td>0.353474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        90.0  0.094972  0.744444   0.955556           0.242424   \n",
       "1       140.0  0.094972  0.678571   0.950000           0.272727   \n",
       "2       169.0  0.094972  0.609467   0.923077           0.333333   \n",
       "3       108.0  0.094972  0.731481   0.953704           0.303030   \n",
       "4       139.0  0.094972  0.647482   0.935252           0.303030   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.727273            0.878788             0.071429   \n",
       "1          0.727273            0.878788             0.081395   \n",
       "2          0.727273            0.878788             0.075377   \n",
       "3          0.787879            0.909091             0.103448   \n",
       "4          0.727273            0.878788             0.090909   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.531746              0.857143  1.491803  0.498489  \n",
       "1            0.488372              0.843023  1.549860  0.453172  \n",
       "2            0.482412              0.854271  1.910163  0.365559  \n",
       "3            0.510345              0.868966  1.516773  0.492447  \n",
       "4            0.490909              0.860606  1.829167  0.353474  "
      ]
     },
     "execution_count": 1178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_400_w_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.861702</td>\n",
       "      <td>1.826030</td>\n",
       "      <td>0.380665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>1.714454</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.079137</td>\n",
       "      <td>0.482014</td>\n",
       "      <td>0.892086</td>\n",
       "      <td>1.827232</td>\n",
       "      <td>0.413897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.084034</td>\n",
       "      <td>0.436975</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>1.764446</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.068627</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.480029</td>\n",
       "      <td>0.498489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        78.0  0.094972  0.794872   0.974359           0.272727   \n",
       "1       125.0  0.094972  0.696000   0.952000           0.242424   \n",
       "2       119.0  0.094972  0.663866   0.941176           0.272727   \n",
       "3        93.0  0.094972  0.741935   0.956989           0.242424   \n",
       "4        72.0  0.094972  0.777778   0.958333           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.818182            0.969697             0.085106   \n",
       "1          0.757576            0.909091             0.086667   \n",
       "2          0.727273            0.909091             0.079137   \n",
       "3          0.696970            0.909091             0.084034   \n",
       "4          0.757576            0.939394             0.068627   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.468085              0.861702  1.826030  0.380665  \n",
       "1            0.466667              0.860000  1.714454  0.432024  \n",
       "2            0.482014              0.892086  1.827232  0.413897  \n",
       "3            0.436975              0.848739  1.764446  0.465257  \n",
       "4            0.480392              0.833333  1.480029  0.498489  "
      ]
     },
     "execution_count": 1180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_400_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>1.484431</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>1.330855</td>\n",
       "      <td>0.534743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.631250</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.076142</td>\n",
       "      <td>0.482234</td>\n",
       "      <td>0.878173</td>\n",
       "      <td>1.462107</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>1.376176</td>\n",
       "      <td>0.531722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.755319</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.092199</td>\n",
       "      <td>0.475177</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>1.363008</td>\n",
       "      <td>0.519637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       116.0  0.094972  0.724138   0.965517           0.272727   \n",
       "1        72.0  0.094972  0.791667   0.958333           0.242424   \n",
       "2       160.0  0.094972  0.631250   0.937500           0.303030   \n",
       "3       109.0  0.094972  0.752294   0.963303           0.242424   \n",
       "4        94.0  0.094972  0.755319   0.957447           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.757576            0.909091             0.055556   \n",
       "1          0.757576            0.939394             0.120000   \n",
       "2          0.818182            0.939394             0.076142   \n",
       "3          0.757576            0.878788             0.070922   \n",
       "4          0.757576            0.939394             0.092199   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.472222              0.861111  1.484431  0.459215  \n",
       "1            0.540000              0.910000  1.330855  0.534743  \n",
       "2            0.482234              0.878173  1.462107  0.480363  \n",
       "3            0.468085              0.872340  1.376176  0.531722  \n",
       "4            0.475177              0.865248  1.363008  0.519637  "
      ]
     },
     "execution_count": 1176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_300_w_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.064935</td>\n",
       "      <td>0.461039</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>1.434881</td>\n",
       "      <td>0.507553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>1.450340</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.533402</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.741573</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.083969</td>\n",
       "      <td>0.488550</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>1.348234</td>\n",
       "      <td>0.513595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.936364</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.496552</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>1.400084</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       123.0  0.094972  0.707317   0.951220           0.272727   \n",
       "1        97.0  0.094972  0.731959   0.969072           0.242424   \n",
       "2        95.0  0.094972  0.789474   0.957895           0.242424   \n",
       "3        89.0  0.094972  0.741573   0.943820           0.272727   \n",
       "4       110.0  0.094972  0.709091   0.936364           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.757576            0.878788             0.064935   \n",
       "1          0.787879            0.939394             0.090909   \n",
       "2          0.787879            0.939394             0.077778   \n",
       "3          0.787879            0.909091             0.083969   \n",
       "4          0.727273            0.909091             0.089655   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.461039              0.844156  1.434881  0.507553  \n",
       "1            0.518182              0.909091  1.450340  0.459215  \n",
       "2            0.488889              0.888889  1.533402  0.471299  \n",
       "3            0.488550              0.870229  1.348234  0.513595  \n",
       "4            0.496552              0.868966  1.400084  0.486405  "
      ]
     },
     "execution_count": 1182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_300_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.948905</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>1.421515</td>\n",
       "      <td>0.510574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.846847</td>\n",
       "      <td>1.469787</td>\n",
       "      <td>0.468278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.636943</td>\n",
       "      <td>0.936306</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.075676</td>\n",
       "      <td>0.470270</td>\n",
       "      <td>0.881081</td>\n",
       "      <td>1.468260</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>1.435143</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.951389</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.076471</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.876471</td>\n",
       "      <td>1.542414</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       137.0  0.094972  0.671533   0.948905           0.272727   \n",
       "1        94.0  0.094972  0.797872   0.957447           0.272727   \n",
       "2       157.0  0.094972  0.636943   0.936306           0.303030   \n",
       "3       115.0  0.094972  0.739130   0.956522           0.303030   \n",
       "4       144.0  0.094972  0.659722   0.951389           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.787879            0.939394             0.088435   \n",
       "1          0.757576            0.909091             0.090090   \n",
       "2          0.727273            0.878788             0.075676   \n",
       "3          0.757576            0.909091             0.101562   \n",
       "4          0.727273            0.909091             0.076471   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.537415              0.863946  1.421515  0.510574  \n",
       "1            0.540541              0.846847  1.469787  0.468278  \n",
       "2            0.470270              0.881081  1.468260  0.465257  \n",
       "3            0.500000              0.859375  1.435143  0.480363  \n",
       "4            0.494118              0.876471  1.542414  0.432024  "
      ]
     },
     "execution_count": 1170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_250_w_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.968000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.093960</td>\n",
       "      <td>0.463087</td>\n",
       "      <td>0.852349</td>\n",
       "      <td>1.548896</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.115044</td>\n",
       "      <td>0.522124</td>\n",
       "      <td>0.858407</td>\n",
       "      <td>1.321403</td>\n",
       "      <td>0.489426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>1.274005</td>\n",
       "      <td>0.546828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.112903</td>\n",
       "      <td>0.532258</td>\n",
       "      <td>0.830645</td>\n",
       "      <td>1.325635</td>\n",
       "      <td>0.489426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.535433</td>\n",
       "      <td>0.881890</td>\n",
       "      <td>1.214646</td>\n",
       "      <td>0.540785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       125.0  0.094972  0.704000   0.968000           0.272727   \n",
       "1        88.0  0.094972  0.784091   0.965909           0.272727   \n",
       "2        70.0  0.094972  0.757143   0.942857           0.242424   \n",
       "3        92.0  0.094972  0.739130   0.934783           0.272727   \n",
       "4        98.0  0.094972  0.775510   0.959184           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.727273            0.909091             0.093960   \n",
       "1          0.757576            0.909091             0.115044   \n",
       "2          0.787879            0.939394             0.048780   \n",
       "3          0.757576            0.939394             0.112903   \n",
       "4          0.818182            0.939394             0.094488   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.463087              0.852349  1.548896  0.432024  \n",
       "1            0.522124              0.858407  1.321403  0.489426  \n",
       "2            0.495935              0.861789  1.274005  0.546828  \n",
       "3            0.532258              0.830645  1.325635  0.489426  \n",
       "4            0.535433              0.881890  1.214646  0.540785  "
      ]
     },
     "execution_count": 1184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_250_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.625767</td>\n",
       "      <td>0.938650</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.465969</td>\n",
       "      <td>0.879581</td>\n",
       "      <td>1.407345</td>\n",
       "      <td>0.474320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0.951389</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.092025</td>\n",
       "      <td>0.496933</td>\n",
       "      <td>0.883436</td>\n",
       "      <td>1.437591</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.669118</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.096970</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>1.464218</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.954887</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.472603</td>\n",
       "      <td>0.842466</td>\n",
       "      <td>1.397490</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.754545</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>1.270403</td>\n",
       "      <td>0.543807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       163.0  0.094972  0.625767   0.938650           0.303030   \n",
       "1       144.0  0.094972  0.659722   0.951389           0.303030   \n",
       "2       136.0  0.094972  0.669118   0.955882           0.303030   \n",
       "3       133.0  0.094972  0.676692   0.954887           0.272727   \n",
       "4       110.0  0.094972  0.754545   0.963636           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.757576            0.909091             0.083770   \n",
       "1          0.787879            0.909091             0.092025   \n",
       "2          0.787879            0.969697             0.096970   \n",
       "3          0.787879            0.909091             0.095890   \n",
       "4          0.787879            0.939394             0.097561   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.465969              0.879581  1.407345  0.474320  \n",
       "1            0.496933              0.883436  1.437591  0.483384  \n",
       "2            0.515152              0.878788  1.464218  0.465257  \n",
       "3            0.472603              0.842466  1.397490  0.480363  \n",
       "4            0.495935              0.886179  1.270403  0.543807  "
      ]
     },
     "execution_count": 1172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200_w_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.094828</td>\n",
       "      <td>0.439655</td>\n",
       "      <td>0.818966</td>\n",
       "      <td>1.196810</td>\n",
       "      <td>0.546828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.718447</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.430894</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>1.307886</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>1.204385</td>\n",
       "      <td>0.525680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>1.294213</td>\n",
       "      <td>0.492447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.093525</td>\n",
       "      <td>0.525180</td>\n",
       "      <td>0.856115</td>\n",
       "      <td>1.235405</td>\n",
       "      <td>0.489426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        93.0  0.094972  0.795699   0.956989           0.272727   \n",
       "1       103.0  0.094972  0.718447   0.932039           0.242424   \n",
       "2        88.0  0.094972  0.784091   0.954545           0.242424   \n",
       "3        82.0  0.094972  0.804878   0.951220           0.242424   \n",
       "4        95.0  0.094972  0.768421   0.957895           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.969697             0.094828   \n",
       "1          0.757576            0.939394             0.097561   \n",
       "2          0.787879            0.939394             0.082569   \n",
       "3          0.787879            0.969697             0.078431   \n",
       "4          0.787879            0.939394             0.093525   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.439655              0.818966  1.196810  0.546828  \n",
       "1            0.430894              0.829268  1.307886  0.483384  \n",
       "2            0.495413              0.871560  1.204385  0.525680  \n",
       "3            0.500000              0.803922  1.294213  0.492447  \n",
       "4            0.525180              0.856115  1.235405  0.489426  "
      ]
     },
     "execution_count": 1186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.107438</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>1.245467</td>\n",
       "      <td>0.580060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.668919</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.461111</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>1.352093</td>\n",
       "      <td>0.513595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.070796</td>\n",
       "      <td>0.469027</td>\n",
       "      <td>0.858407</td>\n",
       "      <td>1.461635</td>\n",
       "      <td>0.407855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.633136</td>\n",
       "      <td>0.934911</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.862245</td>\n",
       "      <td>1.357489</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.947059</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>0.524064</td>\n",
       "      <td>0.887701</td>\n",
       "      <td>1.349806</td>\n",
       "      <td>0.474320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        76.0  0.094972  0.815789   0.960526           0.212121   \n",
       "1       148.0  0.094972  0.668919   0.952703           0.272727   \n",
       "2       180.0  0.094972  0.611111   0.916667           0.303030   \n",
       "3       169.0  0.094972  0.633136   0.934911           0.272727   \n",
       "4       170.0  0.094972  0.623529   0.947059           0.303030   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.787879            0.969697             0.107438   \n",
       "1          0.818182            0.939394             0.077778   \n",
       "2          0.727273            0.909091             0.070796   \n",
       "3          0.757576            0.909091             0.071429   \n",
       "4          0.787879            0.939394             0.096257   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.545455              0.884298  1.245467  0.580060  \n",
       "1            0.461111              0.827778  1.352093  0.513595  \n",
       "2            0.469027              0.858407  1.461635  0.407855  \n",
       "3            0.464286              0.862245  1.357489  0.483384  \n",
       "4            0.524064              0.887701  1.349806  0.474320  "
      ]
     },
     "execution_count": 1174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_150_w_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.081481</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.184601</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.745614</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.175836</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.953947</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>1.226775</td>\n",
       "      <td>0.474320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.950704</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.506250</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>1.246916</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.475862</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>1.179244</td>\n",
       "      <td>0.513595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       106.0  0.094972  0.754717   0.952830           0.272727   \n",
       "1       114.0  0.094972  0.745614   0.956140           0.303030   \n",
       "2       152.0  0.094972  0.651316   0.953947           0.303030   \n",
       "3       142.0  0.094972  0.676056   0.950704           0.333333   \n",
       "4       113.0  0.094972  0.734513   0.938053           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.787879            0.939394             0.081481   \n",
       "1          0.757576            0.878788             0.070312   \n",
       "2          0.757576            0.909091             0.084746   \n",
       "3          0.818182            0.939394             0.093750   \n",
       "4          0.787879            0.939394             0.110345   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.525926              0.851852  1.184601  0.483384  \n",
       "1            0.507812              0.843750  1.175836  0.480363  \n",
       "2            0.525424              0.847458  1.226775  0.474320  \n",
       "3            0.506250              0.862500  1.246916  0.459215  \n",
       "4            0.475862              0.882759  1.179244  0.513595  "
      ]
     },
     "execution_count": 1188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_150_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113,\n",
       " 0.13274336283185842,\n",
       " 0.7345132743362832,\n",
       " 0.9380530973451328,\n",
       " 0.24242424242424243,\n",
       " 0.7878787878787878,\n",
       " 0.9393939393939394,\n",
       " 0.47586206896551725,\n",
       " 0.1103448275862069,\n",
       " 0.8827586206896552)"
      ]
     },
     "execution_count": 1189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "model_metrics(predictions, y_cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 11,  6,  4,  1],\n",
       "       [ 3, 18, 21,  6,  1],\n",
       "       [ 1, 15, 84, 34,  4],\n",
       "       [ 0,  5, 27, 53,  7],\n",
       "       [ 0,  3,  6, 13,  7]])"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrP8H+9Y0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12754fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(zillow_model_2.history['loss'])\n",
    "plt.plot(zillow_model_2.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(130,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/1000\n",
      "2839/2839 [==============================] - 5s 2ms/step - loss: 1.6146 - acc: 0.3251 - val_loss: 1.4759 - val_acc: 0.3293\n",
      "Epoch 2/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4039 - acc: 0.3730 - val_loss: 1.4592 - val_acc: 0.3535\n",
      "Epoch 3/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3817 - acc: 0.3720 - val_loss: 1.4878 - val_acc: 0.3535\n",
      "Epoch 4/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.3932 - acc: 0.3529 - val_loss: 1.4750 - val_acc: 0.3505\n",
      "Epoch 5/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3626 - acc: 0.3688 - val_loss: 1.4523 - val_acc: 0.3323\n",
      "Epoch 6/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3616 - acc: 0.3614 - val_loss: 1.4692 - val_acc: 0.3323\n",
      "Epoch 7/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3562 - acc: 0.3656 - val_loss: 1.4888 - val_acc: 0.3323\n",
      "Epoch 8/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3320 - acc: 0.3794 - val_loss: 1.4582 - val_acc: 0.3323\n",
      "Epoch 9/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3463 - acc: 0.3815 - val_loss: 1.4573 - val_acc: 0.3323\n",
      "Epoch 10/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.3245 - acc: 0.3829 - val_loss: 1.4494 - val_acc: 0.3323\n",
      "Epoch 11/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3292 - acc: 0.3713 - val_loss: 1.4515 - val_acc: 0.3323\n",
      "Epoch 12/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.3234 - acc: 0.3755 - val_loss: 1.4307 - val_acc: 0.3323\n",
      "Epoch 13/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.3215 - acc: 0.3808 - val_loss: 1.4363 - val_acc: 0.3323\n",
      "Epoch 14/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.3162 - acc: 0.3878 - val_loss: 1.4145 - val_acc: 0.3384\n",
      "Epoch 15/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2990 - acc: 0.3871 - val_loss: 1.4237 - val_acc: 0.3323\n",
      "Epoch 16/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.3180 - acc: 0.3801 - val_loss: 1.3984 - val_acc: 0.3293\n",
      "Epoch 17/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3181 - acc: 0.3744 - val_loss: 1.4298 - val_acc: 0.3293\n",
      "Epoch 18/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2993 - acc: 0.3973 - val_loss: 1.4042 - val_acc: 0.3414\n",
      "Epoch 19/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3044 - acc: 0.3783 - val_loss: 1.4360 - val_acc: 0.3051\n",
      "Epoch 20/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3045 - acc: 0.3568 - val_loss: 1.4130 - val_acc: 0.3293\n",
      "Epoch 21/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2992 - acc: 0.3839 - val_loss: 1.3915 - val_acc: 0.3323\n",
      "Epoch 22/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2997 - acc: 0.3853 - val_loss: 1.4007 - val_acc: 0.3293\n",
      "Epoch 23/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2676 - acc: 0.4033 - val_loss: 1.4583 - val_acc: 0.3293\n",
      "Epoch 24/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2740 - acc: 0.3959 - val_loss: 1.3773 - val_acc: 0.3293\n",
      "Epoch 25/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2655 - acc: 0.4019 - val_loss: 1.4761 - val_acc: 0.3293\n",
      "Epoch 26/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2648 - acc: 0.3938 - val_loss: 1.4065 - val_acc: 0.3323\n",
      "Epoch 27/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2629 - acc: 0.4015 - val_loss: 1.4253 - val_acc: 0.3263\n",
      "Epoch 28/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2654 - acc: 0.3825 - val_loss: 1.4425 - val_acc: 0.3202\n",
      "Epoch 29/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2442 - acc: 0.3970 - val_loss: 1.4375 - val_acc: 0.3293\n",
      "Epoch 30/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2510 - acc: 0.3977 - val_loss: 1.3842 - val_acc: 0.3444\n",
      "Epoch 31/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2430 - acc: 0.4111 - val_loss: 1.4037 - val_acc: 0.3414\n",
      "Epoch 32/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2281 - acc: 0.4114 - val_loss: 1.3639 - val_acc: 0.3535\n",
      "Epoch 33/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2245 - acc: 0.4160 - val_loss: 1.3878 - val_acc: 0.3474\n",
      "Epoch 34/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2142 - acc: 0.4107 - val_loss: 1.3691 - val_acc: 0.3444\n",
      "Epoch 35/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2126 - acc: 0.4160 - val_loss: 1.4413 - val_acc: 0.3505\n",
      "Epoch 36/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2244 - acc: 0.4005 - val_loss: 1.4144 - val_acc: 0.3263\n",
      "Epoch 37/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2199 - acc: 0.4019 - val_loss: 1.3702 - val_acc: 0.3323\n",
      "Epoch 38/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2050 - acc: 0.4199 - val_loss: 1.3627 - val_acc: 0.3474\n",
      "Epoch 39/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.2066 - acc: 0.4100 - val_loss: 1.3711 - val_acc: 0.3414\n",
      "Epoch 40/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1984 - acc: 0.4174 - val_loss: 1.4355 - val_acc: 0.3172\n",
      "Epoch 41/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2049 - acc: 0.4111 - val_loss: 1.3900 - val_acc: 0.3444\n",
      "Epoch 42/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1906 - acc: 0.4086 - val_loss: 1.3889 - val_acc: 0.3293\n",
      "Epoch 43/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1906 - acc: 0.4209 - val_loss: 1.3354 - val_acc: 0.3505\n",
      "Epoch 44/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1867 - acc: 0.4297 - val_loss: 1.3664 - val_acc: 0.3625\n",
      "Epoch 45/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1797 - acc: 0.4174 - val_loss: 1.4137 - val_acc: 0.3112\n",
      "Epoch 46/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1825 - acc: 0.4283 - val_loss: 1.4627 - val_acc: 0.3263\n",
      "Epoch 47/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1878 - acc: 0.4174 - val_loss: 1.3735 - val_acc: 0.3414\n",
      "Epoch 48/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1693 - acc: 0.4192 - val_loss: 1.4061 - val_acc: 0.3202\n",
      "Epoch 49/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1675 - acc: 0.4132 - val_loss: 1.3539 - val_acc: 0.3686\n",
      "Epoch 50/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1582 - acc: 0.4322 - val_loss: 1.3457 - val_acc: 0.3535\n",
      "Epoch 51/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1618 - acc: 0.4340 - val_loss: 1.3120 - val_acc: 0.3686\n",
      "Epoch 52/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1528 - acc: 0.4255 - val_loss: 1.3387 - val_acc: 0.3625\n",
      "Epoch 53/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1562 - acc: 0.4294 - val_loss: 1.3455 - val_acc: 0.3414\n",
      "Epoch 54/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1471 - acc: 0.4333 - val_loss: 1.3627 - val_acc: 0.3625\n",
      "Epoch 55/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1606 - acc: 0.4213 - val_loss: 1.2551 - val_acc: 0.4502\n",
      "Epoch 56/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1539 - acc: 0.4477 - val_loss: 1.2649 - val_acc: 0.4230\n",
      "Epoch 57/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1467 - acc: 0.4399 - val_loss: 1.3231 - val_acc: 0.3595\n",
      "Epoch 58/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1419 - acc: 0.4350 - val_loss: 1.3564 - val_acc: 0.3384\n",
      "Epoch 59/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1426 - acc: 0.4301 - val_loss: 1.4476 - val_acc: 0.3112\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1434 - acc: 0.4174 - val_loss: 1.3567 - val_acc: 0.3625\n",
      "Epoch 61/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1196 - acc: 0.4477 - val_loss: 1.3434 - val_acc: 0.3505\n",
      "Epoch 62/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1252 - acc: 0.4406 - val_loss: 1.3087 - val_acc: 0.3625\n",
      "Epoch 63/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1172 - acc: 0.4477 - val_loss: 1.3571 - val_acc: 0.3565\n",
      "Epoch 64/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1252 - acc: 0.4347 - val_loss: 1.3387 - val_acc: 0.3414\n",
      "Epoch 65/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1044 - acc: 0.4396 - val_loss: 1.3362 - val_acc: 0.3505\n",
      "Epoch 66/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1102 - acc: 0.4480 - val_loss: 1.2925 - val_acc: 0.4320\n",
      "Epoch 67/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1240 - acc: 0.4438 - val_loss: 1.2620 - val_acc: 0.4350\n",
      "Epoch 68/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1114 - acc: 0.4417 - val_loss: 1.2473 - val_acc: 0.4622\n",
      "Epoch 69/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1066 - acc: 0.4727 - val_loss: 1.2935 - val_acc: 0.4411\n",
      "Epoch 70/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1020 - acc: 0.4642 - val_loss: 1.2810 - val_acc: 0.4169\n",
      "Epoch 71/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 1.0939 - acc: 0.4646 - val_loss: 1.2720 - val_acc: 0.3927\n",
      "Epoch 72/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0909 - acc: 0.4498 - val_loss: 1.2123 - val_acc: 0.4502\n",
      "Epoch 73/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1055 - acc: 0.4639 - val_loss: 1.2561 - val_acc: 0.4350\n",
      "Epoch 74/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1022 - acc: 0.4466 - val_loss: 1.2782 - val_acc: 0.4260\n",
      "Epoch 75/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0830 - acc: 0.4660 - val_loss: 1.3221 - val_acc: 0.3867\n",
      "Epoch 76/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0859 - acc: 0.4544 - val_loss: 1.3511 - val_acc: 0.3656\n",
      "Epoch 77/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0896 - acc: 0.4516 - val_loss: 1.3002 - val_acc: 0.4139\n",
      "Epoch 78/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0850 - acc: 0.4477 - val_loss: 1.2226 - val_acc: 0.4622\n",
      "Epoch 79/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0815 - acc: 0.4600 - val_loss: 1.2471 - val_acc: 0.4411\n",
      "Epoch 80/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0807 - acc: 0.4442 - val_loss: 1.1873 - val_acc: 0.4683\n",
      "Epoch 81/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0869 - acc: 0.4597 - val_loss: 1.2450 - val_acc: 0.4532\n",
      "Epoch 82/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0636 - acc: 0.4558 - val_loss: 1.2305 - val_acc: 0.4411\n",
      "Epoch 83/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0645 - acc: 0.4660 - val_loss: 1.2707 - val_acc: 0.4350\n",
      "Epoch 84/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0578 - acc: 0.4600 - val_loss: 1.3117 - val_acc: 0.3837\n",
      "Epoch 85/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0624 - acc: 0.4484 - val_loss: 1.3259 - val_acc: 0.3776\n",
      "Epoch 86/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0848 - acc: 0.4487 - val_loss: 1.3175 - val_acc: 0.3837\n",
      "Epoch 87/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0473 - acc: 0.4579 - val_loss: 1.2852 - val_acc: 0.4169\n",
      "Epoch 88/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0520 - acc: 0.4586 - val_loss: 1.2526 - val_acc: 0.4381\n",
      "Epoch 89/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0315 - acc: 0.4745 - val_loss: 1.2716 - val_acc: 0.4169\n",
      "Epoch 90/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0552 - acc: 0.4547 - val_loss: 1.3050 - val_acc: 0.4018\n",
      "Epoch 91/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0668 - acc: 0.4551 - val_loss: 1.2884 - val_acc: 0.3686\n",
      "Epoch 92/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0683 - acc: 0.4477 - val_loss: 1.2883 - val_acc: 0.4139\n",
      "Epoch 93/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0420 - acc: 0.4530 - val_loss: 1.2688 - val_acc: 0.4139\n",
      "Epoch 94/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0345 - acc: 0.4593 - val_loss: 1.2846 - val_acc: 0.4230\n",
      "Epoch 95/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0385 - acc: 0.4632 - val_loss: 1.2740 - val_acc: 0.4199\n",
      "Epoch 96/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0456 - acc: 0.4664 - val_loss: 1.2343 - val_acc: 0.4381\n",
      "Epoch 97/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0418 - acc: 0.4773 - val_loss: 1.2743 - val_acc: 0.4048\n",
      "Epoch 98/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0520 - acc: 0.4650 - val_loss: 1.2542 - val_acc: 0.4350\n",
      "Epoch 99/1000\n",
      "2839/2839 [==============================] - 0s 42us/step - loss: 1.0242 - acc: 0.4635 - val_loss: 1.2421 - val_acc: 0.4502\n",
      "Epoch 100/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.0196 - acc: 0.4635 - val_loss: 1.2586 - val_acc: 0.4290\n",
      "Epoch 101/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0287 - acc: 0.4790 - val_loss: 1.2386 - val_acc: 0.4683\n",
      "Epoch 102/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0081 - acc: 0.4805 - val_loss: 1.2556 - val_acc: 0.4350\n",
      "Epoch 103/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0159 - acc: 0.4776 - val_loss: 1.2445 - val_acc: 0.4471\n",
      "Epoch 104/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0161 - acc: 0.4738 - val_loss: 1.2159 - val_acc: 0.4502\n",
      "Epoch 105/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0052 - acc: 0.4678 - val_loss: 1.1746 - val_acc: 0.4562\n",
      "Epoch 106/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9995 - acc: 0.4819 - val_loss: 1.1820 - val_acc: 0.5166\n",
      "Epoch 107/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9797 - acc: 0.5002 - val_loss: 1.1868 - val_acc: 0.5347\n",
      "Epoch 108/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0042 - acc: 0.4893 - val_loss: 1.1633 - val_acc: 0.5438\n",
      "Epoch 109/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9995 - acc: 0.4822 - val_loss: 1.1023 - val_acc: 0.5740\n",
      "Epoch 110/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0690 - acc: 0.4773 - val_loss: 1.1573 - val_acc: 0.5378\n",
      "Epoch 111/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0073 - acc: 0.5041 - val_loss: 1.1743 - val_acc: 0.5619\n",
      "Epoch 112/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9985 - acc: 0.4843 - val_loss: 1.1494 - val_acc: 0.5559\n",
      "Epoch 113/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0076 - acc: 0.4970 - val_loss: 1.1690 - val_acc: 0.5498\n",
      "Epoch 114/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9885 - acc: 0.5097 - val_loss: 1.1985 - val_acc: 0.5650\n",
      "Epoch 115/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9826 - acc: 0.4959 - val_loss: 1.2312 - val_acc: 0.5650\n",
      "Epoch 116/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9990 - acc: 0.4935 - val_loss: 1.1903 - val_acc: 0.5861\n",
      "Epoch 117/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9774 - acc: 0.5146 - val_loss: 1.1458 - val_acc: 0.5680\n",
      "Epoch 118/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0085 - acc: 0.5016 - val_loss: 1.1307 - val_acc: 0.5740\n",
      "Epoch 119/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0072 - acc: 0.5076 - val_loss: 1.1273 - val_acc: 0.5710\n",
      "Epoch 120/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9842 - acc: 0.5002 - val_loss: 1.1674 - val_acc: 0.5650\n",
      "Epoch 121/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9590 - acc: 0.5037 - val_loss: 1.1780 - val_acc: 0.5589\n",
      "Epoch 122/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9567 - acc: 0.5065 - val_loss: 1.1523 - val_acc: 0.5589\n",
      "Epoch 123/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9834 - acc: 0.4819 - val_loss: 1.1470 - val_acc: 0.5438\n",
      "Epoch 124/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.9692 - acc: 0.4970 - val_loss: 1.1280 - val_acc: 0.5740\n",
      "Epoch 125/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9772 - acc: 0.5016 - val_loss: 1.1181 - val_acc: 0.5770\n",
      "Epoch 126/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9814 - acc: 0.5203 - val_loss: 1.1405 - val_acc: 0.5680\n",
      "Epoch 127/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9805 - acc: 0.5114 - val_loss: 1.1525 - val_acc: 0.5740\n",
      "Epoch 128/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9700 - acc: 0.5213 - val_loss: 1.1625 - val_acc: 0.5861\n",
      "Epoch 129/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9461 - acc: 0.5252 - val_loss: 1.1490 - val_acc: 0.5801\n",
      "Epoch 130/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9507 - acc: 0.5262 - val_loss: 1.1950 - val_acc: 0.5438\n",
      "Epoch 131/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9680 - acc: 0.5079 - val_loss: 1.2354 - val_acc: 0.5136\n",
      "Epoch 132/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9622 - acc: 0.4956 - val_loss: 1.2254 - val_acc: 0.5287\n",
      "Epoch 133/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9651 - acc: 0.4931 - val_loss: 1.2277 - val_acc: 0.5106\n",
      "Epoch 134/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9448 - acc: 0.5033 - val_loss: 1.1809 - val_acc: 0.5498\n",
      "Epoch 135/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9416 - acc: 0.5090 - val_loss: 1.1509 - val_acc: 0.5801\n",
      "Epoch 136/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9233 - acc: 0.5125 - val_loss: 1.0976 - val_acc: 0.5710\n",
      "Epoch 137/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9642 - acc: 0.5107 - val_loss: 1.1181 - val_acc: 0.5740\n",
      "Epoch 138/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9406 - acc: 0.5241 - val_loss: 1.1509 - val_acc: 0.5710\n",
      "Epoch 139/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9380 - acc: 0.5280 - val_loss: 1.1028 - val_acc: 0.5891\n",
      "Epoch 140/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9867 - acc: 0.5107 - val_loss: 1.1217 - val_acc: 0.5831\n",
      "Epoch 141/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9517 - acc: 0.5224 - val_loss: 1.1685 - val_acc: 0.5710\n",
      "Epoch 142/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9070 - acc: 0.5234 - val_loss: 1.2109 - val_acc: 0.5106\n",
      "Epoch 143/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9193 - acc: 0.5185 - val_loss: 1.2269 - val_acc: 0.4985\n",
      "Epoch 144/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9473 - acc: 0.5213 - val_loss: 1.2520 - val_acc: 0.5015\n",
      "Epoch 145/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9448 - acc: 0.4963 - val_loss: 1.1476 - val_acc: 0.5801\n",
      "Epoch 146/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9265 - acc: 0.5308 - val_loss: 1.1424 - val_acc: 0.5740\n",
      "Epoch 147/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9086 - acc: 0.5350 - val_loss: 1.1787 - val_acc: 0.5559\n",
      "Epoch 148/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9069 - acc: 0.5220 - val_loss: 1.2271 - val_acc: 0.5227\n",
      "Epoch 149/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9557 - acc: 0.5083 - val_loss: 1.2360 - val_acc: 0.5045\n",
      "Epoch 150/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9105 - acc: 0.5178 - val_loss: 1.1672 - val_acc: 0.5317\n",
      "Epoch 151/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9050 - acc: 0.5273 - val_loss: 1.1542 - val_acc: 0.5650\n",
      "Epoch 152/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9150 - acc: 0.5220 - val_loss: 1.1904 - val_acc: 0.5317\n",
      "Epoch 153/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9065 - acc: 0.5058 - val_loss: 1.1674 - val_acc: 0.5408\n",
      "Epoch 154/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.8905 - acc: 0.5305 - val_loss: 1.1158 - val_acc: 0.5891\n",
      "Epoch 155/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9117 - acc: 0.5301 - val_loss: 1.1161 - val_acc: 0.5801\n",
      "Epoch 156/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9220 - acc: 0.5428 - val_loss: 1.1441 - val_acc: 0.5650\n",
      "Epoch 157/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9114 - acc: 0.5400 - val_loss: 1.1486 - val_acc: 0.5408\n",
      "Epoch 158/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9284 - acc: 0.5389 - val_loss: 1.1984 - val_acc: 0.5136\n",
      "Epoch 159/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9043 - acc: 0.5431 - val_loss: 1.1402 - val_acc: 0.5619\n",
      "Epoch 160/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8821 - acc: 0.5470 - val_loss: 1.1390 - val_acc: 0.5559\n",
      "Epoch 161/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8862 - acc: 0.5347 - val_loss: 1.1418 - val_acc: 0.5589\n",
      "Epoch 162/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8867 - acc: 0.5396 - val_loss: 1.1121 - val_acc: 0.5619\n",
      "Epoch 163/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9359 - acc: 0.5527 - val_loss: 1.1420 - val_acc: 0.5559\n",
      "Epoch 164/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8783 - acc: 0.5537 - val_loss: 1.1238 - val_acc: 0.5740\n",
      "Epoch 165/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8728 - acc: 0.5548 - val_loss: 1.1225 - val_acc: 0.5861\n",
      "Epoch 166/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8769 - acc: 0.5470 - val_loss: 1.0986 - val_acc: 0.5861\n",
      "Epoch 167/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8975 - acc: 0.5417 - val_loss: 1.1224 - val_acc: 0.5619\n",
      "Epoch 168/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8911 - acc: 0.5467 - val_loss: 1.1443 - val_acc: 0.5317\n",
      "Epoch 169/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9048 - acc: 0.5449 - val_loss: 1.1881 - val_acc: 0.5166\n",
      "Epoch 170/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8875 - acc: 0.5347 - val_loss: 1.1674 - val_acc: 0.5287\n",
      "Epoch 171/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8388 - acc: 0.5597 - val_loss: 1.1335 - val_acc: 0.5438\n",
      "Epoch 172/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.8608 - acc: 0.5467 - val_loss: 1.1468 - val_acc: 0.5408\n",
      "Epoch 173/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8682 - acc: 0.5551 - val_loss: 1.1104 - val_acc: 0.5498\n",
      "Epoch 174/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9388 - acc: 0.5368 - val_loss: 1.1247 - val_acc: 0.5438\n",
      "Epoch 175/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8885 - acc: 0.5608 - val_loss: 1.1443 - val_acc: 0.5498\n",
      "Epoch 176/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8489 - acc: 0.5498 - val_loss: 1.1971 - val_acc: 0.5166\n",
      "Epoch 177/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.8368 - acc: 0.5597 - val_loss: 1.2343 - val_acc: 0.4924\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8581 - acc: 0.5523 - val_loss: 1.1922 - val_acc: 0.4985\n",
      "Epoch 179/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8660 - acc: 0.5608 - val_loss: 1.1334 - val_acc: 0.5438\n",
      "Epoch 180/1000\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.9014 - acc: 0.5544 - val_loss: 1.1159 - val_acc: 0.5589\n",
      "Epoch 181/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.8800 - acc: 0.5460 - val_loss: 1.1383 - val_acc: 0.5498\n",
      "Epoch 182/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.8430 - acc: 0.5706 - val_loss: 1.1845 - val_acc: 0.5378\n",
      "Epoch 183/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.8312 - acc: 0.5615 - val_loss: 1.2498 - val_acc: 0.4773\n",
      "Epoch 184/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8577 - acc: 0.5280 - val_loss: 1.1801 - val_acc: 0.5317\n",
      "Epoch 185/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8264 - acc: 0.5650 - val_loss: 1.2028 - val_acc: 0.4985\n",
      "Epoch 186/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8315 - acc: 0.5583 - val_loss: 1.1682 - val_acc: 0.5347\n",
      "Epoch 187/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8727 - acc: 0.5513 - val_loss: 1.1848 - val_acc: 0.5317\n",
      "Epoch 188/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8496 - acc: 0.5477 - val_loss: 1.2268 - val_acc: 0.4985\n",
      "Epoch 189/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8705 - acc: 0.5343 - val_loss: 1.2661 - val_acc: 0.4592\n",
      "Epoch 190/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8525 - acc: 0.5358 - val_loss: 1.2194 - val_acc: 0.4924\n",
      "Epoch 191/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8321 - acc: 0.5586 - val_loss: 1.2338 - val_acc: 0.4894\n",
      "Epoch 192/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8182 - acc: 0.5650 - val_loss: 1.2225 - val_acc: 0.5015\n",
      "Epoch 193/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8524 - acc: 0.5456 - val_loss: 1.2335 - val_acc: 0.4955\n",
      "Epoch 194/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8529 - acc: 0.5449 - val_loss: 1.3085 - val_acc: 0.4199\n",
      "Epoch 195/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8512 - acc: 0.5319 - val_loss: 1.2345 - val_acc: 0.4924\n",
      "Epoch 196/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8010 - acc: 0.5689 - val_loss: 1.1965 - val_acc: 0.5136\n",
      "Epoch 197/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8105 - acc: 0.5713 - val_loss: 1.2074 - val_acc: 0.4834\n",
      "Epoch 198/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8266 - acc: 0.5756 - val_loss: 1.1627 - val_acc: 0.5378\n",
      "Epoch 199/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8150 - acc: 0.5840 - val_loss: 1.1410 - val_acc: 0.5498\n",
      "Epoch 200/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8304 - acc: 0.5791 - val_loss: 1.1458 - val_acc: 0.5166\n",
      "Epoch 201/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8846 - acc: 0.5657 - val_loss: 1.1239 - val_acc: 0.5529\n",
      "Epoch 202/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8516 - acc: 0.5766 - val_loss: 1.1517 - val_acc: 0.5589\n",
      "Epoch 203/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8070 - acc: 0.5586 - val_loss: 1.1370 - val_acc: 0.5650\n",
      "Epoch 204/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8059 - acc: 0.5787 - val_loss: 1.1525 - val_acc: 0.5378\n",
      "Epoch 205/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7981 - acc: 0.5780 - val_loss: 1.1272 - val_acc: 0.5650\n",
      "Epoch 206/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8353 - acc: 0.5604 - val_loss: 1.1533 - val_acc: 0.4985\n",
      "Epoch 207/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8348 - acc: 0.5675 - val_loss: 1.1788 - val_acc: 0.4985\n",
      "Epoch 208/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8068 - acc: 0.5861 - val_loss: 1.1738 - val_acc: 0.5015\n",
      "Epoch 209/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.8050 - acc: 0.5953 - val_loss: 1.1461 - val_acc: 0.5468\n",
      "Epoch 210/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8160 - acc: 0.5773 - val_loss: 1.1837 - val_acc: 0.5287\n",
      "Epoch 211/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7688 - acc: 0.5756 - val_loss: 1.1584 - val_acc: 0.5196\n",
      "Epoch 212/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7888 - acc: 0.5801 - val_loss: 1.1691 - val_acc: 0.5076\n",
      "Epoch 213/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8013 - acc: 0.5918 - val_loss: 1.2122 - val_acc: 0.4743\n",
      "Epoch 214/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8224 - acc: 0.5780 - val_loss: 1.1998 - val_acc: 0.4864\n",
      "Epoch 215/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8121 - acc: 0.5812 - val_loss: 1.1792 - val_acc: 0.5106\n",
      "Epoch 216/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7903 - acc: 0.5763 - val_loss: 1.1429 - val_acc: 0.5559\n",
      "Epoch 217/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7931 - acc: 0.5749 - val_loss: 1.1324 - val_acc: 0.5589\n",
      "Epoch 218/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8011 - acc: 0.5738 - val_loss: 1.1810 - val_acc: 0.5529\n",
      "Epoch 219/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7778 - acc: 0.5805 - val_loss: 1.1480 - val_acc: 0.5710\n",
      "Epoch 220/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7815 - acc: 0.5696 - val_loss: 1.1609 - val_acc: 0.5408\n",
      "Epoch 221/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8122 - acc: 0.5812 - val_loss: 1.1626 - val_acc: 0.5136\n",
      "Epoch 222/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.8154 - acc: 0.5805 - val_loss: 1.1972 - val_acc: 0.4985\n",
      "Epoch 223/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7950 - acc: 0.5875 - val_loss: 1.1931 - val_acc: 0.5166\n",
      "Epoch 224/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7685 - acc: 0.5946 - val_loss: 1.1936 - val_acc: 0.5106\n",
      "Epoch 225/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7551 - acc: 0.5970 - val_loss: 1.2188 - val_acc: 0.4924\n",
      "Epoch 226/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7751 - acc: 0.5896 - val_loss: 1.3232 - val_acc: 0.3958\n",
      "Epoch 227/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.8171 - acc: 0.5805 - val_loss: 1.2269 - val_acc: 0.4864\n",
      "Epoch 228/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7469 - acc: 0.5893 - val_loss: 1.2985 - val_acc: 0.4411\n",
      "Epoch 229/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7844 - acc: 0.5773 - val_loss: 1.2630 - val_acc: 0.4834\n",
      "Epoch 230/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7953 - acc: 0.5749 - val_loss: 1.3141 - val_acc: 0.4532\n",
      "Epoch 231/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8216 - acc: 0.5421 - val_loss: 1.2493 - val_acc: 0.4924\n",
      "Epoch 232/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7642 - acc: 0.5889 - val_loss: 1.2174 - val_acc: 0.5166\n",
      "Epoch 233/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7586 - acc: 0.5791 - val_loss: 1.1721 - val_acc: 0.5680\n",
      "Epoch 234/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7796 - acc: 0.5847 - val_loss: 1.1549 - val_acc: 0.5559\n",
      "Epoch 235/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7716 - acc: 0.5844 - val_loss: 1.1667 - val_acc: 0.5498\n",
      "Epoch 236/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7952 - acc: 0.5822 - val_loss: 1.1734 - val_acc: 0.5529\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7549 - acc: 0.5970 - val_loss: 1.2186 - val_acc: 0.4924\n",
      "Epoch 238/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7460 - acc: 0.6006 - val_loss: 1.2622 - val_acc: 0.4562\n",
      "Epoch 239/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7772 - acc: 0.5903 - val_loss: 1.2153 - val_acc: 0.4743\n",
      "Epoch 240/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8066 - acc: 0.5879 - val_loss: 1.1779 - val_acc: 0.5438\n",
      "Epoch 241/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7605 - acc: 0.5992 - val_loss: 1.1643 - val_acc: 0.5589\n",
      "Epoch 242/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7626 - acc: 0.5949 - val_loss: 1.1857 - val_acc: 0.5468\n",
      "Epoch 243/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7487 - acc: 0.5953 - val_loss: 1.1862 - val_acc: 0.5408\n",
      "Epoch 244/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7546 - acc: 0.5995 - val_loss: 1.2159 - val_acc: 0.4894\n",
      "Epoch 245/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7267 - acc: 0.6111 - val_loss: 1.2299 - val_acc: 0.4955\n",
      "Epoch 246/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7501 - acc: 0.6097 - val_loss: 1.2445 - val_acc: 0.4532\n",
      "Epoch 247/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7715 - acc: 0.6087 - val_loss: 1.2093 - val_acc: 0.4985\n",
      "Epoch 248/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7611 - acc: 0.5995 - val_loss: 1.2234 - val_acc: 0.4773\n",
      "Epoch 249/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7854 - acc: 0.5865 - val_loss: 1.1695 - val_acc: 0.5619\n",
      "Epoch 250/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7833 - acc: 0.5911 - val_loss: 1.1668 - val_acc: 0.5801\n",
      "Epoch 251/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7470 - acc: 0.6041 - val_loss: 1.1997 - val_acc: 0.5408\n",
      "Epoch 252/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7205 - acc: 0.6055 - val_loss: 1.1894 - val_acc: 0.5378\n",
      "Epoch 253/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7196 - acc: 0.6185 - val_loss: 1.2071 - val_acc: 0.5287\n",
      "Epoch 254/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7260 - acc: 0.6132 - val_loss: 1.2145 - val_acc: 0.4924\n",
      "Epoch 255/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7745 - acc: 0.5970 - val_loss: 1.1965 - val_acc: 0.5045\n",
      "Epoch 256/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7718 - acc: 0.6087 - val_loss: 1.2148 - val_acc: 0.5347\n",
      "Epoch 257/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7284 - acc: 0.6080 - val_loss: 1.2019 - val_acc: 0.5378\n",
      "Epoch 258/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7142 - acc: 0.6164 - val_loss: 1.1953 - val_acc: 0.5529\n",
      "Epoch 259/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7287 - acc: 0.6125 - val_loss: 1.1901 - val_acc: 0.5559\n",
      "Epoch 260/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7307 - acc: 0.6013 - val_loss: 1.2057 - val_acc: 0.5317\n",
      "Epoch 261/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7352 - acc: 0.5833 - val_loss: 1.2634 - val_acc: 0.4834\n",
      "Epoch 262/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7520 - acc: 0.5875 - val_loss: 1.3136 - val_acc: 0.4471\n",
      "Epoch 263/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7582 - acc: 0.5903 - val_loss: 1.3491 - val_acc: 0.4109\n",
      "Epoch 264/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.7358 - acc: 0.5900 - val_loss: 1.2288 - val_acc: 0.5015\n",
      "Epoch 265/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7203 - acc: 0.6030 - val_loss: 1.2193 - val_acc: 0.5378\n",
      "Epoch 266/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7124 - acc: 0.6044 - val_loss: 1.1959 - val_acc: 0.5529\n",
      "Epoch 267/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7178 - acc: 0.6034 - val_loss: 1.2023 - val_acc: 0.5559\n",
      "Epoch 268/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6981 - acc: 0.6192 - val_loss: 1.2257 - val_acc: 0.5378\n",
      "Epoch 269/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6986 - acc: 0.6298 - val_loss: 1.2238 - val_acc: 0.5015\n",
      "Epoch 270/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.8001 - acc: 0.5974 - val_loss: 1.2254 - val_acc: 0.4804\n",
      "Epoch 271/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7568 - acc: 0.6118 - val_loss: 1.2803 - val_acc: 0.4532\n",
      "Epoch 272/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7104 - acc: 0.6221 - val_loss: 1.2744 - val_acc: 0.4441\n",
      "Epoch 273/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7001 - acc: 0.6284 - val_loss: 1.2986 - val_acc: 0.4471\n",
      "Epoch 274/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6879 - acc: 0.6263 - val_loss: 1.3455 - val_acc: 0.4260\n",
      "Epoch 275/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7205 - acc: 0.5992 - val_loss: 1.4283 - val_acc: 0.3474\n",
      "Epoch 276/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7403 - acc: 0.5932 - val_loss: 1.3212 - val_acc: 0.4139\n",
      "Epoch 277/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7085 - acc: 0.6129 - val_loss: 1.3305 - val_acc: 0.4290\n",
      "Epoch 278/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7069 - acc: 0.6139 - val_loss: 1.3939 - val_acc: 0.3988\n",
      "Epoch 279/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7982 - acc: 0.5685 - val_loss: 1.3676 - val_acc: 0.4139\n",
      "Epoch 280/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7679 - acc: 0.5773 - val_loss: 1.2620 - val_acc: 0.4894\n",
      "Epoch 281/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7109 - acc: 0.6097 - val_loss: 1.2660 - val_acc: 0.4894\n",
      "Epoch 282/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7083 - acc: 0.6080 - val_loss: 1.2744 - val_acc: 0.4653\n",
      "Epoch 283/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6952 - acc: 0.5985 - val_loss: 1.2593 - val_acc: 0.4713\n",
      "Epoch 284/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.6991 - acc: 0.6037 - val_loss: 1.2427 - val_acc: 0.5287\n",
      "Epoch 285/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6763 - acc: 0.6242 - val_loss: 1.2310 - val_acc: 0.5408\n",
      "Epoch 286/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6776 - acc: 0.6175 - val_loss: 1.2179 - val_acc: 0.5498\n",
      "Epoch 287/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7235 - acc: 0.6111 - val_loss: 1.2326 - val_acc: 0.5166\n",
      "Epoch 288/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7483 - acc: 0.6178 - val_loss: 1.2614 - val_acc: 0.4804\n",
      "Epoch 289/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7208 - acc: 0.6270 - val_loss: 1.2733 - val_acc: 0.4894\n",
      "Epoch 290/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6577 - acc: 0.6319 - val_loss: 1.2580 - val_acc: 0.5257\n",
      "Epoch 291/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6804 - acc: 0.6252 - val_loss: 1.2596 - val_acc: 0.5045\n",
      "Epoch 292/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7081 - acc: 0.6175 - val_loss: 1.2944 - val_acc: 0.4653\n",
      "Epoch 293/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7202 - acc: 0.6199 - val_loss: 1.3148 - val_acc: 0.4350\n",
      "Epoch 294/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6775 - acc: 0.6432 - val_loss: 1.3755 - val_acc: 0.3988\n",
      "Epoch 295/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6931 - acc: 0.6168 - val_loss: 1.3427 - val_acc: 0.4199\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6779 - acc: 0.6333 - val_loss: 1.3068 - val_acc: 0.4471\n",
      "Epoch 297/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6922 - acc: 0.6164 - val_loss: 1.2524 - val_acc: 0.5438\n",
      "Epoch 298/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7540 - acc: 0.6157 - val_loss: 1.2515 - val_acc: 0.5166\n",
      "Epoch 299/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.7005 - acc: 0.6309 - val_loss: 1.2825 - val_acc: 0.5045\n",
      "Epoch 300/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6422 - acc: 0.6393 - val_loss: 1.2678 - val_acc: 0.5227\n",
      "Epoch 301/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6570 - acc: 0.6323 - val_loss: 1.2778 - val_acc: 0.5166\n",
      "Epoch 302/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.6695 - acc: 0.6330 - val_loss: 1.2575 - val_acc: 0.5166\n",
      "Epoch 303/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6798 - acc: 0.6270 - val_loss: 1.2329 - val_acc: 0.5619\n",
      "Epoch 304/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7463 - acc: 0.5946 - val_loss: 1.2472 - val_acc: 0.5468\n",
      "Epoch 305/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7209 - acc: 0.6270 - val_loss: 1.3491 - val_acc: 0.4109\n",
      "Epoch 306/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7037 - acc: 0.6386 - val_loss: 1.3962 - val_acc: 0.4018\n",
      "Epoch 307/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6800 - acc: 0.6340 - val_loss: 1.3301 - val_acc: 0.4471\n",
      "Epoch 308/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6423 - acc: 0.6456 - val_loss: 1.3453 - val_acc: 0.4260\n",
      "Epoch 309/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6556 - acc: 0.6541 - val_loss: 1.3047 - val_acc: 0.4834\n",
      "Epoch 310/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6542 - acc: 0.6485 - val_loss: 1.2874 - val_acc: 0.5045\n",
      "Epoch 311/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7307 - acc: 0.6273 - val_loss: 1.2674 - val_acc: 0.5196\n",
      "Epoch 312/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6930 - acc: 0.6221 - val_loss: 1.2703 - val_acc: 0.5347\n",
      "Epoch 313/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6559 - acc: 0.6471 - val_loss: 1.2729 - val_acc: 0.5347\n",
      "Epoch 314/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6400 - acc: 0.6375 - val_loss: 1.2742 - val_acc: 0.5378\n",
      "Epoch 315/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6769 - acc: 0.6326 - val_loss: 1.2777 - val_acc: 0.5378\n",
      "Epoch 316/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7208 - acc: 0.6185 - val_loss: 1.3130 - val_acc: 0.4713\n",
      "Epoch 317/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6703 - acc: 0.6538 - val_loss: 1.3739 - val_acc: 0.4290\n",
      "Epoch 318/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6726 - acc: 0.6499 - val_loss: 1.4118 - val_acc: 0.3958\n",
      "Epoch 319/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.6889 - acc: 0.6428 - val_loss: 1.3605 - val_acc: 0.4139\n",
      "Epoch 320/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.6281 - acc: 0.6513 - val_loss: 1.3050 - val_acc: 0.5015\n",
      "Epoch 321/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6450 - acc: 0.6418 - val_loss: 1.2919 - val_acc: 0.5227\n",
      "Epoch 322/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6756 - acc: 0.6323 - val_loss: 1.2924 - val_acc: 0.5076\n",
      "Epoch 323/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7293 - acc: 0.6252 - val_loss: 1.3017 - val_acc: 0.4834\n",
      "Epoch 324/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6763 - acc: 0.6495 - val_loss: 1.3032 - val_acc: 0.4894\n",
      "Epoch 325/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6469 - acc: 0.6337 - val_loss: 1.3190 - val_acc: 0.5045\n",
      "Epoch 326/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6343 - acc: 0.6513 - val_loss: 1.3134 - val_acc: 0.5136\n",
      "Epoch 327/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6525 - acc: 0.6340 - val_loss: 1.2708 - val_acc: 0.5529\n",
      "Epoch 328/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6497 - acc: 0.6442 - val_loss: 1.2959 - val_acc: 0.5257\n",
      "Epoch 329/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6383 - acc: 0.6513 - val_loss: 1.3110 - val_acc: 0.4894\n",
      "Epoch 330/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6521 - acc: 0.6319 - val_loss: 1.3078 - val_acc: 0.4924\n",
      "Epoch 331/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6400 - acc: 0.6319 - val_loss: 1.3471 - val_acc: 0.4804\n",
      "Epoch 332/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6444 - acc: 0.6453 - val_loss: 1.4979 - val_acc: 0.3837\n",
      "Epoch 333/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.7237 - acc: 0.5875 - val_loss: 1.4495 - val_acc: 0.4079\n",
      "Epoch 334/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6511 - acc: 0.6316 - val_loss: 1.3538 - val_acc: 0.4622\n",
      "Epoch 335/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6148 - acc: 0.6516 - val_loss: 1.3303 - val_acc: 0.4924\n",
      "Epoch 336/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5997 - acc: 0.6626 - val_loss: 1.3542 - val_acc: 0.4924\n",
      "Epoch 337/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6376 - acc: 0.6520 - val_loss: 1.3254 - val_acc: 0.4924\n",
      "Epoch 338/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6669 - acc: 0.6305 - val_loss: 1.3505 - val_acc: 0.4804\n",
      "Epoch 339/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.6964 - acc: 0.6157 - val_loss: 1.3763 - val_acc: 0.4381\n",
      "Epoch 340/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6421 - acc: 0.6256 - val_loss: 1.4209 - val_acc: 0.4199\n",
      "Epoch 341/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6427 - acc: 0.6287 - val_loss: 1.5100 - val_acc: 0.3595\n",
      "Epoch 342/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6618 - acc: 0.6337 - val_loss: 1.3832 - val_acc: 0.4592\n",
      "Epoch 343/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6348 - acc: 0.6464 - val_loss: 1.4248 - val_acc: 0.4411\n",
      "Epoch 344/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6154 - acc: 0.6587 - val_loss: 1.4469 - val_acc: 0.4139\n",
      "Epoch 345/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6714 - acc: 0.6326 - val_loss: 1.4122 - val_acc: 0.4350\n",
      "Epoch 346/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.6335 - acc: 0.6428 - val_loss: 1.3666 - val_acc: 0.4773\n",
      "Epoch 347/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6176 - acc: 0.6467 - val_loss: 1.3570 - val_acc: 0.4834\n",
      "Epoch 348/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6175 - acc: 0.6428 - val_loss: 1.4180 - val_acc: 0.4592\n",
      "Epoch 349/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6443 - acc: 0.6460 - val_loss: 1.4661 - val_acc: 0.4079\n",
      "Epoch 350/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6592 - acc: 0.6161 - val_loss: 1.3597 - val_acc: 0.4804\n",
      "Epoch 351/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6240 - acc: 0.6460 - val_loss: 1.3108 - val_acc: 0.5166\n",
      "Epoch 352/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6235 - acc: 0.6513 - val_loss: 1.2875 - val_acc: 0.5468\n",
      "Epoch 353/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6299 - acc: 0.6386 - val_loss: 1.2973 - val_acc: 0.5317\n",
      "Epoch 354/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6502 - acc: 0.6375 - val_loss: 1.3047 - val_acc: 0.5227\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6054 - acc: 0.6576 - val_loss: 1.3524 - val_acc: 0.4985\n",
      "Epoch 356/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6131 - acc: 0.6615 - val_loss: 1.3875 - val_acc: 0.4592\n",
      "Epoch 357/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6850 - acc: 0.6467 - val_loss: 1.3602 - val_acc: 0.4773\n",
      "Epoch 358/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6203 - acc: 0.6626 - val_loss: 1.3408 - val_acc: 0.5015\n",
      "Epoch 359/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5910 - acc: 0.6735 - val_loss: 1.3500 - val_acc: 0.4834\n",
      "Epoch 360/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5842 - acc: 0.6756 - val_loss: 1.3325 - val_acc: 0.5136\n",
      "Epoch 361/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5977 - acc: 0.6622 - val_loss: 1.3584 - val_acc: 0.5076\n",
      "Epoch 362/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6160 - acc: 0.6629 - val_loss: 1.3485 - val_acc: 0.5287\n",
      "Epoch 363/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6370 - acc: 0.6471 - val_loss: 1.3399 - val_acc: 0.5196\n",
      "Epoch 364/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.6068 - acc: 0.6516 - val_loss: 1.3435 - val_acc: 0.5227\n",
      "Epoch 365/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.6141 - acc: 0.6478 - val_loss: 1.3238 - val_acc: 0.5408\n",
      "Epoch 366/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6903 - acc: 0.6185 - val_loss: 1.3777 - val_acc: 0.5196\n",
      "Epoch 367/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.7491 - acc: 0.6383 - val_loss: 1.3509 - val_acc: 0.4713\n",
      "Epoch 368/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5944 - acc: 0.6784 - val_loss: 1.3775 - val_acc: 0.4653\n",
      "Epoch 369/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5717 - acc: 0.6759 - val_loss: 1.3859 - val_acc: 0.4713\n",
      "Epoch 370/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5771 - acc: 0.6650 - val_loss: 1.3889 - val_acc: 0.4804\n",
      "Epoch 371/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5664 - acc: 0.6671 - val_loss: 1.4022 - val_acc: 0.4804\n",
      "Epoch 372/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5611 - acc: 0.6805 - val_loss: 1.3858 - val_acc: 0.4804\n",
      "Epoch 373/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6067 - acc: 0.6611 - val_loss: 1.4182 - val_acc: 0.4955\n",
      "Epoch 374/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.7486 - acc: 0.6368 - val_loss: 1.3641 - val_acc: 0.4683\n",
      "Epoch 375/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6198 - acc: 0.6749 - val_loss: 1.4141 - val_acc: 0.4562\n",
      "Epoch 376/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5888 - acc: 0.6728 - val_loss: 1.4261 - val_acc: 0.4532\n",
      "Epoch 377/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5671 - acc: 0.6876 - val_loss: 1.3711 - val_acc: 0.4924\n",
      "Epoch 378/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5901 - acc: 0.6587 - val_loss: 1.3497 - val_acc: 0.5317\n",
      "Epoch 379/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.6096 - acc: 0.6559 - val_loss: 1.3253 - val_acc: 0.5559\n",
      "Epoch 380/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6337 - acc: 0.6488 - val_loss: 1.3553 - val_acc: 0.5136\n",
      "Epoch 381/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5880 - acc: 0.6844 - val_loss: 1.4061 - val_acc: 0.4743\n",
      "Epoch 382/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6286 - acc: 0.6696 - val_loss: 1.4367 - val_acc: 0.4622\n",
      "Epoch 383/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.6583 - acc: 0.6608 - val_loss: 1.3689 - val_acc: 0.4713\n",
      "Epoch 384/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.5669 - acc: 0.6886 - val_loss: 1.3935 - val_acc: 0.4653\n",
      "Epoch 385/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.5601 - acc: 0.6826 - val_loss: 1.3868 - val_acc: 0.4834\n",
      "Epoch 386/1000\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.5793 - acc: 0.6710 - val_loss: 1.3591 - val_acc: 0.5227\n",
      "Epoch 387/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.6475 - acc: 0.6471 - val_loss: 1.3393 - val_acc: 0.5498\n",
      "Epoch 388/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6385 - acc: 0.6562 - val_loss: 1.3684 - val_acc: 0.5136\n",
      "Epoch 389/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5934 - acc: 0.6770 - val_loss: 1.4022 - val_acc: 0.4804\n",
      "Epoch 390/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6006 - acc: 0.6696 - val_loss: 1.4033 - val_acc: 0.4834\n",
      "Epoch 391/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5736 - acc: 0.6904 - val_loss: 1.4354 - val_acc: 0.4622\n",
      "Epoch 392/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5707 - acc: 0.6872 - val_loss: 1.4834 - val_acc: 0.4290\n",
      "Epoch 393/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6021 - acc: 0.6851 - val_loss: 1.4568 - val_acc: 0.4260\n",
      "Epoch 394/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6249 - acc: 0.6710 - val_loss: 1.4469 - val_acc: 0.4562\n",
      "Epoch 395/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5924 - acc: 0.6826 - val_loss: 1.4779 - val_acc: 0.4260\n",
      "Epoch 396/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5656 - acc: 0.6950 - val_loss: 1.4819 - val_acc: 0.4381\n",
      "Epoch 397/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5718 - acc: 0.6784 - val_loss: 1.5148 - val_acc: 0.4079\n",
      "Epoch 398/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6073 - acc: 0.6689 - val_loss: 1.5679 - val_acc: 0.3716\n",
      "Epoch 399/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6240 - acc: 0.6534 - val_loss: 1.4917 - val_acc: 0.4169\n",
      "Epoch 400/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6213 - acc: 0.6471 - val_loss: 1.3942 - val_acc: 0.4924\n",
      "Epoch 401/1000\n",
      "2839/2839 [==============================] - 0s 43us/step - loss: 0.5756 - acc: 0.6675 - val_loss: 1.3883 - val_acc: 0.5015\n",
      "Epoch 402/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.5740 - acc: 0.6774 - val_loss: 1.4142 - val_acc: 0.4683\n",
      "Epoch 403/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5445 - acc: 0.6904 - val_loss: 1.4830 - val_acc: 0.4350\n",
      "Epoch 404/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5724 - acc: 0.6752 - val_loss: 1.6074 - val_acc: 0.3958\n",
      "Epoch 405/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6705 - acc: 0.6383 - val_loss: 1.5633 - val_acc: 0.3776\n",
      "Epoch 406/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6136 - acc: 0.6446 - val_loss: 1.4613 - val_acc: 0.4230\n",
      "Epoch 407/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5519 - acc: 0.6897 - val_loss: 1.4741 - val_acc: 0.4381\n",
      "Epoch 408/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5509 - acc: 0.6890 - val_loss: 1.4524 - val_acc: 0.4562\n",
      "Epoch 409/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5346 - acc: 0.7038 - val_loss: 1.5048 - val_acc: 0.4532\n",
      "Epoch 410/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5409 - acc: 0.6964 - val_loss: 1.4998 - val_acc: 0.4381\n",
      "Epoch 411/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5931 - acc: 0.6664 - val_loss: 1.5317 - val_acc: 0.3867\n",
      "Epoch 412/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6022 - acc: 0.6580 - val_loss: 1.4495 - val_acc: 0.4471\n",
      "Epoch 413/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5642 - acc: 0.6742 - val_loss: 1.4346 - val_acc: 0.4955\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5864 - acc: 0.6728 - val_loss: 1.4192 - val_acc: 0.5166\n",
      "Epoch 415/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5925 - acc: 0.6604 - val_loss: 1.4075 - val_acc: 0.5166\n",
      "Epoch 416/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5680 - acc: 0.6742 - val_loss: 1.4051 - val_acc: 0.4985\n",
      "Epoch 417/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5385 - acc: 0.6974 - val_loss: 1.4271 - val_acc: 0.5136\n",
      "Epoch 418/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5695 - acc: 0.6833 - val_loss: 1.4629 - val_acc: 0.4924\n",
      "Epoch 419/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6408 - acc: 0.6664 - val_loss: 1.4902 - val_acc: 0.4773\n",
      "Epoch 420/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6436 - acc: 0.6816 - val_loss: 1.4800 - val_acc: 0.4350\n",
      "Epoch 421/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5505 - acc: 0.7066 - val_loss: 1.4845 - val_acc: 0.4320\n",
      "Epoch 422/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5251 - acc: 0.7069 - val_loss: 1.5301 - val_acc: 0.4471\n",
      "Epoch 423/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5395 - acc: 0.6957 - val_loss: 1.5209 - val_acc: 0.4260\n",
      "Epoch 424/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5341 - acc: 0.7002 - val_loss: 1.5307 - val_acc: 0.4471\n",
      "Epoch 425/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5538 - acc: 0.6904 - val_loss: 1.5845 - val_acc: 0.4079\n",
      "Epoch 426/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5804 - acc: 0.6925 - val_loss: 1.5492 - val_acc: 0.4260\n",
      "Epoch 427/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6085 - acc: 0.6851 - val_loss: 1.4763 - val_acc: 0.4622\n",
      "Epoch 428/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5684 - acc: 0.6967 - val_loss: 1.4547 - val_acc: 0.5106\n",
      "Epoch 429/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5948 - acc: 0.6650 - val_loss: 1.4280 - val_acc: 0.5227\n",
      "Epoch 430/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5750 - acc: 0.6795 - val_loss: 1.4368 - val_acc: 0.4985\n",
      "Epoch 431/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5421 - acc: 0.6936 - val_loss: 1.4521 - val_acc: 0.4924\n",
      "Epoch 432/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5285 - acc: 0.7101 - val_loss: 1.4467 - val_acc: 0.5106\n",
      "Epoch 433/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5408 - acc: 0.6914 - val_loss: 1.4683 - val_acc: 0.5106\n",
      "Epoch 434/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5561 - acc: 0.6928 - val_loss: 1.4288 - val_acc: 0.5287\n",
      "Epoch 435/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6078 - acc: 0.6763 - val_loss: 1.4553 - val_acc: 0.4955\n",
      "Epoch 436/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5708 - acc: 0.6918 - val_loss: 1.5306 - val_acc: 0.4562\n",
      "Epoch 437/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5723 - acc: 0.6960 - val_loss: 1.5920 - val_acc: 0.4079\n",
      "Epoch 438/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5564 - acc: 0.7013 - val_loss: 1.5277 - val_acc: 0.4169\n",
      "Epoch 439/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.5224 - acc: 0.7108 - val_loss: 1.5177 - val_acc: 0.4471\n",
      "Epoch 440/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5360 - acc: 0.6960 - val_loss: 1.5444 - val_acc: 0.4562\n",
      "Epoch 441/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5288 - acc: 0.7119 - val_loss: 1.5075 - val_acc: 0.4804\n",
      "Epoch 442/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5844 - acc: 0.6907 - val_loss: 1.4729 - val_acc: 0.5136\n",
      "Epoch 443/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5394 - acc: 0.7175 - val_loss: 1.5209 - val_acc: 0.4864\n",
      "Epoch 444/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5658 - acc: 0.6921 - val_loss: 1.5450 - val_acc: 0.4411\n",
      "Epoch 445/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5461 - acc: 0.7129 - val_loss: 1.5590 - val_acc: 0.4320\n",
      "Epoch 446/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5269 - acc: 0.6985 - val_loss: 1.5815 - val_acc: 0.4079\n",
      "Epoch 447/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5278 - acc: 0.7186 - val_loss: 1.6069 - val_acc: 0.4199\n",
      "Epoch 448/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5592 - acc: 0.6950 - val_loss: 1.6756 - val_acc: 0.3686\n",
      "Epoch 449/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5804 - acc: 0.6770 - val_loss: 1.6437 - val_acc: 0.3716\n",
      "Epoch 450/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5609 - acc: 0.6847 - val_loss: 1.6882 - val_acc: 0.3565\n",
      "Epoch 451/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.6387 - acc: 0.6555 - val_loss: 1.6226 - val_acc: 0.3897\n",
      "Epoch 452/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5808 - acc: 0.6781 - val_loss: 1.5104 - val_acc: 0.4502\n",
      "Epoch 453/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5019 - acc: 0.7017 - val_loss: 1.5438 - val_acc: 0.4441\n",
      "Epoch 454/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5023 - acc: 0.7288 - val_loss: 1.5509 - val_acc: 0.4592\n",
      "Epoch 455/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5272 - acc: 0.7221 - val_loss: 1.5925 - val_acc: 0.4532\n",
      "Epoch 456/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5616 - acc: 0.7161 - val_loss: 1.5369 - val_acc: 0.4653\n",
      "Epoch 457/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5249 - acc: 0.7136 - val_loss: 1.5553 - val_acc: 0.4350\n",
      "Epoch 458/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5113 - acc: 0.7224 - val_loss: 1.5435 - val_acc: 0.4622\n",
      "Epoch 459/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5055 - acc: 0.7091 - val_loss: 1.5215 - val_acc: 0.4955\n",
      "Epoch 460/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5437 - acc: 0.6932 - val_loss: 1.5330 - val_acc: 0.5076\n",
      "Epoch 461/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5883 - acc: 0.6928 - val_loss: 1.5000 - val_acc: 0.4955\n",
      "Epoch 462/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.5103 - acc: 0.7168 - val_loss: 1.5415 - val_acc: 0.4834\n",
      "Epoch 463/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4964 - acc: 0.7200 - val_loss: 1.5465 - val_acc: 0.4985\n",
      "Epoch 464/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5252 - acc: 0.7024 - val_loss: 1.5962 - val_acc: 0.4471\n",
      "Epoch 465/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5114 - acc: 0.6914 - val_loss: 1.5304 - val_acc: 0.5076\n",
      "Epoch 466/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5275 - acc: 0.6936 - val_loss: 1.5535 - val_acc: 0.5106\n",
      "Epoch 467/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5271 - acc: 0.6855 - val_loss: 1.5573 - val_acc: 0.4864\n",
      "Epoch 468/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5469 - acc: 0.6766 - val_loss: 1.6501 - val_acc: 0.3927\n",
      "Epoch 469/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5719 - acc: 0.6724 - val_loss: 1.6828 - val_acc: 0.3686\n",
      "Epoch 470/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5284 - acc: 0.6946 - val_loss: 1.6048 - val_acc: 0.4079\n",
      "Epoch 471/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4994 - acc: 0.7246 - val_loss: 1.5975 - val_acc: 0.4381\n",
      "Epoch 472/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4986 - acc: 0.7154 - val_loss: 1.5929 - val_acc: 0.4532\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4763 - acc: 0.7281 - val_loss: 1.6324 - val_acc: 0.4320\n",
      "Epoch 474/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4798 - acc: 0.7224 - val_loss: 1.5717 - val_acc: 0.4834\n",
      "Epoch 475/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5186 - acc: 0.7010 - val_loss: 1.5448 - val_acc: 0.5468\n",
      "Epoch 476/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5895 - acc: 0.6766 - val_loss: 1.4863 - val_acc: 0.5287\n",
      "Epoch 477/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5215 - acc: 0.7038 - val_loss: 1.5639 - val_acc: 0.4592\n",
      "Epoch 478/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5150 - acc: 0.7055 - val_loss: 1.6006 - val_acc: 0.4622\n",
      "Epoch 479/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.6084 - acc: 0.6847 - val_loss: 1.5969 - val_acc: 0.4622\n",
      "Epoch 480/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5697 - acc: 0.7087 - val_loss: 1.5683 - val_acc: 0.4653\n",
      "Epoch 481/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4919 - acc: 0.7253 - val_loss: 1.6056 - val_acc: 0.4502\n",
      "Epoch 482/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4931 - acc: 0.7172 - val_loss: 1.6780 - val_acc: 0.3958\n",
      "Epoch 483/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4967 - acc: 0.7168 - val_loss: 1.6827 - val_acc: 0.3746\n",
      "Epoch 484/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5331 - acc: 0.7027 - val_loss: 1.6658 - val_acc: 0.4079\n",
      "Epoch 485/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5131 - acc: 0.7066 - val_loss: 1.6497 - val_acc: 0.4109\n",
      "Epoch 486/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4973 - acc: 0.7189 - val_loss: 1.7257 - val_acc: 0.3867\n",
      "Epoch 487/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5238 - acc: 0.7115 - val_loss: 1.7220 - val_acc: 0.3988\n",
      "Epoch 488/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5444 - acc: 0.7175 - val_loss: 1.6253 - val_acc: 0.4471\n",
      "Epoch 489/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5137 - acc: 0.7291 - val_loss: 1.6041 - val_acc: 0.4653\n",
      "Epoch 490/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5599 - acc: 0.7143 - val_loss: 1.5830 - val_acc: 0.5166\n",
      "Epoch 491/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5520 - acc: 0.7038 - val_loss: 1.5500 - val_acc: 0.4864\n",
      "Epoch 492/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4900 - acc: 0.7281 - val_loss: 1.6073 - val_acc: 0.4773\n",
      "Epoch 493/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5003 - acc: 0.7348 - val_loss: 1.6177 - val_acc: 0.4834\n",
      "Epoch 494/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4838 - acc: 0.7231 - val_loss: 1.6565 - val_acc: 0.4320\n",
      "Epoch 495/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.4993 - acc: 0.7246 - val_loss: 1.7587 - val_acc: 0.4048\n",
      "Epoch 496/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5420 - acc: 0.7087 - val_loss: 1.6643 - val_acc: 0.4260\n",
      "Epoch 497/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5216 - acc: 0.7316 - val_loss: 1.6101 - val_acc: 0.4532\n",
      "Epoch 498/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4801 - acc: 0.7302 - val_loss: 1.6471 - val_acc: 0.4653\n",
      "Epoch 499/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5188 - acc: 0.7076 - val_loss: 1.6180 - val_acc: 0.5106\n",
      "Epoch 500/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5793 - acc: 0.6904 - val_loss: 1.5427 - val_acc: 0.5408\n",
      "Epoch 501/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5408 - acc: 0.6992 - val_loss: 1.5569 - val_acc: 0.5166\n",
      "Epoch 502/1000\n",
      "2839/2839 [==============================] - 0s 27us/step - loss: 0.5001 - acc: 0.7126 - val_loss: 1.5422 - val_acc: 0.5136\n",
      "Epoch 503/1000\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.5179 - acc: 0.7020 - val_loss: 1.5899 - val_acc: 0.5015\n",
      "Epoch 504/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4977 - acc: 0.7038 - val_loss: 1.6343 - val_acc: 0.4592\n",
      "Epoch 505/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4660 - acc: 0.7189 - val_loss: 1.6202 - val_acc: 0.4713\n",
      "Epoch 506/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4667 - acc: 0.7242 - val_loss: 1.6578 - val_acc: 0.4532\n",
      "Epoch 507/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4726 - acc: 0.7291 - val_loss: 1.7503 - val_acc: 0.3807\n",
      "Epoch 508/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5586 - acc: 0.6851 - val_loss: 1.7477 - val_acc: 0.3625\n",
      "Epoch 509/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5350 - acc: 0.7062 - val_loss: 1.7101 - val_acc: 0.4109\n",
      "Epoch 510/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4751 - acc: 0.7337 - val_loss: 1.6707 - val_acc: 0.4381\n",
      "Epoch 511/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4681 - acc: 0.7319 - val_loss: 1.7114 - val_acc: 0.4350\n",
      "Epoch 512/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.4732 - acc: 0.7429 - val_loss: 1.7583 - val_acc: 0.4109\n",
      "Epoch 513/1000\n",
      "2839/2839 [==============================] - 0s 45us/step - loss: 0.5346 - acc: 0.7196 - val_loss: 1.7125 - val_acc: 0.4411\n",
      "Epoch 514/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5302 - acc: 0.7231 - val_loss: 1.6210 - val_acc: 0.4592\n",
      "Epoch 515/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4933 - acc: 0.7348 - val_loss: 1.6442 - val_acc: 0.4683\n",
      "Epoch 516/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4626 - acc: 0.7453 - val_loss: 1.6646 - val_acc: 0.4743\n",
      "Epoch 517/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4435 - acc: 0.7503 - val_loss: 1.6841 - val_acc: 0.4592\n",
      "Epoch 518/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4640 - acc: 0.7415 - val_loss: 1.6293 - val_acc: 0.5287\n",
      "Epoch 519/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5185 - acc: 0.7083 - val_loss: 1.6131 - val_acc: 0.5559\n",
      "Epoch 520/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.6000 - acc: 0.6830 - val_loss: 1.6016 - val_acc: 0.4924\n",
      "Epoch 521/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4686 - acc: 0.7316 - val_loss: 1.6471 - val_acc: 0.4894\n",
      "Epoch 522/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4475 - acc: 0.7439 - val_loss: 1.7371 - val_acc: 0.4502\n",
      "Epoch 523/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4761 - acc: 0.7253 - val_loss: 1.7840 - val_acc: 0.4411\n",
      "Epoch 524/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5615 - acc: 0.7017 - val_loss: 1.6327 - val_acc: 0.4924\n",
      "Epoch 525/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4756 - acc: 0.7591 - val_loss: 1.6307 - val_acc: 0.5045\n",
      "Epoch 526/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4610 - acc: 0.7534 - val_loss: 1.6692 - val_acc: 0.4743\n",
      "Epoch 527/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4614 - acc: 0.7312 - val_loss: 1.6700 - val_acc: 0.4773\n",
      "Epoch 528/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4640 - acc: 0.7418 - val_loss: 1.7023 - val_acc: 0.4864\n",
      "Epoch 529/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4592 - acc: 0.7555 - val_loss: 1.6828 - val_acc: 0.4834\n",
      "Epoch 530/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4936 - acc: 0.7253 - val_loss: 1.7162 - val_acc: 0.4502\n",
      "Epoch 531/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.6051 - acc: 0.6714 - val_loss: 1.7377 - val_acc: 0.3958\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.5438 - acc: 0.6869 - val_loss: 1.7172 - val_acc: 0.4350\n",
      "Epoch 533/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4698 - acc: 0.7291 - val_loss: 1.7266 - val_acc: 0.4290\n",
      "Epoch 534/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4745 - acc: 0.7390 - val_loss: 1.7652 - val_acc: 0.4169\n",
      "Epoch 535/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4531 - acc: 0.7443 - val_loss: 1.8121 - val_acc: 0.4018\n",
      "Epoch 536/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4907 - acc: 0.7408 - val_loss: 1.8143 - val_acc: 0.4139\n",
      "Epoch 537/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5148 - acc: 0.7390 - val_loss: 1.7456 - val_acc: 0.4290\n",
      "Epoch 538/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4563 - acc: 0.7460 - val_loss: 1.7292 - val_acc: 0.4411\n",
      "Epoch 539/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4646 - acc: 0.7369 - val_loss: 1.8179 - val_acc: 0.3867\n",
      "Epoch 540/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4705 - acc: 0.7400 - val_loss: 1.7113 - val_acc: 0.4622\n",
      "Epoch 541/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4632 - acc: 0.7344 - val_loss: 1.7128 - val_acc: 0.4562\n",
      "Epoch 542/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4603 - acc: 0.7496 - val_loss: 1.7094 - val_acc: 0.5015\n",
      "Epoch 543/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.5124 - acc: 0.6978 - val_loss: 1.8036 - val_acc: 0.4109\n",
      "Epoch 544/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5636 - acc: 0.6685 - val_loss: 1.6854 - val_acc: 0.4592\n",
      "Epoch 545/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4626 - acc: 0.7365 - val_loss: 1.7284 - val_acc: 0.4441\n",
      "Epoch 546/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4657 - acc: 0.7263 - val_loss: 1.8248 - val_acc: 0.3746\n",
      "Epoch 547/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4873 - acc: 0.7270 - val_loss: 1.7706 - val_acc: 0.4260\n",
      "Epoch 548/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4825 - acc: 0.7351 - val_loss: 1.7872 - val_acc: 0.4109\n",
      "Epoch 549/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4484 - acc: 0.7503 - val_loss: 1.7985 - val_acc: 0.4169\n",
      "Epoch 550/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4416 - acc: 0.7467 - val_loss: 1.7935 - val_acc: 0.4199\n",
      "Epoch 551/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4482 - acc: 0.7496 - val_loss: 1.7930 - val_acc: 0.4441\n",
      "Epoch 552/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4911 - acc: 0.7253 - val_loss: 1.8699 - val_acc: 0.4320\n",
      "Epoch 553/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5808 - acc: 0.7073 - val_loss: 1.7032 - val_acc: 0.4713\n",
      "Epoch 554/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5162 - acc: 0.7348 - val_loss: 1.7075 - val_acc: 0.4743\n",
      "Epoch 555/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4295 - acc: 0.7510 - val_loss: 1.7284 - val_acc: 0.4743\n",
      "Epoch 556/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4563 - acc: 0.7478 - val_loss: 1.6844 - val_acc: 0.5347\n",
      "Epoch 557/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.4796 - acc: 0.7221 - val_loss: 1.7170 - val_acc: 0.5015\n",
      "Epoch 558/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4517 - acc: 0.7471 - val_loss: 1.7410 - val_acc: 0.4804\n",
      "Epoch 559/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4371 - acc: 0.7718 - val_loss: 1.8153 - val_acc: 0.4471\n",
      "Epoch 560/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4568 - acc: 0.7520 - val_loss: 1.7557 - val_acc: 0.4622\n",
      "Epoch 561/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4843 - acc: 0.7365 - val_loss: 1.7502 - val_acc: 0.4622\n",
      "Epoch 562/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5212 - acc: 0.7193 - val_loss: 1.6815 - val_acc: 0.5106\n",
      "Epoch 563/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5188 - acc: 0.7376 - val_loss: 1.6821 - val_acc: 0.5076\n",
      "Epoch 564/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4541 - acc: 0.7534 - val_loss: 1.7302 - val_acc: 0.4864\n",
      "Epoch 565/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4534 - acc: 0.7460 - val_loss: 1.7117 - val_acc: 0.5106\n",
      "Epoch 566/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4726 - acc: 0.7200 - val_loss: 1.6845 - val_acc: 0.5227\n",
      "Epoch 567/1000\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.4577 - acc: 0.7372 - val_loss: 1.7394 - val_acc: 0.5257\n",
      "Epoch 568/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4914 - acc: 0.7235 - val_loss: 1.7466 - val_acc: 0.4864\n",
      "Epoch 569/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4655 - acc: 0.7069 - val_loss: 1.7842 - val_acc: 0.4290\n",
      "Epoch 570/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4741 - acc: 0.7256 - val_loss: 1.8927 - val_acc: 0.3746\n",
      "Epoch 571/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.5362 - acc: 0.7013 - val_loss: 1.8534 - val_acc: 0.3837\n",
      "Epoch 572/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.4909 - acc: 0.7344 - val_loss: 1.7519 - val_acc: 0.4532\n",
      "Epoch 573/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4225 - acc: 0.7478 - val_loss: 1.7563 - val_acc: 0.4441\n",
      "Epoch 574/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4659 - acc: 0.7337 - val_loss: 1.7864 - val_acc: 0.4471\n",
      "Epoch 575/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.5000 - acc: 0.7091 - val_loss: 1.7264 - val_acc: 0.4592\n",
      "Epoch 576/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4536 - acc: 0.7319 - val_loss: 1.7229 - val_acc: 0.4834\n",
      "Epoch 577/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4233 - acc: 0.7548 - val_loss: 1.7122 - val_acc: 0.5196\n",
      "Epoch 578/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.4471 - acc: 0.7351 - val_loss: 1.7492 - val_acc: 0.5045\n",
      "Epoch 579/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4418 - acc: 0.7453 - val_loss: 1.7423 - val_acc: 0.5257\n",
      "Epoch 580/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4573 - acc: 0.7457 - val_loss: 1.7290 - val_acc: 0.4955\n",
      "Epoch 581/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4273 - acc: 0.7545 - val_loss: 1.7597 - val_acc: 0.5015\n",
      "Epoch 582/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4527 - acc: 0.7496 - val_loss: 1.7191 - val_acc: 0.5408\n",
      "Epoch 583/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.5214 - acc: 0.7246 - val_loss: 1.7639 - val_acc: 0.4924\n",
      "Epoch 584/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4616 - acc: 0.7457 - val_loss: 1.7655 - val_acc: 0.4834\n",
      "Epoch 585/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4442 - acc: 0.7573 - val_loss: 1.8140 - val_acc: 0.4532\n",
      "Epoch 586/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4443 - acc: 0.7471 - val_loss: 1.8358 - val_acc: 0.4290\n",
      "Epoch 587/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4432 - acc: 0.7622 - val_loss: 1.8637 - val_acc: 0.4290\n",
      "Epoch 588/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4323 - acc: 0.7587 - val_loss: 1.8201 - val_acc: 0.4471\n",
      "Epoch 589/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4454 - acc: 0.7524 - val_loss: 1.8651 - val_acc: 0.4260\n",
      "Epoch 590/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4810 - acc: 0.7524 - val_loss: 1.8370 - val_acc: 0.4502\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4840 - acc: 0.7538 - val_loss: 1.7835 - val_acc: 0.4441\n",
      "Epoch 592/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4370 - acc: 0.7531 - val_loss: 1.7909 - val_acc: 0.4924\n",
      "Epoch 593/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4927 - acc: 0.7309 - val_loss: 1.7372 - val_acc: 0.5227\n",
      "Epoch 594/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4499 - acc: 0.7584 - val_loss: 1.7562 - val_acc: 0.5045\n",
      "Epoch 595/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4192 - acc: 0.7510 - val_loss: 1.7417 - val_acc: 0.5196\n",
      "Epoch 596/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4385 - acc: 0.7524 - val_loss: 1.7531 - val_acc: 0.5106\n",
      "Epoch 597/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4510 - acc: 0.7397 - val_loss: 1.7830 - val_acc: 0.5136\n",
      "Epoch 598/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4339 - acc: 0.7577 - val_loss: 1.8200 - val_acc: 0.4592\n",
      "Epoch 599/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4332 - acc: 0.7538 - val_loss: 1.8716 - val_acc: 0.4471\n",
      "Epoch 600/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4305 - acc: 0.7703 - val_loss: 1.9265 - val_acc: 0.4441\n",
      "Epoch 601/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4999 - acc: 0.7404 - val_loss: 1.8405 - val_acc: 0.4532\n",
      "Epoch 602/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4507 - acc: 0.7594 - val_loss: 1.8306 - val_acc: 0.4804\n",
      "Epoch 603/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4090 - acc: 0.7679 - val_loss: 1.8226 - val_acc: 0.4894\n",
      "Epoch 604/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.4440 - acc: 0.7563 - val_loss: 1.7899 - val_acc: 0.5257\n",
      "Epoch 605/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5395 - acc: 0.7263 - val_loss: 1.8221 - val_acc: 0.4713\n",
      "Epoch 606/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4679 - acc: 0.7573 - val_loss: 1.7718 - val_acc: 0.4743\n",
      "Epoch 607/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4151 - acc: 0.7746 - val_loss: 1.7994 - val_acc: 0.4773\n",
      "Epoch 608/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4205 - acc: 0.7658 - val_loss: 1.8384 - val_acc: 0.4713\n",
      "Epoch 609/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4055 - acc: 0.7570 - val_loss: 1.8060 - val_acc: 0.4924\n",
      "Epoch 610/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4306 - acc: 0.7485 - val_loss: 1.7999 - val_acc: 0.5136\n",
      "Epoch 611/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4458 - acc: 0.7425 - val_loss: 1.7926 - val_acc: 0.5136\n",
      "Epoch 612/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4464 - acc: 0.7471 - val_loss: 1.8290 - val_acc: 0.4834\n",
      "Epoch 613/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4129 - acc: 0.7619 - val_loss: 1.8361 - val_acc: 0.5045\n",
      "Epoch 614/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4371 - acc: 0.7408 - val_loss: 1.8694 - val_acc: 0.5015\n",
      "Epoch 615/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5182 - acc: 0.7277 - val_loss: 1.8612 - val_acc: 0.4924\n",
      "Epoch 616/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5275 - acc: 0.7464 - val_loss: 1.8050 - val_acc: 0.4592\n",
      "Epoch 617/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4083 - acc: 0.7791 - val_loss: 1.8387 - val_acc: 0.4683\n",
      "Epoch 618/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3866 - acc: 0.7862 - val_loss: 1.8907 - val_acc: 0.4471\n",
      "Epoch 619/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3903 - acc: 0.7707 - val_loss: 1.9037 - val_acc: 0.4562\n",
      "Epoch 620/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3994 - acc: 0.7735 - val_loss: 1.9628 - val_acc: 0.4350\n",
      "Epoch 621/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4197 - acc: 0.7672 - val_loss: 1.9890 - val_acc: 0.4290\n",
      "Epoch 622/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4288 - acc: 0.7679 - val_loss: 2.0111 - val_acc: 0.4230\n",
      "Epoch 623/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4638 - acc: 0.7478 - val_loss: 1.9655 - val_acc: 0.4381\n",
      "Epoch 624/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4383 - acc: 0.7626 - val_loss: 1.9155 - val_acc: 0.4441\n",
      "Epoch 625/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4243 - acc: 0.7689 - val_loss: 1.8593 - val_acc: 0.4592\n",
      "Epoch 626/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4034 - acc: 0.7732 - val_loss: 1.9111 - val_acc: 0.4592\n",
      "Epoch 627/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3999 - acc: 0.7770 - val_loss: 1.8909 - val_acc: 0.4924\n",
      "Epoch 628/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4117 - acc: 0.7619 - val_loss: 1.8816 - val_acc: 0.5287\n",
      "Epoch 629/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4880 - acc: 0.7457 - val_loss: 1.8881 - val_acc: 0.4894\n",
      "Epoch 630/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4456 - acc: 0.7686 - val_loss: 1.9289 - val_acc: 0.4834\n",
      "Epoch 631/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4600 - acc: 0.7503 - val_loss: 1.9263 - val_acc: 0.4411\n",
      "Epoch 632/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4349 - acc: 0.7601 - val_loss: 1.9946 - val_acc: 0.4350\n",
      "Epoch 633/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4135 - acc: 0.7802 - val_loss: 1.9837 - val_acc: 0.4471\n",
      "Epoch 634/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4213 - acc: 0.7781 - val_loss: 2.0123 - val_acc: 0.4532\n",
      "Epoch 635/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4234 - acc: 0.7749 - val_loss: 1.9588 - val_acc: 0.4471\n",
      "Epoch 636/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4050 - acc: 0.7820 - val_loss: 1.9981 - val_acc: 0.4260\n",
      "Epoch 637/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3967 - acc: 0.7858 - val_loss: 2.0517 - val_acc: 0.4350\n",
      "Epoch 638/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4056 - acc: 0.7813 - val_loss: 2.0352 - val_acc: 0.4350\n",
      "Epoch 639/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4246 - acc: 0.7725 - val_loss: 2.0420 - val_acc: 0.3897\n",
      "Epoch 640/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4111 - acc: 0.7661 - val_loss: 1.9753 - val_acc: 0.4290\n",
      "Epoch 641/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4227 - acc: 0.7675 - val_loss: 1.9844 - val_acc: 0.4441\n",
      "Epoch 642/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4739 - acc: 0.7548 - val_loss: 2.0589 - val_acc: 0.3444\n",
      "Epoch 643/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.5093 - acc: 0.7351 - val_loss: 1.8956 - val_acc: 0.4411\n",
      "Epoch 644/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4144 - acc: 0.7598 - val_loss: 1.9322 - val_acc: 0.4653\n",
      "Epoch 645/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4036 - acc: 0.7665 - val_loss: 1.9014 - val_acc: 0.4894\n",
      "Epoch 646/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4064 - acc: 0.7661 - val_loss: 1.9193 - val_acc: 0.4683\n",
      "Epoch 647/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4061 - acc: 0.7672 - val_loss: 1.9182 - val_acc: 0.4653\n",
      "Epoch 648/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3908 - acc: 0.7739 - val_loss: 1.9979 - val_acc: 0.4320\n",
      "Epoch 649/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4245 - acc: 0.7531 - val_loss: 1.9838 - val_acc: 0.4320\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4344 - acc: 0.7474 - val_loss: 1.9657 - val_acc: 0.4381\n",
      "Epoch 651/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4154 - acc: 0.7608 - val_loss: 1.9353 - val_acc: 0.4804\n",
      "Epoch 652/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4084 - acc: 0.7598 - val_loss: 1.9285 - val_acc: 0.5045\n",
      "Epoch 653/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4901 - acc: 0.7126 - val_loss: 1.8760 - val_acc: 0.5136\n",
      "Epoch 654/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4496 - acc: 0.7327 - val_loss: 1.9373 - val_acc: 0.4773\n",
      "Epoch 655/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3860 - acc: 0.7732 - val_loss: 1.9604 - val_acc: 0.4804\n",
      "Epoch 656/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3930 - acc: 0.7732 - val_loss: 1.9718 - val_acc: 0.4864\n",
      "Epoch 657/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4218 - acc: 0.7693 - val_loss: 1.9690 - val_acc: 0.4773\n",
      "Epoch 658/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4283 - acc: 0.7654 - val_loss: 1.9972 - val_acc: 0.4683\n",
      "Epoch 659/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4243 - acc: 0.7777 - val_loss: 1.9656 - val_acc: 0.4743\n",
      "Epoch 660/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4276 - acc: 0.7770 - val_loss: 2.0063 - val_acc: 0.4653\n",
      "Epoch 661/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3954 - acc: 0.7788 - val_loss: 2.0340 - val_acc: 0.4592\n",
      "Epoch 662/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3694 - acc: 0.7848 - val_loss: 1.9967 - val_acc: 0.4622\n",
      "Epoch 663/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4054 - acc: 0.7661 - val_loss: 1.9635 - val_acc: 0.4894\n",
      "Epoch 664/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4793 - acc: 0.7238 - val_loss: 1.9218 - val_acc: 0.5045\n",
      "Epoch 665/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4553 - acc: 0.7467 - val_loss: 1.9947 - val_acc: 0.4290\n",
      "Epoch 666/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4679 - acc: 0.7425 - val_loss: 1.9839 - val_acc: 0.4260\n",
      "Epoch 667/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4093 - acc: 0.7636 - val_loss: 1.9916 - val_acc: 0.4502\n",
      "Epoch 668/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3875 - acc: 0.7844 - val_loss: 1.9685 - val_acc: 0.4562\n",
      "Epoch 669/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3846 - acc: 0.7746 - val_loss: 2.0122 - val_acc: 0.4441\n",
      "Epoch 670/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3841 - acc: 0.7806 - val_loss: 1.9975 - val_acc: 0.4592\n",
      "Epoch 671/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3880 - acc: 0.7788 - val_loss: 2.0107 - val_acc: 0.4411\n",
      "Epoch 672/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.4027 - acc: 0.7725 - val_loss: 1.9665 - val_acc: 0.4290\n",
      "Epoch 673/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3942 - acc: 0.7802 - val_loss: 1.9668 - val_acc: 0.4592\n",
      "Epoch 674/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3740 - acc: 0.7749 - val_loss: 2.0438 - val_acc: 0.4381\n",
      "Epoch 675/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3949 - acc: 0.7777 - val_loss: 2.1709 - val_acc: 0.3686\n",
      "Epoch 676/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4335 - acc: 0.7474 - val_loss: 2.0845 - val_acc: 0.4018\n",
      "Epoch 677/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4299 - acc: 0.7629 - val_loss: 2.1094 - val_acc: 0.4169\n",
      "Epoch 678/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4407 - acc: 0.7686 - val_loss: 2.1278 - val_acc: 0.3958\n",
      "Epoch 679/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4253 - acc: 0.7728 - val_loss: 2.1053 - val_acc: 0.4139\n",
      "Epoch 680/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3874 - acc: 0.7989 - val_loss: 2.1014 - val_acc: 0.4139\n",
      "Epoch 681/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3838 - acc: 0.7894 - val_loss: 2.0288 - val_acc: 0.4502\n",
      "Epoch 682/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3750 - acc: 0.7848 - val_loss: 2.1535 - val_acc: 0.4109\n",
      "Epoch 683/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4072 - acc: 0.7774 - val_loss: 2.1810 - val_acc: 0.4109\n",
      "Epoch 684/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4488 - acc: 0.7573 - val_loss: 2.0944 - val_acc: 0.4230\n",
      "Epoch 685/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3876 - acc: 0.7844 - val_loss: 2.1077 - val_acc: 0.4441\n",
      "Epoch 686/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3797 - acc: 0.7894 - val_loss: 2.1099 - val_acc: 0.4411\n",
      "Epoch 687/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3860 - acc: 0.7837 - val_loss: 2.1205 - val_acc: 0.4502\n",
      "Epoch 688/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.5172 - acc: 0.7474 - val_loss: 2.0292 - val_acc: 0.4653\n",
      "Epoch 689/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4751 - acc: 0.7661 - val_loss: 1.9501 - val_acc: 0.4804\n",
      "Epoch 690/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3665 - acc: 0.7880 - val_loss: 1.9730 - val_acc: 0.4894\n",
      "Epoch 691/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3730 - acc: 0.7770 - val_loss: 1.9737 - val_acc: 0.4894\n",
      "Epoch 692/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3640 - acc: 0.7890 - val_loss: 2.0430 - val_acc: 0.4713\n",
      "Epoch 693/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3642 - acc: 0.7964 - val_loss: 2.0265 - val_acc: 0.5015\n",
      "Epoch 694/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3800 - acc: 0.7806 - val_loss: 2.0355 - val_acc: 0.5045\n",
      "Epoch 695/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4454 - acc: 0.7460 - val_loss: 2.0008 - val_acc: 0.5015\n",
      "Epoch 696/1000\n",
      "2839/2839 [==============================] - 0s 40us/step - loss: 0.4339 - acc: 0.7467 - val_loss: 2.0056 - val_acc: 0.4411\n",
      "Epoch 697/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.4013 - acc: 0.7714 - val_loss: 2.0294 - val_acc: 0.4441\n",
      "Epoch 698/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3637 - acc: 0.7957 - val_loss: 2.0808 - val_acc: 0.4441\n",
      "Epoch 699/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3436 - acc: 0.8045 - val_loss: 2.1478 - val_acc: 0.4350\n",
      "Epoch 700/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3744 - acc: 0.7883 - val_loss: 2.1773 - val_acc: 0.4320\n",
      "Epoch 701/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3895 - acc: 0.7834 - val_loss: 2.1823 - val_acc: 0.4230\n",
      "Epoch 702/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4194 - acc: 0.7816 - val_loss: 2.0947 - val_acc: 0.4290\n",
      "Epoch 703/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4224 - acc: 0.7735 - val_loss: 2.0708 - val_acc: 0.4381\n",
      "Epoch 704/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4302 - acc: 0.7555 - val_loss: 1.9513 - val_acc: 0.4924\n",
      "Epoch 705/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4032 - acc: 0.7714 - val_loss: 1.9982 - val_acc: 0.4924\n",
      "Epoch 706/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3553 - acc: 0.7943 - val_loss: 2.0447 - val_acc: 0.4894\n",
      "Epoch 707/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3560 - acc: 0.7968 - val_loss: 2.0955 - val_acc: 0.4683\n",
      "Epoch 708/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3590 - acc: 0.7925 - val_loss: 2.1612 - val_acc: 0.4743\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4011 - acc: 0.7693 - val_loss: 2.1153 - val_acc: 0.4804\n",
      "Epoch 710/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4292 - acc: 0.7770 - val_loss: 2.0807 - val_acc: 0.4622\n",
      "Epoch 711/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3673 - acc: 0.8006 - val_loss: 2.0683 - val_acc: 0.4834\n",
      "Epoch 712/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3503 - acc: 0.8105 - val_loss: 2.1499 - val_acc: 0.4441\n",
      "Epoch 713/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3608 - acc: 0.7968 - val_loss: 2.1500 - val_acc: 0.4532\n",
      "Epoch 714/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3721 - acc: 0.7858 - val_loss: 2.2279 - val_acc: 0.4320\n",
      "Epoch 715/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3813 - acc: 0.7932 - val_loss: 2.3359 - val_acc: 0.3837\n",
      "Epoch 716/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4551 - acc: 0.7654 - val_loss: 2.2285 - val_acc: 0.4079\n",
      "Epoch 717/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4050 - acc: 0.7795 - val_loss: 2.1222 - val_acc: 0.4381\n",
      "Epoch 718/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3497 - acc: 0.8027 - val_loss: 2.1320 - val_acc: 0.4622\n",
      "Epoch 719/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3399 - acc: 0.8052 - val_loss: 2.1520 - val_acc: 0.4955\n",
      "Epoch 720/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4074 - acc: 0.7749 - val_loss: 2.1576 - val_acc: 0.4834\n",
      "Epoch 721/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4280 - acc: 0.7809 - val_loss: 2.0914 - val_acc: 0.4683\n",
      "Epoch 722/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3912 - acc: 0.7982 - val_loss: 2.1391 - val_acc: 0.4411\n",
      "Epoch 723/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3782 - acc: 0.8045 - val_loss: 2.2272 - val_acc: 0.4441\n",
      "Epoch 724/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3938 - acc: 0.7774 - val_loss: 2.2431 - val_acc: 0.4199\n",
      "Epoch 725/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3994 - acc: 0.7932 - val_loss: 2.1368 - val_acc: 0.4441\n",
      "Epoch 726/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3737 - acc: 0.7985 - val_loss: 2.1239 - val_acc: 0.4622\n",
      "Epoch 727/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3462 - acc: 0.8094 - val_loss: 2.1291 - val_acc: 0.4502\n",
      "Epoch 728/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3827 - acc: 0.7904 - val_loss: 2.1616 - val_acc: 0.4169\n",
      "Epoch 729/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4528 - acc: 0.7665 - val_loss: 2.1053 - val_acc: 0.4411\n",
      "Epoch 730/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4531 - acc: 0.7573 - val_loss: 2.1518 - val_acc: 0.4320\n",
      "Epoch 731/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4195 - acc: 0.7622 - val_loss: 2.0967 - val_acc: 0.4441\n",
      "Epoch 732/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3645 - acc: 0.7989 - val_loss: 2.1812 - val_acc: 0.4350\n",
      "Epoch 733/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3465 - acc: 0.8042 - val_loss: 2.2273 - val_acc: 0.4199\n",
      "Epoch 734/1000\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.3536 - acc: 0.7985 - val_loss: 2.2484 - val_acc: 0.4441\n",
      "Epoch 735/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3819 - acc: 0.7869 - val_loss: 2.2572 - val_acc: 0.4199\n",
      "Epoch 736/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3574 - acc: 0.8010 - val_loss: 2.1796 - val_acc: 0.4441\n",
      "Epoch 737/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3605 - acc: 0.8070 - val_loss: 2.2257 - val_acc: 0.4653\n",
      "Epoch 738/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4067 - acc: 0.7957 - val_loss: 2.1635 - val_acc: 0.4864\n",
      "Epoch 739/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4246 - acc: 0.7957 - val_loss: 2.0785 - val_acc: 0.4773\n",
      "Epoch 740/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3489 - acc: 0.8179 - val_loss: 2.0875 - val_acc: 0.4864\n",
      "Epoch 741/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3429 - acc: 0.8112 - val_loss: 2.2293 - val_acc: 0.4532\n",
      "Epoch 742/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3605 - acc: 0.8024 - val_loss: 2.2632 - val_acc: 0.4350\n",
      "Epoch 743/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4043 - acc: 0.7788 - val_loss: 2.3422 - val_acc: 0.4230\n",
      "Epoch 744/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4087 - acc: 0.7813 - val_loss: 2.1643 - val_acc: 0.4502\n",
      "Epoch 745/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3592 - acc: 0.8073 - val_loss: 2.1929 - val_acc: 0.4532\n",
      "Epoch 746/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3943 - acc: 0.7943 - val_loss: 2.1578 - val_acc: 0.4713\n",
      "Epoch 747/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3658 - acc: 0.8080 - val_loss: 2.1442 - val_acc: 0.4743\n",
      "Epoch 748/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3449 - acc: 0.8126 - val_loss: 2.1550 - val_acc: 0.4864\n",
      "Epoch 749/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3326 - acc: 0.8130 - val_loss: 2.1881 - val_acc: 0.4713\n",
      "Epoch 750/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3751 - acc: 0.8006 - val_loss: 2.2328 - val_acc: 0.4622\n",
      "Epoch 751/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4165 - acc: 0.7883 - val_loss: 2.2158 - val_acc: 0.4713\n",
      "Epoch 752/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4524 - acc: 0.7844 - val_loss: 2.1730 - val_acc: 0.4683\n",
      "Epoch 753/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3529 - acc: 0.8087 - val_loss: 2.1916 - val_acc: 0.4381\n",
      "Epoch 754/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3344 - acc: 0.8168 - val_loss: 2.3029 - val_acc: 0.4411\n",
      "Epoch 755/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3743 - acc: 0.7961 - val_loss: 2.3165 - val_acc: 0.4290\n",
      "Epoch 756/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3798 - acc: 0.7908 - val_loss: 2.2412 - val_acc: 0.4350\n",
      "Epoch 757/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3594 - acc: 0.8101 - val_loss: 2.2093 - val_acc: 0.4532\n",
      "Epoch 758/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3217 - acc: 0.8175 - val_loss: 2.2538 - val_acc: 0.4622\n",
      "Epoch 759/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3342 - acc: 0.8214 - val_loss: 2.2758 - val_acc: 0.4683\n",
      "Epoch 760/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3327 - acc: 0.8144 - val_loss: 2.2742 - val_acc: 0.4864\n",
      "Epoch 761/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3848 - acc: 0.8003 - val_loss: 2.3010 - val_acc: 0.4834\n",
      "Epoch 762/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4111 - acc: 0.7865 - val_loss: 2.2140 - val_acc: 0.4804\n",
      "Epoch 763/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3964 - acc: 0.8038 - val_loss: 2.1972 - val_acc: 0.4804\n",
      "Epoch 764/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3568 - acc: 0.8119 - val_loss: 2.1538 - val_acc: 0.5076\n",
      "Epoch 765/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3561 - acc: 0.7890 - val_loss: 2.1281 - val_acc: 0.4955\n",
      "Epoch 766/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3958 - acc: 0.7527 - val_loss: 2.1155 - val_acc: 0.5045\n",
      "Epoch 767/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3782 - acc: 0.7784 - val_loss: 2.1194 - val_acc: 0.4924\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3596 - acc: 0.8010 - val_loss: 2.1307 - val_acc: 0.4924\n",
      "Epoch 769/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3850 - acc: 0.7813 - val_loss: 2.1863 - val_acc: 0.4743\n",
      "Epoch 770/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3639 - acc: 0.7837 - val_loss: 2.1767 - val_acc: 0.4592\n",
      "Epoch 771/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3490 - acc: 0.7989 - val_loss: 2.2360 - val_acc: 0.4230\n",
      "Epoch 772/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3561 - acc: 0.8087 - val_loss: 2.2669 - val_acc: 0.4139\n",
      "Epoch 773/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3750 - acc: 0.7915 - val_loss: 2.2552 - val_acc: 0.4350\n",
      "Epoch 774/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3520 - acc: 0.8137 - val_loss: 2.3940 - val_acc: 0.4320\n",
      "Epoch 775/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3969 - acc: 0.7901 - val_loss: 2.3332 - val_acc: 0.4260\n",
      "Epoch 776/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3812 - acc: 0.7922 - val_loss: 2.3841 - val_acc: 0.4199\n",
      "Epoch 777/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3955 - acc: 0.7897 - val_loss: 2.2238 - val_acc: 0.4381\n",
      "Epoch 778/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3524 - acc: 0.8119 - val_loss: 2.2527 - val_acc: 0.4653\n",
      "Epoch 779/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3454 - acc: 0.8211 - val_loss: 2.2505 - val_acc: 0.4653\n",
      "Epoch 780/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3356 - acc: 0.8278 - val_loss: 2.2726 - val_acc: 0.4773\n",
      "Epoch 781/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3531 - acc: 0.8144 - val_loss: 2.2931 - val_acc: 0.4834\n",
      "Epoch 782/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3864 - acc: 0.7851 - val_loss: 2.2185 - val_acc: 0.5015\n",
      "Epoch 783/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3587 - acc: 0.8087 - val_loss: 2.2338 - val_acc: 0.4894\n",
      "Epoch 784/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3419 - acc: 0.8101 - val_loss: 2.2164 - val_acc: 0.4743\n",
      "Epoch 785/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3344 - acc: 0.8059 - val_loss: 2.1735 - val_acc: 0.5076\n",
      "Epoch 786/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3680 - acc: 0.7820 - val_loss: 2.1767 - val_acc: 0.5015\n",
      "Epoch 787/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.4042 - acc: 0.7513 - val_loss: 2.1332 - val_acc: 0.5015\n",
      "Epoch 788/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3526 - acc: 0.7876 - val_loss: 2.2132 - val_acc: 0.4773\n",
      "Epoch 789/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3143 - acc: 0.8200 - val_loss: 2.2420 - val_acc: 0.4985\n",
      "Epoch 790/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3246 - acc: 0.8235 - val_loss: 2.2750 - val_acc: 0.4894\n",
      "Epoch 791/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4197 - acc: 0.7848 - val_loss: 2.2859 - val_acc: 0.4834\n",
      "Epoch 792/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3937 - acc: 0.7961 - val_loss: 2.1964 - val_acc: 0.4804\n",
      "Epoch 793/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3240 - acc: 0.8186 - val_loss: 2.1994 - val_acc: 0.4804\n",
      "Epoch 794/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3693 - acc: 0.7982 - val_loss: 2.1729 - val_acc: 0.4955\n",
      "Epoch 795/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3828 - acc: 0.7640 - val_loss: 2.1517 - val_acc: 0.4894\n",
      "Epoch 796/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3515 - acc: 0.7813 - val_loss: 2.1954 - val_acc: 0.4894\n",
      "Epoch 797/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3188 - acc: 0.8070 - val_loss: 2.2613 - val_acc: 0.4743\n",
      "Epoch 798/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3105 - acc: 0.8242 - val_loss: 2.2334 - val_acc: 0.4924\n",
      "Epoch 799/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3221 - acc: 0.8225 - val_loss: 2.2640 - val_acc: 0.4924\n",
      "Epoch 800/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3440 - acc: 0.8295 - val_loss: 2.3028 - val_acc: 0.4834\n",
      "Epoch 801/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3449 - acc: 0.8140 - val_loss: 2.3429 - val_acc: 0.4592\n",
      "Epoch 802/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3933 - acc: 0.8073 - val_loss: 2.2955 - val_acc: 0.4713\n",
      "Epoch 803/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3656 - acc: 0.8154 - val_loss: 2.3454 - val_acc: 0.4532\n",
      "Epoch 804/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3576 - acc: 0.8035 - val_loss: 2.2959 - val_acc: 0.4502\n",
      "Epoch 805/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3301 - acc: 0.8278 - val_loss: 2.2778 - val_acc: 0.4622\n",
      "Epoch 806/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3142 - acc: 0.8263 - val_loss: 2.3400 - val_acc: 0.4653\n",
      "Epoch 807/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3152 - acc: 0.8292 - val_loss: 2.4043 - val_acc: 0.4471\n",
      "Epoch 808/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3383 - acc: 0.8207 - val_loss: 2.5351 - val_acc: 0.4230\n",
      "Epoch 809/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3774 - acc: 0.7929 - val_loss: 2.4012 - val_acc: 0.4290\n",
      "Epoch 810/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3481 - acc: 0.8070 - val_loss: 2.3481 - val_acc: 0.4381\n",
      "Epoch 811/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3167 - acc: 0.8200 - val_loss: 2.3751 - val_acc: 0.4592\n",
      "Epoch 812/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3166 - acc: 0.8256 - val_loss: 2.3335 - val_acc: 0.4592\n",
      "Epoch 813/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3059 - acc: 0.8422 - val_loss: 2.2513 - val_acc: 0.4894\n",
      "Epoch 814/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3611 - acc: 0.8035 - val_loss: 2.3647 - val_acc: 0.4350\n",
      "Epoch 815/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4645 - acc: 0.7372 - val_loss: 2.3158 - val_acc: 0.4260\n",
      "Epoch 816/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3964 - acc: 0.7982 - val_loss: 2.3072 - val_acc: 0.4290\n",
      "Epoch 817/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3433 - acc: 0.8168 - val_loss: 2.2882 - val_acc: 0.4653\n",
      "Epoch 818/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3088 - acc: 0.8316 - val_loss: 2.3355 - val_acc: 0.4653\n",
      "Epoch 819/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3059 - acc: 0.8182 - val_loss: 2.3842 - val_acc: 0.4773\n",
      "Epoch 820/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3096 - acc: 0.8274 - val_loss: 2.4414 - val_acc: 0.4411\n",
      "Epoch 821/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3146 - acc: 0.8348 - val_loss: 2.3810 - val_acc: 0.4713\n",
      "Epoch 822/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3123 - acc: 0.8299 - val_loss: 2.3522 - val_acc: 0.4592\n",
      "Epoch 823/1000\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 0.3220 - acc: 0.8253 - val_loss: 2.3422 - val_acc: 0.4471\n",
      "Epoch 824/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3502 - acc: 0.8137 - val_loss: 2.4946 - val_acc: 0.4018\n",
      "Epoch 825/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3819 - acc: 0.7932 - val_loss: 2.5137 - val_acc: 0.4290\n",
      "Epoch 826/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3596 - acc: 0.8010 - val_loss: 2.4490 - val_acc: 0.4290\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3463 - acc: 0.8175 - val_loss: 2.3973 - val_acc: 0.4622\n",
      "Epoch 828/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3257 - acc: 0.8274 - val_loss: 2.4001 - val_acc: 0.4441\n",
      "Epoch 829/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3385 - acc: 0.8228 - val_loss: 2.4106 - val_acc: 0.4350\n",
      "Epoch 830/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3276 - acc: 0.8267 - val_loss: 2.4299 - val_acc: 0.4411\n",
      "Epoch 831/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3360 - acc: 0.8256 - val_loss: 2.4576 - val_acc: 0.4532\n",
      "Epoch 832/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3335 - acc: 0.8214 - val_loss: 2.4817 - val_acc: 0.4471\n",
      "Epoch 833/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3592 - acc: 0.8098 - val_loss: 2.3778 - val_acc: 0.4532\n",
      "Epoch 834/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3254 - acc: 0.8302 - val_loss: 2.3868 - val_acc: 0.4622\n",
      "Epoch 835/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3317 - acc: 0.8193 - val_loss: 2.3289 - val_acc: 0.5015\n",
      "Epoch 836/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3335 - acc: 0.8263 - val_loss: 2.3256 - val_acc: 0.4924\n",
      "Epoch 837/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3155 - acc: 0.8221 - val_loss: 2.3056 - val_acc: 0.4955\n",
      "Epoch 838/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.3311 - acc: 0.8179 - val_loss: 2.2782 - val_acc: 0.5045\n",
      "Epoch 839/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3494 - acc: 0.8123 - val_loss: 2.3842 - val_acc: 0.4743\n",
      "Epoch 840/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.3427 - acc: 0.8211 - val_loss: 2.3065 - val_acc: 0.5045\n",
      "Epoch 841/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.3324 - acc: 0.8295 - val_loss: 2.3467 - val_acc: 0.4834\n",
      "Epoch 842/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.2948 - acc: 0.8390 - val_loss: 2.4777 - val_acc: 0.4743\n",
      "Epoch 843/1000\n",
      "2839/2839 [==============================] - 0s 50us/step - loss: 0.3443 - acc: 0.8211 - val_loss: 2.5259 - val_acc: 0.4290\n",
      "Epoch 844/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3541 - acc: 0.8126 - val_loss: 2.5114 - val_acc: 0.4290\n",
      "Epoch 845/1000\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 0.3524 - acc: 0.8147 - val_loss: 2.4380 - val_acc: 0.4381\n",
      "Epoch 846/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3083 - acc: 0.8352 - val_loss: 2.4243 - val_acc: 0.4622\n",
      "Epoch 847/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2826 - acc: 0.8440 - val_loss: 2.5024 - val_acc: 0.4502\n",
      "Epoch 848/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3327 - acc: 0.8077 - val_loss: 2.3730 - val_acc: 0.4683\n",
      "Epoch 849/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3112 - acc: 0.8271 - val_loss: 2.4078 - val_acc: 0.4683\n",
      "Epoch 850/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3089 - acc: 0.8344 - val_loss: 2.4787 - val_acc: 0.4592\n",
      "Epoch 851/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3207 - acc: 0.8260 - val_loss: 2.7022 - val_acc: 0.4139\n",
      "Epoch 852/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.4122 - acc: 0.7865 - val_loss: 2.5696 - val_acc: 0.4290\n",
      "Epoch 853/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4604 - acc: 0.7742 - val_loss: 2.3097 - val_acc: 0.4834\n",
      "Epoch 854/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.4121 - acc: 0.7915 - val_loss: 2.3415 - val_acc: 0.4683\n",
      "Epoch 855/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.2891 - acc: 0.8489 - val_loss: 2.3788 - val_acc: 0.4653\n",
      "Epoch 856/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.2985 - acc: 0.8337 - val_loss: 2.3362 - val_acc: 0.4894\n",
      "Epoch 857/1000\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 0.2860 - acc: 0.8344 - val_loss: 2.3892 - val_acc: 0.4804\n",
      "Epoch 858/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.2899 - acc: 0.8355 - val_loss: 2.3724 - val_acc: 0.4894\n",
      "Epoch 859/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.3030 - acc: 0.8235 - val_loss: 2.4300 - val_acc: 0.4804\n",
      "Epoch 860/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2920 - acc: 0.8295 - val_loss: 2.4023 - val_acc: 0.4924\n",
      "Epoch 861/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2875 - acc: 0.8341 - val_loss: 2.4045 - val_acc: 0.4924\n",
      "Epoch 862/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3077 - acc: 0.8225 - val_loss: 2.3608 - val_acc: 0.4894\n",
      "Epoch 863/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3399 - acc: 0.8027 - val_loss: 2.4176 - val_acc: 0.4894\n",
      "Epoch 864/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3818 - acc: 0.7777 - val_loss: 2.3763 - val_acc: 0.4985\n",
      "Epoch 865/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.4583 - acc: 0.7665 - val_loss: 2.4018 - val_acc: 0.4622\n",
      "Epoch 866/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.3742 - acc: 0.7975 - val_loss: 2.3572 - val_acc: 0.4683\n",
      "Epoch 867/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3076 - acc: 0.8373 - val_loss: 2.4141 - val_acc: 0.4622\n",
      "Epoch 868/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2902 - acc: 0.8383 - val_loss: 2.4493 - val_acc: 0.4713\n",
      "Epoch 869/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2995 - acc: 0.8348 - val_loss: 2.4026 - val_acc: 0.4834\n",
      "Epoch 870/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3103 - acc: 0.8292 - val_loss: 2.3779 - val_acc: 0.4773\n",
      "Epoch 871/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2987 - acc: 0.8313 - val_loss: 2.4866 - val_acc: 0.4532\n",
      "Epoch 872/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2914 - acc: 0.8422 - val_loss: 2.5896 - val_acc: 0.4411\n",
      "Epoch 873/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3342 - acc: 0.8140 - val_loss: 2.6078 - val_acc: 0.4290\n",
      "Epoch 874/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3464 - acc: 0.8242 - val_loss: 2.5665 - val_acc: 0.4320\n",
      "Epoch 875/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3242 - acc: 0.8390 - val_loss: 2.4864 - val_acc: 0.4562\n",
      "Epoch 876/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3051 - acc: 0.8454 - val_loss: 2.4507 - val_acc: 0.4713\n",
      "Epoch 877/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3103 - acc: 0.8366 - val_loss: 2.4178 - val_acc: 0.4985\n",
      "Epoch 878/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3675 - acc: 0.8197 - val_loss: 2.4256 - val_acc: 0.4955\n",
      "Epoch 879/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3832 - acc: 0.8260 - val_loss: 2.4173 - val_acc: 0.4653\n",
      "Epoch 880/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3185 - acc: 0.8330 - val_loss: 2.3962 - val_acc: 0.4713\n",
      "Epoch 881/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2786 - acc: 0.8538 - val_loss: 2.5035 - val_acc: 0.4622\n",
      "Epoch 882/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2977 - acc: 0.8418 - val_loss: 2.4719 - val_acc: 0.4713\n",
      "Epoch 883/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2986 - acc: 0.8327 - val_loss: 2.5394 - val_acc: 0.4411\n",
      "Epoch 884/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2957 - acc: 0.8337 - val_loss: 2.6126 - val_acc: 0.4532\n",
      "Epoch 885/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3195 - acc: 0.8299 - val_loss: 2.6060 - val_acc: 0.4381\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3377 - acc: 0.8221 - val_loss: 2.5212 - val_acc: 0.4502\n",
      "Epoch 887/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3167 - acc: 0.8281 - val_loss: 2.4643 - val_acc: 0.4592\n",
      "Epoch 888/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2930 - acc: 0.8366 - val_loss: 2.5055 - val_acc: 0.4532\n",
      "Epoch 889/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3002 - acc: 0.8422 - val_loss: 2.4225 - val_acc: 0.4562\n",
      "Epoch 890/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3794 - acc: 0.8077 - val_loss: 2.4469 - val_acc: 0.4441\n",
      "Epoch 891/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3448 - acc: 0.8101 - val_loss: 2.3573 - val_acc: 0.4743\n",
      "Epoch 892/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2797 - acc: 0.8401 - val_loss: 2.3886 - val_acc: 0.4894\n",
      "Epoch 893/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2820 - acc: 0.8373 - val_loss: 2.4849 - val_acc: 0.4713\n",
      "Epoch 894/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2830 - acc: 0.8514 - val_loss: 2.5682 - val_acc: 0.4441\n",
      "Epoch 895/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3088 - acc: 0.8313 - val_loss: 2.4605 - val_acc: 0.4502\n",
      "Epoch 896/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2970 - acc: 0.8369 - val_loss: 2.5023 - val_acc: 0.4562\n",
      "Epoch 897/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3030 - acc: 0.8359 - val_loss: 2.6337 - val_acc: 0.4381\n",
      "Epoch 898/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3273 - acc: 0.8235 - val_loss: 2.6520 - val_acc: 0.4562\n",
      "Epoch 899/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3491 - acc: 0.8242 - val_loss: 2.5637 - val_acc: 0.4502\n",
      "Epoch 900/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3304 - acc: 0.8344 - val_loss: 2.4493 - val_acc: 0.4622\n",
      "Epoch 901/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.2803 - acc: 0.8514 - val_loss: 2.4818 - val_acc: 0.4683\n",
      "Epoch 902/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2912 - acc: 0.8418 - val_loss: 2.4715 - val_acc: 0.4804\n",
      "Epoch 903/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2839 - acc: 0.8503 - val_loss: 2.4481 - val_acc: 0.4924\n",
      "Epoch 904/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2737 - acc: 0.8496 - val_loss: 2.4315 - val_acc: 0.5076\n",
      "Epoch 905/1000\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 0.3060 - acc: 0.8200 - val_loss: 2.4177 - val_acc: 0.5227\n",
      "Epoch 906/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3952 - acc: 0.7767 - val_loss: 2.3872 - val_acc: 0.5076\n",
      "Epoch 907/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3230 - acc: 0.8186 - val_loss: 2.3924 - val_acc: 0.4864\n",
      "Epoch 908/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2891 - acc: 0.8288 - val_loss: 2.4499 - val_acc: 0.4683\n",
      "Epoch 909/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3281 - acc: 0.8130 - val_loss: 2.5075 - val_acc: 0.4441\n",
      "Epoch 910/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3953 - acc: 0.8098 - val_loss: 2.5002 - val_acc: 0.4532\n",
      "Epoch 911/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3852 - acc: 0.7901 - val_loss: 2.4320 - val_acc: 0.4864\n",
      "Epoch 912/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2905 - acc: 0.8359 - val_loss: 2.4939 - val_acc: 0.4683\n",
      "Epoch 913/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2826 - acc: 0.8485 - val_loss: 2.5274 - val_acc: 0.4713\n",
      "Epoch 914/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2998 - acc: 0.8418 - val_loss: 2.5466 - val_acc: 0.4653\n",
      "Epoch 915/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3070 - acc: 0.8422 - val_loss: 2.5146 - val_acc: 0.4562\n",
      "Epoch 916/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3122 - acc: 0.8422 - val_loss: 2.4959 - val_acc: 0.4622\n",
      "Epoch 917/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2834 - acc: 0.8482 - val_loss: 2.4816 - val_acc: 0.4743\n",
      "Epoch 918/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2836 - acc: 0.8566 - val_loss: 2.4570 - val_acc: 0.4773\n",
      "Epoch 919/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2752 - acc: 0.8552 - val_loss: 2.4929 - val_acc: 0.4864\n",
      "Epoch 920/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3708 - acc: 0.8179 - val_loss: 2.4883 - val_acc: 0.4804\n",
      "Epoch 921/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3305 - acc: 0.8306 - val_loss: 2.3665 - val_acc: 0.4894\n",
      "Epoch 922/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2853 - acc: 0.8426 - val_loss: 2.4211 - val_acc: 0.4743\n",
      "Epoch 923/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2652 - acc: 0.8573 - val_loss: 2.4322 - val_acc: 0.4955\n",
      "Epoch 924/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3050 - acc: 0.8348 - val_loss: 2.4835 - val_acc: 0.5015\n",
      "Epoch 925/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3567 - acc: 0.7989 - val_loss: 2.4672 - val_acc: 0.4894\n",
      "Epoch 926/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3162 - acc: 0.8052 - val_loss: 2.4635 - val_acc: 0.4804\n",
      "Epoch 927/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2937 - acc: 0.8172 - val_loss: 2.4715 - val_acc: 0.4985\n",
      "Epoch 928/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2920 - acc: 0.8510 - val_loss: 2.4857 - val_acc: 0.4864\n",
      "Epoch 929/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2588 - acc: 0.8588 - val_loss: 2.5262 - val_acc: 0.4834\n",
      "Epoch 930/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3082 - acc: 0.8172 - val_loss: 2.4419 - val_acc: 0.4985\n",
      "Epoch 931/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3229 - acc: 0.8147 - val_loss: 2.4363 - val_acc: 0.5045\n",
      "Epoch 932/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3352 - acc: 0.8003 - val_loss: 2.4383 - val_acc: 0.5106\n",
      "Epoch 933/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3072 - acc: 0.8154 - val_loss: 2.4437 - val_acc: 0.5045\n",
      "Epoch 934/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2806 - acc: 0.8397 - val_loss: 2.5896 - val_acc: 0.4834\n",
      "Epoch 935/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3085 - acc: 0.8390 - val_loss: 2.5691 - val_acc: 0.4804\n",
      "Epoch 936/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3469 - acc: 0.8306 - val_loss: 2.5199 - val_acc: 0.4955\n",
      "Epoch 937/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3174 - acc: 0.8394 - val_loss: 2.5193 - val_acc: 0.4804\n",
      "Epoch 938/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2834 - acc: 0.8528 - val_loss: 2.4562 - val_acc: 0.4864\n",
      "Epoch 939/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2709 - acc: 0.8478 - val_loss: 2.5042 - val_acc: 0.5136\n",
      "Epoch 940/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2904 - acc: 0.8337 - val_loss: 2.4571 - val_acc: 0.5106\n",
      "Epoch 941/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3671 - acc: 0.7985 - val_loss: 2.4927 - val_acc: 0.5136\n",
      "Epoch 942/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3268 - acc: 0.8144 - val_loss: 2.4638 - val_acc: 0.4743\n",
      "Epoch 943/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2844 - acc: 0.8471 - val_loss: 2.4744 - val_acc: 0.4894\n",
      "Epoch 944/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2773 - acc: 0.8580 - val_loss: 2.4785 - val_acc: 0.4804\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2620 - acc: 0.8507 - val_loss: 2.4797 - val_acc: 0.4804\n",
      "Epoch 946/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2850 - acc: 0.8492 - val_loss: 2.5642 - val_acc: 0.4653\n",
      "Epoch 947/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2791 - acc: 0.8563 - val_loss: 2.6377 - val_acc: 0.4532\n",
      "Epoch 948/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3217 - acc: 0.8418 - val_loss: 2.6458 - val_acc: 0.4532\n",
      "Epoch 949/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3255 - acc: 0.8337 - val_loss: 2.5759 - val_acc: 0.4622\n",
      "Epoch 950/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3131 - acc: 0.8440 - val_loss: 2.6168 - val_acc: 0.4411\n",
      "Epoch 951/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3006 - acc: 0.8422 - val_loss: 2.5850 - val_acc: 0.4653\n",
      "Epoch 952/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2876 - acc: 0.8499 - val_loss: 2.6165 - val_acc: 0.4683\n",
      "Epoch 953/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.2942 - acc: 0.8499 - val_loss: 2.6574 - val_acc: 0.4441\n",
      "Epoch 954/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2978 - acc: 0.8528 - val_loss: 2.5753 - val_acc: 0.4713\n",
      "Epoch 955/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.3197 - acc: 0.8362 - val_loss: 2.6281 - val_acc: 0.4502\n",
      "Epoch 956/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3241 - acc: 0.8267 - val_loss: 2.6396 - val_acc: 0.4381\n",
      "Epoch 957/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2934 - acc: 0.8450 - val_loss: 2.6030 - val_acc: 0.4502\n",
      "Epoch 958/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.2898 - acc: 0.8461 - val_loss: 2.5541 - val_acc: 0.4592\n",
      "Epoch 959/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2898 - acc: 0.8397 - val_loss: 2.6140 - val_acc: 0.4471\n",
      "Epoch 960/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3140 - acc: 0.8253 - val_loss: 2.6276 - val_acc: 0.4471\n",
      "Epoch 961/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3735 - acc: 0.8221 - val_loss: 2.4994 - val_acc: 0.4864\n",
      "Epoch 962/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3272 - acc: 0.8239 - val_loss: 2.4842 - val_acc: 0.4894\n",
      "Epoch 963/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2925 - acc: 0.8281 - val_loss: 2.4751 - val_acc: 0.4955\n",
      "Epoch 964/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2724 - acc: 0.8492 - val_loss: 2.4902 - val_acc: 0.4955\n",
      "Epoch 965/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2641 - acc: 0.8637 - val_loss: 2.5271 - val_acc: 0.5015\n",
      "Epoch 966/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2987 - acc: 0.8327 - val_loss: 2.4948 - val_acc: 0.5227\n",
      "Epoch 967/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3273 - acc: 0.8101 - val_loss: 2.4410 - val_acc: 0.5166\n",
      "Epoch 968/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2714 - acc: 0.8471 - val_loss: 2.4771 - val_acc: 0.5045\n",
      "Epoch 969/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.2676 - acc: 0.8482 - val_loss: 2.5176 - val_acc: 0.5106\n",
      "Epoch 970/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2778 - acc: 0.8499 - val_loss: 2.5820 - val_acc: 0.5015\n",
      "Epoch 971/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2675 - acc: 0.8521 - val_loss: 2.5849 - val_acc: 0.4864\n",
      "Epoch 972/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2626 - acc: 0.8542 - val_loss: 2.5401 - val_acc: 0.5136\n",
      "Epoch 973/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2759 - acc: 0.8415 - val_loss: 2.5857 - val_acc: 0.4894\n",
      "Epoch 974/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.3141 - acc: 0.8175 - val_loss: 2.6478 - val_acc: 0.4773\n",
      "Epoch 975/1000\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 0.3480 - acc: 0.776 - 0s 36us/step - loss: 0.3355 - acc: 0.8035 - val_loss: 2.6063 - val_acc: 0.4804\n",
      "Epoch 976/1000\n",
      "2839/2839 [==============================] - 0s 41us/step - loss: 0.2954 - acc: 0.8373 - val_loss: 2.5910 - val_acc: 0.4864\n",
      "Epoch 977/1000\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 0.2831 - acc: 0.8415 - val_loss: 2.5154 - val_acc: 0.5136\n",
      "Epoch 978/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2913 - acc: 0.8418 - val_loss: 2.5455 - val_acc: 0.4894\n",
      "Epoch 979/1000\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.2984 - acc: 0.8471 - val_loss: 2.5203 - val_acc: 0.5076\n",
      "Epoch 980/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2916 - acc: 0.8503 - val_loss: 2.5685 - val_acc: 0.5106\n",
      "Epoch 981/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2863 - acc: 0.8271 - val_loss: 2.5966 - val_acc: 0.4985\n",
      "Epoch 982/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2781 - acc: 0.8397 - val_loss: 2.6044 - val_acc: 0.5076\n",
      "Epoch 983/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2767 - acc: 0.8521 - val_loss: 2.5928 - val_acc: 0.5136\n",
      "Epoch 984/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.2967 - acc: 0.8221 - val_loss: 2.5694 - val_acc: 0.5045\n",
      "Epoch 985/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2662 - acc: 0.8531 - val_loss: 2.6344 - val_acc: 0.4924\n",
      "Epoch 986/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2568 - acc: 0.8609 - val_loss: 2.6829 - val_acc: 0.4743\n",
      "Epoch 987/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2590 - acc: 0.8573 - val_loss: 2.5948 - val_acc: 0.5015\n",
      "Epoch 988/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2610 - acc: 0.8690 - val_loss: 2.7245 - val_acc: 0.4592\n",
      "Epoch 989/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.3426 - acc: 0.8295 - val_loss: 2.6956 - val_acc: 0.4471\n",
      "Epoch 990/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.4858 - acc: 0.7679 - val_loss: 2.5839 - val_acc: 0.4562\n",
      "Epoch 991/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.3143 - acc: 0.8066 - val_loss: 2.5929 - val_acc: 0.4834\n",
      "Epoch 992/1000\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.2598 - acc: 0.8531 - val_loss: 2.5779 - val_acc: 0.4894\n",
      "Epoch 993/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2592 - acc: 0.8531 - val_loss: 2.5546 - val_acc: 0.4924\n",
      "Epoch 994/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2587 - acc: 0.8637 - val_loss: 2.6138 - val_acc: 0.4834\n",
      "Epoch 995/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2714 - acc: 0.8588 - val_loss: 2.5827 - val_acc: 0.4955\n",
      "Epoch 996/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2635 - acc: 0.8549 - val_loss: 2.6478 - val_acc: 0.5045\n",
      "Epoch 997/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2843 - acc: 0.8344 - val_loss: 2.6001 - val_acc: 0.4894\n",
      "Epoch 998/1000\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 0.2886 - acc: 0.8352 - val_loss: 2.6179 - val_acc: 0.4955\n",
      "Epoch 999/1000\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.2998 - acc: 0.8175 - val_loss: 2.5810 - val_acc: 0.5257\n",
      "Epoch 1000/1000\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.2989 - acc: 0.8440 - val_loss: 2.6420 - val_acc: 0.5106\n"
     ]
    }
   ],
   "source": [
    "business_model_2 = model.fit(x=X_train_business, y=y_cat_train_business, \n",
    "          batch_size=2000, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_business, y_cat_test_business),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/250\n",
      "2839/2839 [==============================] - 1s 516us/step - loss: 2.1981 - acc: 0.2360 - val_loss: 1.8420 - val_acc: 0.3293\n",
      "Epoch 2/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.7698 - acc: 0.3149 - val_loss: 1.8009 - val_acc: 0.3323\n",
      "Epoch 3/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.7221 - acc: 0.3325 - val_loss: 1.7912 - val_acc: 0.3323\n",
      "Epoch 4/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.6885 - acc: 0.3540 - val_loss: 1.7672 - val_acc: 0.3323\n",
      "Epoch 5/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6601 - acc: 0.3614 - val_loss: 1.7720 - val_acc: 0.3323\n",
      "Epoch 6/250\n",
      "2839/2839 [==============================] - 0s 36us/step - loss: 1.6488 - acc: 0.3850 - val_loss: 1.7565 - val_acc: 0.3323\n",
      "Epoch 7/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.6404 - acc: 0.3572 - val_loss: 1.7659 - val_acc: 0.3293\n",
      "Epoch 8/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6382 - acc: 0.3677 - val_loss: 1.7343 - val_acc: 0.3323\n",
      "Epoch 9/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.6078 - acc: 0.3801 - val_loss: 1.7448 - val_acc: 0.3323\n",
      "Epoch 10/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.6021 - acc: 0.3751 - val_loss: 1.7282 - val_acc: 0.3323\n",
      "Epoch 11/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.5920 - acc: 0.3709 - val_loss: 1.7005 - val_acc: 0.3293\n",
      "Epoch 12/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5809 - acc: 0.3758 - val_loss: 1.6796 - val_acc: 0.3323\n",
      "Epoch 13/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5698 - acc: 0.3794 - val_loss: 1.6652 - val_acc: 0.3293\n",
      "Epoch 14/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5734 - acc: 0.3765 - val_loss: 1.6677 - val_acc: 0.3293\n",
      "Epoch 15/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.5554 - acc: 0.3751 - val_loss: 1.6973 - val_acc: 0.3263\n",
      "Epoch 16/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5563 - acc: 0.3610 - val_loss: 1.6713 - val_acc: 0.3293\n",
      "Epoch 17/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5357 - acc: 0.3836 - val_loss: 1.6613 - val_acc: 0.3293\n",
      "Epoch 18/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5334 - acc: 0.3875 - val_loss: 1.6403 - val_acc: 0.3293\n",
      "Epoch 19/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5073 - acc: 0.3889 - val_loss: 1.6341 - val_acc: 0.3323\n",
      "Epoch 20/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.5172 - acc: 0.3762 - val_loss: 1.6253 - val_acc: 0.3263\n",
      "Epoch 21/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.4918 - acc: 0.3822 - val_loss: 1.6290 - val_acc: 0.3263\n",
      "Epoch 22/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4979 - acc: 0.3663 - val_loss: 1.5957 - val_acc: 0.3293\n",
      "Epoch 23/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.5000 - acc: 0.3839 - val_loss: 1.6033 - val_acc: 0.3263\n",
      "Epoch 24/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4848 - acc: 0.3903 - val_loss: 1.6281 - val_acc: 0.3263\n",
      "Epoch 25/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4735 - acc: 0.3839 - val_loss: 1.5804 - val_acc: 0.3263\n",
      "Epoch 26/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.4637 - acc: 0.3832 - val_loss: 1.5671 - val_acc: 0.3323\n",
      "Epoch 27/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4629 - acc: 0.3956 - val_loss: 1.6194 - val_acc: 0.3233\n",
      "Epoch 28/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4434 - acc: 0.3913 - val_loss: 1.6161 - val_acc: 0.3233\n",
      "Epoch 29/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.4579 - acc: 0.3755 - val_loss: 1.6134 - val_acc: 0.3263\n",
      "Epoch 30/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4288 - acc: 0.3906 - val_loss: 1.6175 - val_acc: 0.3112\n",
      "Epoch 31/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.4193 - acc: 0.3991 - val_loss: 1.6595 - val_acc: 0.2689\n",
      "Epoch 32/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4245 - acc: 0.3720 - val_loss: 1.5601 - val_acc: 0.3263\n",
      "Epoch 33/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4193 - acc: 0.3878 - val_loss: 1.5369 - val_acc: 0.3233\n",
      "Epoch 34/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4170 - acc: 0.3966 - val_loss: 1.5615 - val_acc: 0.3233\n",
      "Epoch 35/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.4023 - acc: 0.4051 - val_loss: 1.5729 - val_acc: 0.3233\n",
      "Epoch 36/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3979 - acc: 0.3959 - val_loss: 1.5208 - val_acc: 0.3233\n",
      "Epoch 37/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3957 - acc: 0.3987 - val_loss: 1.5260 - val_acc: 0.3293\n",
      "Epoch 38/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3989 - acc: 0.3991 - val_loss: 1.5192 - val_acc: 0.3263\n",
      "Epoch 39/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3815 - acc: 0.4156 - val_loss: 1.5501 - val_acc: 0.3263\n",
      "Epoch 40/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.3627 - acc: 0.4058 - val_loss: 1.6056 - val_acc: 0.2991\n",
      "Epoch 41/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3769 - acc: 0.3815 - val_loss: 1.5674 - val_acc: 0.3233\n",
      "Epoch 42/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3592 - acc: 0.3970 - val_loss: 1.5259 - val_acc: 0.3233\n",
      "Epoch 43/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3493 - acc: 0.3977 - val_loss: 1.5067 - val_acc: 0.3233\n",
      "Epoch 44/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3638 - acc: 0.3977 - val_loss: 1.4995 - val_acc: 0.3233\n",
      "Epoch 45/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3462 - acc: 0.4026 - val_loss: 1.4877 - val_acc: 0.3233\n",
      "Epoch 46/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3462 - acc: 0.3998 - val_loss: 1.4590 - val_acc: 0.3293\n",
      "Epoch 47/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.3308 - acc: 0.4089 - val_loss: 1.5078 - val_acc: 0.3263\n",
      "Epoch 48/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3387 - acc: 0.4086 - val_loss: 1.4750 - val_acc: 0.3263\n",
      "Epoch 49/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3209 - acc: 0.4132 - val_loss: 1.4913 - val_acc: 0.3293\n",
      "Epoch 50/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3295 - acc: 0.4047 - val_loss: 1.4510 - val_acc: 0.3293\n",
      "Epoch 51/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3182 - acc: 0.4128 - val_loss: 1.4562 - val_acc: 0.3293\n",
      "Epoch 52/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3127 - acc: 0.4216 - val_loss: 1.4876 - val_acc: 0.3172\n",
      "Epoch 53/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.3083 - acc: 0.4097 - val_loss: 1.4467 - val_acc: 0.3233\n",
      "Epoch 54/250\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.3133 - acc: 0.4167 - val_loss: 1.4445 - val_acc: 0.3263\n",
      "Epoch 55/250\n",
      "2839/2839 [==============================] - 0s 48us/step - loss: 1.3081 - acc: 0.4234 - val_loss: 1.5528 - val_acc: 0.3112\n",
      "Epoch 56/250\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.3041 - acc: 0.4104 - val_loss: 1.6068 - val_acc: 0.2991\n",
      "Epoch 57/250\n",
      "2839/2839 [==============================] - 0s 37us/step - loss: 1.3116 - acc: 0.3987 - val_loss: 1.5433 - val_acc: 0.2991\n",
      "Epoch 58/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3034 - acc: 0.3934 - val_loss: 1.5707 - val_acc: 0.2991\n",
      "Epoch 59/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.3090 - acc: 0.3910 - val_loss: 1.5046 - val_acc: 0.3051\n",
      "Epoch 60/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2869 - acc: 0.4121 - val_loss: 1.5097 - val_acc: 0.2961\n",
      "Epoch 61/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2874 - acc: 0.4280 - val_loss: 1.5474 - val_acc: 0.2719\n",
      "Epoch 62/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2810 - acc: 0.4160 - val_loss: 1.4881 - val_acc: 0.3112\n",
      "Epoch 63/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.2770 - acc: 0.4216 - val_loss: 1.5005 - val_acc: 0.3082\n",
      "Epoch 64/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2756 - acc: 0.4132 - val_loss: 1.5105 - val_acc: 0.3051\n",
      "Epoch 65/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2736 - acc: 0.4280 - val_loss: 1.4736 - val_acc: 0.3172\n",
      "Epoch 66/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2560 - acc: 0.4160 - val_loss: 1.4788 - val_acc: 0.3142\n",
      "Epoch 67/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2512 - acc: 0.4389 - val_loss: 1.4785 - val_acc: 0.3293\n",
      "Epoch 68/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2703 - acc: 0.4223 - val_loss: 1.3919 - val_acc: 0.3384\n",
      "Epoch 69/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2785 - acc: 0.4259 - val_loss: 1.4144 - val_acc: 0.3474\n",
      "Epoch 70/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2451 - acc: 0.4325 - val_loss: 1.4497 - val_acc: 0.3293\n",
      "Epoch 71/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2459 - acc: 0.4403 - val_loss: 1.3971 - val_acc: 0.3474\n",
      "Epoch 72/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2599 - acc: 0.4202 - val_loss: 1.3592 - val_acc: 0.3776\n",
      "Epoch 73/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2798 - acc: 0.4480 - val_loss: 1.4251 - val_acc: 0.3595\n",
      "Epoch 74/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2395 - acc: 0.4375 - val_loss: 1.3963 - val_acc: 0.3837\n",
      "Epoch 75/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2332 - acc: 0.4368 - val_loss: 1.4038 - val_acc: 0.3927\n",
      "Epoch 76/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2368 - acc: 0.4498 - val_loss: 1.4030 - val_acc: 0.4199\n",
      "Epoch 77/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2420 - acc: 0.4565 - val_loss: 1.3660 - val_acc: 0.3988\n",
      "Epoch 78/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2347 - acc: 0.4512 - val_loss: 1.3650 - val_acc: 0.3958\n",
      "Epoch 79/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2346 - acc: 0.4428 - val_loss: 1.3922 - val_acc: 0.3776\n",
      "Epoch 80/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2290 - acc: 0.4449 - val_loss: 1.4325 - val_acc: 0.3505\n",
      "Epoch 81/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2230 - acc: 0.4435 - val_loss: 1.3721 - val_acc: 0.3988\n",
      "Epoch 82/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2204 - acc: 0.4438 - val_loss: 1.3306 - val_acc: 0.4381\n",
      "Epoch 83/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.2392 - acc: 0.4642 - val_loss: 1.3829 - val_acc: 0.3807\n",
      "Epoch 84/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2123 - acc: 0.4445 - val_loss: 1.3917 - val_acc: 0.3988\n",
      "Epoch 85/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1907 - acc: 0.4597 - val_loss: 1.4157 - val_acc: 0.4139\n",
      "Epoch 86/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2034 - acc: 0.4495 - val_loss: 1.4629 - val_acc: 0.3958\n",
      "Epoch 87/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2220 - acc: 0.4375 - val_loss: 1.4368 - val_acc: 0.4018\n",
      "Epoch 88/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1970 - acc: 0.4583 - val_loss: 1.4139 - val_acc: 0.4441\n",
      "Epoch 89/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1956 - acc: 0.4526 - val_loss: 1.3209 - val_acc: 0.5468\n",
      "Epoch 90/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2582 - acc: 0.4576 - val_loss: 1.3746 - val_acc: 0.4924\n",
      "Epoch 91/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.2231 - acc: 0.4776 - val_loss: 1.4594 - val_acc: 0.3837\n",
      "Epoch 92/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.2069 - acc: 0.4382 - val_loss: 1.4632 - val_acc: 0.3746\n",
      "Epoch 93/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.2072 - acc: 0.4456 - val_loss: 1.4334 - val_acc: 0.4199\n",
      "Epoch 94/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1940 - acc: 0.4533 - val_loss: 1.3424 - val_acc: 0.5076\n",
      "Epoch 95/250\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.2103 - acc: 0.4759 - val_loss: 1.3438 - val_acc: 0.4894\n",
      "Epoch 96/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1884 - acc: 0.4748 - val_loss: 1.4028 - val_acc: 0.4411\n",
      "Epoch 97/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1735 - acc: 0.4678 - val_loss: 1.4259 - val_acc: 0.4230\n",
      "Epoch 98/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1936 - acc: 0.4505 - val_loss: 1.4417 - val_acc: 0.4018\n",
      "Epoch 99/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1772 - acc: 0.4561 - val_loss: 1.4079 - val_acc: 0.4169\n",
      "Epoch 100/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1918 - acc: 0.4653 - val_loss: 1.3936 - val_acc: 0.3958\n",
      "Epoch 101/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1872 - acc: 0.4621 - val_loss: 1.4668 - val_acc: 0.3927\n",
      "Epoch 102/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.2054 - acc: 0.4354 - val_loss: 1.4398 - val_acc: 0.4139\n",
      "Epoch 103/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1724 - acc: 0.4667 - val_loss: 1.3509 - val_acc: 0.4411\n",
      "Epoch 104/250\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.1737 - acc: 0.4650 - val_loss: 1.3969 - val_acc: 0.4230\n",
      "Epoch 105/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.1810 - acc: 0.4590 - val_loss: 1.4108 - val_acc: 0.4230\n",
      "Epoch 106/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1742 - acc: 0.4551 - val_loss: 1.3999 - val_acc: 0.4350\n",
      "Epoch 107/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1707 - acc: 0.4678 - val_loss: 1.4577 - val_acc: 0.4199\n",
      "Epoch 108/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.1862 - acc: 0.4664 - val_loss: 1.4094 - val_acc: 0.4350\n",
      "Epoch 109/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1632 - acc: 0.4681 - val_loss: 1.4092 - val_acc: 0.4109\n",
      "Epoch 110/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1726 - acc: 0.4727 - val_loss: 1.3724 - val_acc: 0.4471\n",
      "Epoch 111/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1635 - acc: 0.4600 - val_loss: 1.3778 - val_acc: 0.4381\n",
      "Epoch 112/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1623 - acc: 0.4706 - val_loss: 1.4530 - val_acc: 0.3958\n",
      "Epoch 113/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1843 - acc: 0.4399 - val_loss: 1.4191 - val_acc: 0.4199\n",
      "Epoch 114/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1698 - acc: 0.4569 - val_loss: 1.4057 - val_acc: 0.4532\n",
      "Epoch 115/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1427 - acc: 0.4709 - val_loss: 1.3659 - val_acc: 0.4894\n",
      "Epoch 116/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1455 - acc: 0.4836 - val_loss: 1.3622 - val_acc: 0.4864\n",
      "Epoch 117/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1410 - acc: 0.4850 - val_loss: 1.2938 - val_acc: 0.5468\n",
      "Epoch 118/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1586 - acc: 0.4914 - val_loss: 1.3022 - val_acc: 0.5347\n",
      "Epoch 119/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1585 - acc: 0.5044 - val_loss: 1.2796 - val_acc: 0.5317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1531 - acc: 0.4921 - val_loss: 1.2831 - val_acc: 0.5196\n",
      "Epoch 121/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1507 - acc: 0.4942 - val_loss: 1.2572 - val_acc: 0.5378\n",
      "Epoch 122/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1620 - acc: 0.4921 - val_loss: 1.3392 - val_acc: 0.4894\n",
      "Epoch 123/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1324 - acc: 0.5016 - val_loss: 1.3508 - val_acc: 0.4864\n",
      "Epoch 124/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1399 - acc: 0.4727 - val_loss: 1.3344 - val_acc: 0.4743\n",
      "Epoch 125/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1372 - acc: 0.4716 - val_loss: 1.3188 - val_acc: 0.4864\n",
      "Epoch 126/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1265 - acc: 0.4826 - val_loss: 1.3532 - val_acc: 0.4804\n",
      "Epoch 127/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1518 - acc: 0.4762 - val_loss: 1.4332 - val_acc: 0.4199\n",
      "Epoch 128/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1625 - acc: 0.4456 - val_loss: 1.3695 - val_acc: 0.4622\n",
      "Epoch 129/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1139 - acc: 0.4773 - val_loss: 1.3271 - val_acc: 0.4985\n",
      "Epoch 130/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1207 - acc: 0.4882 - val_loss: 1.3603 - val_acc: 0.4864\n",
      "Epoch 131/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1274 - acc: 0.4889 - val_loss: 1.4295 - val_acc: 0.4169\n",
      "Epoch 132/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1515 - acc: 0.4692 - val_loss: 1.3695 - val_acc: 0.4592\n",
      "Epoch 133/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1256 - acc: 0.4783 - val_loss: 1.3133 - val_acc: 0.5045\n",
      "Epoch 134/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1233 - acc: 0.4942 - val_loss: 1.3613 - val_acc: 0.4622\n",
      "Epoch 135/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1424 - acc: 0.4889 - val_loss: 1.3661 - val_acc: 0.4562\n",
      "Epoch 136/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1422 - acc: 0.4713 - val_loss: 1.3326 - val_acc: 0.4683\n",
      "Epoch 137/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1065 - acc: 0.4868 - val_loss: 1.2964 - val_acc: 0.5076\n",
      "Epoch 138/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1138 - acc: 0.4914 - val_loss: 1.2518 - val_acc: 0.5498\n",
      "Epoch 139/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1320 - acc: 0.4910 - val_loss: 1.2424 - val_acc: 0.5317\n",
      "Epoch 140/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1438 - acc: 0.5248 - val_loss: 1.2925 - val_acc: 0.5106\n",
      "Epoch 141/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0886 - acc: 0.5153 - val_loss: 1.2778 - val_acc: 0.5045\n",
      "Epoch 142/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.0997 - acc: 0.5079 - val_loss: 1.2732 - val_acc: 0.5166\n",
      "Epoch 143/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.1009 - acc: 0.5097 - val_loss: 1.2593 - val_acc: 0.5378\n",
      "Epoch 144/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1014 - acc: 0.4977 - val_loss: 1.2344 - val_acc: 0.5408\n",
      "Epoch 145/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1356 - acc: 0.4995 - val_loss: 1.2986 - val_acc: 0.5015\n",
      "Epoch 146/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1761 - acc: 0.5083 - val_loss: 1.2935 - val_acc: 0.4955\n",
      "Epoch 147/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.1063 - acc: 0.5062 - val_loss: 1.2807 - val_acc: 0.5015\n",
      "Epoch 148/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0941 - acc: 0.5118 - val_loss: 1.2973 - val_acc: 0.4955\n",
      "Epoch 149/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0854 - acc: 0.5174 - val_loss: 1.3479 - val_acc: 0.4381\n",
      "Epoch 150/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0967 - acc: 0.4952 - val_loss: 1.4251 - val_acc: 0.4048\n",
      "Epoch 151/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1253 - acc: 0.4741 - val_loss: 1.3375 - val_acc: 0.4924\n",
      "Epoch 152/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1132 - acc: 0.4871 - val_loss: 1.4136 - val_acc: 0.4079\n",
      "Epoch 153/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1435 - acc: 0.4586 - val_loss: 1.3472 - val_acc: 0.4653\n",
      "Epoch 154/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1001 - acc: 0.4896 - val_loss: 1.2984 - val_acc: 0.5076\n",
      "Epoch 155/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0682 - acc: 0.5069 - val_loss: 1.3034 - val_acc: 0.5227\n",
      "Epoch 156/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0725 - acc: 0.5065 - val_loss: 1.2506 - val_acc: 0.5045\n",
      "Epoch 157/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0974 - acc: 0.5058 - val_loss: 1.2380 - val_acc: 0.5106\n",
      "Epoch 158/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1037 - acc: 0.5079 - val_loss: 1.2452 - val_acc: 0.5106\n",
      "Epoch 159/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1005 - acc: 0.5269 - val_loss: 1.3043 - val_acc: 0.4773\n",
      "Epoch 160/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.1177 - acc: 0.5030 - val_loss: 1.3153 - val_acc: 0.4773\n",
      "Epoch 161/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0903 - acc: 0.5195 - val_loss: 1.2973 - val_acc: 0.4955\n",
      "Epoch 162/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0721 - acc: 0.5065 - val_loss: 1.2987 - val_acc: 0.5045\n",
      "Epoch 163/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0557 - acc: 0.5224 - val_loss: 1.2292 - val_acc: 0.5408\n",
      "Epoch 164/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0825 - acc: 0.5259 - val_loss: 1.2777 - val_acc: 0.5015\n",
      "Epoch 165/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.1228 - acc: 0.5114 - val_loss: 1.3020 - val_acc: 0.4683\n",
      "Epoch 166/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0721 - acc: 0.5097 - val_loss: 1.2739 - val_acc: 0.4985\n",
      "Epoch 167/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0611 - acc: 0.5136 - val_loss: 1.2483 - val_acc: 0.5106\n",
      "Epoch 168/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0878 - acc: 0.5262 - val_loss: 1.3009 - val_acc: 0.4804\n",
      "Epoch 169/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0920 - acc: 0.5150 - val_loss: 1.2522 - val_acc: 0.4955\n",
      "Epoch 170/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0977 - acc: 0.5231 - val_loss: 1.3081 - val_acc: 0.4713\n",
      "Epoch 171/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0588 - acc: 0.5255 - val_loss: 1.3220 - val_acc: 0.4592\n",
      "Epoch 172/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0745 - acc: 0.5058 - val_loss: 1.3734 - val_acc: 0.4502\n",
      "Epoch 173/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0631 - acc: 0.5044 - val_loss: 1.3030 - val_acc: 0.4743\n",
      "Epoch 174/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0574 - acc: 0.5114 - val_loss: 1.2754 - val_acc: 0.4834\n",
      "Epoch 175/250\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0776 - acc: 0.5125 - val_loss: 1.2684 - val_acc: 0.5015\n",
      "Epoch 176/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1200 - acc: 0.5269 - val_loss: 1.2406 - val_acc: 0.5227\n",
      "Epoch 177/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0858 - acc: 0.5217 - val_loss: 1.2655 - val_acc: 0.4924\n",
      "Epoch 178/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0541 - acc: 0.5333 - val_loss: 1.2970 - val_acc: 0.4804\n",
      "Epoch 179/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.0365 - acc: 0.5308 - val_loss: 1.3815 - val_acc: 0.4350\n",
      "Epoch 180/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0683 - acc: 0.5139 - val_loss: 1.3955 - val_acc: 0.4139\n",
      "Epoch 181/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0782 - acc: 0.4931 - val_loss: 1.3770 - val_acc: 0.4471\n",
      "Epoch 182/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0878 - acc: 0.4935 - val_loss: 1.3362 - val_acc: 0.4622\n",
      "Epoch 183/250\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0639 - acc: 0.4924 - val_loss: 1.2986 - val_acc: 0.4894\n",
      "Epoch 184/250\n",
      "2839/2839 [==============================] - 0s 35us/step - loss: 1.0547 - acc: 0.5150 - val_loss: 1.3317 - val_acc: 0.4653\n",
      "Epoch 185/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0668 - acc: 0.4868 - val_loss: 1.3422 - val_acc: 0.4622\n",
      "Epoch 186/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0591 - acc: 0.5072 - val_loss: 1.3279 - val_acc: 0.4713\n",
      "Epoch 187/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0460 - acc: 0.5016 - val_loss: 1.3262 - val_acc: 0.4773\n",
      "Epoch 188/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0645 - acc: 0.5019 - val_loss: 1.2840 - val_acc: 0.4985\n",
      "Epoch 189/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0376 - acc: 0.5019 - val_loss: 1.2892 - val_acc: 0.4955\n",
      "Epoch 190/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0223 - acc: 0.5213 - val_loss: 1.3329 - val_acc: 0.4502\n",
      "Epoch 191/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0428 - acc: 0.5164 - val_loss: 1.4490 - val_acc: 0.4109\n",
      "Epoch 192/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.1083 - acc: 0.4706 - val_loss: 1.3992 - val_acc: 0.4199\n",
      "Epoch 193/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0548 - acc: 0.5023 - val_loss: 1.2915 - val_acc: 0.4924\n",
      "Epoch 194/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0362 - acc: 0.5146 - val_loss: 1.3073 - val_acc: 0.4743\n",
      "Epoch 195/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0171 - acc: 0.5195 - val_loss: 1.4022 - val_acc: 0.4260\n",
      "Epoch 196/250\n",
      "2839/2839 [==============================] - 0s 39us/step - loss: 1.0739 - acc: 0.4981 - val_loss: 1.3643 - val_acc: 0.4411\n",
      "Epoch 197/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0312 - acc: 0.5118 - val_loss: 1.3220 - val_acc: 0.4622\n",
      "Epoch 198/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0261 - acc: 0.5224 - val_loss: 1.3290 - val_acc: 0.4622\n",
      "Epoch 199/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.1019 - acc: 0.4952 - val_loss: 1.3622 - val_acc: 0.4411\n",
      "Epoch 200/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0558 - acc: 0.4956 - val_loss: 1.3567 - val_acc: 0.4532\n",
      "Epoch 201/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0343 - acc: 0.5125 - val_loss: 1.3383 - val_acc: 0.4532\n",
      "Epoch 202/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9994 - acc: 0.5424 - val_loss: 1.2741 - val_acc: 0.4864\n",
      "Epoch 203/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0322 - acc: 0.5213 - val_loss: 1.2545 - val_acc: 0.4894\n",
      "Epoch 204/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0548 - acc: 0.5245 - val_loss: 1.2838 - val_acc: 0.4683\n",
      "Epoch 205/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0373 - acc: 0.5231 - val_loss: 1.2767 - val_acc: 0.4773\n",
      "Epoch 206/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0566 - acc: 0.5259 - val_loss: 1.2617 - val_acc: 0.4894\n",
      "Epoch 207/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0663 - acc: 0.5319 - val_loss: 1.3774 - val_acc: 0.4320\n",
      "Epoch 208/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 1.0381 - acc: 0.5305 - val_loss: 1.3303 - val_acc: 0.4471\n",
      "Epoch 209/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0262 - acc: 0.5308 - val_loss: 1.3555 - val_acc: 0.4381\n",
      "Epoch 210/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0478 - acc: 0.5030 - val_loss: 1.4287 - val_acc: 0.4169\n",
      "Epoch 211/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0720 - acc: 0.4910 - val_loss: 1.3955 - val_acc: 0.4230\n",
      "Epoch 212/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0314 - acc: 0.5079 - val_loss: 1.3554 - val_acc: 0.4471\n",
      "Epoch 213/250\n",
      "2839/2839 [==============================] - 0s 34us/step - loss: 1.0312 - acc: 0.5350 - val_loss: 1.3023 - val_acc: 0.4653\n",
      "Epoch 214/250\n",
      "2839/2839 [==============================] - 0s 29us/step - loss: 1.0320 - acc: 0.5368 - val_loss: 1.2780 - val_acc: 0.4924\n",
      "Epoch 215/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0442 - acc: 0.5259 - val_loss: 1.2607 - val_acc: 0.4804\n",
      "Epoch 216/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0370 - acc: 0.5527 - val_loss: 1.2831 - val_acc: 0.4834\n",
      "Epoch 217/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0005 - acc: 0.5484 - val_loss: 1.2726 - val_acc: 0.4924\n",
      "Epoch 218/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9914 - acc: 0.5530 - val_loss: 1.3107 - val_acc: 0.4743\n",
      "Epoch 219/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0031 - acc: 0.5273 - val_loss: 1.2528 - val_acc: 0.5196\n",
      "Epoch 220/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0364 - acc: 0.5181 - val_loss: 1.2516 - val_acc: 0.5166\n",
      "Epoch 221/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0266 - acc: 0.5298 - val_loss: 1.2792 - val_acc: 0.4924\n",
      "Epoch 222/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0061 - acc: 0.5516 - val_loss: 1.3838 - val_acc: 0.4290\n",
      "Epoch 223/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0916 - acc: 0.4952 - val_loss: 1.3897 - val_acc: 0.4290\n",
      "Epoch 224/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0289 - acc: 0.5100 - val_loss: 1.3630 - val_acc: 0.4532\n",
      "Epoch 225/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9964 - acc: 0.5372 - val_loss: 1.3072 - val_acc: 0.4773\n",
      "Epoch 226/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9763 - acc: 0.5520 - val_loss: 1.3046 - val_acc: 0.4743\n",
      "Epoch 227/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0069 - acc: 0.5340 - val_loss: 1.3956 - val_acc: 0.4441\n",
      "Epoch 228/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0841 - acc: 0.4956 - val_loss: 1.3229 - val_acc: 0.4864\n",
      "Epoch 229/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9953 - acc: 0.5340 - val_loss: 1.3512 - val_acc: 0.4622\n",
      "Epoch 230/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9868 - acc: 0.5470 - val_loss: 1.3551 - val_acc: 0.4471\n",
      "Epoch 231/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9784 - acc: 0.5523 - val_loss: 1.3881 - val_acc: 0.4320\n",
      "Epoch 232/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0267 - acc: 0.5266 - val_loss: 1.4632 - val_acc: 0.4169\n",
      "Epoch 233/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0531 - acc: 0.5139 - val_loss: 1.4112 - val_acc: 0.4169\n",
      "Epoch 234/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0440 - acc: 0.4988 - val_loss: 1.3797 - val_acc: 0.4350\n",
      "Epoch 235/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0286 - acc: 0.5153 - val_loss: 1.3187 - val_acc: 0.4924\n",
      "Epoch 236/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0161 - acc: 0.5220 - val_loss: 1.3185 - val_acc: 0.4834\n",
      "Epoch 237/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0070 - acc: 0.5248 - val_loss: 1.3425 - val_acc: 0.4622\n",
      "Epoch 238/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9869 - acc: 0.5266 - val_loss: 1.3365 - val_acc: 0.4683\n",
      "Epoch 239/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 0.9885 - acc: 0.5315 - val_loss: 1.4109 - val_acc: 0.4320\n",
      "Epoch 240/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0420 - acc: 0.5178 - val_loss: 1.3792 - val_acc: 0.4471\n",
      "Epoch 241/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0174 - acc: 0.5076 - val_loss: 1.2851 - val_acc: 0.4894\n",
      "Epoch 242/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9850 - acc: 0.5347 - val_loss: 1.3016 - val_acc: 0.5076\n",
      "Epoch 243/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 1.0172 - acc: 0.5217 - val_loss: 1.3259 - val_acc: 0.4713\n",
      "Epoch 244/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 1.0203 - acc: 0.5245 - val_loss: 1.3556 - val_acc: 0.4502\n",
      "Epoch 245/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9970 - acc: 0.5273 - val_loss: 1.3932 - val_acc: 0.4199\n",
      "Epoch 246/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9934 - acc: 0.5210 - val_loss: 1.3947 - val_acc: 0.4260\n",
      "Epoch 247/250\n",
      "2839/2839 [==============================] - 0s 30us/step - loss: 0.9993 - acc: 0.5315 - val_loss: 1.4238 - val_acc: 0.4230\n",
      "Epoch 248/250\n",
      "2839/2839 [==============================] - 0s 31us/step - loss: 1.0000 - acc: 0.5273 - val_loss: 1.3386 - val_acc: 0.4653\n",
      "Epoch 249/250\n",
      "2839/2839 [==============================] - 0s 32us/step - loss: 0.9787 - acc: 0.5358 - val_loss: 1.3626 - val_acc: 0.4471\n",
      "Epoch 250/250\n",
      "2839/2839 [==============================] - 0s 33us/step - loss: 0.9919 - acc: 0.5220 - val_loss: 1.3412 - val_acc: 0.4683\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gain_large_true_large' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-0ce54a57d9a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mgain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain_true_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain_stay_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfour_five_split_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfour_five_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfour_five_split_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes_gain_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes_gain_large_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes_gain_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cat_test_business\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdf_business_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pred_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf_business_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gain_10%'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain_large_true_large\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mdf_business_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gain_3%'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdf_business_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gain_mean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgain_stay_true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gain_large_true_large' is not defined"
     ]
    }
   ],
   "source": [
    "df_business_analysis = pd.DataFrame()\n",
    "#, kernel_regularizer=regularizers.l1(0.0001)\n",
    "for num in range(5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_shape=(130,), activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    ada = keras.optimizers.Adagrad()\n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "    sgd = keras.optimizers.SGD(lr=0.75)\n",
    "    model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    business_model_2 = model.fit(x=X_train_business, y=y_cat_train_business, \n",
    "          batch_size=2000, \n",
    "          epochs=250, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_business, y_cat_test_business),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)\n",
    "    \n",
    "    predictions = model.predict(X_test_business)\n",
    "    loss = business_model_2.history['val_loss'][-1]\n",
    "    acc = business_model_2.history['val_acc'][-1]\n",
    "    gain_pred, gain_true_large, gain_true, gain_stay_true, four_five_split_large, four_five_split, four_five_split_mean, predicted_classes_gain_true, predicted_classes_gain_large_true, predicted_classes_gain_mean = model_metrics(predictions, y_cat_test_business)\n",
    "    df_business_analysis.loc[num,'pred_count'] = gain_pred\n",
    "    df_business_analysis.loc[num,'gain_10%'] = gain_large_true_large\n",
    "    df_business_analysis.loc[num,'gain_3%'] = gain_true\n",
    "    df_business_analysis.loc[num,'gain_mean'] = gain_stay_true\n",
    "    df_business_analysis.loc[num,'top_pred_prob_10%'] = four_five_split_large\n",
    "    df_business_analysis.loc[num,'top_pred_prob_3%'] = four_five_split\n",
    "    df_business_analysis.loc[num,'top_pred_prob_mean'] = four_five_split_mean\n",
    "    df_business_analysis.loc[num,'model_pred_prob_10%'] = predicted_classes_gain_large_true\n",
    "    df_business_analysis.loc[num,'model_pred_prob_3%'] = predicted_classes_gain_true\n",
    "    df_business_analysis.loc[num,'model_pred_prob_mean'] = predicted_classes_gain_mean\n",
    "    df_business_analysis.loc[num,'loss'] = loss\n",
    "    df_business_analysis.loc[num,'acc'] = acc\n",
    "    print(num)\n",
    "df_175_w_b = df_business_analysis\n",
    "df_business_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.072848</td>\n",
       "      <td>0.516556</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>1.355781</td>\n",
       "      <td>0.489426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.943925</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.578571</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>1.449290</td>\n",
       "      <td>0.450151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.873874</td>\n",
       "      <td>1.410280</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.516393</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>1.345150</td>\n",
       "      <td>0.489426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>83.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.106796</td>\n",
       "      <td>0.592233</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>1.437661</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       115.0  0.094972  0.730435   0.947826           0.242424   \n",
       "1       107.0  0.094972  0.747664   0.943925           0.242424   \n",
       "2        90.0  0.094972  0.777778   0.955556           0.242424   \n",
       "3        91.0  0.094972  0.769231   0.945055           0.272727   \n",
       "4        83.0  0.094972  0.783133   0.963855           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.818182            0.939394             0.072848   \n",
       "1          0.848485            0.969697             0.092857   \n",
       "2          0.878788            0.969697             0.108108   \n",
       "3          0.848485            0.969697             0.106557   \n",
       "4          0.848485            0.969697             0.106796   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.516556              0.860927  1.355781  0.489426  \n",
       "1            0.578571              0.914286  1.449290  0.450151  \n",
       "2            0.558559              0.873874  1.410280  0.459215  \n",
       "3            0.516393              0.852459  1.345150  0.489426  \n",
       "4            0.592233              0.873786  1.437661  0.459215  "
      ]
     },
     "execution_count": 1140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_400_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.117117</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>1.373044</td>\n",
       "      <td>0.543807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.604796</td>\n",
       "      <td>0.477341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.987342</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>1.497453</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.503876</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>1.519598</td>\n",
       "      <td>0.441088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>1.441763</td>\n",
       "      <td>0.510574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        79.0  0.094972  0.810127   0.987342           0.272727   \n",
       "1        53.0  0.094972  0.830189   0.981132           0.303030   \n",
       "2        79.0  0.094972  0.848101   0.987342           0.212121   \n",
       "3       108.0  0.094972  0.731481   0.953704           0.272727   \n",
       "4       102.0  0.094972  0.754902   0.950980           0.303030   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.787879            0.969697             0.117117   \n",
       "1          0.848485            1.000000             0.098765   \n",
       "2          0.818182            1.000000             0.099099   \n",
       "3          0.848485            0.969697             0.100775   \n",
       "4          0.757576            0.939394             0.062500   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.549550              0.864865  1.373044  0.543807  \n",
       "1            0.604938              0.888889  1.604796  0.477341  \n",
       "2            0.558559              0.882883  1.497453  0.486405  \n",
       "3            0.503876              0.883721  1.519598  0.441088  \n",
       "4            0.506944              0.847222  1.441763  0.510574  "
      ]
     },
     "execution_count": 1151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_400_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>1.312308</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.105960</td>\n",
       "      <td>0.543046</td>\n",
       "      <td>0.913907</td>\n",
       "      <td>1.336576</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.575472</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>1.291740</td>\n",
       "      <td>0.498489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.090226</td>\n",
       "      <td>0.541353</td>\n",
       "      <td>0.879699</td>\n",
       "      <td>1.287987</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.558333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.278520</td>\n",
       "      <td>0.507553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        76.0  0.094972  0.789474   0.973684           0.242424   \n",
       "1       120.0  0.094972  0.700000   0.950000           0.272727   \n",
       "2        88.0  0.094972  0.795455   0.977273           0.242424   \n",
       "3       113.0  0.094972  0.707965   0.946903           0.242424   \n",
       "4       100.0  0.094972  0.750000   0.960000           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.969697             0.142857   \n",
       "1          0.818182            0.939394             0.105960   \n",
       "2          0.848485            0.969697             0.094340   \n",
       "3          0.848485            0.969697             0.090226   \n",
       "4          0.848485            0.969697             0.100000   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.604396              0.879121  1.312308  0.486405  \n",
       "1            0.543046              0.913907  1.336576  0.483384  \n",
       "2            0.575472              0.924528  1.291740  0.498489  \n",
       "3            0.541353              0.879699  1.287987  0.486405  \n",
       "4            0.558333              0.916667  1.278520  0.507553  "
      ]
     },
     "execution_count": 1141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_300_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.699115</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.886525</td>\n",
       "      <td>1.383570</td>\n",
       "      <td>0.429003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.573034</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>1.445286</td>\n",
       "      <td>0.477341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077922</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.424749</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>1.346164</td>\n",
       "      <td>0.429003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>1.408535</td>\n",
       "      <td>0.444109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       113.0  0.094972  0.699115   0.955752           0.242424   \n",
       "1        70.0  0.094972  0.800000   0.971429           0.303030   \n",
       "2        47.0  0.094972  0.851064   0.978723           0.242424   \n",
       "3        54.0  0.094972  0.851852   0.981481           0.242424   \n",
       "4       120.0  0.094972  0.700000   0.950000           0.333333   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.099291   \n",
       "1          0.878788            1.000000             0.101124   \n",
       "2          0.878788            1.000000             0.077922   \n",
       "3          0.878788            0.969697             0.123077   \n",
       "4          0.878788            0.969697             0.096774   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.560284              0.886525  1.383570  0.429003  \n",
       "1            0.573034              0.898876  1.445286  0.477341  \n",
       "2            0.571429              0.857143  1.424749  0.480363  \n",
       "3            0.553846              0.907692  1.346164  0.429003  \n",
       "4            0.522581              0.851613  1.408535  0.444109  "
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_300_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.584158</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>1.359825</td>\n",
       "      <td>0.435045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.644295</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>1.311240</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.974684</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.604938</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>1.358138</td>\n",
       "      <td>0.447130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.959016</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>1.275713</td>\n",
       "      <td>0.516616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.092437</td>\n",
       "      <td>0.554622</td>\n",
       "      <td>0.890756</td>\n",
       "      <td>1.231637</td>\n",
       "      <td>0.522659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        94.0  0.094972  0.744681   0.957447           0.212121   \n",
       "1       149.0  0.094972  0.644295   0.939597           0.272727   \n",
       "2        79.0  0.094972  0.797468   0.974684           0.212121   \n",
       "3       122.0  0.094972  0.704918   0.959016           0.242424   \n",
       "4        87.0  0.094972  0.793103   0.965517           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.818182            0.939394             0.089109   \n",
       "1          0.878788            0.969697             0.080000   \n",
       "2          0.878788            0.969697             0.111111   \n",
       "3          0.787879            0.939394             0.106667   \n",
       "4          0.818182            0.939394             0.092437   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.584158              0.900990  1.359825  0.435045  \n",
       "1            0.520000              0.891429  1.311240  0.459215  \n",
       "2            0.604938              0.950617  1.358138  0.447130  \n",
       "3            0.533333              0.886667  1.275713  0.516616  \n",
       "4            0.554622              0.890756  1.231637  0.522659  "
      ]
     },
     "execution_count": 1165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_250_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>1.310063</td>\n",
       "      <td>0.462236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.902913</td>\n",
       "      <td>1.340102</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>1.218421</td>\n",
       "      <td>0.495468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.074468</td>\n",
       "      <td>0.521277</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>1.308485</td>\n",
       "      <td>0.495468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>1.210499</td>\n",
       "      <td>0.504532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       100.0  0.094972  0.740000   0.970000           0.242424   \n",
       "1        82.0  0.094972  0.768293   0.975610           0.242424   \n",
       "2        90.0  0.094972  0.800000   0.966667           0.272727   \n",
       "3        59.0  0.094972  0.813559   0.966102           0.272727   \n",
       "4        60.0  0.094972  0.833333   0.966667           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.094017   \n",
       "1          0.818182            0.939394             0.097087   \n",
       "2          0.848485            0.939394             0.134615   \n",
       "3          0.848485            0.969697             0.074468   \n",
       "4          0.878788            0.969697             0.072289   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.564103              0.880342  1.310063  0.462236  \n",
       "1            0.572816              0.902913  1.340102  0.465257  \n",
       "2            0.605769              0.903846  1.218421  0.495468  \n",
       "3            0.521277              0.851064  1.308485  0.495468  \n",
       "4            0.530120              0.903614  1.210499  0.504532  "
      ]
     },
     "execution_count": 1163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_250_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.716814</td>\n",
       "      <td>0.955752</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.094488</td>\n",
       "      <td>0.527559</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>1.344561</td>\n",
       "      <td>0.416918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.733945</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.296870</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.089655</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.882759</td>\n",
       "      <td>1.373079</td>\n",
       "      <td>0.422961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>129.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.945736</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.077844</td>\n",
       "      <td>0.508982</td>\n",
       "      <td>0.862275</td>\n",
       "      <td>1.287742</td>\n",
       "      <td>0.474320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.649254</td>\n",
       "      <td>0.932836</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>1.338315</td>\n",
       "      <td>0.453172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       113.0  0.094972  0.716814   0.955752           0.272727   \n",
       "1       109.0  0.094972  0.733945   0.972477           0.212121   \n",
       "2       129.0  0.094972  0.697674   0.953488           0.272727   \n",
       "3       129.0  0.094972  0.666667   0.945736           0.272727   \n",
       "4       134.0  0.094972  0.649254   0.932836           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.818182            0.939394             0.094488   \n",
       "1          0.848485            0.969697             0.108333   \n",
       "2          0.878788            0.969697             0.089655   \n",
       "3          0.848485            0.969697             0.077844   \n",
       "4          0.878788            0.969697             0.085366   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.527559              0.897638  1.344561  0.416918  \n",
       "1            0.575000              0.900000  1.296870  0.486405  \n",
       "2            0.551724              0.882759  1.373079  0.422961  \n",
       "3            0.508982              0.862275  1.287742  0.474320  \n",
       "4            0.536585              0.926829  1.338315  0.453172  "
      ]
     },
     "execution_count": 1143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.552083</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>1.344601</td>\n",
       "      <td>0.416918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.102362</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.897638</td>\n",
       "      <td>1.417642</td>\n",
       "      <td>0.365559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>1.303192</td>\n",
       "      <td>0.444109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.101124</td>\n",
       "      <td>0.629213</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>1.126423</td>\n",
       "      <td>0.534743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.243225</td>\n",
       "      <td>0.462236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        94.0  0.094972  0.776596   0.957447           0.303030   \n",
       "1       116.0  0.094972  0.715517   0.956897           0.333333   \n",
       "2        95.0  0.094972  0.757895   0.968421           0.333333   \n",
       "3        73.0  0.094972  0.808219   0.972603           0.242424   \n",
       "4        75.0  0.094972  0.813333   0.973333           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.114583   \n",
       "1          0.818182            0.939394             0.102362   \n",
       "2          0.848485            0.969697             0.097087   \n",
       "3          0.848485            0.939394             0.101124   \n",
       "4          0.848485            0.939394             0.125000   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.552083              0.854167  1.344601  0.416918  \n",
       "1            0.566929              0.897638  1.417642  0.365559  \n",
       "2            0.563107              0.893204  1.303192  0.444109  \n",
       "3            0.629213              0.898876  1.126423  0.534743  \n",
       "4            0.650000              0.950000  1.243225  0.462236  "
      ]
     },
     "execution_count": 1155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.709402</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>1.256521</td>\n",
       "      <td>0.498489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.607955</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>0.502591</td>\n",
       "      <td>0.906736</td>\n",
       "      <td>1.368041</td>\n",
       "      <td>0.416918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.715517</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.559055</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>1.239193</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.577465</td>\n",
       "      <td>0.908451</td>\n",
       "      <td>1.334022</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>1.254909</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       117.0  0.094972  0.709402   0.974359           0.272727   \n",
       "1       176.0  0.094972  0.607955   0.931818           0.272727   \n",
       "2       116.0  0.094972  0.715517   0.974138           0.212121   \n",
       "3       118.0  0.094972  0.694915   0.966102           0.242424   \n",
       "4        94.0  0.094972  0.776596   0.968085           0.303030   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.096774   \n",
       "1          0.909091            0.969697             0.088083   \n",
       "2          0.909091            0.969697             0.086614   \n",
       "3          0.878788            0.969697             0.098592   \n",
       "4          0.848485            0.969697             0.104762   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.548387              0.879032  1.256521  0.498489  \n",
       "1            0.502591              0.906736  1.368041  0.416918  \n",
       "2            0.559055              0.929134  1.239193  0.486405  \n",
       "3            0.577465              0.908451  1.334022  0.471299  \n",
       "4            0.600000              0.876190  1.254909  0.465257  "
      ]
     },
     "execution_count": 1147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_175_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.085470</td>\n",
       "      <td>0.547009</td>\n",
       "      <td>0.880342</td>\n",
       "      <td>1.122820</td>\n",
       "      <td>0.510574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.889908</td>\n",
       "      <td>1.209980</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.975904</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.925234</td>\n",
       "      <td>1.187737</td>\n",
       "      <td>0.495468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.091603</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>1.238466</td>\n",
       "      <td>0.425982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.086022</td>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>1.262683</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        88.0  0.094972  0.750000   0.977273           0.212121   \n",
       "1        84.0  0.094972  0.797619   0.976190           0.303030   \n",
       "2        83.0  0.094972  0.795181   0.975904           0.242424   \n",
       "3       118.0  0.094972  0.703390   0.949153           0.272727   \n",
       "4        86.0  0.094972  0.790698   0.976744           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.909091            0.969697             0.085470   \n",
       "1          0.878788            0.969697             0.091743   \n",
       "2          0.878788            0.969697             0.102804   \n",
       "3          0.909091            0.969697             0.091603   \n",
       "4          0.878788            0.969697             0.086022   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.547009              0.880342  1.122820  0.510574  \n",
       "1            0.596330              0.889908  1.209980  0.465257  \n",
       "2            0.672897              0.925234  1.187737  0.495468  \n",
       "3            0.595420              0.870229  1.238466  0.425982  \n",
       "4            0.602151              0.924731  1.262683  0.432024  "
      ]
     },
     "execution_count": 1157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_175_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.095652</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.895652</td>\n",
       "      <td>1.287202</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.958678</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>1.296553</td>\n",
       "      <td>0.486405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.912621</td>\n",
       "      <td>1.235172</td>\n",
       "      <td>0.477341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.628743</td>\n",
       "      <td>0.940120</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.469072</td>\n",
       "      <td>0.855670</td>\n",
       "      <td>1.307907</td>\n",
       "      <td>0.468278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>144.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.095808</td>\n",
       "      <td>0.550898</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>1.262887</td>\n",
       "      <td>0.492447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       100.0  0.094972  0.770000   0.960000           0.272727   \n",
       "1       121.0  0.094972  0.694215   0.958678           0.242424   \n",
       "2        78.0  0.094972  0.769231   0.961538           0.242424   \n",
       "3       167.0  0.094972  0.628743   0.940120           0.242424   \n",
       "4       144.0  0.094972  0.673611   0.958333           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.095652   \n",
       "1          0.909091            0.969697             0.086667   \n",
       "2          0.878788            0.969697             0.097087   \n",
       "3          0.878788            0.969697             0.061856   \n",
       "4          0.878788            0.969697             0.095808   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.565217              0.895652  1.287202  0.480363  \n",
       "1            0.500000              0.873333  1.296553  0.486405  \n",
       "2            0.582524              0.912621  1.235172  0.477341  \n",
       "3            0.469072              0.855670  1.307907  0.468278  \n",
       "4            0.550898              0.880240  1.262887  0.492447  "
      ]
     },
     "execution_count": 1145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_150_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.762376</td>\n",
       "      <td>0.960396</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.097345</td>\n",
       "      <td>0.566372</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>1.193644</td>\n",
       "      <td>0.462236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.768421</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.850467</td>\n",
       "      <td>1.160346</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>97.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.969072</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>1.125869</td>\n",
       "      <td>0.507553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>1.230654</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.121495</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>1.229660</td>\n",
       "      <td>0.422961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       101.0  0.094972  0.762376   0.960396           0.272727   \n",
       "1        95.0  0.094972  0.768421   0.978947           0.272727   \n",
       "2        97.0  0.094972  0.731959   0.969072           0.242424   \n",
       "3        99.0  0.094972  0.787879   0.969697           0.242424   \n",
       "4        99.0  0.094972  0.767677   0.959596           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.939394             0.097345   \n",
       "1          0.909091            0.969697             0.102804   \n",
       "2          0.878788            0.969697             0.099099   \n",
       "3          0.848485            0.969697             0.091743   \n",
       "4          0.818182            0.939394             0.121495   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.566372              0.911504  1.193644  0.462236  \n",
       "1            0.514019              0.850467  1.160346  0.471299  \n",
       "2            0.567568              0.891892  1.125869  0.507553  \n",
       "3            0.568807              0.871560  1.230654  0.459215  \n",
       "4            0.598131              0.887850  1.229660  0.422961  "
      ]
     },
     "execution_count": 1159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_150_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.512658</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>1.287235</td>\n",
       "      <td>0.495468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>175.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.594286</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>0.497326</td>\n",
       "      <td>0.877005</td>\n",
       "      <td>1.263305</td>\n",
       "      <td>0.501511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.074890</td>\n",
       "      <td>0.462555</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>1.357800</td>\n",
       "      <td>0.404834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.651613</td>\n",
       "      <td>0.941935</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>1.321788</td>\n",
       "      <td>0.477341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>1.252489</td>\n",
       "      <td>0.507553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       109.0  0.094972  0.715596   0.963303           0.272727   \n",
       "1       175.0  0.094972  0.594286   0.942857           0.242424   \n",
       "2       200.0  0.094972  0.570000   0.910000           0.272727   \n",
       "3       155.0  0.094972  0.651613   0.941935           0.242424   \n",
       "4       130.0  0.094972  0.676923   0.953846           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.088608   \n",
       "1          0.848485            0.969697             0.069519   \n",
       "2          0.848485            0.969697             0.074890   \n",
       "3          0.878788            0.969697             0.091954   \n",
       "4          0.909091            0.969697             0.071429   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.512658              0.879747  1.287235  0.495468  \n",
       "1            0.497326              0.877005  1.263305  0.501511  \n",
       "2            0.462555              0.859031  1.357800  0.404834  \n",
       "3            0.500000              0.862069  1.321788  0.477341  \n",
       "4            0.494505              0.895604  1.252489  0.507553  "
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_125_w_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.621118</td>\n",
       "      <td>0.925466</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.486772</td>\n",
       "      <td>0.878307</td>\n",
       "      <td>1.159453</td>\n",
       "      <td>0.480363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.762376</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>1.137560</td>\n",
       "      <td>0.492447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.978947</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>1.065054</td>\n",
       "      <td>0.549849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.591398</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>1.223903</td>\n",
       "      <td>0.447130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.722772</td>\n",
       "      <td>0.960396</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.106195</td>\n",
       "      <td>0.592920</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>1.195224</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       161.0  0.094972  0.621118   0.925466           0.242424   \n",
       "1       101.0  0.094972  0.762376   0.980198           0.272727   \n",
       "2        95.0  0.094972  0.789474   0.978947           0.272727   \n",
       "3       186.0  0.094972  0.591398   0.919355           0.303030   \n",
       "4       101.0  0.094972  0.722772   0.960396           0.242424   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.969697             0.074074   \n",
       "1          0.878788            0.969697             0.093023   \n",
       "2          0.878788            0.969697             0.117647   \n",
       "3          0.848485            0.969697             0.080000   \n",
       "4          0.818182            0.969697             0.106195   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.486772              0.878307  1.159453  0.480363  \n",
       "1            0.558140              0.906977  1.137560  0.492447  \n",
       "2            0.588235              0.882353  1.065054  0.549849  \n",
       "3            0.485000              0.870000  1.223903  0.447130  \n",
       "4            0.592920              0.911504  1.195224  0.465257  "
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_125_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "0.711864406779661\n",
      "0.9322033898305084\n",
      "25\n",
      "0.72\n",
      "0.96\n",
      "0.7575757575757576\n",
      "0.1694915254237288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   0,   0],\n",
       "       [  3,  21,  17,   5,   4],\n",
       "       [  1,  25, 105,  61,   6],\n",
       "       [  0,   3,  17,  38,   6],\n",
       "       [  0,   1,   2,   6,   4]])"
      ]
     },
     "execution_count": 922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_business)\n",
    "model_metrics(predictions, y_cat_test_business)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test_business.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSExJQUREYkoKIiISU1IQEZHY/wEuvPTJTqMiVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14a30c208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(business_model_2.history['loss'])\n",
    "plt.plot(business_model_2.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(631,), activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(631,), activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/400\n",
      "2839/2839 [==============================] - 7s 2ms/step - loss: 2.3837 - acc: 0.2740 - val_loss: 2.2945 - val_acc: 0.3353\n",
      "Epoch 2/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 2.2386 - acc: 0.3244 - val_loss: 2.2558 - val_acc: 0.3323\n",
      "Epoch 3/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 2.1757 - acc: 0.3329 - val_loss: 2.2510 - val_acc: 0.3323\n",
      "Epoch 4/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 2.1451 - acc: 0.3286 - val_loss: 2.1958 - val_acc: 0.3323\n",
      "Epoch 5/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 2.0881 - acc: 0.3589 - val_loss: 2.1864 - val_acc: 0.3353\n",
      "Epoch 6/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 2.0750 - acc: 0.3417 - val_loss: 2.1477 - val_acc: 0.3323\n",
      "Epoch 7/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 2.0568 - acc: 0.3603 - val_loss: 2.1203 - val_acc: 0.3353\n",
      "Epoch 8/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 2.0114 - acc: 0.3846 - val_loss: 2.0961 - val_acc: 0.3323\n",
      "Epoch 9/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 2.0028 - acc: 0.3695 - val_loss: 2.1204 - val_acc: 0.3323\n",
      "Epoch 10/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.9978 - acc: 0.3586 - val_loss: 2.0930 - val_acc: 0.3353\n",
      "Epoch 11/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.9754 - acc: 0.3628 - val_loss: 2.1358 - val_acc: 0.3353\n",
      "Epoch 12/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.9568 - acc: 0.3670 - val_loss: 2.0670 - val_acc: 0.3323\n",
      "Epoch 13/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.9263 - acc: 0.3695 - val_loss: 2.0424 - val_acc: 0.3353\n",
      "Epoch 14/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.9039 - acc: 0.3744 - val_loss: 2.0580 - val_acc: 0.3263\n",
      "Epoch 15/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.9030 - acc: 0.3698 - val_loss: 2.0471 - val_acc: 0.3263\n",
      "Epoch 16/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.9015 - acc: 0.3741 - val_loss: 1.9721 - val_acc: 0.3233\n",
      "Epoch 17/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.8548 - acc: 0.3836 - val_loss: 1.9935 - val_acc: 0.3263\n",
      "Epoch 18/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.8332 - acc: 0.3959 - val_loss: 2.0808 - val_acc: 0.2508\n",
      "Epoch 19/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.8437 - acc: 0.3322 - val_loss: 1.9951 - val_acc: 0.3263\n",
      "Epoch 20/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.8068 - acc: 0.3822 - val_loss: 1.9396 - val_acc: 0.3233\n",
      "Epoch 21/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.7860 - acc: 0.3913 - val_loss: 1.9143 - val_acc: 0.3263\n",
      "Epoch 22/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.7650 - acc: 0.4093 - val_loss: 1.9959 - val_acc: 0.2961\n",
      "Epoch 23/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.7933 - acc: 0.3540 - val_loss: 1.9183 - val_acc: 0.3263\n",
      "Epoch 24/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.7414 - acc: 0.4019 - val_loss: 1.9100 - val_acc: 0.3142\n",
      "Epoch 25/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.7167 - acc: 0.3994 - val_loss: 2.0684 - val_acc: 0.2236\n",
      "Epoch 26/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.7714 - acc: 0.3389 - val_loss: 1.9335 - val_acc: 0.3293\n",
      "Epoch 27/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.7278 - acc: 0.3910 - val_loss: 1.8586 - val_acc: 0.3323\n",
      "Epoch 28/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.7046 - acc: 0.3970 - val_loss: 1.8256 - val_acc: 0.3293\n",
      "Epoch 29/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.6866 - acc: 0.4111 - val_loss: 1.8159 - val_acc: 0.3323\n",
      "Epoch 30/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.6858 - acc: 0.4047 - val_loss: 1.8424 - val_acc: 0.3293\n",
      "Epoch 31/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.6823 - acc: 0.3787 - val_loss: 1.9172 - val_acc: 0.2538\n",
      "Epoch 32/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.6730 - acc: 0.3706 - val_loss: 1.9086 - val_acc: 0.3202\n",
      "Epoch 33/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.6540 - acc: 0.3832 - val_loss: 1.8301 - val_acc: 0.3263\n",
      "Epoch 34/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.6249 - acc: 0.4019 - val_loss: 1.8832 - val_acc: 0.2900\n",
      "Epoch 35/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.6241 - acc: 0.3878 - val_loss: 1.8697 - val_acc: 0.2931\n",
      "Epoch 36/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.6288 - acc: 0.3681 - val_loss: 1.8320 - val_acc: 0.3142\n",
      "Epoch 37/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.5922 - acc: 0.4001 - val_loss: 1.8922 - val_acc: 0.3172\n",
      "Epoch 38/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.5977 - acc: 0.3906 - val_loss: 1.8170 - val_acc: 0.3172\n",
      "Epoch 39/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5833 - acc: 0.3913 - val_loss: 1.7776 - val_acc: 0.3263\n",
      "Epoch 40/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5753 - acc: 0.3853 - val_loss: 1.7641 - val_acc: 0.3233\n",
      "Epoch 41/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.5839 - acc: 0.3864 - val_loss: 1.6500 - val_acc: 0.3353\n",
      "Epoch 42/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.5940 - acc: 0.4125 - val_loss: 1.7018 - val_acc: 0.3263\n",
      "Epoch 43/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5571 - acc: 0.4012 - val_loss: 1.6601 - val_acc: 0.3353\n",
      "Epoch 44/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.5740 - acc: 0.4174 - val_loss: 1.7537 - val_acc: 0.3263\n",
      "Epoch 45/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5290 - acc: 0.4128 - val_loss: 1.8381 - val_acc: 0.3142\n",
      "Epoch 46/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5490 - acc: 0.4128 - val_loss: 1.8284 - val_acc: 0.3142\n",
      "Epoch 47/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.5296 - acc: 0.3970 - val_loss: 1.8016 - val_acc: 0.3202\n",
      "Epoch 48/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5304 - acc: 0.3956 - val_loss: 1.8057 - val_acc: 0.2689\n",
      "Epoch 49/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.5377 - acc: 0.3751 - val_loss: 1.7038 - val_acc: 0.3293\n",
      "Epoch 50/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5003 - acc: 0.4139 - val_loss: 1.8823 - val_acc: 0.2175\n",
      "Epoch 51/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.5689 - acc: 0.3360 - val_loss: 1.7245 - val_acc: 0.3293\n",
      "Epoch 52/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4911 - acc: 0.4107 - val_loss: 1.7064 - val_acc: 0.3233\n",
      "Epoch 53/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4655 - acc: 0.4227 - val_loss: 1.6481 - val_acc: 0.3323\n",
      "Epoch 54/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4951 - acc: 0.4188 - val_loss: 1.5687 - val_acc: 0.3595\n",
      "Epoch 55/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.5356 - acc: 0.4368 - val_loss: 1.7132 - val_acc: 0.3202\n",
      "Epoch 56/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4546 - acc: 0.4223 - val_loss: 1.6797 - val_acc: 0.3293\n",
      "Epoch 57/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4393 - acc: 0.4287 - val_loss: 1.7112 - val_acc: 0.3202\n",
      "Epoch 58/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.4477 - acc: 0.4209 - val_loss: 1.7862 - val_acc: 0.3142\n",
      "Epoch 59/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4548 - acc: 0.4202 - val_loss: 1.8282 - val_acc: 0.2810\n",
      "Epoch 60/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4898 - acc: 0.3906 - val_loss: 1.8081 - val_acc: 0.2508\n",
      "Epoch 61/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.5052 - acc: 0.3639 - val_loss: 1.7340 - val_acc: 0.3021\n",
      "Epoch 62/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4502 - acc: 0.4037 - val_loss: 1.6947 - val_acc: 0.3202\n",
      "Epoch 63/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4134 - acc: 0.4174 - val_loss: 1.7921 - val_acc: 0.2749\n",
      "Epoch 64/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4533 - acc: 0.3973 - val_loss: 1.7374 - val_acc: 0.3172\n",
      "Epoch 65/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4222 - acc: 0.4033 - val_loss: 1.6813 - val_acc: 0.3202\n",
      "Epoch 66/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4050 - acc: 0.4170 - val_loss: 1.7966 - val_acc: 0.2689\n",
      "Epoch 67/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4657 - acc: 0.3779 - val_loss: 1.6729 - val_acc: 0.3202\n",
      "Epoch 68/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4172 - acc: 0.4153 - val_loss: 1.5791 - val_acc: 0.3474\n",
      "Epoch 69/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.4248 - acc: 0.4417 - val_loss: 1.6487 - val_acc: 0.3082\n",
      "Epoch 70/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4511 - acc: 0.4371 - val_loss: 1.7456 - val_acc: 0.2719\n",
      "Epoch 71/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4282 - acc: 0.4033 - val_loss: 1.7275 - val_acc: 0.2991\n",
      "Epoch 72/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3996 - acc: 0.4174 - val_loss: 1.7346 - val_acc: 0.3051\n",
      "Epoch 73/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.4069 - acc: 0.4170 - val_loss: 1.7263 - val_acc: 0.3051\n",
      "Epoch 74/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3971 - acc: 0.4149 - val_loss: 1.7309 - val_acc: 0.2870\n",
      "Epoch 75/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4066 - acc: 0.4199 - val_loss: 1.8299 - val_acc: 0.2568\n",
      "Epoch 76/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4319 - acc: 0.3720 - val_loss: 1.6308 - val_acc: 0.3263\n",
      "Epoch 77/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3778 - acc: 0.4318 - val_loss: 1.6002 - val_acc: 0.3384\n",
      "Epoch 78/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3843 - acc: 0.4449 - val_loss: 1.5511 - val_acc: 0.3686\n",
      "Epoch 79/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.4094 - acc: 0.4392 - val_loss: 1.5161 - val_acc: 0.3565\n",
      "Epoch 80/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4018 - acc: 0.4251 - val_loss: 1.5873 - val_acc: 0.3535\n",
      "Epoch 81/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3670 - acc: 0.4421 - val_loss: 1.8091 - val_acc: 0.3142\n",
      "Epoch 82/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4190 - acc: 0.4153 - val_loss: 1.6695 - val_acc: 0.3051\n",
      "Epoch 83/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3640 - acc: 0.4082 - val_loss: 1.6436 - val_acc: 0.3082\n",
      "Epoch 84/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3544 - acc: 0.4251 - val_loss: 1.6054 - val_acc: 0.3263\n",
      "Epoch 85/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3636 - acc: 0.4354 - val_loss: 1.6584 - val_acc: 0.3082\n",
      "Epoch 86/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3658 - acc: 0.4283 - val_loss: 1.8204 - val_acc: 0.2387\n",
      "Epoch 87/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4077 - acc: 0.3723 - val_loss: 1.6494 - val_acc: 0.3323\n",
      "Epoch 88/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3426 - acc: 0.4297 - val_loss: 1.5829 - val_acc: 0.3474\n",
      "Epoch 89/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.3523 - acc: 0.4304 - val_loss: 1.6532 - val_acc: 0.3353\n",
      "Epoch 90/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3516 - acc: 0.4350 - val_loss: 1.7245 - val_acc: 0.3051\n",
      "Epoch 91/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3710 - acc: 0.4255 - val_loss: 1.7998 - val_acc: 0.2417\n",
      "Epoch 92/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3933 - acc: 0.3639 - val_loss: 1.6392 - val_acc: 0.3323\n",
      "Epoch 93/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3138 - acc: 0.4322 - val_loss: 1.6009 - val_acc: 0.3474\n",
      "Epoch 94/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3190 - acc: 0.4456 - val_loss: 1.6134 - val_acc: 0.3505\n",
      "Epoch 95/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3727 - acc: 0.4259 - val_loss: 1.5428 - val_acc: 0.3837\n",
      "Epoch 96/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3496 - acc: 0.4421 - val_loss: 1.4443 - val_acc: 0.3958\n",
      "Epoch 97/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.4200 - acc: 0.4368 - val_loss: 1.5831 - val_acc: 0.3414\n",
      "Epoch 98/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3366 - acc: 0.4301 - val_loss: 1.5616 - val_acc: 0.3565\n",
      "Epoch 99/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2907 - acc: 0.4565 - val_loss: 1.5952 - val_acc: 0.3444\n",
      "Epoch 100/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3026 - acc: 0.4445 - val_loss: 1.9758 - val_acc: 0.2900\n",
      "Epoch 101/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.4584 - acc: 0.4072 - val_loss: 1.6225 - val_acc: 0.3142\n",
      "Epoch 102/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3605 - acc: 0.4297 - val_loss: 1.6151 - val_acc: 0.3293\n",
      "Epoch 103/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2999 - acc: 0.4329 - val_loss: 1.6174 - val_acc: 0.3202\n",
      "Epoch 104/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3262 - acc: 0.4280 - val_loss: 1.7365 - val_acc: 0.2659\n",
      "Epoch 105/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3452 - acc: 0.4008 - val_loss: 1.6475 - val_acc: 0.3323\n",
      "Epoch 106/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2938 - acc: 0.4435 - val_loss: 1.6487 - val_acc: 0.3384\n",
      "Epoch 107/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3096 - acc: 0.4477 - val_loss: 1.6606 - val_acc: 0.3263\n",
      "Epoch 108/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3091 - acc: 0.4389 - val_loss: 1.6156 - val_acc: 0.3474\n",
      "Epoch 109/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2778 - acc: 0.4396 - val_loss: 1.6983 - val_acc: 0.3263\n",
      "Epoch 110/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3297 - acc: 0.4385 - val_loss: 1.7401 - val_acc: 0.2508\n",
      "Epoch 111/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3476 - acc: 0.3945 - val_loss: 1.5924 - val_acc: 0.3474\n",
      "Epoch 112/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2854 - acc: 0.4431 - val_loss: 1.5376 - val_acc: 0.3716\n",
      "Epoch 113/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2893 - acc: 0.4452 - val_loss: 1.4452 - val_acc: 0.4169\n",
      "Epoch 114/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3444 - acc: 0.4473 - val_loss: 1.4355 - val_acc: 0.3867\n",
      "Epoch 115/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3466 - acc: 0.4614 - val_loss: 1.6216 - val_acc: 0.3414\n",
      "Epoch 116/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2964 - acc: 0.4378 - val_loss: 1.5112 - val_acc: 0.3716\n",
      "Epoch 117/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2848 - acc: 0.4554 - val_loss: 1.4768 - val_acc: 0.4169\n",
      "Epoch 118/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2955 - acc: 0.4459 - val_loss: 1.4256 - val_acc: 0.4381\n",
      "Epoch 119/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3260 - acc: 0.4540 - val_loss: 1.4267 - val_acc: 0.4562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.3046 - acc: 0.4713 - val_loss: 1.5285 - val_acc: 0.3958\n",
      "Epoch 121/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3003 - acc: 0.4565 - val_loss: 1.6226 - val_acc: 0.3293\n",
      "Epoch 122/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3089 - acc: 0.4192 - val_loss: 1.6246 - val_acc: 0.3353\n",
      "Epoch 123/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2545 - acc: 0.4435 - val_loss: 1.7287 - val_acc: 0.3323\n",
      "Epoch 124/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3013 - acc: 0.4364 - val_loss: 1.5000 - val_acc: 0.3625\n",
      "Epoch 125/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2517 - acc: 0.4406 - val_loss: 1.4358 - val_acc: 0.4350\n",
      "Epoch 126/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3038 - acc: 0.4421 - val_loss: 1.4389 - val_acc: 0.4350\n",
      "Epoch 127/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2717 - acc: 0.4727 - val_loss: 1.4586 - val_acc: 0.4290\n",
      "Epoch 128/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2739 - acc: 0.4628 - val_loss: 1.4181 - val_acc: 0.4320\n",
      "Epoch 129/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3051 - acc: 0.4502 - val_loss: 1.4925 - val_acc: 0.3656\n",
      "Epoch 130/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3062 - acc: 0.4516 - val_loss: 1.4797 - val_acc: 0.4139\n",
      "Epoch 131/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2501 - acc: 0.4530 - val_loss: 1.4335 - val_acc: 0.4290\n",
      "Epoch 132/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2598 - acc: 0.4586 - val_loss: 1.3885 - val_acc: 0.4743\n",
      "Epoch 133/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3259 - acc: 0.4657 - val_loss: 1.4725 - val_acc: 0.3746\n",
      "Epoch 134/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2460 - acc: 0.4600 - val_loss: 1.4435 - val_acc: 0.4169\n",
      "Epoch 135/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2594 - acc: 0.4745 - val_loss: 1.4606 - val_acc: 0.4230\n",
      "Epoch 136/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2800 - acc: 0.4787 - val_loss: 1.5475 - val_acc: 0.3716\n",
      "Epoch 137/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2360 - acc: 0.4491 - val_loss: 1.5713 - val_acc: 0.3535\n",
      "Epoch 138/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2608 - acc: 0.4597 - val_loss: 1.6479 - val_acc: 0.3202\n",
      "Epoch 139/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2889 - acc: 0.4086 - val_loss: 1.7416 - val_acc: 0.3293\n",
      "Epoch 140/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.3113 - acc: 0.4206 - val_loss: 1.5762 - val_acc: 0.3414\n",
      "Epoch 141/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2325 - acc: 0.4410 - val_loss: 1.5366 - val_acc: 0.3867\n",
      "Epoch 142/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2054 - acc: 0.4600 - val_loss: 1.4818 - val_acc: 0.3897\n",
      "Epoch 143/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3022 - acc: 0.4297 - val_loss: 1.4254 - val_acc: 0.4199\n",
      "Epoch 144/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.2485 - acc: 0.4600 - val_loss: 1.4026 - val_acc: 0.4350\n",
      "Epoch 145/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.2341 - acc: 0.4801 - val_loss: 1.3997 - val_acc: 0.4622\n",
      "Epoch 146/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.2983 - acc: 0.4713 - val_loss: 1.4842 - val_acc: 0.3625\n",
      "Epoch 147/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2085 - acc: 0.4600 - val_loss: 1.4558 - val_acc: 0.4230\n",
      "Epoch 148/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1943 - acc: 0.4819 - val_loss: 1.5513 - val_acc: 0.3927\n",
      "Epoch 149/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2818 - acc: 0.4364 - val_loss: 1.4999 - val_acc: 0.4320\n",
      "Epoch 150/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2799 - acc: 0.4664 - val_loss: 1.3770 - val_acc: 0.4502\n",
      "Epoch 151/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3765 - acc: 0.4526 - val_loss: 1.4609 - val_acc: 0.3988\n",
      "Epoch 152/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2003 - acc: 0.4713 - val_loss: 1.4420 - val_acc: 0.4290\n",
      "Epoch 153/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1839 - acc: 0.4847 - val_loss: 1.4199 - val_acc: 0.4502\n",
      "Epoch 154/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2047 - acc: 0.4921 - val_loss: 1.3777 - val_acc: 0.4743\n",
      "Epoch 155/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2940 - acc: 0.4551 - val_loss: 1.4045 - val_acc: 0.4018\n",
      "Epoch 156/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2963 - acc: 0.4723 - val_loss: 1.5288 - val_acc: 0.3927\n",
      "Epoch 157/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2189 - acc: 0.4790 - val_loss: 1.5251 - val_acc: 0.3807\n",
      "Epoch 158/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2039 - acc: 0.4667 - val_loss: 1.4513 - val_acc: 0.4471\n",
      "Epoch 159/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1849 - acc: 0.4692 - val_loss: 1.3566 - val_acc: 0.4985\n",
      "Epoch 160/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3157 - acc: 0.4597 - val_loss: 1.4460 - val_acc: 0.3988\n",
      "Epoch 161/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2715 - acc: 0.4685 - val_loss: 1.4672 - val_acc: 0.4471\n",
      "Epoch 162/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2255 - acc: 0.4642 - val_loss: 1.3749 - val_acc: 0.4894\n",
      "Epoch 163/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2040 - acc: 0.4850 - val_loss: 1.3766 - val_acc: 0.4683\n",
      "Epoch 164/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2233 - acc: 0.4769 - val_loss: 1.3789 - val_acc: 0.4502\n",
      "Epoch 165/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2183 - acc: 0.4886 - val_loss: 1.3871 - val_acc: 0.4441\n",
      "Epoch 166/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2127 - acc: 0.4917 - val_loss: 1.4171 - val_acc: 0.4411\n",
      "Epoch 167/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1744 - acc: 0.4840 - val_loss: 1.3681 - val_acc: 0.4622\n",
      "Epoch 168/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2175 - acc: 0.4731 - val_loss: 1.3758 - val_acc: 0.4502\n",
      "Epoch 169/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2343 - acc: 0.4878 - val_loss: 1.4246 - val_acc: 0.4441\n",
      "Epoch 170/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1644 - acc: 0.4819 - val_loss: 1.4408 - val_acc: 0.4562\n",
      "Epoch 171/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1921 - acc: 0.4822 - val_loss: 1.6996 - val_acc: 0.3595\n",
      "Epoch 172/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.3241 - acc: 0.4502 - val_loss: 1.6238 - val_acc: 0.2991\n",
      "Epoch 173/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2436 - acc: 0.4294 - val_loss: 1.5864 - val_acc: 0.3444\n",
      "Epoch 174/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2056 - acc: 0.4449 - val_loss: 1.4874 - val_acc: 0.3958\n",
      "Epoch 175/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1703 - acc: 0.4805 - val_loss: 1.5609 - val_acc: 0.3625\n",
      "Epoch 176/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2092 - acc: 0.4692 - val_loss: 1.6484 - val_acc: 0.3414\n",
      "Epoch 177/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.2159 - acc: 0.4456 - val_loss: 1.5242 - val_acc: 0.3837\n",
      "Epoch 178/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1863 - acc: 0.4685 - val_loss: 1.5227 - val_acc: 0.3927\n",
      "Epoch 179/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1867 - acc: 0.4720 - val_loss: 1.6634 - val_acc: 0.3293\n",
      "Epoch 180/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2394 - acc: 0.4456 - val_loss: 1.5029 - val_acc: 0.3746\n",
      "Epoch 181/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2135 - acc: 0.4576 - val_loss: 1.4129 - val_acc: 0.4713\n",
      "Epoch 182/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1735 - acc: 0.4878 - val_loss: 1.3770 - val_acc: 0.4622\n",
      "Epoch 183/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1785 - acc: 0.4850 - val_loss: 1.3542 - val_acc: 0.4894\n",
      "Epoch 184/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1802 - acc: 0.4935 - val_loss: 1.4084 - val_acc: 0.4562\n",
      "Epoch 185/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.2604 - acc: 0.4639 - val_loss: 1.3664 - val_acc: 0.4411\n",
      "Epoch 186/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.2300 - acc: 0.4773 - val_loss: 1.4291 - val_acc: 0.4290\n",
      "Epoch 187/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1828 - acc: 0.4780 - val_loss: 1.3788 - val_acc: 0.4562\n",
      "Epoch 188/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1706 - acc: 0.4822 - val_loss: 1.3901 - val_acc: 0.4834\n",
      "Epoch 189/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1625 - acc: 0.4882 - val_loss: 1.3653 - val_acc: 0.4502\n",
      "Epoch 190/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1761 - acc: 0.4861 - val_loss: 1.3613 - val_acc: 0.4290\n",
      "Epoch 191/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2863 - acc: 0.4889 - val_loss: 1.4783 - val_acc: 0.4018\n",
      "Epoch 192/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1853 - acc: 0.4745 - val_loss: 1.5145 - val_acc: 0.3927\n",
      "Epoch 193/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1710 - acc: 0.4685 - val_loss: 1.4772 - val_acc: 0.4109\n",
      "Epoch 194/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1609 - acc: 0.4790 - val_loss: 1.6803 - val_acc: 0.3172\n",
      "Epoch 195/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2634 - acc: 0.4294 - val_loss: 1.4935 - val_acc: 0.3897\n",
      "Epoch 196/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1587 - acc: 0.4738 - val_loss: 1.4816 - val_acc: 0.4260\n",
      "Epoch 197/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1421 - acc: 0.4907 - val_loss: 1.5408 - val_acc: 0.4109\n",
      "Epoch 198/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1968 - acc: 0.4727 - val_loss: 1.4767 - val_acc: 0.4079\n",
      "Epoch 199/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1746 - acc: 0.4600 - val_loss: 1.4368 - val_acc: 0.4471\n",
      "Epoch 200/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1228 - acc: 0.4903 - val_loss: 1.4923 - val_acc: 0.4199\n",
      "Epoch 201/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1603 - acc: 0.4783 - val_loss: 1.5303 - val_acc: 0.4290\n",
      "Epoch 202/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1703 - acc: 0.4843 - val_loss: 1.6469 - val_acc: 0.3505\n",
      "Epoch 203/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.2526 - acc: 0.4385 - val_loss: 1.5559 - val_acc: 0.3958\n",
      "Epoch 204/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1995 - acc: 0.4590 - val_loss: 1.4441 - val_acc: 0.4350\n",
      "Epoch 205/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1456 - acc: 0.5005 - val_loss: 1.3877 - val_acc: 0.4532\n",
      "Epoch 206/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1497 - acc: 0.5093 - val_loss: 1.3773 - val_acc: 0.4804\n",
      "Epoch 207/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1346 - acc: 0.4967 - val_loss: 1.3519 - val_acc: 0.4743\n",
      "Epoch 208/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1761 - acc: 0.5041 - val_loss: 1.3579 - val_acc: 0.4441\n",
      "Epoch 209/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2069 - acc: 0.4931 - val_loss: 1.3745 - val_acc: 0.4532\n",
      "Epoch 210/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1693 - acc: 0.4776 - val_loss: 1.3688 - val_acc: 0.4773\n",
      "Epoch 211/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1189 - acc: 0.5093 - val_loss: 1.4011 - val_acc: 0.4532\n",
      "Epoch 212/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1160 - acc: 0.5072 - val_loss: 1.6204 - val_acc: 0.3474\n",
      "Epoch 213/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2629 - acc: 0.4442 - val_loss: 1.4950 - val_acc: 0.3867\n",
      "Epoch 214/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1937 - acc: 0.4561 - val_loss: 1.4768 - val_acc: 0.4290\n",
      "Epoch 215/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1794 - acc: 0.4826 - val_loss: 1.4782 - val_acc: 0.4260\n",
      "Epoch 216/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1585 - acc: 0.4843 - val_loss: 1.3618 - val_acc: 0.4713\n",
      "Epoch 217/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1174 - acc: 0.4984 - val_loss: 1.3970 - val_acc: 0.4441\n",
      "Epoch 218/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1671 - acc: 0.4995 - val_loss: 1.3599 - val_acc: 0.4743\n",
      "Epoch 219/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1868 - acc: 0.4801 - val_loss: 1.3422 - val_acc: 0.4562\n",
      "Epoch 220/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.2256 - acc: 0.4847 - val_loss: 1.4007 - val_acc: 0.4441\n",
      "Epoch 221/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1269 - acc: 0.5055 - val_loss: 1.4326 - val_acc: 0.4381\n",
      "Epoch 222/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1085 - acc: 0.5072 - val_loss: 1.5736 - val_acc: 0.3867\n",
      "Epoch 223/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1921 - acc: 0.4660 - val_loss: 1.4990 - val_acc: 0.3867\n",
      "Epoch 224/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1130 - acc: 0.4819 - val_loss: 1.4728 - val_acc: 0.3988\n",
      "Epoch 225/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1193 - acc: 0.5016 - val_loss: 1.5950 - val_acc: 0.3293\n",
      "Epoch 226/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1760 - acc: 0.4385 - val_loss: 1.5047 - val_acc: 0.4169\n",
      "Epoch 227/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1347 - acc: 0.4769 - val_loss: 1.4543 - val_acc: 0.4381\n",
      "Epoch 228/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1549 - acc: 0.4843 - val_loss: 1.4104 - val_acc: 0.4471\n",
      "Epoch 229/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1866 - acc: 0.4840 - val_loss: 1.3411 - val_acc: 0.4955\n",
      "Epoch 230/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1943 - acc: 0.4833 - val_loss: 1.3541 - val_acc: 0.4471\n",
      "Epoch 231/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1457 - acc: 0.4945 - val_loss: 1.3458 - val_acc: 0.4924\n",
      "Epoch 232/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1156 - acc: 0.5097 - val_loss: 1.3329 - val_acc: 0.4683\n",
      "Epoch 233/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.1728 - acc: 0.4959 - val_loss: 1.3637 - val_acc: 0.4592\n",
      "Epoch 234/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1196 - acc: 0.5090 - val_loss: 1.3253 - val_acc: 0.4743\n",
      "Epoch 235/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1548 - acc: 0.5093 - val_loss: 1.4202 - val_acc: 0.4411\n",
      "Epoch 236/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1577 - acc: 0.5136 - val_loss: 1.4650 - val_acc: 0.4350\n",
      "Epoch 237/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1586 - acc: 0.4833 - val_loss: 1.5421 - val_acc: 0.3746\n",
      "Epoch 238/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1645 - acc: 0.4653 - val_loss: 1.5486 - val_acc: 0.3656\n",
      "Epoch 239/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1327 - acc: 0.4727 - val_loss: 1.4424 - val_acc: 0.4350\n",
      "Epoch 240/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0962 - acc: 0.5178 - val_loss: 1.5915 - val_acc: 0.3686\n",
      "Epoch 241/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1941 - acc: 0.4657 - val_loss: 1.4650 - val_acc: 0.4048\n",
      "Epoch 242/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1038 - acc: 0.4826 - val_loss: 1.4059 - val_acc: 0.4502\n",
      "Epoch 243/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0704 - acc: 0.5213 - val_loss: 1.5137 - val_acc: 0.3927\n",
      "Epoch 244/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1411 - acc: 0.4864 - val_loss: 1.6314 - val_acc: 0.3323\n",
      "Epoch 245/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1905 - acc: 0.4160 - val_loss: 1.3771 - val_acc: 0.4622\n",
      "Epoch 246/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0755 - acc: 0.5336 - val_loss: 1.3805 - val_acc: 0.4592\n",
      "Epoch 247/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1239 - acc: 0.5076 - val_loss: 1.3631 - val_acc: 0.4441\n",
      "Epoch 248/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1740 - acc: 0.5090 - val_loss: 1.3679 - val_acc: 0.4653\n",
      "Epoch 249/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1094 - acc: 0.4928 - val_loss: 1.3229 - val_acc: 0.4955\n",
      "Epoch 250/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1580 - acc: 0.5037 - val_loss: 1.4210 - val_acc: 0.4411\n",
      "Epoch 251/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0985 - acc: 0.5072 - val_loss: 1.6542 - val_acc: 0.4018\n",
      "Epoch 252/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2201 - acc: 0.4597 - val_loss: 1.4161 - val_acc: 0.4350\n",
      "Epoch 253/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0736 - acc: 0.5044 - val_loss: 1.4114 - val_acc: 0.4441\n",
      "Epoch 254/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0612 - acc: 0.5199 - val_loss: 1.3857 - val_acc: 0.4592\n",
      "Epoch 255/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0553 - acc: 0.5188 - val_loss: 1.3379 - val_acc: 0.4713\n",
      "Epoch 256/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2106 - acc: 0.4956 - val_loss: 1.3611 - val_acc: 0.4350\n",
      "Epoch 257/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2694 - acc: 0.4755 - val_loss: 1.4839 - val_acc: 0.4320\n",
      "Epoch 258/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1501 - acc: 0.4931 - val_loss: 1.4737 - val_acc: 0.4230\n",
      "Epoch 259/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0782 - acc: 0.5016 - val_loss: 1.4057 - val_acc: 0.4592\n",
      "Epoch 260/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0565 - acc: 0.5252 - val_loss: 1.4119 - val_acc: 0.4502\n",
      "Epoch 261/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0435 - acc: 0.5277 - val_loss: 1.4501 - val_acc: 0.4411\n",
      "Epoch 262/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0798 - acc: 0.5188 - val_loss: 1.6847 - val_acc: 0.3263\n",
      "Epoch 263/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2031 - acc: 0.4435 - val_loss: 1.4517 - val_acc: 0.4381\n",
      "Epoch 264/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0859 - acc: 0.5136 - val_loss: 1.4507 - val_acc: 0.4411\n",
      "Epoch 265/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1148 - acc: 0.4945 - val_loss: 1.4851 - val_acc: 0.4109\n",
      "Epoch 266/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1770 - acc: 0.4678 - val_loss: 1.3513 - val_acc: 0.4562\n",
      "Epoch 267/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1428 - acc: 0.5030 - val_loss: 1.3471 - val_acc: 0.4773\n",
      "Epoch 268/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0861 - acc: 0.5174 - val_loss: 1.3315 - val_acc: 0.4804\n",
      "Epoch 269/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0971 - acc: 0.5107 - val_loss: 1.3339 - val_acc: 0.4502\n",
      "Epoch 270/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1348 - acc: 0.5118 - val_loss: 1.3608 - val_acc: 0.4471\n",
      "Epoch 271/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0903 - acc: 0.5203 - val_loss: 1.3886 - val_acc: 0.4502\n",
      "Epoch 272/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1515 - acc: 0.5100 - val_loss: 1.4202 - val_acc: 0.4532\n",
      "Epoch 273/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0928 - acc: 0.5181 - val_loss: 1.5197 - val_acc: 0.4048\n",
      "Epoch 274/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1275 - acc: 0.4868 - val_loss: 1.4102 - val_acc: 0.4471\n",
      "Epoch 275/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0673 - acc: 0.5255 - val_loss: 1.5706 - val_acc: 0.3746\n",
      "Epoch 276/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1836 - acc: 0.4547 - val_loss: 1.5480 - val_acc: 0.3837\n",
      "Epoch 277/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1316 - acc: 0.4815 - val_loss: 1.4245 - val_acc: 0.4381\n",
      "Epoch 278/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0900 - acc: 0.4952 - val_loss: 1.3855 - val_acc: 0.4562\n",
      "Epoch 279/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0682 - acc: 0.5150 - val_loss: 1.3320 - val_acc: 0.4955\n",
      "Epoch 280/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0892 - acc: 0.4988 - val_loss: 1.3159 - val_acc: 0.4864\n",
      "Epoch 281/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1815 - acc: 0.4991 - val_loss: 1.3930 - val_acc: 0.4471\n",
      "Epoch 282/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0559 - acc: 0.5252 - val_loss: 1.4051 - val_acc: 0.4562\n",
      "Epoch 283/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0401 - acc: 0.5354 - val_loss: 1.5901 - val_acc: 0.3837\n",
      "Epoch 284/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1525 - acc: 0.4773 - val_loss: 1.4318 - val_acc: 0.4199\n",
      "Epoch 285/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0484 - acc: 0.5086 - val_loss: 1.3886 - val_acc: 0.4622\n",
      "Epoch 286/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0310 - acc: 0.5238 - val_loss: 1.3301 - val_acc: 0.5136\n",
      "Epoch 287/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1230 - acc: 0.4938 - val_loss: 1.3217 - val_acc: 0.4622\n",
      "Epoch 288/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1348 - acc: 0.5174 - val_loss: 1.3733 - val_acc: 0.4773\n",
      "Epoch 289/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0680 - acc: 0.5305 - val_loss: 1.4550 - val_acc: 0.4290\n",
      "Epoch 290/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1471 - acc: 0.4903 - val_loss: 1.4426 - val_acc: 0.4562\n",
      "Epoch 291/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0947 - acc: 0.5118 - val_loss: 1.4430 - val_acc: 0.4260\n",
      "Epoch 292/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0513 - acc: 0.5104 - val_loss: 1.5508 - val_acc: 0.4048\n",
      "Epoch 293/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1451 - acc: 0.4797 - val_loss: 1.5912 - val_acc: 0.3505\n",
      "Epoch 294/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1149 - acc: 0.4695 - val_loss: 1.3931 - val_acc: 0.4320\n",
      "Epoch 295/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0305 - acc: 0.5195 - val_loss: 1.4291 - val_acc: 0.4502\n",
      "Epoch 296/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0855 - acc: 0.5030 - val_loss: 1.4545 - val_acc: 0.4230\n",
      "Epoch 297/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0747 - acc: 0.4868 - val_loss: 1.3915 - val_acc: 0.4562\n",
      "Epoch 298/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0233 - acc: 0.5315 - val_loss: 1.4195 - val_acc: 0.4622\n",
      "Epoch 299/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0490 - acc: 0.5167 - val_loss: 1.4312 - val_acc: 0.4532\n",
      "Epoch 300/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0955 - acc: 0.4903 - val_loss: 1.3348 - val_acc: 0.5136\n",
      "Epoch 301/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0918 - acc: 0.5072 - val_loss: 1.3152 - val_acc: 0.4804\n",
      "Epoch 302/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1944 - acc: 0.5164 - val_loss: 1.3850 - val_acc: 0.4441\n",
      "Epoch 303/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0783 - acc: 0.5206 - val_loss: 1.3817 - val_acc: 0.4532\n",
      "Epoch 304/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0343 - acc: 0.5326 - val_loss: 1.3971 - val_acc: 0.4622\n",
      "Epoch 305/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0222 - acc: 0.5431 - val_loss: 1.5806 - val_acc: 0.3716\n",
      "Epoch 306/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1496 - acc: 0.4495 - val_loss: 1.5892 - val_acc: 0.3656\n",
      "Epoch 307/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1618 - acc: 0.4463 - val_loss: 1.4791 - val_acc: 0.4320\n",
      "Epoch 308/400\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 1.0954 - acc: 0.5072 - val_loss: 1.4953 - val_acc: 0.3988\n",
      "Epoch 309/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0520 - acc: 0.5030 - val_loss: 1.4413 - val_acc: 0.4230\n",
      "Epoch 310/400\n",
      "2839/2839 [==============================] - 0s 81us/step - loss: 1.0335 - acc: 0.5178 - val_loss: 1.5222 - val_acc: 0.3746\n",
      "Epoch 311/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.1120 - acc: 0.4833 - val_loss: 1.5175 - val_acc: 0.4018\n",
      "Epoch 312/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0707 - acc: 0.4928 - val_loss: 1.4541 - val_acc: 0.4260\n",
      "Epoch 313/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0448 - acc: 0.5213 - val_loss: 1.5112 - val_acc: 0.4048\n",
      "Epoch 314/400\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 1.0728 - acc: 0.4974 - val_loss: 1.4887 - val_acc: 0.4169\n",
      "Epoch 315/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0810 - acc: 0.4988 - val_loss: 1.5062 - val_acc: 0.4109\n",
      "Epoch 316/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0867 - acc: 0.4938 - val_loss: 1.4129 - val_acc: 0.4834\n",
      "Epoch 317/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.0444 - acc: 0.5291 - val_loss: 1.4545 - val_acc: 0.4562\n",
      "Epoch 318/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0493 - acc: 0.5157 - val_loss: 1.3979 - val_acc: 0.4683\n",
      "Epoch 319/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0257 - acc: 0.5185 - val_loss: 1.3493 - val_acc: 0.4955\n",
      "Epoch 320/400\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 1.0478 - acc: 0.5129 - val_loss: 1.3265 - val_acc: 0.4834\n",
      "Epoch 321/400\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 1.1547 - acc: 0.5153 - val_loss: 1.3849 - val_acc: 0.4773\n",
      "Epoch 322/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0326 - acc: 0.5463 - val_loss: 1.4121 - val_acc: 0.4532\n",
      "Epoch 323/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0308 - acc: 0.5414 - val_loss: 1.4514 - val_acc: 0.4441\n",
      "Epoch 324/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.1017 - acc: 0.5241 - val_loss: 1.4051 - val_acc: 0.4592\n",
      "Epoch 325/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0550 - acc: 0.5343 - val_loss: 1.3642 - val_acc: 0.4743\n",
      "Epoch 326/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.1046 - acc: 0.5305 - val_loss: 1.3685 - val_acc: 0.4411\n",
      "Epoch 327/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1605 - acc: 0.4854 - val_loss: 1.3516 - val_acc: 0.4622\n",
      "Epoch 328/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0292 - acc: 0.5347 - val_loss: 1.3565 - val_acc: 0.4773\n",
      "Epoch 329/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0067 - acc: 0.5474 - val_loss: 1.3552 - val_acc: 0.4653\n",
      "Epoch 330/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.0833 - acc: 0.5284 - val_loss: 1.3616 - val_acc: 0.4502\n",
      "Epoch 331/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0847 - acc: 0.5280 - val_loss: 1.3825 - val_acc: 0.4592\n",
      "Epoch 332/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 0.9903 - acc: 0.5456 - val_loss: 1.3675 - val_acc: 0.4653\n",
      "Epoch 333/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0445 - acc: 0.5467 - val_loss: 1.3826 - val_acc: 0.4502\n",
      "Epoch 334/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1113 - acc: 0.5252 - val_loss: 1.3708 - val_acc: 0.4653\n",
      "Epoch 335/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0152 - acc: 0.5558 - val_loss: 1.3452 - val_acc: 0.4864\n",
      "Epoch 336/400\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 1.0372 - acc: 0.5375 - val_loss: 1.3988 - val_acc: 0.4471\n",
      "Epoch 337/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.1028 - acc: 0.5016 - val_loss: 1.4187 - val_acc: 0.4743\n",
      "Epoch 338/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0521 - acc: 0.5146 - val_loss: 1.4663 - val_acc: 0.4199\n",
      "Epoch 339/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0261 - acc: 0.5298 - val_loss: 1.5750 - val_acc: 0.3716\n",
      "Epoch 340/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.1077 - acc: 0.4854 - val_loss: 1.5525 - val_acc: 0.3807\n",
      "Epoch 341/400\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 1.0611 - acc: 0.4924 - val_loss: 1.4524 - val_acc: 0.4411\n",
      "Epoch 342/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0041 - acc: 0.5403 - val_loss: 1.4448 - val_acc: 0.4350\n",
      "Epoch 343/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0527 - acc: 0.5424 - val_loss: 1.4253 - val_acc: 0.4622\n",
      "Epoch 344/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0657 - acc: 0.5354 - val_loss: 1.3729 - val_acc: 0.4834\n",
      "Epoch 345/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.9950 - acc: 0.5523 - val_loss: 1.3580 - val_acc: 0.4683\n",
      "Epoch 346/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0861 - acc: 0.5167 - val_loss: 1.3599 - val_acc: 0.4592\n",
      "Epoch 347/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0662 - acc: 0.5358 - val_loss: 1.3990 - val_acc: 0.4592\n",
      "Epoch 348/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0044 - acc: 0.5548 - val_loss: 1.4073 - val_acc: 0.4653\n",
      "Epoch 349/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0364 - acc: 0.5446 - val_loss: 1.3866 - val_acc: 0.4471\n",
      "Epoch 350/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0428 - acc: 0.5382 - val_loss: 1.3581 - val_acc: 0.4622\n",
      "Epoch 351/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1051 - acc: 0.4935 - val_loss: 1.3643 - val_acc: 0.4894\n",
      "Epoch 352/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.9895 - acc: 0.5495 - val_loss: 1.4487 - val_acc: 0.4350\n",
      "Epoch 353/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0337 - acc: 0.5312 - val_loss: 1.5861 - val_acc: 0.3807\n",
      "Epoch 354/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0744 - acc: 0.4907 - val_loss: 1.4383 - val_acc: 0.4290\n",
      "Epoch 355/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 0.9939 - acc: 0.5326 - val_loss: 1.4326 - val_acc: 0.4773\n",
      "Epoch 356/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0288 - acc: 0.5287 - val_loss: 1.4663 - val_acc: 0.4562\n",
      "Epoch 357/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0736 - acc: 0.5026 - val_loss: 1.4152 - val_acc: 0.4683\n",
      "Epoch 358/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0082 - acc: 0.5329 - val_loss: 1.4627 - val_acc: 0.4381\n",
      "Epoch 359/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.9954 - acc: 0.5277 - val_loss: 1.4210 - val_acc: 0.4592\n",
      "Epoch 360/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 0.9732 - acc: 0.5548 - val_loss: 1.4633 - val_acc: 0.4381\n",
      "Epoch 361/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0077 - acc: 0.5481 - val_loss: 1.7632 - val_acc: 0.2870\n",
      "Epoch 362/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.1877 - acc: 0.4396 - val_loss: 1.5964 - val_acc: 0.3776\n",
      "Epoch 363/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.1299 - acc: 0.4917 - val_loss: 1.4516 - val_acc: 0.4320\n",
      "Epoch 364/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0427 - acc: 0.5449 - val_loss: 1.4099 - val_acc: 0.4471\n",
      "Epoch 365/400\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.9690 - acc: 0.5639 - val_loss: 1.4535 - val_acc: 0.4320\n",
      "Epoch 366/400\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.9827 - acc: 0.5417 - val_loss: 1.7383 - val_acc: 0.3172\n",
      "Epoch 367/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1837 - acc: 0.4410 - val_loss: 1.4537 - val_acc: 0.4169\n",
      "Epoch 368/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0192 - acc: 0.5255 - val_loss: 1.3869 - val_acc: 0.4622\n",
      "Epoch 369/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.9788 - acc: 0.5513 - val_loss: 1.3560 - val_acc: 0.5076\n",
      "Epoch 370/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0016 - acc: 0.5498 - val_loss: 1.3727 - val_acc: 0.4441\n",
      "Epoch 371/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0365 - acc: 0.5474 - val_loss: 1.4067 - val_acc: 0.4411\n",
      "Epoch 372/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0529 - acc: 0.5442 - val_loss: 1.4167 - val_acc: 0.4441\n",
      "Epoch 373/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.9888 - acc: 0.5667 - val_loss: 1.4162 - val_acc: 0.4562\n",
      "Epoch 374/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0101 - acc: 0.5520 - val_loss: 1.3855 - val_acc: 0.4653\n",
      "Epoch 375/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0590 - acc: 0.5329 - val_loss: 1.3467 - val_acc: 0.4834\n",
      "Epoch 376/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0361 - acc: 0.5435 - val_loss: 1.3877 - val_acc: 0.4622\n",
      "Epoch 377/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.9606 - acc: 0.5692 - val_loss: 1.5007 - val_acc: 0.4320\n",
      "Epoch 378/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0585 - acc: 0.5002 - val_loss: 1.4564 - val_acc: 0.4622\n",
      "Epoch 379/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0244 - acc: 0.5291 - val_loss: 1.4632 - val_acc: 0.4260\n",
      "Epoch 380/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.9867 - acc: 0.5417 - val_loss: 1.3961 - val_acc: 0.4924\n",
      "Epoch 381/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0028 - acc: 0.5259 - val_loss: 1.3637 - val_acc: 0.4713\n",
      "Epoch 382/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 1.0062 - acc: 0.5343 - val_loss: 1.3414 - val_acc: 0.4894\n",
      "Epoch 383/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0645 - acc: 0.5255 - val_loss: 1.3731 - val_acc: 0.4653\n",
      "Epoch 384/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0133 - acc: 0.5555 - val_loss: 1.3993 - val_acc: 0.4532\n",
      "Epoch 385/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0124 - acc: 0.5562 - val_loss: 1.3929 - val_acc: 0.4381\n",
      "Epoch 386/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0227 - acc: 0.5562 - val_loss: 1.4298 - val_acc: 0.4471\n",
      "Epoch 387/400\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 1.0121 - acc: 0.5597 - val_loss: 1.5310 - val_acc: 0.4048\n",
      "Epoch 388/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0177 - acc: 0.5308 - val_loss: 1.5515 - val_acc: 0.4230\n",
      "Epoch 389/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0870 - acc: 0.5136 - val_loss: 1.5955 - val_acc: 0.3746\n",
      "Epoch 390/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 1.0398 - acc: 0.4974 - val_loss: 1.4210 - val_acc: 0.4502\n",
      "Epoch 391/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 0.9557 - acc: 0.5763 - val_loss: 1.4714 - val_acc: 0.4260\n",
      "Epoch 392/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.9689 - acc: 0.5403 - val_loss: 1.5467 - val_acc: 0.3867\n",
      "Epoch 393/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0507 - acc: 0.5160 - val_loss: 1.5133 - val_acc: 0.4109\n",
      "Epoch 394/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.9707 - acc: 0.5343 - val_loss: 1.4215 - val_acc: 0.4411\n",
      "Epoch 395/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 0.9597 - acc: 0.5604 - val_loss: 1.4550 - val_acc: 0.4562\n",
      "Epoch 396/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0449 - acc: 0.5033 - val_loss: 1.3646 - val_acc: 0.5378\n",
      "Epoch 397/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0424 - acc: 0.5266 - val_loss: 1.3604 - val_acc: 0.4592\n",
      "Epoch 398/400\n",
      "2839/2839 [==============================] - 0s 60us/step - loss: 1.0491 - acc: 0.5262 - val_loss: 1.3725 - val_acc: 0.4683\n",
      "Epoch 399/400\n",
      "2839/2839 [==============================] - 0s 61us/step - loss: 0.9935 - acc: 0.5583 - val_loss: 1.3857 - val_acc: 0.4592\n",
      "Epoch 400/400\n",
      "2839/2839 [==============================] - 0s 62us/step - loss: 0.9756 - acc: 0.5611 - val_loss: 1.4185 - val_acc: 0.4592\n"
     ]
    }
   ],
   "source": [
    "w2v_model_2 = model.fit(x=X_train_w2v, y=y_cat_train_w2v, \n",
    "          batch_size=2000, \n",
    "          epochs=400, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_w2v, y_cat_test_w2v),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the cell where I adjusted the \"top_pred_prob\" from 10% to 15% of the top predictions. The predictability lowered but hopefully when I run all of the metrics this will differentiate some of them a bit further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2831 samples, validate on 330 samples\n",
      "Epoch 1/300\n",
      "2831/2831 [==============================] - 2s 572us/step - loss: 2.9504 - acc: 0.1533 - val_loss: 2.3532 - val_acc: 0.4121\n",
      "Epoch 2/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.3800 - acc: 0.2889 - val_loss: 2.3306 - val_acc: 0.3242\n",
      "Epoch 3/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 2.3207 - acc: 0.3013 - val_loss: 2.3075 - val_acc: 0.3273\n",
      "Epoch 4/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.2430 - acc: 0.3402 - val_loss: 2.2414 - val_acc: 0.3303\n",
      "Epoch 5/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.2180 - acc: 0.3373 - val_loss: 2.2399 - val_acc: 0.3242\n",
      "Epoch 6/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1744 - acc: 0.3430 - val_loss: 2.2283 - val_acc: 0.3242\n",
      "Epoch 7/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1550 - acc: 0.3370 - val_loss: 2.2061 - val_acc: 0.3273\n",
      "Epoch 8/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1212 - acc: 0.3532 - val_loss: 2.1917 - val_acc: 0.3121\n",
      "Epoch 9/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1072 - acc: 0.3342 - val_loss: 2.1727 - val_acc: 0.3182\n",
      "Epoch 10/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.0746 - acc: 0.3440 - val_loss: 2.1564 - val_acc: 0.3182\n",
      "Epoch 11/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 2.0653 - acc: 0.3426 - val_loss: 2.1077 - val_acc: 0.3333\n",
      "Epoch 12/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 2.0508 - acc: 0.3497 - val_loss: 2.1098 - val_acc: 0.3242\n",
      "Epoch 13/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 2.0180 - acc: 0.3621 - val_loss: 2.1146 - val_acc: 0.3212\n",
      "Epoch 14/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.0208 - acc: 0.3557 - val_loss: 2.1041 - val_acc: 0.3182\n",
      "Epoch 15/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.9881 - acc: 0.3596 - val_loss: 2.0906 - val_acc: 0.2879\n",
      "Epoch 16/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.9563 - acc: 0.3578 - val_loss: 2.0737 - val_acc: 0.3242\n",
      "Epoch 17/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.9363 - acc: 0.3596 - val_loss: 2.0315 - val_acc: 0.3182\n",
      "Epoch 18/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.9418 - acc: 0.3720 - val_loss: 2.0030 - val_acc: 0.3242\n",
      "Epoch 19/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.9238 - acc: 0.3606 - val_loss: 1.9968 - val_acc: 0.3242\n",
      "Epoch 20/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8908 - acc: 0.3557 - val_loss: 1.9764 - val_acc: 0.3242\n",
      "Epoch 21/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8740 - acc: 0.3833 - val_loss: 2.0445 - val_acc: 0.2212\n",
      "Epoch 22/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8853 - acc: 0.3451 - val_loss: 1.9953 - val_acc: 0.3182\n",
      "Epoch 23/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8469 - acc: 0.3797 - val_loss: 1.9749 - val_acc: 0.2939\n",
      "Epoch 24/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8369 - acc: 0.3642 - val_loss: 1.9808 - val_acc: 0.2697\n",
      "Epoch 25/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8233 - acc: 0.3592 - val_loss: 1.9703 - val_acc: 0.2636\n",
      "Epoch 26/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.8110 - acc: 0.3554 - val_loss: 1.9706 - val_acc: 0.2152\n",
      "Epoch 27/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8054 - acc: 0.3380 - val_loss: 1.9574 - val_acc: 0.2424\n",
      "Epoch 28/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7861 - acc: 0.3437 - val_loss: 1.8745 - val_acc: 0.3182\n",
      "Epoch 29/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7629 - acc: 0.3836 - val_loss: 1.8925 - val_acc: 0.3091\n",
      "Epoch 30/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7460 - acc: 0.3758 - val_loss: 1.8455 - val_acc: 0.3212\n",
      "Epoch 31/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7601 - acc: 0.3695 - val_loss: 1.8816 - val_acc: 0.3212\n",
      "Epoch 32/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.7423 - acc: 0.3599 - val_loss: 1.8107 - val_acc: 0.3212\n",
      "Epoch 33/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6885 - acc: 0.3931 - val_loss: 1.8034 - val_acc: 0.3152\n",
      "Epoch 34/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6906 - acc: 0.3769 - val_loss: 1.8221 - val_acc: 0.3212\n",
      "Epoch 35/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.7355 - acc: 0.3864 - val_loss: 1.8193 - val_acc: 0.3273\n",
      "Epoch 36/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.6784 - acc: 0.3928 - val_loss: 1.7836 - val_acc: 0.3242\n",
      "Epoch 37/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.6613 - acc: 0.3818 - val_loss: 1.7773 - val_acc: 0.3212\n",
      "Epoch 38/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.6564 - acc: 0.3900 - val_loss: 1.7370 - val_acc: 0.3152\n",
      "Epoch 39/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.6533 - acc: 0.3833 - val_loss: 1.7385 - val_acc: 0.3152\n",
      "Epoch 40/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.6171 - acc: 0.3981 - val_loss: 1.7337 - val_acc: 0.3242\n",
      "Epoch 41/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.6481 - acc: 0.3946 - val_loss: 1.8194 - val_acc: 0.3091\n",
      "Epoch 42/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.6308 - acc: 0.3797 - val_loss: 1.7799 - val_acc: 0.3091\n",
      "Epoch 43/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.6149 - acc: 0.3931 - val_loss: 1.8193 - val_acc: 0.2545\n",
      "Epoch 44/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.6032 - acc: 0.3797 - val_loss: 1.7631 - val_acc: 0.2879\n",
      "Epoch 45/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.5950 - acc: 0.3882 - val_loss: 1.7379 - val_acc: 0.3152\n",
      "Epoch 46/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.5919 - acc: 0.3963 - val_loss: 1.7609 - val_acc: 0.2788\n",
      "Epoch 47/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5938 - acc: 0.3801 - val_loss: 1.7616 - val_acc: 0.3061\n",
      "Epoch 48/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5779 - acc: 0.3875 - val_loss: 1.7437 - val_acc: 0.2788\n",
      "Epoch 49/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.5797 - acc: 0.3907 - val_loss: 1.6449 - val_acc: 0.3212\n",
      "Epoch 50/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5937 - acc: 0.3995 - val_loss: 1.6937 - val_acc: 0.3182\n",
      "Epoch 51/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5481 - acc: 0.3967 - val_loss: 1.6424 - val_acc: 0.3152\n",
      "Epoch 52/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.5404 - acc: 0.3900 - val_loss: 1.6867 - val_acc: 0.3182\n",
      "Epoch 53/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5545 - acc: 0.3963 - val_loss: 1.6639 - val_acc: 0.3212\n",
      "Epoch 54/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.5327 - acc: 0.4002 - val_loss: 1.6397 - val_acc: 0.3152\n",
      "Epoch 55/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5189 - acc: 0.4087 - val_loss: 1.6249 - val_acc: 0.3182\n",
      "Epoch 56/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5278 - acc: 0.4083 - val_loss: 1.6382 - val_acc: 0.3182\n",
      "Epoch 57/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.5336 - acc: 0.3977 - val_loss: 1.6283 - val_acc: 0.3182\n",
      "Epoch 58/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5144 - acc: 0.4115 - val_loss: 1.6325 - val_acc: 0.3152\n",
      "Epoch 59/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.4970 - acc: 0.4115 - val_loss: 1.7220 - val_acc: 0.2667\n",
      "Epoch 60/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.5289 - acc: 0.3999 - val_loss: 1.6588 - val_acc: 0.3030\n",
      "Epoch 61/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4844 - acc: 0.3946 - val_loss: 1.6880 - val_acc: 0.2848\n",
      "Epoch 62/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4848 - acc: 0.4002 - val_loss: 1.8168 - val_acc: 0.2303\n",
      "Epoch 63/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5312 - acc: 0.3289 - val_loss: 1.6927 - val_acc: 0.3182\n",
      "Epoch 64/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4643 - acc: 0.4041 - val_loss: 1.6361 - val_acc: 0.3061\n",
      "Epoch 65/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4635 - acc: 0.4126 - val_loss: 1.6459 - val_acc: 0.3000\n",
      "Epoch 66/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4484 - acc: 0.4097 - val_loss: 1.6293 - val_acc: 0.3000\n",
      "Epoch 67/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4621 - acc: 0.3942 - val_loss: 1.6189 - val_acc: 0.3121\n",
      "Epoch 68/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4642 - acc: 0.4066 - val_loss: 1.5823 - val_acc: 0.3182\n",
      "Epoch 69/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4444 - acc: 0.4172 - val_loss: 1.5657 - val_acc: 0.3212\n",
      "Epoch 70/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4744 - acc: 0.4083 - val_loss: 1.5885 - val_acc: 0.3242\n",
      "Epoch 71/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4704 - acc: 0.3992 - val_loss: 1.5888 - val_acc: 0.3182\n",
      "Epoch 72/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4378 - acc: 0.4076 - val_loss: 1.6593 - val_acc: 0.3182\n",
      "Epoch 73/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4403 - acc: 0.4112 - val_loss: 1.6338 - val_acc: 0.3212\n",
      "Epoch 74/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4167 - acc: 0.4059 - val_loss: 1.5358 - val_acc: 0.3273\n",
      "Epoch 75/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4684 - acc: 0.4055 - val_loss: 1.5408 - val_acc: 0.3182\n",
      "Epoch 76/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4416 - acc: 0.4288 - val_loss: 1.6171 - val_acc: 0.3182\n",
      "Epoch 77/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4186 - acc: 0.4034 - val_loss: 1.5992 - val_acc: 0.3152\n",
      "Epoch 78/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4190 - acc: 0.3903 - val_loss: 1.6326 - val_acc: 0.3152\n",
      "Epoch 79/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4203 - acc: 0.4225 - val_loss: 1.6516 - val_acc: 0.2939\n",
      "Epoch 80/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4062 - acc: 0.4069 - val_loss: 1.7829 - val_acc: 0.2758\n",
      "Epoch 81/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4497 - acc: 0.3769 - val_loss: 1.6560 - val_acc: 0.3091\n",
      "Epoch 82/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4137 - acc: 0.3970 - val_loss: 1.5607 - val_acc: 0.3182\n",
      "Epoch 83/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3951 - acc: 0.4334 - val_loss: 1.5390 - val_acc: 0.3212\n",
      "Epoch 84/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3961 - acc: 0.4066 - val_loss: 1.5339 - val_acc: 0.3242\n",
      "Epoch 85/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4271 - acc: 0.4119 - val_loss: 1.5537 - val_acc: 0.3212\n",
      "Epoch 86/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3944 - acc: 0.4299 - val_loss: 1.5942 - val_acc: 0.3212\n",
      "Epoch 87/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3893 - acc: 0.4140 - val_loss: 1.7169 - val_acc: 0.3121\n",
      "Epoch 88/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4068 - acc: 0.4126 - val_loss: 1.6351 - val_acc: 0.3182\n",
      "Epoch 89/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3674 - acc: 0.4242 - val_loss: 1.6639 - val_acc: 0.3091\n",
      "Epoch 90/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3738 - acc: 0.4112 - val_loss: 1.7025 - val_acc: 0.2727\n",
      "Epoch 91/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3836 - acc: 0.3963 - val_loss: 1.6872 - val_acc: 0.2697\n",
      "Epoch 92/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3847 - acc: 0.3868 - val_loss: 1.6269 - val_acc: 0.2758\n",
      "Epoch 93/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3490 - acc: 0.4175 - val_loss: 1.7366 - val_acc: 0.2818\n",
      "Epoch 94/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3891 - acc: 0.3808 - val_loss: 1.6056 - val_acc: 0.3152\n",
      "Epoch 95/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3373 - acc: 0.4264 - val_loss: 1.6104 - val_acc: 0.3394\n",
      "Epoch 96/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3605 - acc: 0.4172 - val_loss: 1.6044 - val_acc: 0.3242\n",
      "Epoch 97/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3923 - acc: 0.4214 - val_loss: 1.5498 - val_acc: 0.3303\n",
      "Epoch 98/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3287 - acc: 0.4221 - val_loss: 1.4604 - val_acc: 0.3424\n",
      "Epoch 99/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3933 - acc: 0.4377 - val_loss: 1.5230 - val_acc: 0.3273\n",
      "Epoch 100/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3422 - acc: 0.4345 - val_loss: 1.4729 - val_acc: 0.3303\n",
      "Epoch 101/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3489 - acc: 0.4437 - val_loss: 1.5348 - val_acc: 0.3394\n",
      "Epoch 102/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3564 - acc: 0.4302 - val_loss: 1.5780 - val_acc: 0.2818\n",
      "Epoch 103/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3529 - acc: 0.4214 - val_loss: 1.6978 - val_acc: 0.2606\n",
      "Epoch 104/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3762 - acc: 0.3758 - val_loss: 1.6589 - val_acc: 0.3030\n",
      "Epoch 105/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3200 - acc: 0.4122 - val_loss: 1.5975 - val_acc: 0.3273\n",
      "Epoch 106/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3054 - acc: 0.4267 - val_loss: 1.7104 - val_acc: 0.2909\n",
      "Epoch 107/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3442 - acc: 0.4069 - val_loss: 1.6122 - val_acc: 0.3091\n",
      "Epoch 108/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3190 - acc: 0.4193 - val_loss: 1.5259 - val_acc: 0.3727\n",
      "Epoch 109/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3571 - acc: 0.4267 - val_loss: 1.5203 - val_acc: 0.3455\n",
      "Epoch 110/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3475 - acc: 0.4440 - val_loss: 1.4797 - val_acc: 0.3273\n",
      "Epoch 111/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.3155 - acc: 0.4352 - val_loss: 1.4788 - val_acc: 0.3273\n",
      "Epoch 112/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3264 - acc: 0.4207 - val_loss: 1.4800 - val_acc: 0.3424\n",
      "Epoch 113/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3379 - acc: 0.4316 - val_loss: 1.5710 - val_acc: 0.3273\n",
      "Epoch 114/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3019 - acc: 0.4271 - val_loss: 1.6416 - val_acc: 0.3242\n",
      "Epoch 115/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3422 - acc: 0.4316 - val_loss: 1.5357 - val_acc: 0.3182\n",
      "Epoch 116/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2843 - acc: 0.4331 - val_loss: 1.4533 - val_acc: 0.3273\n",
      "Epoch 117/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3543 - acc: 0.4285 - val_loss: 1.4531 - val_acc: 0.3364\n",
      "Epoch 118/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.3665 - acc: 0.4324 - val_loss: 1.4973 - val_acc: 0.3485\n",
      "Epoch 119/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2892 - acc: 0.4500 - val_loss: 1.5712 - val_acc: 0.3242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2791 - acc: 0.4437 - val_loss: 1.5559 - val_acc: 0.3697\n",
      "Epoch 121/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3040 - acc: 0.4468 - val_loss: 1.6377 - val_acc: 0.2939\n",
      "Epoch 122/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3088 - acc: 0.4062 - val_loss: 1.5964 - val_acc: 0.3182\n",
      "Epoch 123/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2730 - acc: 0.4458 - val_loss: 1.7058 - val_acc: 0.3182\n",
      "Epoch 124/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3272 - acc: 0.4271 - val_loss: 1.6282 - val_acc: 0.3091\n",
      "Epoch 125/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2905 - acc: 0.4196 - val_loss: 1.6998 - val_acc: 0.2697\n",
      "Epoch 126/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3074 - acc: 0.3910 - val_loss: 1.5327 - val_acc: 0.3455\n",
      "Epoch 127/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2686 - acc: 0.4479 - val_loss: 1.6231 - val_acc: 0.3303\n",
      "Epoch 128/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2992 - acc: 0.4115 - val_loss: 1.5246 - val_acc: 0.3303\n",
      "Epoch 129/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2469 - acc: 0.4412 - val_loss: 1.4651 - val_acc: 0.3788\n",
      "Epoch 130/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3016 - acc: 0.4380 - val_loss: 1.4049 - val_acc: 0.3485\n",
      "Epoch 131/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3389 - acc: 0.4588 - val_loss: 1.5135 - val_acc: 0.3303\n",
      "Epoch 132/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.2480 - acc: 0.4525 - val_loss: 1.5266 - val_acc: 0.3606\n",
      "Epoch 133/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.2687 - acc: 0.4458 - val_loss: 1.6008 - val_acc: 0.3182\n",
      "Epoch 134/300\n",
      "2831/2831 [==============================] - 0s 58us/step - loss: 1.2776 - acc: 0.4394 - val_loss: 1.5409 - val_acc: 0.3485\n",
      "Epoch 135/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3024 - acc: 0.4454 - val_loss: 1.4914 - val_acc: 0.3485\n",
      "Epoch 136/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2794 - acc: 0.4384 - val_loss: 1.4055 - val_acc: 0.3667\n",
      "Epoch 137/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3002 - acc: 0.4281 - val_loss: 1.4003 - val_acc: 0.3455\n",
      "Epoch 138/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3194 - acc: 0.4486 - val_loss: 1.4996 - val_acc: 0.3364\n",
      "Epoch 139/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2287 - acc: 0.4483 - val_loss: 1.4381 - val_acc: 0.3606\n",
      "Epoch 140/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2610 - acc: 0.4550 - val_loss: 1.4229 - val_acc: 0.3667\n",
      "Epoch 141/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2956 - acc: 0.4581 - val_loss: 1.4721 - val_acc: 0.3545\n",
      "Epoch 142/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2198 - acc: 0.4588 - val_loss: 1.5587 - val_acc: 0.3364\n",
      "Epoch 143/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2257 - acc: 0.4610 - val_loss: 1.7801 - val_acc: 0.2667\n",
      "Epoch 144/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3305 - acc: 0.3599 - val_loss: 1.5329 - val_acc: 0.3636\n",
      "Epoch 145/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2496 - acc: 0.4528 - val_loss: 1.5965 - val_acc: 0.3424\n",
      "Epoch 146/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2410 - acc: 0.4493 - val_loss: 1.6455 - val_acc: 0.3242\n",
      "Epoch 147/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2664 - acc: 0.4313 - val_loss: 1.5082 - val_acc: 0.3394\n",
      "Epoch 148/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2240 - acc: 0.4398 - val_loss: 1.4938 - val_acc: 0.3364\n",
      "Epoch 149/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2765 - acc: 0.4327 - val_loss: 1.4681 - val_acc: 0.3545\n",
      "Epoch 150/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2238 - acc: 0.4564 - val_loss: 1.5211 - val_acc: 0.3424\n",
      "Epoch 151/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2180 - acc: 0.4550 - val_loss: 1.8140 - val_acc: 0.3000\n",
      "Epoch 152/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.3734 - acc: 0.4189 - val_loss: 1.5467 - val_acc: 0.3273\n",
      "Epoch 153/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2481 - acc: 0.4278 - val_loss: 1.5506 - val_acc: 0.3273\n",
      "Epoch 154/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2246 - acc: 0.4394 - val_loss: 1.5984 - val_acc: 0.3152\n",
      "Epoch 155/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2320 - acc: 0.4313 - val_loss: 1.6135 - val_acc: 0.3121\n",
      "Epoch 156/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2429 - acc: 0.4454 - val_loss: 1.6267 - val_acc: 0.3182\n",
      "Epoch 157/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2505 - acc: 0.4292 - val_loss: 1.6358 - val_acc: 0.3152\n",
      "Epoch 158/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2533 - acc: 0.4309 - val_loss: 1.6735 - val_acc: 0.3000\n",
      "Epoch 159/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2550 - acc: 0.4154 - val_loss: 1.5128 - val_acc: 0.3545\n",
      "Epoch 160/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1988 - acc: 0.4631 - val_loss: 1.5820 - val_acc: 0.3303\n",
      "Epoch 161/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2577 - acc: 0.4352 - val_loss: 1.5655 - val_acc: 0.3485\n",
      "Epoch 162/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2271 - acc: 0.4535 - val_loss: 1.5262 - val_acc: 0.3455\n",
      "Epoch 163/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1961 - acc: 0.4550 - val_loss: 1.7377 - val_acc: 0.2848\n",
      "Epoch 164/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2846 - acc: 0.4150 - val_loss: 1.5642 - val_acc: 0.3455\n",
      "Epoch 165/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2083 - acc: 0.4479 - val_loss: 1.5110 - val_acc: 0.3909\n",
      "Epoch 166/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2093 - acc: 0.4458 - val_loss: 1.4782 - val_acc: 0.3879\n",
      "Epoch 167/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1837 - acc: 0.4723 - val_loss: 1.6062 - val_acc: 0.3606\n",
      "Epoch 168/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2259 - acc: 0.4295 - val_loss: 1.6082 - val_acc: 0.3091\n",
      "Epoch 169/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2119 - acc: 0.4419 - val_loss: 1.6321 - val_acc: 0.3242\n",
      "Epoch 170/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2495 - acc: 0.4366 - val_loss: 1.5622 - val_acc: 0.3364\n",
      "Epoch 171/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2307 - acc: 0.4571 - val_loss: 1.4144 - val_acc: 0.3545\n",
      "Epoch 172/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2174 - acc: 0.4557 - val_loss: 1.3895 - val_acc: 0.3879\n",
      "Epoch 173/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2141 - acc: 0.4546 - val_loss: 1.3582 - val_acc: 0.4424\n",
      "Epoch 174/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2499 - acc: 0.4783 - val_loss: 1.4114 - val_acc: 0.3788\n",
      "Epoch 175/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1919 - acc: 0.4627 - val_loss: 1.4300 - val_acc: 0.4000\n",
      "Epoch 176/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1859 - acc: 0.4599 - val_loss: 1.3685 - val_acc: 0.4333\n",
      "Epoch 177/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2276 - acc: 0.4702 - val_loss: 1.3898 - val_acc: 0.4303\n",
      "Epoch 178/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2206 - acc: 0.4822 - val_loss: 1.4279 - val_acc: 0.3970\n",
      "Epoch 179/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1881 - acc: 0.4596 - val_loss: 1.4500 - val_acc: 0.3606\n",
      "Epoch 180/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2491 - acc: 0.4585 - val_loss: 1.5452 - val_acc: 0.3606\n",
      "Epoch 181/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2250 - acc: 0.4564 - val_loss: 1.6031 - val_acc: 0.3121\n",
      "Epoch 182/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2320 - acc: 0.4444 - val_loss: 1.5727 - val_acc: 0.3333\n",
      "Epoch 183/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1909 - acc: 0.4394 - val_loss: 1.5135 - val_acc: 0.3788\n",
      "Epoch 184/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1587 - acc: 0.4702 - val_loss: 1.5056 - val_acc: 0.4182\n",
      "Epoch 185/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1752 - acc: 0.4588 - val_loss: 1.4939 - val_acc: 0.4152\n",
      "Epoch 186/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2082 - acc: 0.4624 - val_loss: 1.3853 - val_acc: 0.4636\n",
      "Epoch 187/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1629 - acc: 0.4723 - val_loss: 1.3452 - val_acc: 0.4515\n",
      "Epoch 188/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2844 - acc: 0.4641 - val_loss: 1.4276 - val_acc: 0.3879\n",
      "Epoch 189/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1646 - acc: 0.4779 - val_loss: 1.4067 - val_acc: 0.4061\n",
      "Epoch 190/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.1476 - acc: 0.4716 - val_loss: 1.3247 - val_acc: 0.4394\n",
      "Epoch 191/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2725 - acc: 0.4543 - val_loss: 1.3807 - val_acc: 0.4121\n",
      "Epoch 192/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1802 - acc: 0.4670 - val_loss: 1.4073 - val_acc: 0.4697\n",
      "Epoch 193/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1819 - acc: 0.4853 - val_loss: 1.4081 - val_acc: 0.4273\n",
      "Epoch 194/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2465 - acc: 0.4698 - val_loss: 1.4585 - val_acc: 0.3879\n",
      "Epoch 195/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1747 - acc: 0.4779 - val_loss: 1.5100 - val_acc: 0.3727\n",
      "Epoch 196/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1788 - acc: 0.4532 - val_loss: 1.6069 - val_acc: 0.3242\n",
      "Epoch 197/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2296 - acc: 0.4076 - val_loss: 1.5493 - val_acc: 0.3576\n",
      "Epoch 198/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1749 - acc: 0.4571 - val_loss: 1.4662 - val_acc: 0.3697\n",
      "Epoch 199/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1493 - acc: 0.4645 - val_loss: 1.5341 - val_acc: 0.3758\n",
      "Epoch 200/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1789 - acc: 0.4666 - val_loss: 1.5103 - val_acc: 0.3485\n",
      "Epoch 201/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1625 - acc: 0.4603 - val_loss: 1.4004 - val_acc: 0.4000\n",
      "Epoch 202/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1590 - acc: 0.4765 - val_loss: 1.3428 - val_acc: 0.4121\n",
      "Epoch 203/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1991 - acc: 0.4691 - val_loss: 1.3671 - val_acc: 0.4333\n",
      "Epoch 204/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1857 - acc: 0.4857 - val_loss: 1.4261 - val_acc: 0.4242\n",
      "Epoch 205/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1807 - acc: 0.4857 - val_loss: 1.4709 - val_acc: 0.3818\n",
      "Epoch 206/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1390 - acc: 0.4836 - val_loss: 1.4980 - val_acc: 0.3758\n",
      "Epoch 207/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1531 - acc: 0.4624 - val_loss: 1.5477 - val_acc: 0.3455\n",
      "Epoch 208/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2098 - acc: 0.4338 - val_loss: 1.6047 - val_acc: 0.3424\n",
      "Epoch 209/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1938 - acc: 0.4309 - val_loss: 1.4719 - val_acc: 0.3727\n",
      "Epoch 210/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1645 - acc: 0.4638 - val_loss: 1.4403 - val_acc: 0.3970\n",
      "Epoch 211/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1631 - acc: 0.4606 - val_loss: 1.3655 - val_acc: 0.4091\n",
      "Epoch 212/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1707 - acc: 0.4786 - val_loss: 1.3315 - val_acc: 0.4515\n",
      "Epoch 213/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1616 - acc: 0.4896 - val_loss: 1.3349 - val_acc: 0.4273\n",
      "Epoch 214/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2110 - acc: 0.4786 - val_loss: 1.3900 - val_acc: 0.4212\n",
      "Epoch 215/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1533 - acc: 0.4998 - val_loss: 1.4286 - val_acc: 0.4061\n",
      "Epoch 216/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1432 - acc: 0.4931 - val_loss: 1.4742 - val_acc: 0.4273\n",
      "Epoch 217/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1523 - acc: 0.4829 - val_loss: 1.6428 - val_acc: 0.3333\n",
      "Epoch 218/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2408 - acc: 0.4105 - val_loss: 1.5017 - val_acc: 0.3667\n",
      "Epoch 219/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1533 - acc: 0.4649 - val_loss: 1.5347 - val_acc: 0.3576\n",
      "Epoch 220/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1605 - acc: 0.4627 - val_loss: 1.5351 - val_acc: 0.3606\n",
      "Epoch 221/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1394 - acc: 0.4705 - val_loss: 1.4719 - val_acc: 0.3727\n",
      "Epoch 222/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1256 - acc: 0.4691 - val_loss: 1.5391 - val_acc: 0.3818\n",
      "Epoch 223/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2153 - acc: 0.4620 - val_loss: 1.3987 - val_acc: 0.4242\n",
      "Epoch 224/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1317 - acc: 0.4853 - val_loss: 1.4165 - val_acc: 0.4000\n",
      "Epoch 225/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1030 - acc: 0.4885 - val_loss: 1.4896 - val_acc: 0.3818\n",
      "Epoch 226/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1319 - acc: 0.4825 - val_loss: 1.7153 - val_acc: 0.3333\n",
      "Epoch 227/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2343 - acc: 0.4285 - val_loss: 1.4344 - val_acc: 0.3727\n",
      "Epoch 228/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1384 - acc: 0.4860 - val_loss: 1.4704 - val_acc: 0.4000\n",
      "Epoch 229/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1194 - acc: 0.4811 - val_loss: 1.5418 - val_acc: 0.3909\n",
      "Epoch 230/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1812 - acc: 0.4380 - val_loss: 1.5655 - val_acc: 0.3242\n",
      "Epoch 231/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1712 - acc: 0.4412 - val_loss: 1.5177 - val_acc: 0.3697\n",
      "Epoch 232/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1249 - acc: 0.4779 - val_loss: 1.5363 - val_acc: 0.3606\n",
      "Epoch 233/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1491 - acc: 0.4726 - val_loss: 1.6373 - val_acc: 0.3303\n",
      "Epoch 234/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2035 - acc: 0.4398 - val_loss: 1.4544 - val_acc: 0.3818\n",
      "Epoch 235/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1284 - acc: 0.4705 - val_loss: 1.3781 - val_acc: 0.4152\n",
      "Epoch 236/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1639 - acc: 0.4610 - val_loss: 1.3894 - val_acc: 0.4515\n",
      "Epoch 237/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1199 - acc: 0.4963 - val_loss: 1.4576 - val_acc: 0.4091\n",
      "Epoch 238/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1233 - acc: 0.4899 - val_loss: 1.5917 - val_acc: 0.3455\n",
      "Epoch 239/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1769 - acc: 0.4451 - val_loss: 1.4588 - val_acc: 0.3939\n",
      "Epoch 240/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1144 - acc: 0.4804 - val_loss: 1.4870 - val_acc: 0.3848\n",
      "Epoch 241/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1421 - acc: 0.4871 - val_loss: 1.6309 - val_acc: 0.3091\n",
      "Epoch 242/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1718 - acc: 0.4430 - val_loss: 1.4437 - val_acc: 0.4030\n",
      "Epoch 243/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0953 - acc: 0.5079 - val_loss: 1.5104 - val_acc: 0.3939\n",
      "Epoch 244/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1417 - acc: 0.4811 - val_loss: 1.5320 - val_acc: 0.3939\n",
      "Epoch 245/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1681 - acc: 0.4511 - val_loss: 1.5138 - val_acc: 0.3667\n",
      "Epoch 246/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1315 - acc: 0.4726 - val_loss: 1.4957 - val_acc: 0.3697\n",
      "Epoch 247/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1151 - acc: 0.4730 - val_loss: 1.4875 - val_acc: 0.3788\n",
      "Epoch 248/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1070 - acc: 0.4836 - val_loss: 1.7081 - val_acc: 0.2818\n",
      "Epoch 249/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2525 - acc: 0.3949 - val_loss: 1.4304 - val_acc: 0.3970\n",
      "Epoch 250/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1222 - acc: 0.4917 - val_loss: 1.4141 - val_acc: 0.4121\n",
      "Epoch 251/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0846 - acc: 0.5012 - val_loss: 1.3886 - val_acc: 0.4333\n",
      "Epoch 252/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1392 - acc: 0.4843 - val_loss: 1.4565 - val_acc: 0.4212\n",
      "Epoch 253/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1629 - acc: 0.4829 - val_loss: 1.4221 - val_acc: 0.3939\n",
      "Epoch 254/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0788 - acc: 0.4935 - val_loss: 1.4681 - val_acc: 0.3939\n",
      "Epoch 255/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1128 - acc: 0.4899 - val_loss: 1.7201 - val_acc: 0.3303\n",
      "Epoch 256/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.2738 - acc: 0.4136 - val_loss: 1.4746 - val_acc: 0.3758\n",
      "Epoch 257/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.1042 - acc: 0.4938 - val_loss: 1.3943 - val_acc: 0.4182\n",
      "Epoch 258/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0715 - acc: 0.5136 - val_loss: 1.4417 - val_acc: 0.4182\n",
      "Epoch 259/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1092 - acc: 0.4875 - val_loss: 1.6073 - val_acc: 0.3758\n",
      "Epoch 260/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2245 - acc: 0.4532 - val_loss: 1.4979 - val_acc: 0.3515\n",
      "Epoch 261/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1047 - acc: 0.4666 - val_loss: 1.4501 - val_acc: 0.3788\n",
      "Epoch 262/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0644 - acc: 0.5062 - val_loss: 1.5912 - val_acc: 0.3606\n",
      "Epoch 263/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1936 - acc: 0.4546 - val_loss: 1.5799 - val_acc: 0.4000\n",
      "Epoch 264/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1567 - acc: 0.4546 - val_loss: 1.3514 - val_acc: 0.4606\n",
      "Epoch 265/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0773 - acc: 0.5224 - val_loss: 1.3498 - val_acc: 0.4333\n",
      "Epoch 266/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.0859 - acc: 0.5161 - val_loss: 1.3278 - val_acc: 0.4606\n",
      "Epoch 267/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1607 - acc: 0.5030 - val_loss: 1.3852 - val_acc: 0.4121\n",
      "Epoch 268/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1138 - acc: 0.5238 - val_loss: 1.4122 - val_acc: 0.4303\n",
      "Epoch 269/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0773 - acc: 0.5002 - val_loss: 1.6631 - val_acc: 0.3152\n",
      "Epoch 270/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1903 - acc: 0.4260 - val_loss: 1.4175 - val_acc: 0.4273\n",
      "Epoch 271/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0826 - acc: 0.4963 - val_loss: 1.4495 - val_acc: 0.3939\n",
      "Epoch 272/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0896 - acc: 0.4921 - val_loss: 1.5210 - val_acc: 0.3758\n",
      "Epoch 273/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1421 - acc: 0.4687 - val_loss: 1.4859 - val_acc: 0.3818\n",
      "Epoch 274/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1083 - acc: 0.4889 - val_loss: 1.4202 - val_acc: 0.3818\n",
      "Epoch 275/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0654 - acc: 0.5072 - val_loss: 1.5387 - val_acc: 0.3788\n",
      "Epoch 276/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1062 - acc: 0.4825 - val_loss: 1.5537 - val_acc: 0.3606\n",
      "Epoch 277/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1267 - acc: 0.4659 - val_loss: 1.5271 - val_acc: 0.3667\n",
      "Epoch 278/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0798 - acc: 0.4892 - val_loss: 1.4978 - val_acc: 0.3636\n",
      "Epoch 279/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.0677 - acc: 0.4998 - val_loss: 1.5234 - val_acc: 0.3697\n",
      "Epoch 280/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1139 - acc: 0.4864 - val_loss: 1.5071 - val_acc: 0.4000\n",
      "Epoch 281/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0856 - acc: 0.5026 - val_loss: 1.4271 - val_acc: 0.4121\n",
      "Epoch 282/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0800 - acc: 0.4966 - val_loss: 1.4646 - val_acc: 0.4152\n",
      "Epoch 283/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1475 - acc: 0.4772 - val_loss: 1.3122 - val_acc: 0.4788\n",
      "Epoch 284/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.0670 - acc: 0.5101 - val_loss: 1.3244 - val_acc: 0.4485\n",
      "Epoch 285/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0781 - acc: 0.5122 - val_loss: 1.2879 - val_acc: 0.4576\n",
      "Epoch 286/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1056 - acc: 0.5171 - val_loss: 1.3221 - val_acc: 0.4667\n",
      "Epoch 287/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.0507 - acc: 0.5302 - val_loss: 1.3739 - val_acc: 0.4424\n",
      "Epoch 288/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.1166 - acc: 0.5129 - val_loss: 1.3532 - val_acc: 0.4515\n",
      "Epoch 289/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1522 - acc: 0.5026 - val_loss: 1.3427 - val_acc: 0.4091\n",
      "Epoch 290/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.1042 - acc: 0.4719 - val_loss: 1.3211 - val_acc: 0.4515\n",
      "Epoch 291/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.1021 - acc: 0.5143 - val_loss: 1.3655 - val_acc: 0.4576\n",
      "Epoch 292/300\n",
      "2831/2831 [==============================] - 0s 58us/step - loss: 1.0616 - acc: 0.5193 - val_loss: 1.3588 - val_acc: 0.4333\n",
      "Epoch 293/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.0454 - acc: 0.5249 - val_loss: 1.4901 - val_acc: 0.4303\n",
      "Epoch 294/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1249 - acc: 0.4899 - val_loss: 1.5078 - val_acc: 0.3515\n",
      "Epoch 295/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0743 - acc: 0.4889 - val_loss: 1.4728 - val_acc: 0.3848\n",
      "Epoch 296/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0469 - acc: 0.4903 - val_loss: 1.5751 - val_acc: 0.3545\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1444 - acc: 0.4447 - val_loss: 1.5228 - val_acc: 0.3727\n",
      "Epoch 298/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1224 - acc: 0.4617 - val_loss: 1.4977 - val_acc: 0.3697\n",
      "Epoch 299/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0877 - acc: 0.4864 - val_loss: 1.5245 - val_acc: 0.3636\n",
      "Epoch 300/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1166 - acc: 0.4677 - val_loss: 1.4334 - val_acc: 0.3879\n",
      "0\n",
      "Train on 2831 samples, validate on 330 samples\n",
      "Epoch 1/300\n",
      "2831/2831 [==============================] - 1s 462us/step - loss: 2.5445 - acc: 0.2176 - val_loss: 2.4234 - val_acc: 0.0818\n",
      "Epoch 2/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.3086 - acc: 0.2303 - val_loss: 2.3210 - val_acc: 0.3303\n",
      "Epoch 3/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.1724 - acc: 0.3289 - val_loss: 2.2781 - val_acc: 0.3303\n",
      "Epoch 4/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1418 - acc: 0.3419 - val_loss: 2.2350 - val_acc: 0.3394\n",
      "Epoch 5/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1230 - acc: 0.3200 - val_loss: 2.2382 - val_acc: 0.3333\n",
      "Epoch 6/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 2.0822 - acc: 0.3352 - val_loss: 2.1811 - val_acc: 0.3303\n",
      "Epoch 7/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.0671 - acc: 0.3617 - val_loss: 2.1998 - val_acc: 0.3303\n",
      "Epoch 8/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 2.0434 - acc: 0.3359 - val_loss: 2.1556 - val_acc: 0.3273\n",
      "Epoch 9/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.0168 - acc: 0.3557 - val_loss: 2.1579 - val_acc: 0.3333\n",
      "Epoch 10/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.9994 - acc: 0.3370 - val_loss: 2.0745 - val_acc: 0.3333\n",
      "Epoch 11/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.9802 - acc: 0.3606 - val_loss: 2.1409 - val_acc: 0.3182\n",
      "Epoch 12/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.9875 - acc: 0.3398 - val_loss: 2.0736 - val_acc: 0.3212\n",
      "Epoch 13/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.9599 - acc: 0.3490 - val_loss: 2.1093 - val_acc: 0.3273\n",
      "Epoch 14/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.9390 - acc: 0.3416 - val_loss: 2.0669 - val_acc: 0.3333\n",
      "Epoch 15/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.8935 - acc: 0.3762 - val_loss: 2.0777 - val_acc: 0.3212\n",
      "Epoch 16/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.8884 - acc: 0.3688 - val_loss: 2.0174 - val_acc: 0.3273\n",
      "Epoch 17/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8533 - acc: 0.3712 - val_loss: 1.9754 - val_acc: 0.3182\n",
      "Epoch 18/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8471 - acc: 0.3723 - val_loss: 1.9404 - val_acc: 0.3364\n",
      "Epoch 19/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.8552 - acc: 0.3992 - val_loss: 1.9336 - val_acc: 0.3273\n",
      "Epoch 20/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8192 - acc: 0.3790 - val_loss: 1.9181 - val_acc: 0.3212\n",
      "Epoch 21/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.8054 - acc: 0.3818 - val_loss: 1.9871 - val_acc: 0.3273\n",
      "Epoch 22/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8083 - acc: 0.3801 - val_loss: 1.8980 - val_acc: 0.3242\n",
      "Epoch 23/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.7743 - acc: 0.3840 - val_loss: 1.9006 - val_acc: 0.3212\n",
      "Epoch 24/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.7543 - acc: 0.3843 - val_loss: 1.8914 - val_acc: 0.3273\n",
      "Epoch 25/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7395 - acc: 0.3822 - val_loss: 1.8144 - val_acc: 0.3273\n",
      "Epoch 26/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.7289 - acc: 0.4112 - val_loss: 1.8550 - val_acc: 0.3273\n",
      "Epoch 27/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.6936 - acc: 0.4052 - val_loss: 1.7968 - val_acc: 0.3212\n",
      "Epoch 28/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7039 - acc: 0.4009 - val_loss: 1.8016 - val_acc: 0.3212\n",
      "Epoch 29/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.6926 - acc: 0.4108 - val_loss: 1.8638 - val_acc: 0.2909\n",
      "Epoch 30/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6945 - acc: 0.3737 - val_loss: 1.8507 - val_acc: 0.3242\n",
      "Epoch 31/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6569 - acc: 0.3939 - val_loss: 1.9265 - val_acc: 0.3091\n",
      "Epoch 32/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.6675 - acc: 0.3776 - val_loss: 1.9005 - val_acc: 0.3182\n",
      "Epoch 33/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.6514 - acc: 0.3995 - val_loss: 1.8702 - val_acc: 0.3212\n",
      "Epoch 34/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.6312 - acc: 0.3963 - val_loss: 1.9624 - val_acc: 0.3152\n",
      "Epoch 35/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.6630 - acc: 0.3928 - val_loss: 1.7318 - val_acc: 0.3303\n",
      "Epoch 36/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.6377 - acc: 0.4045 - val_loss: 1.7279 - val_acc: 0.3242\n",
      "Epoch 37/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.6105 - acc: 0.3928 - val_loss: 1.7092 - val_acc: 0.3394\n",
      "Epoch 38/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5939 - acc: 0.4225 - val_loss: 1.7427 - val_acc: 0.3273\n",
      "Epoch 39/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5767 - acc: 0.4182 - val_loss: 1.8281 - val_acc: 0.3242\n",
      "Epoch 40/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5944 - acc: 0.3931 - val_loss: 1.7388 - val_acc: 0.3242\n",
      "Epoch 41/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5775 - acc: 0.4112 - val_loss: 1.7848 - val_acc: 0.3182\n",
      "Epoch 42/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5587 - acc: 0.4186 - val_loss: 1.9149 - val_acc: 0.3242\n",
      "Epoch 43/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5841 - acc: 0.4006 - val_loss: 1.7223 - val_acc: 0.3182\n",
      "Epoch 44/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.5373 - acc: 0.4119 - val_loss: 1.6956 - val_acc: 0.3212\n",
      "Epoch 45/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.5244 - acc: 0.4119 - val_loss: 1.6306 - val_acc: 0.3333\n",
      "Epoch 46/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.6581 - acc: 0.4105 - val_loss: 1.7521 - val_acc: 0.3212\n",
      "Epoch 47/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5353 - acc: 0.4097 - val_loss: 1.8090 - val_acc: 0.3152\n",
      "Epoch 48/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5494 - acc: 0.4246 - val_loss: 1.8658 - val_acc: 0.2879\n",
      "Epoch 49/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.5498 - acc: 0.3815 - val_loss: 1.7654 - val_acc: 0.3091\n",
      "Epoch 50/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5251 - acc: 0.3907 - val_loss: 1.6929 - val_acc: 0.3121\n",
      "Epoch 51/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5140 - acc: 0.4073 - val_loss: 1.6377 - val_acc: 0.3212\n",
      "Epoch 52/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5091 - acc: 0.4168 - val_loss: 1.6495 - val_acc: 0.3364\n",
      "Epoch 53/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5209 - acc: 0.4182 - val_loss: 1.6266 - val_acc: 0.3545\n",
      "Epoch 54/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5152 - acc: 0.4405 - val_loss: 1.6889 - val_acc: 0.3182\n",
      "Epoch 55/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4625 - acc: 0.4278 - val_loss: 1.8348 - val_acc: 0.2333\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5416 - acc: 0.3642 - val_loss: 1.7348 - val_acc: 0.3152\n",
      "Epoch 57/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4936 - acc: 0.3999 - val_loss: 1.6528 - val_acc: 0.3182\n",
      "Epoch 58/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4623 - acc: 0.4140 - val_loss: 1.6082 - val_acc: 0.3273\n",
      "Epoch 59/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4747 - acc: 0.4221 - val_loss: 1.6001 - val_acc: 0.3303\n",
      "Epoch 60/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4692 - acc: 0.4256 - val_loss: 1.5965 - val_acc: 0.3273\n",
      "Epoch 61/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4711 - acc: 0.4412 - val_loss: 1.6266 - val_acc: 0.3152\n",
      "Epoch 62/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4430 - acc: 0.4260 - val_loss: 1.5424 - val_acc: 0.3455\n",
      "Epoch 63/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.5165 - acc: 0.4172 - val_loss: 1.7189 - val_acc: 0.3182\n",
      "Epoch 64/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4454 - acc: 0.4256 - val_loss: 1.7079 - val_acc: 0.3182\n",
      "Epoch 65/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4592 - acc: 0.4182 - val_loss: 1.6844 - val_acc: 0.3091\n",
      "Epoch 66/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4304 - acc: 0.4207 - val_loss: 1.6265 - val_acc: 0.3242\n",
      "Epoch 67/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4257 - acc: 0.4158 - val_loss: 1.5140 - val_acc: 0.3364\n",
      "Epoch 68/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.5064 - acc: 0.4327 - val_loss: 1.6124 - val_acc: 0.3333\n",
      "Epoch 69/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4355 - acc: 0.4327 - val_loss: 1.5636 - val_acc: 0.3879\n",
      "Epoch 70/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4417 - acc: 0.4405 - val_loss: 1.6690 - val_acc: 0.3030\n",
      "Epoch 71/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3992 - acc: 0.4271 - val_loss: 1.8772 - val_acc: 0.2424\n",
      "Epoch 72/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4688 - acc: 0.3762 - val_loss: 1.6071 - val_acc: 0.3182\n",
      "Epoch 73/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4044 - acc: 0.4430 - val_loss: 1.6409 - val_acc: 0.3152\n",
      "Epoch 74/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4020 - acc: 0.4235 - val_loss: 1.7152 - val_acc: 0.2576\n",
      "Epoch 75/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4425 - acc: 0.3857 - val_loss: 1.7454 - val_acc: 0.2697\n",
      "Epoch 76/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4198 - acc: 0.3977 - val_loss: 1.7002 - val_acc: 0.3152\n",
      "Epoch 77/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3901 - acc: 0.4239 - val_loss: 1.7696 - val_acc: 0.3152\n",
      "Epoch 78/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4241 - acc: 0.4242 - val_loss: 1.6776 - val_acc: 0.3030\n",
      "Epoch 79/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3819 - acc: 0.4090 - val_loss: 1.7767 - val_acc: 0.2818\n",
      "Epoch 80/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4040 - acc: 0.4002 - val_loss: 1.6377 - val_acc: 0.3212\n",
      "Epoch 81/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3617 - acc: 0.4302 - val_loss: 1.6847 - val_acc: 0.3152\n",
      "Epoch 82/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3676 - acc: 0.4211 - val_loss: 1.6850 - val_acc: 0.3030\n",
      "Epoch 83/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3571 - acc: 0.4225 - val_loss: 1.7171 - val_acc: 0.2879\n",
      "Epoch 84/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3788 - acc: 0.4221 - val_loss: 1.6181 - val_acc: 0.3182\n",
      "Epoch 85/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3378 - acc: 0.4377 - val_loss: 1.6184 - val_acc: 0.3333\n",
      "Epoch 86/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4324 - acc: 0.4105 - val_loss: 1.5175 - val_acc: 0.3515\n",
      "Epoch 87/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3713 - acc: 0.4419 - val_loss: 1.4892 - val_acc: 0.3667\n",
      "Epoch 88/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3906 - acc: 0.4218 - val_loss: 1.4932 - val_acc: 0.3242\n",
      "Epoch 89/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3640 - acc: 0.4461 - val_loss: 1.5734 - val_acc: 0.3242\n",
      "Epoch 90/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3200 - acc: 0.4415 - val_loss: 1.6051 - val_acc: 0.3091\n",
      "Epoch 91/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3535 - acc: 0.4320 - val_loss: 1.5888 - val_acc: 0.3152\n",
      "Epoch 92/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3442 - acc: 0.4278 - val_loss: 1.5769 - val_acc: 0.3030\n",
      "Epoch 93/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3289 - acc: 0.4486 - val_loss: 1.6754 - val_acc: 0.3000\n",
      "Epoch 94/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3815 - acc: 0.4027 - val_loss: 1.7270 - val_acc: 0.3000\n",
      "Epoch 95/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3678 - acc: 0.4002 - val_loss: 1.6957 - val_acc: 0.3212\n",
      "Epoch 96/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3478 - acc: 0.4313 - val_loss: 1.5719 - val_acc: 0.3212\n",
      "Epoch 97/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3060 - acc: 0.4437 - val_loss: 1.6165 - val_acc: 0.3212\n",
      "Epoch 98/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3029 - acc: 0.4493 - val_loss: 1.9015 - val_acc: 0.2515\n",
      "Epoch 99/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4345 - acc: 0.3730 - val_loss: 1.5990 - val_acc: 0.3303\n",
      "Epoch 100/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3946 - acc: 0.4302 - val_loss: 1.5495 - val_acc: 0.3394\n",
      "Epoch 101/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3479 - acc: 0.4327 - val_loss: 1.4850 - val_acc: 0.3333\n",
      "Epoch 102/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3104 - acc: 0.4525 - val_loss: 1.4939 - val_acc: 0.3515\n",
      "Epoch 103/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3098 - acc: 0.4479 - val_loss: 1.4751 - val_acc: 0.3606\n",
      "Epoch 104/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3526 - acc: 0.4475 - val_loss: 1.4966 - val_acc: 0.3364\n",
      "Epoch 105/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3435 - acc: 0.4076 - val_loss: 1.4526 - val_acc: 0.3515\n",
      "Epoch 106/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3759 - acc: 0.4408 - val_loss: 1.5588 - val_acc: 0.3394\n",
      "Epoch 107/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2805 - acc: 0.4504 - val_loss: 1.5433 - val_acc: 0.3424\n",
      "Epoch 108/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2816 - acc: 0.4479 - val_loss: 1.6884 - val_acc: 0.3273\n",
      "Epoch 109/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3644 - acc: 0.4348 - val_loss: 1.6764 - val_acc: 0.2939\n",
      "Epoch 110/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3153 - acc: 0.4097 - val_loss: 1.5802 - val_acc: 0.3152\n",
      "Epoch 111/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2907 - acc: 0.4313 - val_loss: 1.5897 - val_acc: 0.3182\n",
      "Epoch 112/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2971 - acc: 0.4207 - val_loss: 1.6667 - val_acc: 0.3030\n",
      "Epoch 113/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3210 - acc: 0.4196 - val_loss: 1.6730 - val_acc: 0.3152\n",
      "Epoch 114/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3022 - acc: 0.4239 - val_loss: 1.7553 - val_acc: 0.2970\n",
      "Epoch 115/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3491 - acc: 0.4136 - val_loss: 1.5246 - val_acc: 0.3424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2761 - acc: 0.4355 - val_loss: 1.4694 - val_acc: 0.3697\n",
      "Epoch 117/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2808 - acc: 0.4377 - val_loss: 1.4420 - val_acc: 0.3545\n",
      "Epoch 118/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3112 - acc: 0.4412 - val_loss: 1.4955 - val_acc: 0.3576\n",
      "Epoch 119/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2602 - acc: 0.4532 - val_loss: 1.4078 - val_acc: 0.3727\n",
      "Epoch 120/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3045 - acc: 0.4564 - val_loss: 1.4447 - val_acc: 0.3939\n",
      "Epoch 121/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3205 - acc: 0.4588 - val_loss: 1.5343 - val_acc: 0.3515\n",
      "Epoch 122/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3143 - acc: 0.4348 - val_loss: 1.5457 - val_acc: 0.3515\n",
      "Epoch 123/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3079 - acc: 0.4398 - val_loss: 1.4880 - val_acc: 0.3273\n",
      "Epoch 124/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2642 - acc: 0.4557 - val_loss: 1.4490 - val_acc: 0.3667\n",
      "Epoch 125/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2856 - acc: 0.4359 - val_loss: 1.4097 - val_acc: 0.3636\n",
      "Epoch 126/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.3823 - acc: 0.4539 - val_loss: 1.6197 - val_acc: 0.3364\n",
      "Epoch 127/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2937 - acc: 0.4419 - val_loss: 1.6238 - val_acc: 0.3242\n",
      "Epoch 128/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2834 - acc: 0.4401 - val_loss: 1.5510 - val_acc: 0.3242\n",
      "Epoch 129/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2367 - acc: 0.4422 - val_loss: 1.5737 - val_acc: 0.3364\n",
      "Epoch 130/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2538 - acc: 0.4620 - val_loss: 1.6493 - val_acc: 0.3333\n",
      "Epoch 131/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2864 - acc: 0.4341 - val_loss: 1.6439 - val_acc: 0.3000\n",
      "Epoch 132/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2960 - acc: 0.4126 - val_loss: 1.5650 - val_acc: 0.3364\n",
      "Epoch 133/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2723 - acc: 0.4242 - val_loss: 1.4680 - val_acc: 0.3788\n",
      "Epoch 134/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.2445 - acc: 0.4627 - val_loss: 1.6051 - val_acc: 0.3394\n",
      "Epoch 135/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2806 - acc: 0.4366 - val_loss: 1.7273 - val_acc: 0.2909\n",
      "Epoch 136/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3016 - acc: 0.4105 - val_loss: 1.5203 - val_acc: 0.3364\n",
      "Epoch 137/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2290 - acc: 0.4521 - val_loss: 1.4794 - val_acc: 0.3515\n",
      "Epoch 138/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2314 - acc: 0.4518 - val_loss: 1.5306 - val_acc: 0.3515\n",
      "Epoch 139/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2629 - acc: 0.4578 - val_loss: 1.5211 - val_acc: 0.3818\n",
      "Epoch 140/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2500 - acc: 0.4557 - val_loss: 1.6432 - val_acc: 0.3394\n",
      "Epoch 141/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2770 - acc: 0.4511 - val_loss: 1.5152 - val_acc: 0.3212\n",
      "Epoch 142/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2220 - acc: 0.4490 - val_loss: 1.5594 - val_acc: 0.3394\n",
      "Epoch 143/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2385 - acc: 0.4525 - val_loss: 1.6276 - val_acc: 0.3394\n",
      "Epoch 144/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2674 - acc: 0.4288 - val_loss: 1.5383 - val_acc: 0.3303\n",
      "Epoch 145/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2045 - acc: 0.4581 - val_loss: 1.6318 - val_acc: 0.3394\n",
      "Epoch 146/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2873 - acc: 0.4567 - val_loss: 1.4844 - val_acc: 0.3455\n",
      "Epoch 147/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2119 - acc: 0.4465 - val_loss: 1.4188 - val_acc: 0.4030\n",
      "Epoch 148/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2289 - acc: 0.4511 - val_loss: 1.3555 - val_acc: 0.4091\n",
      "Epoch 149/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3423 - acc: 0.4627 - val_loss: 1.4786 - val_acc: 0.3424\n",
      "Epoch 150/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2031 - acc: 0.4712 - val_loss: 1.5121 - val_acc: 0.3333\n",
      "Epoch 151/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2095 - acc: 0.4684 - val_loss: 1.6974 - val_acc: 0.3333\n",
      "Epoch 152/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3081 - acc: 0.4433 - val_loss: 1.5097 - val_acc: 0.3182\n",
      "Epoch 153/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2067 - acc: 0.4433 - val_loss: 1.5117 - val_acc: 0.3394\n",
      "Epoch 154/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2147 - acc: 0.4532 - val_loss: 1.7561 - val_acc: 0.2848\n",
      "Epoch 155/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3465 - acc: 0.3628 - val_loss: 1.5248 - val_acc: 0.3455\n",
      "Epoch 156/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2382 - acc: 0.4553 - val_loss: 1.5434 - val_acc: 0.3424\n",
      "Epoch 157/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2197 - acc: 0.4645 - val_loss: 1.5253 - val_acc: 0.3394\n",
      "Epoch 158/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.2254 - acc: 0.4613 - val_loss: 1.4715 - val_acc: 0.3636\n",
      "Epoch 159/300\n",
      "2831/2831 [==============================] - 0s 58us/step - loss: 1.1910 - acc: 0.4691 - val_loss: 1.4660 - val_acc: 0.3606\n",
      "Epoch 160/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1944 - acc: 0.4659 - val_loss: 1.4001 - val_acc: 0.3970\n",
      "Epoch 161/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2518 - acc: 0.4387 - val_loss: 1.3883 - val_acc: 0.4121\n",
      "Epoch 162/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2891 - acc: 0.4656 - val_loss: 1.4603 - val_acc: 0.3970\n",
      "Epoch 163/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1985 - acc: 0.4716 - val_loss: 1.4672 - val_acc: 0.4030\n",
      "Epoch 164/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1999 - acc: 0.4507 - val_loss: 1.4608 - val_acc: 0.3939\n",
      "Epoch 165/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1777 - acc: 0.4807 - val_loss: 1.6296 - val_acc: 0.3273\n",
      "Epoch 166/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2957 - acc: 0.4083 - val_loss: 1.5461 - val_acc: 0.3394\n",
      "Epoch 167/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1995 - acc: 0.4550 - val_loss: 1.4995 - val_acc: 0.3788\n",
      "Epoch 168/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1816 - acc: 0.4567 - val_loss: 1.6527 - val_acc: 0.3212\n",
      "Epoch 169/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2708 - acc: 0.4380 - val_loss: 1.5086 - val_acc: 0.3606\n",
      "Epoch 170/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1866 - acc: 0.4472 - val_loss: 1.4687 - val_acc: 0.3879\n",
      "Epoch 171/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1832 - acc: 0.4634 - val_loss: 1.4456 - val_acc: 0.4242\n",
      "Epoch 172/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2289 - acc: 0.4567 - val_loss: 1.4448 - val_acc: 0.3909\n",
      "Epoch 173/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2009 - acc: 0.4680 - val_loss: 1.3788 - val_acc: 0.3727\n",
      "Epoch 174/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2480 - acc: 0.4362 - val_loss: 1.3762 - val_acc: 0.3909\n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2042 - acc: 0.4815 - val_loss: 1.4785 - val_acc: 0.4000\n",
      "Epoch 176/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2037 - acc: 0.4596 - val_loss: 1.5671 - val_acc: 0.3848\n",
      "Epoch 177/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1989 - acc: 0.4422 - val_loss: 1.5779 - val_acc: 0.3485\n",
      "Epoch 178/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2069 - acc: 0.4546 - val_loss: 1.6037 - val_acc: 0.3273\n",
      "Epoch 179/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2240 - acc: 0.4525 - val_loss: 1.4995 - val_acc: 0.3364\n",
      "Epoch 180/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1849 - acc: 0.4603 - val_loss: 1.4224 - val_acc: 0.3606\n",
      "Epoch 181/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1507 - acc: 0.4783 - val_loss: 1.3633 - val_acc: 0.4091\n",
      "Epoch 182/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2345 - acc: 0.4564 - val_loss: 1.3699 - val_acc: 0.4152\n",
      "Epoch 183/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2682 - acc: 0.4730 - val_loss: 1.4477 - val_acc: 0.4091\n",
      "Epoch 184/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1947 - acc: 0.4793 - val_loss: 1.4040 - val_acc: 0.4091\n",
      "Epoch 185/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1498 - acc: 0.4952 - val_loss: 1.3846 - val_acc: 0.4000\n",
      "Epoch 186/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1837 - acc: 0.4712 - val_loss: 1.3569 - val_acc: 0.3939\n",
      "Epoch 187/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2212 - acc: 0.4606 - val_loss: 1.3807 - val_acc: 0.3788\n",
      "Epoch 188/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1710 - acc: 0.4800 - val_loss: 1.3894 - val_acc: 0.3848\n",
      "Epoch 189/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1502 - acc: 0.4793 - val_loss: 1.3567 - val_acc: 0.4212\n",
      "Epoch 190/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2007 - acc: 0.4751 - val_loss: 1.3850 - val_acc: 0.4121\n",
      "Epoch 191/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2020 - acc: 0.4765 - val_loss: 1.4420 - val_acc: 0.4303\n",
      "Epoch 192/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2264 - acc: 0.4800 - val_loss: 1.5295 - val_acc: 0.3848\n",
      "Epoch 193/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2076 - acc: 0.4384 - val_loss: 1.5766 - val_acc: 0.3636\n",
      "Epoch 194/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2026 - acc: 0.4334 - val_loss: 1.5147 - val_acc: 0.3515\n",
      "Epoch 195/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1689 - acc: 0.4705 - val_loss: 1.5814 - val_acc: 0.3545\n",
      "Epoch 196/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1984 - acc: 0.4610 - val_loss: 1.4641 - val_acc: 0.3606\n",
      "Epoch 197/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1326 - acc: 0.4832 - val_loss: 1.4410 - val_acc: 0.3848\n",
      "Epoch 198/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1385 - acc: 0.4807 - val_loss: 1.6440 - val_acc: 0.3485\n",
      "Epoch 199/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2313 - acc: 0.4571 - val_loss: 1.4593 - val_acc: 0.3697\n",
      "Epoch 200/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1289 - acc: 0.4800 - val_loss: 1.5379 - val_acc: 0.3606\n",
      "Epoch 201/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2041 - acc: 0.4525 - val_loss: 1.6668 - val_acc: 0.3455\n",
      "Epoch 202/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2342 - acc: 0.3871 - val_loss: 1.4164 - val_acc: 0.4061\n",
      "Epoch 203/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1139 - acc: 0.4988 - val_loss: 1.4685 - val_acc: 0.4091\n",
      "Epoch 204/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1458 - acc: 0.4705 - val_loss: 1.5302 - val_acc: 0.3970\n",
      "Epoch 205/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1786 - acc: 0.4550 - val_loss: 1.6056 - val_acc: 0.3545\n",
      "Epoch 206/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2007 - acc: 0.4539 - val_loss: 1.5065 - val_acc: 0.3576\n",
      "Epoch 207/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1621 - acc: 0.4730 - val_loss: 1.5480 - val_acc: 0.3364\n",
      "Epoch 208/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1933 - acc: 0.4620 - val_loss: 1.4873 - val_acc: 0.3758\n",
      "Epoch 209/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1496 - acc: 0.4666 - val_loss: 1.5290 - val_acc: 0.3818\n",
      "Epoch 210/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1637 - acc: 0.4624 - val_loss: 1.5538 - val_acc: 0.3727\n",
      "Epoch 211/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1636 - acc: 0.4620 - val_loss: 1.5488 - val_acc: 0.3848\n",
      "Epoch 212/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1830 - acc: 0.4408 - val_loss: 1.5778 - val_acc: 0.3515\n",
      "Epoch 213/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1791 - acc: 0.4578 - val_loss: 1.4842 - val_acc: 0.3576\n",
      "Epoch 214/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1370 - acc: 0.4705 - val_loss: 1.4649 - val_acc: 0.3606\n",
      "Epoch 215/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.1468 - acc: 0.4822 - val_loss: 1.4311 - val_acc: 0.3667\n",
      "Epoch 216/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1277 - acc: 0.4733 - val_loss: 1.4304 - val_acc: 0.3909\n",
      "Epoch 217/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1083 - acc: 0.4811 - val_loss: 1.5205 - val_acc: 0.3636\n",
      "Epoch 218/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1499 - acc: 0.4779 - val_loss: 1.6588 - val_acc: 0.3273\n",
      "Epoch 219/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2553 - acc: 0.3956 - val_loss: 1.4768 - val_acc: 0.4030\n",
      "Epoch 220/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1537 - acc: 0.4829 - val_loss: 1.4005 - val_acc: 0.4364\n",
      "Epoch 221/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1199 - acc: 0.4959 - val_loss: 1.4080 - val_acc: 0.4212\n",
      "Epoch 222/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1324 - acc: 0.4945 - val_loss: 1.3654 - val_acc: 0.4242\n",
      "Epoch 223/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1433 - acc: 0.4906 - val_loss: 1.3515 - val_acc: 0.4545\n",
      "Epoch 224/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1818 - acc: 0.4853 - val_loss: 1.3428 - val_acc: 0.4091\n",
      "Epoch 225/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1609 - acc: 0.4716 - val_loss: 1.3699 - val_acc: 0.3788\n",
      "Epoch 226/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1486 - acc: 0.4744 - val_loss: 1.3855 - val_acc: 0.4242\n",
      "Epoch 227/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.0773 - acc: 0.5171 - val_loss: 1.3808 - val_acc: 0.4455\n",
      "Epoch 228/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0858 - acc: 0.4991 - val_loss: 1.3178 - val_acc: 0.4636\n",
      "Epoch 229/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2490 - acc: 0.4751 - val_loss: 1.3592 - val_acc: 0.4394\n",
      "Epoch 230/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1907 - acc: 0.4896 - val_loss: 1.4367 - val_acc: 0.4152\n",
      "Epoch 231/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.1487 - acc: 0.4991 - val_loss: 1.4908 - val_acc: 0.4152\n",
      "Epoch 232/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1532 - acc: 0.4726 - val_loss: 1.4201 - val_acc: 0.4333\n",
      "Epoch 233/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1170 - acc: 0.4804 - val_loss: 1.4369 - val_acc: 0.4394\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1166 - acc: 0.4839 - val_loss: 1.5441 - val_acc: 0.3939\n",
      "Epoch 235/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1847 - acc: 0.4525 - val_loss: 1.5340 - val_acc: 0.3576\n",
      "Epoch 236/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1457 - acc: 0.4670 - val_loss: 1.4627 - val_acc: 0.4030\n",
      "Epoch 237/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1234 - acc: 0.4885 - val_loss: 1.4533 - val_acc: 0.3909\n",
      "Epoch 238/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1152 - acc: 0.4765 - val_loss: 1.4764 - val_acc: 0.3879\n",
      "Epoch 239/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1514 - acc: 0.4641 - val_loss: 1.5584 - val_acc: 0.3758\n",
      "Epoch 240/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1719 - acc: 0.4553 - val_loss: 1.5070 - val_acc: 0.3909\n",
      "Epoch 241/300\n",
      "2831/2831 [==============================] - 0s 80us/step - loss: 1.1341 - acc: 0.4733 - val_loss: 1.4422 - val_acc: 0.4061\n",
      "Epoch 242/300\n",
      "2831/2831 [==============================] - 0s 59us/step - loss: 1.1144 - acc: 0.4857 - val_loss: 1.5497 - val_acc: 0.3879\n",
      "Epoch 243/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.1454 - acc: 0.4479 - val_loss: 1.4547 - val_acc: 0.4061\n",
      "Epoch 244/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0970 - acc: 0.4797 - val_loss: 1.4803 - val_acc: 0.4061\n",
      "Epoch 245/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.1109 - acc: 0.4807 - val_loss: 1.5622 - val_acc: 0.3758\n",
      "Epoch 246/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1536 - acc: 0.4737 - val_loss: 1.5444 - val_acc: 0.3788\n",
      "Epoch 247/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1412 - acc: 0.4500 - val_loss: 1.4914 - val_acc: 0.4121\n",
      "Epoch 248/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1151 - acc: 0.4709 - val_loss: 1.5118 - val_acc: 0.4000\n",
      "Epoch 249/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1196 - acc: 0.4719 - val_loss: 1.4745 - val_acc: 0.3939\n",
      "Epoch 250/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0840 - acc: 0.4839 - val_loss: 1.4603 - val_acc: 0.4152\n",
      "Epoch 251/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1293 - acc: 0.4776 - val_loss: 1.5017 - val_acc: 0.3697\n",
      "Epoch 252/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1670 - acc: 0.4747 - val_loss: 1.3479 - val_acc: 0.4333\n",
      "Epoch 253/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1019 - acc: 0.4836 - val_loss: 1.3204 - val_acc: 0.4424\n",
      "Epoch 254/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1473 - acc: 0.4910 - val_loss: 1.3433 - val_acc: 0.4333\n",
      "Epoch 255/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0938 - acc: 0.5101 - val_loss: 1.3802 - val_acc: 0.4212\n",
      "Epoch 256/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0727 - acc: 0.4981 - val_loss: 1.3078 - val_acc: 0.4788\n",
      "Epoch 257/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1523 - acc: 0.4733 - val_loss: 1.3197 - val_acc: 0.4394\n",
      "Epoch 258/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0941 - acc: 0.5030 - val_loss: 1.3461 - val_acc: 0.4515\n",
      "Epoch 259/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0891 - acc: 0.5228 - val_loss: 1.3605 - val_acc: 0.4455\n",
      "Epoch 260/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.1961 - acc: 0.4804 - val_loss: 1.3311 - val_acc: 0.4091\n",
      "Epoch 261/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.1454 - acc: 0.4733 - val_loss: 1.3871 - val_acc: 0.4182\n",
      "Epoch 262/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1241 - acc: 0.4963 - val_loss: 1.4818 - val_acc: 0.4273\n",
      "Epoch 263/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1591 - acc: 0.4825 - val_loss: 1.5105 - val_acc: 0.3697\n",
      "Epoch 264/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.1059 - acc: 0.4769 - val_loss: 1.4518 - val_acc: 0.4152\n",
      "Epoch 265/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.0761 - acc: 0.4974 - val_loss: 1.5243 - val_acc: 0.4030\n",
      "Epoch 266/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.1232 - acc: 0.4716 - val_loss: 1.5118 - val_acc: 0.4030\n",
      "Epoch 267/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1047 - acc: 0.4825 - val_loss: 1.4490 - val_acc: 0.4182\n",
      "Epoch 268/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.0918 - acc: 0.5041 - val_loss: 1.4614 - val_acc: 0.4424\n",
      "Epoch 269/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1034 - acc: 0.4871 - val_loss: 1.4630 - val_acc: 0.3970\n",
      "Epoch 270/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.0979 - acc: 0.4875 - val_loss: 1.4442 - val_acc: 0.4152\n",
      "Epoch 271/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.0555 - acc: 0.5111 - val_loss: 1.4713 - val_acc: 0.4061\n",
      "Epoch 272/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0927 - acc: 0.4924 - val_loss: 1.4876 - val_acc: 0.3970\n",
      "Epoch 273/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1001 - acc: 0.4793 - val_loss: 1.4766 - val_acc: 0.3970\n",
      "Epoch 274/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.0890 - acc: 0.4797 - val_loss: 1.6192 - val_acc: 0.3394\n",
      "Epoch 275/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1621 - acc: 0.4451 - val_loss: 1.4484 - val_acc: 0.4212\n",
      "Epoch 276/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.0982 - acc: 0.5012 - val_loss: 1.4178 - val_acc: 0.4364\n",
      "Epoch 277/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.1242 - acc: 0.4882 - val_loss: 1.3728 - val_acc: 0.4697\n",
      "Epoch 278/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1073 - acc: 0.5263 - val_loss: 1.3686 - val_acc: 0.4364\n",
      "Epoch 279/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0611 - acc: 0.5122 - val_loss: 1.3663 - val_acc: 0.4455\n",
      "Epoch 280/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0693 - acc: 0.5210 - val_loss: 1.3242 - val_acc: 0.4636\n",
      "Epoch 281/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1740 - acc: 0.4818 - val_loss: 1.3643 - val_acc: 0.4121\n",
      "Epoch 282/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1124 - acc: 0.4832 - val_loss: 1.4176 - val_acc: 0.4424\n",
      "Epoch 283/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0835 - acc: 0.4988 - val_loss: 1.4012 - val_acc: 0.4303\n",
      "Epoch 284/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0659 - acc: 0.5048 - val_loss: 1.4067 - val_acc: 0.4303\n",
      "Epoch 285/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0597 - acc: 0.5171 - val_loss: 1.4351 - val_acc: 0.4273\n",
      "Epoch 286/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0941 - acc: 0.5051 - val_loss: 1.3973 - val_acc: 0.4606\n",
      "Epoch 287/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.0649 - acc: 0.5072 - val_loss: 1.5136 - val_acc: 0.3818\n",
      "Epoch 288/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1148 - acc: 0.4677 - val_loss: 1.6065 - val_acc: 0.3576\n",
      "Epoch 289/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1439 - acc: 0.4398 - val_loss: 1.4411 - val_acc: 0.3939\n",
      "Epoch 290/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.0514 - acc: 0.5101 - val_loss: 1.3716 - val_acc: 0.4515\n",
      "Epoch 291/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.0276 - acc: 0.5203 - val_loss: 1.3516 - val_acc: 0.4727\n",
      "Epoch 292/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1009 - acc: 0.4811 - val_loss: 1.2987 - val_acc: 0.4697\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.1576 - acc: 0.5065 - val_loss: 1.3644 - val_acc: 0.4667\n",
      "Epoch 294/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0549 - acc: 0.5390 - val_loss: 1.3627 - val_acc: 0.4394\n",
      "Epoch 295/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.0920 - acc: 0.5083 - val_loss: 1.3700 - val_acc: 0.4424\n",
      "Epoch 296/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.1336 - acc: 0.5034 - val_loss: 1.3839 - val_acc: 0.4394\n",
      "Epoch 297/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.0351 - acc: 0.5253 - val_loss: 1.5123 - val_acc: 0.4121\n",
      "Epoch 298/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1325 - acc: 0.4673 - val_loss: 1.5577 - val_acc: 0.3909\n",
      "Epoch 299/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1241 - acc: 0.4546 - val_loss: 1.4050 - val_acc: 0.4667\n",
      "Epoch 300/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0391 - acc: 0.5242 - val_loss: 1.5816 - val_acc: 0.3909\n",
      "1\n",
      "Train on 2831 samples, validate on 330 samples\n",
      "Epoch 1/300\n",
      "2831/2831 [==============================] - 1s 496us/step - loss: 2.4142 - acc: 0.3130 - val_loss: 2.3023 - val_acc: 0.3424\n",
      "Epoch 2/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 2.2624 - acc: 0.3419 - val_loss: 2.3152 - val_acc: 0.3242\n",
      "Epoch 3/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 2.1922 - acc: 0.3214 - val_loss: 2.2071 - val_acc: 0.3273\n",
      "Epoch 4/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 2.1401 - acc: 0.3529 - val_loss: 2.2351 - val_acc: 0.3303\n",
      "Epoch 5/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 2.1215 - acc: 0.3585 - val_loss: 2.1452 - val_acc: 0.3273\n",
      "Epoch 6/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 2.0810 - acc: 0.3603 - val_loss: 2.1343 - val_acc: 0.3303\n",
      "Epoch 7/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 2.0333 - acc: 0.3755 - val_loss: 2.1097 - val_acc: 0.3242\n",
      "Epoch 8/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.0287 - acc: 0.3737 - val_loss: 2.1115 - val_acc: 0.3273\n",
      "Epoch 9/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.0079 - acc: 0.3727 - val_loss: 2.0761 - val_acc: 0.3242\n",
      "Epoch 10/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.9814 - acc: 0.3868 - val_loss: 2.1415 - val_acc: 0.3152\n",
      "Epoch 11/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.9919 - acc: 0.3493 - val_loss: 2.0888 - val_acc: 0.3273\n",
      "Epoch 12/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.9540 - acc: 0.3659 - val_loss: 2.0503 - val_acc: 0.3242\n",
      "Epoch 13/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.9087 - acc: 0.3861 - val_loss: 2.0678 - val_acc: 0.3273\n",
      "Epoch 14/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8960 - acc: 0.3921 - val_loss: 2.0465 - val_acc: 0.3212\n",
      "Epoch 15/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8822 - acc: 0.3836 - val_loss: 2.0401 - val_acc: 0.2970\n",
      "Epoch 16/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8848 - acc: 0.3462 - val_loss: 2.0286 - val_acc: 0.3182\n",
      "Epoch 17/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.8603 - acc: 0.3886 - val_loss: 2.0135 - val_acc: 0.3212\n",
      "Epoch 18/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8303 - acc: 0.3871 - val_loss: 2.0106 - val_acc: 0.3242\n",
      "Epoch 19/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8161 - acc: 0.3702 - val_loss: 1.9065 - val_acc: 0.3303\n",
      "Epoch 20/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8076 - acc: 0.3854 - val_loss: 1.9155 - val_acc: 0.3303\n",
      "Epoch 21/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8125 - acc: 0.3886 - val_loss: 1.8485 - val_acc: 0.3273\n",
      "Epoch 22/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7849 - acc: 0.3882 - val_loss: 1.8665 - val_acc: 0.3242\n",
      "Epoch 23/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7476 - acc: 0.4073 - val_loss: 1.8719 - val_acc: 0.3212\n",
      "Epoch 24/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7344 - acc: 0.3917 - val_loss: 1.8179 - val_acc: 0.3273\n",
      "Epoch 25/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.7200 - acc: 0.3907 - val_loss: 1.7805 - val_acc: 0.3303\n",
      "Epoch 26/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.7383 - acc: 0.3967 - val_loss: 1.8489 - val_acc: 0.3273\n",
      "Epoch 27/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7272 - acc: 0.3928 - val_loss: 1.7905 - val_acc: 0.3273\n",
      "Epoch 28/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6942 - acc: 0.3977 - val_loss: 1.7805 - val_acc: 0.3273\n",
      "Epoch 29/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6958 - acc: 0.3896 - val_loss: 1.7908 - val_acc: 0.3273\n",
      "Epoch 30/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.6762 - acc: 0.4002 - val_loss: 1.8095 - val_acc: 0.3152\n",
      "Epoch 31/300\n",
      "2831/2831 [==============================] - 0s 80us/step - loss: 1.6502 - acc: 0.3829 - val_loss: 1.7407 - val_acc: 0.3303\n",
      "Epoch 32/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.6340 - acc: 0.4076 - val_loss: 1.7450 - val_acc: 0.3303\n",
      "Epoch 33/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.6435 - acc: 0.3928 - val_loss: 1.6984 - val_acc: 0.3303\n",
      "Epoch 34/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6367 - acc: 0.4105 - val_loss: 1.7599 - val_acc: 0.3212\n",
      "Epoch 35/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.6128 - acc: 0.3928 - val_loss: 1.7553 - val_acc: 0.3273\n",
      "Epoch 36/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.6470 - acc: 0.3956 - val_loss: 1.7592 - val_acc: 0.3303\n",
      "Epoch 37/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.6312 - acc: 0.3953 - val_loss: 1.7014 - val_acc: 0.3303\n",
      "Epoch 38/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.5889 - acc: 0.4023 - val_loss: 1.7168 - val_acc: 0.3273\n",
      "Epoch 39/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.5766 - acc: 0.3984 - val_loss: 1.6852 - val_acc: 0.3273\n",
      "Epoch 40/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5693 - acc: 0.4094 - val_loss: 1.6748 - val_acc: 0.3303\n",
      "Epoch 41/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.5647 - acc: 0.4115 - val_loss: 1.6396 - val_acc: 0.3333\n",
      "Epoch 42/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.5723 - acc: 0.4232 - val_loss: 1.7146 - val_acc: 0.3182\n",
      "Epoch 43/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.5174 - acc: 0.4069 - val_loss: 1.6974 - val_acc: 0.3212\n",
      "Epoch 44/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.5671 - acc: 0.3857 - val_loss: 1.6031 - val_acc: 0.3394\n",
      "Epoch 45/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.5580 - acc: 0.4080 - val_loss: 1.7072 - val_acc: 0.3182\n",
      "Epoch 46/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.5108 - acc: 0.4147 - val_loss: 1.9138 - val_acc: 0.3152\n",
      "Epoch 47/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5670 - acc: 0.3882 - val_loss: 1.6918 - val_acc: 0.3152\n",
      "Epoch 48/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.5102 - acc: 0.4158 - val_loss: 1.6867 - val_acc: 0.3152\n",
      "Epoch 49/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.5318 - acc: 0.4066 - val_loss: 1.7730 - val_acc: 0.2424\n",
      "Epoch 50/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.5323 - acc: 0.3709 - val_loss: 1.7852 - val_acc: 0.2970\n",
      "Epoch 51/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.5004 - acc: 0.3829 - val_loss: 1.7654 - val_acc: 0.3121\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.4851 - acc: 0.4037 - val_loss: 1.7047 - val_acc: 0.3152\n",
      "Epoch 53/300\n",
      "2831/2831 [==============================] - 0s 59us/step - loss: 1.4665 - acc: 0.4161 - val_loss: 1.7199 - val_acc: 0.3182\n",
      "Epoch 54/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.4824 - acc: 0.4059 - val_loss: 1.6919 - val_acc: 0.3152\n",
      "Epoch 55/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.4718 - acc: 0.4122 - val_loss: 1.6375 - val_acc: 0.3212\n",
      "Epoch 56/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.4530 - acc: 0.4221 - val_loss: 1.5994 - val_acc: 0.3242\n",
      "Epoch 57/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.4684 - acc: 0.4037 - val_loss: 1.5453 - val_acc: 0.3333\n",
      "Epoch 58/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.4947 - acc: 0.4299 - val_loss: 1.6543 - val_acc: 0.3121\n",
      "Epoch 59/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.4384 - acc: 0.4165 - val_loss: 1.7529 - val_acc: 0.2788\n",
      "Epoch 60/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.4622 - acc: 0.3939 - val_loss: 1.7218 - val_acc: 0.2879\n",
      "Epoch 61/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4456 - acc: 0.3917 - val_loss: 1.6577 - val_acc: 0.3121\n",
      "Epoch 62/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4149 - acc: 0.4094 - val_loss: 1.5632 - val_acc: 0.3152\n",
      "Epoch 63/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4652 - acc: 0.4115 - val_loss: 1.5009 - val_acc: 0.3333\n",
      "Epoch 64/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5007 - acc: 0.4221 - val_loss: 1.7033 - val_acc: 0.3242\n",
      "Epoch 65/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4592 - acc: 0.4048 - val_loss: 1.6287 - val_acc: 0.3152\n",
      "Epoch 66/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4097 - acc: 0.4165 - val_loss: 1.6317 - val_acc: 0.3030\n",
      "Epoch 67/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3938 - acc: 0.4182 - val_loss: 1.6629 - val_acc: 0.3121\n",
      "Epoch 68/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4172 - acc: 0.4214 - val_loss: 1.8310 - val_acc: 0.2909\n",
      "Epoch 69/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4733 - acc: 0.4037 - val_loss: 1.6983 - val_acc: 0.2636\n",
      "Epoch 70/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4297 - acc: 0.3822 - val_loss: 1.6840 - val_acc: 0.2788\n",
      "Epoch 71/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4021 - acc: 0.4023 - val_loss: 1.7804 - val_acc: 0.2606\n",
      "Epoch 72/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4339 - acc: 0.3847 - val_loss: 1.6536 - val_acc: 0.3152\n",
      "Epoch 73/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3845 - acc: 0.4200 - val_loss: 1.5534 - val_acc: 0.3091\n",
      "Epoch 74/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3794 - acc: 0.4267 - val_loss: 1.5658 - val_acc: 0.3303\n",
      "Epoch 75/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3973 - acc: 0.4143 - val_loss: 1.5781 - val_acc: 0.3273\n",
      "Epoch 76/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4043 - acc: 0.4165 - val_loss: 1.6762 - val_acc: 0.3152\n",
      "Epoch 77/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4065 - acc: 0.4267 - val_loss: 1.7252 - val_acc: 0.3091\n",
      "Epoch 78/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3745 - acc: 0.4105 - val_loss: 1.7405 - val_acc: 0.2727\n",
      "Epoch 79/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3844 - acc: 0.4016 - val_loss: 1.6054 - val_acc: 0.3182\n",
      "Epoch 80/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3438 - acc: 0.4271 - val_loss: 1.5469 - val_acc: 0.3121\n",
      "Epoch 81/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3672 - acc: 0.4097 - val_loss: 1.4549 - val_acc: 0.3364\n",
      "Epoch 82/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4348 - acc: 0.4193 - val_loss: 1.6083 - val_acc: 0.3091\n",
      "Epoch 83/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3627 - acc: 0.4175 - val_loss: 1.6964 - val_acc: 0.3152\n",
      "Epoch 84/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3912 - acc: 0.4235 - val_loss: 1.6651 - val_acc: 0.2970\n",
      "Epoch 85/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3436 - acc: 0.4200 - val_loss: 1.6787 - val_acc: 0.2788\n",
      "Epoch 86/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3620 - acc: 0.4122 - val_loss: 1.6992 - val_acc: 0.2848\n",
      "Epoch 87/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3564 - acc: 0.4115 - val_loss: 1.6458 - val_acc: 0.2758\n",
      "Epoch 88/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3505 - acc: 0.3999 - val_loss: 1.6714 - val_acc: 0.2394\n",
      "Epoch 89/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3746 - acc: 0.3878 - val_loss: 1.5146 - val_acc: 0.3242\n",
      "Epoch 90/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3263 - acc: 0.4274 - val_loss: 1.4840 - val_acc: 0.3212\n",
      "Epoch 91/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3701 - acc: 0.4242 - val_loss: 1.4384 - val_acc: 0.3394\n",
      "Epoch 92/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4080 - acc: 0.4401 - val_loss: 1.5790 - val_acc: 0.3273\n",
      "Epoch 93/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3451 - acc: 0.4228 - val_loss: 1.5545 - val_acc: 0.3242\n",
      "Epoch 94/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3443 - acc: 0.4249 - val_loss: 1.5647 - val_acc: 0.3212\n",
      "Epoch 95/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3418 - acc: 0.4172 - val_loss: 1.5430 - val_acc: 0.3242\n",
      "Epoch 96/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3059 - acc: 0.4324 - val_loss: 1.4664 - val_acc: 0.3273\n",
      "Epoch 97/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3349 - acc: 0.4292 - val_loss: 1.4344 - val_acc: 0.3333\n",
      "Epoch 98/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3677 - acc: 0.4331 - val_loss: 1.5368 - val_acc: 0.3182\n",
      "Epoch 99/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3052 - acc: 0.4267 - val_loss: 1.5162 - val_acc: 0.3273\n",
      "Epoch 100/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2990 - acc: 0.4214 - val_loss: 1.4736 - val_acc: 0.3303\n",
      "Epoch 101/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3707 - acc: 0.4313 - val_loss: 1.6841 - val_acc: 0.3121\n",
      "Epoch 102/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3456 - acc: 0.4281 - val_loss: 1.6928 - val_acc: 0.2788\n",
      "Epoch 103/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3278 - acc: 0.4193 - val_loss: 1.5765 - val_acc: 0.3121\n",
      "Epoch 104/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2907 - acc: 0.4327 - val_loss: 1.6363 - val_acc: 0.3152\n",
      "Epoch 105/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3045 - acc: 0.4158 - val_loss: 1.7233 - val_acc: 0.2879\n",
      "Epoch 106/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3417 - acc: 0.4062 - val_loss: 1.6279 - val_acc: 0.3000\n",
      "Epoch 107/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3167 - acc: 0.4016 - val_loss: 1.5543 - val_acc: 0.3000\n",
      "Epoch 108/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3207 - acc: 0.4150 - val_loss: 1.4228 - val_acc: 0.3364\n",
      "Epoch 109/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3231 - acc: 0.4461 - val_loss: 1.5640 - val_acc: 0.2909\n",
      "Epoch 110/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3092 - acc: 0.4196 - val_loss: 1.7440 - val_acc: 0.2152\n",
      "Epoch 111/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3627 - acc: 0.3624 - val_loss: 1.6081 - val_acc: 0.3242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3075 - acc: 0.4327 - val_loss: 1.7429 - val_acc: 0.2727\n",
      "Epoch 113/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.3260 - acc: 0.4094 - val_loss: 1.5699 - val_acc: 0.3030\n",
      "Epoch 114/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2651 - acc: 0.4348 - val_loss: 1.5772 - val_acc: 0.3030\n",
      "Epoch 115/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2615 - acc: 0.4398 - val_loss: 1.5776 - val_acc: 0.3061\n",
      "Epoch 116/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2852 - acc: 0.4246 - val_loss: 1.5068 - val_acc: 0.3273\n",
      "Epoch 117/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3007 - acc: 0.4419 - val_loss: 1.5477 - val_acc: 0.3242\n",
      "Epoch 118/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2621 - acc: 0.4398 - val_loss: 1.8103 - val_acc: 0.2545\n",
      "Epoch 119/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3756 - acc: 0.4006 - val_loss: 1.5902 - val_acc: 0.2788\n",
      "Epoch 120/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2975 - acc: 0.4246 - val_loss: 1.4879 - val_acc: 0.3152\n",
      "Epoch 121/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2722 - acc: 0.4369 - val_loss: 1.4782 - val_acc: 0.3182\n",
      "Epoch 122/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2491 - acc: 0.4440 - val_loss: 1.5059 - val_acc: 0.3091\n",
      "Epoch 123/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2623 - acc: 0.4345 - val_loss: 1.4288 - val_acc: 0.3364\n",
      "Epoch 124/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2998 - acc: 0.4419 - val_loss: 1.4122 - val_acc: 0.3424\n",
      "Epoch 125/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2984 - acc: 0.4437 - val_loss: 1.4249 - val_acc: 0.3364\n",
      "Epoch 126/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2996 - acc: 0.4285 - val_loss: 1.4599 - val_acc: 0.3455\n",
      "Epoch 127/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2615 - acc: 0.4352 - val_loss: 1.4821 - val_acc: 0.3303\n",
      "Epoch 128/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2482 - acc: 0.4472 - val_loss: 1.4667 - val_acc: 0.3364\n",
      "Epoch 129/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2573 - acc: 0.4468 - val_loss: 1.4977 - val_acc: 0.3424\n",
      "Epoch 130/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2712 - acc: 0.4398 - val_loss: 1.6439 - val_acc: 0.2970\n",
      "Epoch 131/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2833 - acc: 0.4415 - val_loss: 1.6108 - val_acc: 0.2879\n",
      "Epoch 132/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2543 - acc: 0.4256 - val_loss: 1.5634 - val_acc: 0.3061\n",
      "Epoch 133/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2416 - acc: 0.4218 - val_loss: 1.5701 - val_acc: 0.3182\n",
      "Epoch 134/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2578 - acc: 0.4377 - val_loss: 1.7928 - val_acc: 0.2455\n",
      "Epoch 135/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3540 - acc: 0.3804 - val_loss: 1.4987 - val_acc: 0.3242\n",
      "Epoch 136/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2744 - acc: 0.4369 - val_loss: 1.5378 - val_acc: 0.3273\n",
      "Epoch 137/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2702 - acc: 0.4331 - val_loss: 1.4954 - val_acc: 0.3061\n",
      "Epoch 138/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2123 - acc: 0.4458 - val_loss: 1.4259 - val_acc: 0.3364\n",
      "Epoch 139/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2279 - acc: 0.4334 - val_loss: 1.3647 - val_acc: 0.3727\n",
      "Epoch 140/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3124 - acc: 0.4451 - val_loss: 1.4965 - val_acc: 0.3697\n",
      "Epoch 141/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2649 - acc: 0.4514 - val_loss: 1.4909 - val_acc: 0.3727\n",
      "Epoch 142/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2725 - acc: 0.4278 - val_loss: 1.3891 - val_acc: 0.3394\n",
      "Epoch 143/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2372 - acc: 0.4546 - val_loss: 1.3920 - val_acc: 0.3545\n",
      "Epoch 144/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2612 - acc: 0.4451 - val_loss: 1.4085 - val_acc: 0.3788\n",
      "Epoch 145/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2411 - acc: 0.4638 - val_loss: 1.4656 - val_acc: 0.3697\n",
      "Epoch 146/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2583 - acc: 0.4334 - val_loss: 1.3509 - val_acc: 0.3606\n",
      "Epoch 147/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3425 - acc: 0.4543 - val_loss: 1.4607 - val_acc: 0.3364\n",
      "Epoch 148/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2095 - acc: 0.4468 - val_loss: 1.5130 - val_acc: 0.3364\n",
      "Epoch 149/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2166 - acc: 0.4401 - val_loss: 1.7531 - val_acc: 0.2848\n",
      "Epoch 150/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3248 - acc: 0.3999 - val_loss: 1.5237 - val_acc: 0.3364\n",
      "Epoch 151/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2194 - acc: 0.4419 - val_loss: 1.4956 - val_acc: 0.3576\n",
      "Epoch 152/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2182 - acc: 0.4433 - val_loss: 1.5997 - val_acc: 0.3303\n",
      "Epoch 153/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2640 - acc: 0.4136 - val_loss: 1.4386 - val_acc: 0.3394\n",
      "Epoch 154/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1938 - acc: 0.4486 - val_loss: 1.4129 - val_acc: 0.3788\n",
      "Epoch 155/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2693 - acc: 0.4479 - val_loss: 1.3729 - val_acc: 0.3758\n",
      "Epoch 156/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2550 - acc: 0.4659 - val_loss: 1.4928 - val_acc: 0.3242\n",
      "Epoch 157/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2208 - acc: 0.4398 - val_loss: 1.4129 - val_acc: 0.3515\n",
      "Epoch 158/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2116 - acc: 0.4437 - val_loss: 1.3744 - val_acc: 0.3848\n",
      "Epoch 159/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2286 - acc: 0.4599 - val_loss: 1.4178 - val_acc: 0.3818\n",
      "Epoch 160/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2687 - acc: 0.4444 - val_loss: 1.4051 - val_acc: 0.3545\n",
      "Epoch 161/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1999 - acc: 0.4581 - val_loss: 1.4048 - val_acc: 0.3515\n",
      "Epoch 162/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1924 - acc: 0.4479 - val_loss: 1.3554 - val_acc: 0.3606\n",
      "Epoch 163/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3145 - acc: 0.4313 - val_loss: 1.4677 - val_acc: 0.3424\n",
      "Epoch 164/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1977 - acc: 0.4500 - val_loss: 1.3897 - val_acc: 0.3727\n",
      "Epoch 165/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1861 - acc: 0.4698 - val_loss: 1.4233 - val_acc: 0.3727\n",
      "Epoch 166/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2563 - acc: 0.4486 - val_loss: 1.4908 - val_acc: 0.3515\n",
      "Epoch 167/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2396 - acc: 0.4571 - val_loss: 1.4513 - val_acc: 0.3636\n",
      "Epoch 168/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1852 - acc: 0.4578 - val_loss: 1.6029 - val_acc: 0.2939\n",
      "Epoch 169/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.2402 - acc: 0.4232 - val_loss: 1.5624 - val_acc: 0.3364\n",
      "Epoch 170/300\n",
      "2831/2831 [==============================] - 0s 59us/step - loss: 1.1995 - acc: 0.4461 - val_loss: 1.5200 - val_acc: 0.3364\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1840 - acc: 0.4571 - val_loss: 1.5836 - val_acc: 0.3242\n",
      "Epoch 172/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2152 - acc: 0.4373 - val_loss: 1.5790 - val_acc: 0.3303\n",
      "Epoch 173/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2107 - acc: 0.4281 - val_loss: 1.5061 - val_acc: 0.3394\n",
      "Epoch 174/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2060 - acc: 0.4415 - val_loss: 1.5564 - val_acc: 0.3212\n",
      "Epoch 175/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2088 - acc: 0.4408 - val_loss: 1.4985 - val_acc: 0.3333\n",
      "Epoch 176/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1755 - acc: 0.4550 - val_loss: 1.5784 - val_acc: 0.3212\n",
      "Epoch 177/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2343 - acc: 0.4377 - val_loss: 1.5991 - val_acc: 0.3273\n",
      "Epoch 178/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2109 - acc: 0.4334 - val_loss: 1.4509 - val_acc: 0.3848\n",
      "Epoch 179/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1672 - acc: 0.4634 - val_loss: 1.4307 - val_acc: 0.3879\n",
      "Epoch 180/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1711 - acc: 0.4726 - val_loss: 1.4113 - val_acc: 0.3909\n",
      "Epoch 181/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2235 - acc: 0.4712 - val_loss: 1.3654 - val_acc: 0.3848\n",
      "Epoch 182/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2166 - acc: 0.4765 - val_loss: 1.3988 - val_acc: 0.3848\n",
      "Epoch 183/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1669 - acc: 0.4790 - val_loss: 1.4300 - val_acc: 0.3758\n",
      "Epoch 184/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1855 - acc: 0.4571 - val_loss: 1.4057 - val_acc: 0.3848\n",
      "Epoch 185/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1590 - acc: 0.4839 - val_loss: 1.5106 - val_acc: 0.3636\n",
      "Epoch 186/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2000 - acc: 0.4497 - val_loss: 1.6909 - val_acc: 0.2697\n",
      "Epoch 187/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2852 - acc: 0.3723 - val_loss: 1.5828 - val_acc: 0.3242\n",
      "Epoch 188/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2178 - acc: 0.4546 - val_loss: 1.4923 - val_acc: 0.3394\n",
      "Epoch 189/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1557 - acc: 0.4617 - val_loss: 1.5379 - val_acc: 0.3515\n",
      "Epoch 190/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2079 - acc: 0.4292 - val_loss: 1.6005 - val_acc: 0.3242\n",
      "Epoch 191/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2065 - acc: 0.4168 - val_loss: 1.4329 - val_acc: 0.3667\n",
      "Epoch 192/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1503 - acc: 0.4783 - val_loss: 1.6038 - val_acc: 0.3121\n",
      "Epoch 193/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2314 - acc: 0.4419 - val_loss: 1.5260 - val_acc: 0.3485\n",
      "Epoch 194/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1715 - acc: 0.4705 - val_loss: 1.4304 - val_acc: 0.3455\n",
      "Epoch 195/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1480 - acc: 0.4638 - val_loss: 1.4180 - val_acc: 0.3636\n",
      "Epoch 196/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1466 - acc: 0.4603 - val_loss: 1.3983 - val_acc: 0.3758\n",
      "Epoch 197/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1938 - acc: 0.4504 - val_loss: 1.3470 - val_acc: 0.3879\n",
      "Epoch 198/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1624 - acc: 0.4673 - val_loss: 1.3565 - val_acc: 0.3970\n",
      "Epoch 199/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2208 - acc: 0.4684 - val_loss: 1.3667 - val_acc: 0.3970\n",
      "Epoch 200/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1729 - acc: 0.4878 - val_loss: 1.4070 - val_acc: 0.3758\n",
      "Epoch 201/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1501 - acc: 0.4755 - val_loss: 1.5637 - val_acc: 0.3424\n",
      "Epoch 202/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2064 - acc: 0.4203 - val_loss: 1.5515 - val_acc: 0.3364\n",
      "Epoch 203/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1873 - acc: 0.4458 - val_loss: 1.6049 - val_acc: 0.3515\n",
      "Epoch 204/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1903 - acc: 0.4433 - val_loss: 1.4536 - val_acc: 0.3515\n",
      "Epoch 205/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1258 - acc: 0.4740 - val_loss: 1.4685 - val_acc: 0.3636\n",
      "Epoch 206/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1377 - acc: 0.4673 - val_loss: 1.5785 - val_acc: 0.3485\n",
      "Epoch 207/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1968 - acc: 0.4387 - val_loss: 1.5363 - val_acc: 0.3424\n",
      "Epoch 208/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1813 - acc: 0.4550 - val_loss: 1.4716 - val_acc: 0.3424\n",
      "Epoch 209/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2088 - acc: 0.4493 - val_loss: 1.3694 - val_acc: 0.3697\n",
      "Epoch 210/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1551 - acc: 0.4737 - val_loss: 1.3260 - val_acc: 0.3909\n",
      "Epoch 211/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1784 - acc: 0.4846 - val_loss: 1.3503 - val_acc: 0.4030\n",
      "Epoch 212/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1464 - acc: 0.4931 - val_loss: 1.3784 - val_acc: 0.4030\n",
      "Epoch 213/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2030 - acc: 0.4634 - val_loss: 1.3540 - val_acc: 0.3939\n",
      "Epoch 214/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1559 - acc: 0.4949 - val_loss: 1.3601 - val_acc: 0.3909\n",
      "Epoch 215/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1332 - acc: 0.4726 - val_loss: 1.3284 - val_acc: 0.4030\n",
      "Epoch 216/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1712 - acc: 0.4659 - val_loss: 1.3536 - val_acc: 0.4000\n",
      "Epoch 217/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1223 - acc: 0.4928 - val_loss: 1.3540 - val_acc: 0.4000\n",
      "Epoch 218/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1351 - acc: 0.4938 - val_loss: 1.3702 - val_acc: 0.3970\n",
      "Epoch 219/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2018 - acc: 0.4892 - val_loss: 1.4241 - val_acc: 0.3727\n",
      "Epoch 220/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1380 - acc: 0.4779 - val_loss: 1.5132 - val_acc: 0.3576\n",
      "Epoch 221/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1676 - acc: 0.4412 - val_loss: 1.4586 - val_acc: 0.3939\n",
      "Epoch 222/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1389 - acc: 0.4857 - val_loss: 1.4529 - val_acc: 0.3879\n",
      "Epoch 223/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1270 - acc: 0.4797 - val_loss: 1.6063 - val_acc: 0.3121\n",
      "Epoch 224/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1893 - acc: 0.4426 - val_loss: 1.4938 - val_acc: 0.3758\n",
      "Epoch 225/300\n",
      "2831/2831 [==============================] - 0s 79us/step - loss: 1.1459 - acc: 0.4740 - val_loss: 1.4527 - val_acc: 0.3636\n",
      "Epoch 226/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.1236 - acc: 0.4687 - val_loss: 1.5437 - val_acc: 0.3394\n",
      "Epoch 227/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1808 - acc: 0.4535 - val_loss: 1.5834 - val_acc: 0.3364\n",
      "Epoch 228/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1751 - acc: 0.4412 - val_loss: 1.4255 - val_acc: 0.3879\n",
      "Epoch 229/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0968 - acc: 0.4864 - val_loss: 1.3741 - val_acc: 0.4121\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1023 - acc: 0.4875 - val_loss: 1.3293 - val_acc: 0.4212\n",
      "Epoch 231/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2176 - acc: 0.4680 - val_loss: 1.3452 - val_acc: 0.4000\n",
      "Epoch 232/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1385 - acc: 0.4938 - val_loss: 1.3783 - val_acc: 0.3848\n",
      "Epoch 233/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1140 - acc: 0.4758 - val_loss: 1.3709 - val_acc: 0.4000\n",
      "Epoch 234/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2074 - acc: 0.4807 - val_loss: 1.3648 - val_acc: 0.4152\n",
      "Epoch 235/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0963 - acc: 0.4956 - val_loss: 1.3741 - val_acc: 0.4182\n",
      "Epoch 236/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0825 - acc: 0.4998 - val_loss: 1.4010 - val_acc: 0.4242\n",
      "Epoch 237/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1567 - acc: 0.4807 - val_loss: 1.3673 - val_acc: 0.4333\n",
      "Epoch 238/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1655 - acc: 0.4981 - val_loss: 1.3172 - val_acc: 0.4152\n",
      "Epoch 239/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1635 - acc: 0.4751 - val_loss: 1.3186 - val_acc: 0.4030\n",
      "Epoch 240/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1327 - acc: 0.5104 - val_loss: 1.3691 - val_acc: 0.3879\n",
      "Epoch 241/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1044 - acc: 0.4981 - val_loss: 1.3929 - val_acc: 0.4091\n",
      "Epoch 242/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1382 - acc: 0.5023 - val_loss: 1.3198 - val_acc: 0.4212\n",
      "Epoch 243/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1972 - acc: 0.4797 - val_loss: 1.3466 - val_acc: 0.4121\n",
      "Epoch 244/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1029 - acc: 0.4977 - val_loss: 1.3793 - val_acc: 0.3909\n",
      "Epoch 245/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0869 - acc: 0.4928 - val_loss: 1.3140 - val_acc: 0.4303\n",
      "Epoch 246/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1595 - acc: 0.4783 - val_loss: 1.3124 - val_acc: 0.4273\n",
      "Epoch 247/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1261 - acc: 0.4931 - val_loss: 1.3548 - val_acc: 0.4091\n",
      "Epoch 248/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0989 - acc: 0.4917 - val_loss: 1.3470 - val_acc: 0.4424\n",
      "Epoch 249/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1140 - acc: 0.4882 - val_loss: 1.4812 - val_acc: 0.4030\n",
      "Epoch 250/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1945 - acc: 0.4811 - val_loss: 1.5032 - val_acc: 0.3818\n",
      "Epoch 251/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1725 - acc: 0.4610 - val_loss: 1.4751 - val_acc: 0.3697\n",
      "Epoch 252/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1162 - acc: 0.4726 - val_loss: 1.4473 - val_acc: 0.3788\n",
      "Epoch 253/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0861 - acc: 0.5044 - val_loss: 1.5810 - val_acc: 0.3485\n",
      "Epoch 254/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1787 - acc: 0.4553 - val_loss: 1.5537 - val_acc: 0.3606\n",
      "Epoch 255/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1362 - acc: 0.4528 - val_loss: 1.4654 - val_acc: 0.3909\n",
      "Epoch 256/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0940 - acc: 0.4903 - val_loss: 1.4052 - val_acc: 0.4273\n",
      "Epoch 257/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0881 - acc: 0.5026 - val_loss: 1.4448 - val_acc: 0.4303\n",
      "Epoch 258/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1036 - acc: 0.4892 - val_loss: 1.3883 - val_acc: 0.4333\n",
      "Epoch 259/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0853 - acc: 0.5009 - val_loss: 1.3349 - val_acc: 0.4273\n",
      "Epoch 260/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1153 - acc: 0.4981 - val_loss: 1.2973 - val_acc: 0.4636\n",
      "Epoch 261/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2301 - acc: 0.4751 - val_loss: 1.3628 - val_acc: 0.4091\n",
      "Epoch 262/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0975 - acc: 0.5044 - val_loss: 1.4080 - val_acc: 0.4000\n",
      "Epoch 263/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1003 - acc: 0.4917 - val_loss: 1.5258 - val_acc: 0.3667\n",
      "Epoch 264/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1429 - acc: 0.4772 - val_loss: 1.3995 - val_acc: 0.3879\n",
      "Epoch 265/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0870 - acc: 0.4928 - val_loss: 1.5118 - val_acc: 0.3667\n",
      "Epoch 266/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1338 - acc: 0.4719 - val_loss: 1.5107 - val_acc: 0.3636\n",
      "Epoch 267/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0869 - acc: 0.4860 - val_loss: 1.4574 - val_acc: 0.3758\n",
      "Epoch 268/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0875 - acc: 0.4988 - val_loss: 1.4636 - val_acc: 0.3970\n",
      "Epoch 269/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1192 - acc: 0.4825 - val_loss: 1.4439 - val_acc: 0.3788\n",
      "Epoch 270/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1464 - acc: 0.4666 - val_loss: 1.3134 - val_acc: 0.4545\n",
      "Epoch 271/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1594 - acc: 0.4702 - val_loss: 1.3168 - val_acc: 0.4485\n",
      "Epoch 272/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1252 - acc: 0.5016 - val_loss: 1.3959 - val_acc: 0.4030\n",
      "Epoch 273/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0846 - acc: 0.5115 - val_loss: 1.3738 - val_acc: 0.4455\n",
      "Epoch 274/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0753 - acc: 0.5207 - val_loss: 1.3603 - val_acc: 0.4515\n",
      "Epoch 275/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1290 - acc: 0.5051 - val_loss: 1.3277 - val_acc: 0.4182\n",
      "Epoch 276/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1267 - acc: 0.5051 - val_loss: 1.3486 - val_acc: 0.4091\n",
      "Epoch 277/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0689 - acc: 0.5125 - val_loss: 1.3481 - val_acc: 0.4515\n",
      "Epoch 278/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1237 - acc: 0.4800 - val_loss: 1.3400 - val_acc: 0.4576\n",
      "Epoch 279/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0729 - acc: 0.5090 - val_loss: 1.3572 - val_acc: 0.4121\n",
      "Epoch 280/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0422 - acc: 0.5291 - val_loss: 1.4482 - val_acc: 0.4273\n",
      "Epoch 281/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1501 - acc: 0.4875 - val_loss: 1.6296 - val_acc: 0.3515\n",
      "Epoch 282/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2117 - acc: 0.4165 - val_loss: 1.5037 - val_acc: 0.3879\n",
      "Epoch 283/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1128 - acc: 0.4938 - val_loss: 1.4173 - val_acc: 0.4212\n",
      "Epoch 284/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0779 - acc: 0.4959 - val_loss: 1.3940 - val_acc: 0.4121\n",
      "Epoch 285/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0421 - acc: 0.5118 - val_loss: 1.4290 - val_acc: 0.4182\n",
      "Epoch 286/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0588 - acc: 0.5094 - val_loss: 1.4868 - val_acc: 0.3667\n",
      "Epoch 287/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1226 - acc: 0.4744 - val_loss: 1.5708 - val_acc: 0.3848\n",
      "Epoch 288/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1433 - acc: 0.4719 - val_loss: 1.4197 - val_acc: 0.3848\n",
      "Epoch 289/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0760 - acc: 0.4928 - val_loss: 1.3508 - val_acc: 0.4485\n",
      "Epoch 290/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0635 - acc: 0.5108 - val_loss: 1.3876 - val_acc: 0.4394\n",
      "Epoch 291/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0734 - acc: 0.5048 - val_loss: 1.5425 - val_acc: 0.3667\n",
      "Epoch 292/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1729 - acc: 0.4574 - val_loss: 1.4598 - val_acc: 0.3697\n",
      "Epoch 293/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0849 - acc: 0.4917 - val_loss: 1.3956 - val_acc: 0.3970\n",
      "Epoch 294/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.0340 - acc: 0.5221 - val_loss: 1.4323 - val_acc: 0.3970\n",
      "Epoch 295/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0936 - acc: 0.5005 - val_loss: 1.7424 - val_acc: 0.2788\n",
      "Epoch 296/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2035 - acc: 0.3946 - val_loss: 1.3635 - val_acc: 0.4455\n",
      "Epoch 297/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1041 - acc: 0.4998 - val_loss: 1.3416 - val_acc: 0.4545\n",
      "Epoch 298/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0975 - acc: 0.5327 - val_loss: 1.3568 - val_acc: 0.4121\n",
      "Epoch 299/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0406 - acc: 0.5263 - val_loss: 1.3646 - val_acc: 0.4485\n",
      "Epoch 300/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0655 - acc: 0.5132 - val_loss: 1.3165 - val_acc: 0.4515\n",
      "2\n",
      "Train on 2831 samples, validate on 330 samples\n",
      "Epoch 1/300\n",
      "2831/2831 [==============================] - 2s 602us/step - loss: 2.6866 - acc: 0.2151 - val_loss: 2.3984 - val_acc: 0.3303\n",
      "Epoch 2/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.3016 - acc: 0.3133 - val_loss: 2.3972 - val_acc: 0.3303\n",
      "Epoch 3/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 2.2573 - acc: 0.3147 - val_loss: 2.3836 - val_acc: 0.3212\n",
      "Epoch 4/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.2131 - acc: 0.3327 - val_loss: 2.3251 - val_acc: 0.3303\n",
      "Epoch 5/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 2.1955 - acc: 0.3229 - val_loss: 2.3386 - val_acc: 0.3242\n",
      "Epoch 6/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1550 - acc: 0.3416 - val_loss: 2.2796 - val_acc: 0.3303\n",
      "Epoch 7/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 2.1279 - acc: 0.3501 - val_loss: 2.2980 - val_acc: 0.3212\n",
      "Epoch 8/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 2.1037 - acc: 0.3543 - val_loss: 2.2989 - val_acc: 0.3303\n",
      "Epoch 9/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 2.1013 - acc: 0.3303 - val_loss: 2.2357 - val_acc: 0.3273\n",
      "Epoch 10/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 2.0592 - acc: 0.3557 - val_loss: 2.2376 - val_acc: 0.3242\n",
      "Epoch 11/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 2.0507 - acc: 0.3568 - val_loss: 2.2153 - val_acc: 0.3273\n",
      "Epoch 12/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 2.0060 - acc: 0.3787 - val_loss: 2.1862 - val_acc: 0.3303\n",
      "Epoch 13/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 2.0161 - acc: 0.3730 - val_loss: 2.1526 - val_acc: 0.3273\n",
      "Epoch 14/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.9847 - acc: 0.3518 - val_loss: 2.1305 - val_acc: 0.3242\n",
      "Epoch 15/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.9689 - acc: 0.3681 - val_loss: 2.1300 - val_acc: 0.3152\n",
      "Epoch 16/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.9617 - acc: 0.3497 - val_loss: 2.1133 - val_acc: 0.3182\n",
      "Epoch 17/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.9407 - acc: 0.3416 - val_loss: 2.0653 - val_acc: 0.3242\n",
      "Epoch 18/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.9085 - acc: 0.3744 - val_loss: 2.0494 - val_acc: 0.3333\n",
      "Epoch 19/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.8925 - acc: 0.3780 - val_loss: 2.0309 - val_acc: 0.3303\n",
      "Epoch 20/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.8834 - acc: 0.3815 - val_loss: 2.0760 - val_acc: 0.2818\n",
      "Epoch 21/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.9042 - acc: 0.3246 - val_loss: 2.0385 - val_acc: 0.3242\n",
      "Epoch 22/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.8478 - acc: 0.3910 - val_loss: 2.0182 - val_acc: 0.3242\n",
      "Epoch 23/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8323 - acc: 0.3907 - val_loss: 1.9774 - val_acc: 0.3303\n",
      "Epoch 24/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.8235 - acc: 0.3765 - val_loss: 1.9968 - val_acc: 0.3303\n",
      "Epoch 25/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8428 - acc: 0.3893 - val_loss: 1.9800 - val_acc: 0.3242\n",
      "Epoch 26/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8094 - acc: 0.3787 - val_loss: 1.9337 - val_acc: 0.3303\n",
      "Epoch 27/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7913 - acc: 0.3984 - val_loss: 1.9135 - val_acc: 0.3273\n",
      "Epoch 28/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7528 - acc: 0.3992 - val_loss: 1.9303 - val_acc: 0.3212\n",
      "Epoch 29/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7739 - acc: 0.3575 - val_loss: 1.9572 - val_acc: 0.3303\n",
      "Epoch 30/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7517 - acc: 0.3564 - val_loss: 1.9030 - val_acc: 0.3273\n",
      "Epoch 31/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7714 - acc: 0.3861 - val_loss: 1.9009 - val_acc: 0.3273\n",
      "Epoch 32/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7247 - acc: 0.3967 - val_loss: 1.9687 - val_acc: 0.3273\n",
      "Epoch 33/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7122 - acc: 0.3850 - val_loss: 1.9116 - val_acc: 0.3333\n",
      "Epoch 34/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6922 - acc: 0.4006 - val_loss: 1.8965 - val_acc: 0.3303\n",
      "Epoch 35/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6946 - acc: 0.3942 - val_loss: 1.9732 - val_acc: 0.2758\n",
      "Epoch 36/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.7260 - acc: 0.3373 - val_loss: 1.8451 - val_acc: 0.3424\n",
      "Epoch 37/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.6834 - acc: 0.4027 - val_loss: 1.8510 - val_acc: 0.3485\n",
      "Epoch 38/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6829 - acc: 0.3896 - val_loss: 1.8575 - val_acc: 0.3273\n",
      "Epoch 39/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.6512 - acc: 0.3900 - val_loss: 1.8403 - val_acc: 0.3303\n",
      "Epoch 40/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6378 - acc: 0.3953 - val_loss: 1.8468 - val_acc: 0.3212\n",
      "Epoch 41/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.6172 - acc: 0.4002 - val_loss: 1.8027 - val_acc: 0.3333\n",
      "Epoch 42/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6209 - acc: 0.3999 - val_loss: 1.7391 - val_acc: 0.3515\n",
      "Epoch 43/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6111 - acc: 0.4168 - val_loss: 1.7438 - val_acc: 0.3333\n",
      "Epoch 44/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6189 - acc: 0.4023 - val_loss: 1.7026 - val_acc: 0.3424\n",
      "Epoch 45/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6337 - acc: 0.4090 - val_loss: 1.8105 - val_acc: 0.3333\n",
      "Epoch 46/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5834 - acc: 0.4108 - val_loss: 1.8778 - val_acc: 0.3364\n",
      "Epoch 47/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.6347 - acc: 0.4041 - val_loss: 1.8005 - val_acc: 0.3242\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5796 - acc: 0.3995 - val_loss: 1.8095 - val_acc: 0.3152\n",
      "Epoch 49/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5551 - acc: 0.4087 - val_loss: 1.7704 - val_acc: 0.3242\n",
      "Epoch 50/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5535 - acc: 0.4037 - val_loss: 1.8771 - val_acc: 0.2788\n",
      "Epoch 51/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5851 - acc: 0.3698 - val_loss: 1.7564 - val_acc: 0.3515\n",
      "Epoch 52/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5659 - acc: 0.4189 - val_loss: 1.6759 - val_acc: 0.3576\n",
      "Epoch 53/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5467 - acc: 0.4115 - val_loss: 1.6742 - val_acc: 0.3485\n",
      "Epoch 54/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5245 - acc: 0.4288 - val_loss: 1.6843 - val_acc: 0.3515\n",
      "Epoch 55/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5381 - acc: 0.4203 - val_loss: 1.7335 - val_acc: 0.4000\n",
      "Epoch 56/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.5640 - acc: 0.4048 - val_loss: 1.7133 - val_acc: 0.3303\n",
      "Epoch 57/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4988 - acc: 0.4186 - val_loss: 1.8343 - val_acc: 0.2697\n",
      "Epoch 58/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5447 - acc: 0.3585 - val_loss: 1.7149 - val_acc: 0.3333\n",
      "Epoch 59/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5127 - acc: 0.4278 - val_loss: 1.6405 - val_acc: 0.3515\n",
      "Epoch 60/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5024 - acc: 0.4302 - val_loss: 1.6538 - val_acc: 0.3394\n",
      "Epoch 61/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5014 - acc: 0.4320 - val_loss: 1.7022 - val_acc: 0.3303\n",
      "Epoch 62/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5175 - acc: 0.4080 - val_loss: 1.7548 - val_acc: 0.3030\n",
      "Epoch 63/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4859 - acc: 0.4059 - val_loss: 1.7621 - val_acc: 0.3000\n",
      "Epoch 64/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4981 - acc: 0.3946 - val_loss: 1.7560 - val_acc: 0.3152\n",
      "Epoch 65/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4787 - acc: 0.4182 - val_loss: 1.7513 - val_acc: 0.3212\n",
      "Epoch 66/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4747 - acc: 0.4211 - val_loss: 1.7080 - val_acc: 0.3152\n",
      "Epoch 67/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.4498 - acc: 0.4207 - val_loss: 1.7811 - val_acc: 0.2879\n",
      "Epoch 68/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4882 - acc: 0.3758 - val_loss: 1.6693 - val_acc: 0.3333\n",
      "Epoch 69/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.4663 - acc: 0.4196 - val_loss: 1.6676 - val_acc: 0.3424\n",
      "Epoch 70/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4561 - acc: 0.4278 - val_loss: 1.6928 - val_acc: 0.3333\n",
      "Epoch 71/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4509 - acc: 0.4027 - val_loss: 1.8075 - val_acc: 0.2970\n",
      "Epoch 72/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4702 - acc: 0.3861 - val_loss: 1.7175 - val_acc: 0.3212\n",
      "Epoch 73/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4307 - acc: 0.4260 - val_loss: 1.7313 - val_acc: 0.3091\n",
      "Epoch 74/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4333 - acc: 0.4235 - val_loss: 1.7378 - val_acc: 0.3061\n",
      "Epoch 75/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.4300 - acc: 0.4225 - val_loss: 1.7054 - val_acc: 0.3182\n",
      "Epoch 76/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.4120 - acc: 0.4165 - val_loss: 1.6976 - val_acc: 0.3303\n",
      "Epoch 77/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.4956 - acc: 0.4052 - val_loss: 1.5745 - val_acc: 0.3636\n",
      "Epoch 78/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4325 - acc: 0.4207 - val_loss: 1.5674 - val_acc: 0.3697\n",
      "Epoch 79/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.4041 - acc: 0.4483 - val_loss: 1.5382 - val_acc: 0.4030\n",
      "Epoch 80/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4119 - acc: 0.4514 - val_loss: 1.5867 - val_acc: 0.3939\n",
      "Epoch 81/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4150 - acc: 0.4599 - val_loss: 1.7028 - val_acc: 0.3030\n",
      "Epoch 82/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4351 - acc: 0.4023 - val_loss: 1.8155 - val_acc: 0.2636\n",
      "Epoch 83/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4536 - acc: 0.3730 - val_loss: 1.6157 - val_acc: 0.3303\n",
      "Epoch 84/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3973 - acc: 0.4295 - val_loss: 1.7040 - val_acc: 0.3303\n",
      "Epoch 85/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3875 - acc: 0.4285 - val_loss: 1.6999 - val_acc: 0.3182\n",
      "Epoch 86/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4027 - acc: 0.4214 - val_loss: 1.6801 - val_acc: 0.3333\n",
      "Epoch 87/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3718 - acc: 0.4316 - val_loss: 1.6899 - val_acc: 0.3242\n",
      "Epoch 88/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3847 - acc: 0.4207 - val_loss: 1.6734 - val_acc: 0.3273\n",
      "Epoch 89/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3847 - acc: 0.4150 - val_loss: 1.7377 - val_acc: 0.2909\n",
      "Epoch 90/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4212 - acc: 0.3833 - val_loss: 1.6252 - val_acc: 0.3727\n",
      "Epoch 91/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3863 - acc: 0.4401 - val_loss: 1.6032 - val_acc: 0.3667\n",
      "Epoch 92/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.3715 - acc: 0.4415 - val_loss: 1.6065 - val_acc: 0.3727\n",
      "Epoch 93/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3555 - acc: 0.4264 - val_loss: 1.5958 - val_acc: 0.3818\n",
      "Epoch 94/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3776 - acc: 0.4426 - val_loss: 1.6678 - val_acc: 0.3303\n",
      "Epoch 95/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3665 - acc: 0.4267 - val_loss: 1.7700 - val_acc: 0.3273\n",
      "Epoch 96/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3992 - acc: 0.4239 - val_loss: 1.6316 - val_acc: 0.3182\n",
      "Epoch 97/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.3442 - acc: 0.4313 - val_loss: 1.5737 - val_acc: 0.3576\n",
      "Epoch 98/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3325 - acc: 0.4405 - val_loss: 1.6481 - val_acc: 0.3485\n",
      "Epoch 99/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3593 - acc: 0.4493 - val_loss: 1.7643 - val_acc: 0.3152\n",
      "Epoch 100/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3761 - acc: 0.4338 - val_loss: 1.6073 - val_acc: 0.3303\n",
      "Epoch 101/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3357 - acc: 0.4355 - val_loss: 1.6129 - val_acc: 0.3455\n",
      "Epoch 102/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3696 - acc: 0.4299 - val_loss: 1.6471 - val_acc: 0.3485\n",
      "Epoch 103/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3426 - acc: 0.4309 - val_loss: 1.6689 - val_acc: 0.3394\n",
      "Epoch 104/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3521 - acc: 0.4179 - val_loss: 1.7049 - val_acc: 0.3152\n",
      "Epoch 105/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3537 - acc: 0.4348 - val_loss: 1.6439 - val_acc: 0.3242\n",
      "Epoch 106/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3293 - acc: 0.4348 - val_loss: 1.6424 - val_acc: 0.3364\n",
      "Epoch 107/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3334 - acc: 0.4391 - val_loss: 1.6642 - val_acc: 0.3212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3290 - acc: 0.4207 - val_loss: 1.6899 - val_acc: 0.2879\n",
      "Epoch 109/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3715 - acc: 0.3826 - val_loss: 1.5993 - val_acc: 0.3303\n",
      "Epoch 110/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3259 - acc: 0.4334 - val_loss: 1.5458 - val_acc: 0.4030\n",
      "Epoch 111/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3441 - acc: 0.4483 - val_loss: 1.4912 - val_acc: 0.4000\n",
      "Epoch 112/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3366 - acc: 0.4521 - val_loss: 1.4481 - val_acc: 0.3606\n",
      "Epoch 113/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.3405 - acc: 0.4525 - val_loss: 1.4864 - val_acc: 0.3818\n",
      "Epoch 114/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.2958 - acc: 0.4610 - val_loss: 1.4879 - val_acc: 0.3909\n",
      "Epoch 115/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2794 - acc: 0.4634 - val_loss: 1.4350 - val_acc: 0.4515\n",
      "Epoch 116/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3101 - acc: 0.4606 - val_loss: 1.4224 - val_acc: 0.4242\n",
      "Epoch 117/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3486 - acc: 0.4649 - val_loss: 1.5718 - val_acc: 0.3606\n",
      "Epoch 118/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3284 - acc: 0.4316 - val_loss: 1.7339 - val_acc: 0.2545\n",
      "Epoch 119/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3777 - acc: 0.3730 - val_loss: 1.5965 - val_acc: 0.3545\n",
      "Epoch 120/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2916 - acc: 0.4483 - val_loss: 1.5801 - val_acc: 0.3576\n",
      "Epoch 121/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2909 - acc: 0.4620 - val_loss: 1.6533 - val_acc: 0.3515\n",
      "Epoch 122/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3223 - acc: 0.4500 - val_loss: 1.6579 - val_acc: 0.3182\n",
      "Epoch 123/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2876 - acc: 0.4239 - val_loss: 1.5725 - val_acc: 0.3364\n",
      "Epoch 124/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2753 - acc: 0.4627 - val_loss: 1.5924 - val_acc: 0.3455\n",
      "Epoch 125/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2771 - acc: 0.4486 - val_loss: 1.6856 - val_acc: 0.3303\n",
      "Epoch 126/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3347 - acc: 0.4200 - val_loss: 1.5691 - val_acc: 0.3606\n",
      "Epoch 127/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3236 - acc: 0.4475 - val_loss: 1.5305 - val_acc: 0.4182\n",
      "Epoch 128/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3138 - acc: 0.4553 - val_loss: 1.4323 - val_acc: 0.4303\n",
      "Epoch 129/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2682 - acc: 0.4723 - val_loss: 1.3862 - val_acc: 0.4030\n",
      "Epoch 130/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3458 - acc: 0.4723 - val_loss: 1.5097 - val_acc: 0.3788\n",
      "Epoch 131/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2579 - acc: 0.4811 - val_loss: 1.5916 - val_acc: 0.3545\n",
      "Epoch 132/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2876 - acc: 0.4440 - val_loss: 1.5748 - val_acc: 0.3333\n",
      "Epoch 133/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2883 - acc: 0.4444 - val_loss: 1.6353 - val_acc: 0.3212\n",
      "Epoch 134/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2791 - acc: 0.4331 - val_loss: 1.6159 - val_acc: 0.3182\n",
      "Epoch 135/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2806 - acc: 0.4218 - val_loss: 1.5370 - val_acc: 0.3848\n",
      "Epoch 136/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2624 - acc: 0.4645 - val_loss: 1.5731 - val_acc: 0.3667\n",
      "Epoch 137/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2726 - acc: 0.4391 - val_loss: 1.6883 - val_acc: 0.3455\n",
      "Epoch 138/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3236 - acc: 0.4112 - val_loss: 1.5349 - val_acc: 0.3485\n",
      "Epoch 139/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2471 - acc: 0.4634 - val_loss: 1.5468 - val_acc: 0.3636\n",
      "Epoch 140/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2861 - acc: 0.4447 - val_loss: 1.5100 - val_acc: 0.3758\n",
      "Epoch 141/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2621 - acc: 0.4603 - val_loss: 1.4381 - val_acc: 0.4091\n",
      "Epoch 142/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2589 - acc: 0.4521 - val_loss: 1.3692 - val_acc: 0.4242\n",
      "Epoch 143/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3002 - acc: 0.4747 - val_loss: 1.4676 - val_acc: 0.3697\n",
      "Epoch 144/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2329 - acc: 0.4649 - val_loss: 1.3775 - val_acc: 0.4788\n",
      "Epoch 145/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2977 - acc: 0.4744 - val_loss: 1.4744 - val_acc: 0.4121\n",
      "Epoch 146/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2791 - acc: 0.4705 - val_loss: 1.5150 - val_acc: 0.3697\n",
      "Epoch 147/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2300 - acc: 0.4574 - val_loss: 1.6831 - val_acc: 0.2909\n",
      "Epoch 148/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.3162 - acc: 0.3946 - val_loss: 1.5585 - val_acc: 0.3576\n",
      "Epoch 149/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2343 - acc: 0.4588 - val_loss: 1.6413 - val_acc: 0.3424\n",
      "Epoch 150/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2724 - acc: 0.4603 - val_loss: 1.5455 - val_acc: 0.3303\n",
      "Epoch 151/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2187 - acc: 0.4461 - val_loss: 1.5263 - val_acc: 0.3667\n",
      "Epoch 152/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2446 - acc: 0.4578 - val_loss: 1.4033 - val_acc: 0.4242\n",
      "Epoch 153/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2230 - acc: 0.4776 - val_loss: 1.3630 - val_acc: 0.4727\n",
      "Epoch 154/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2681 - acc: 0.4815 - val_loss: 1.4163 - val_acc: 0.4000\n",
      "Epoch 155/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2442 - acc: 0.4723 - val_loss: 1.3890 - val_acc: 0.4515\n",
      "Epoch 156/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2450 - acc: 0.4974 - val_loss: 1.4072 - val_acc: 0.4333\n",
      "Epoch 157/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.2326 - acc: 0.5083 - val_loss: 1.3804 - val_acc: 0.4636\n",
      "Epoch 158/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2588 - acc: 0.4702 - val_loss: 1.3903 - val_acc: 0.3879\n",
      "Epoch 159/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3011 - acc: 0.4680 - val_loss: 1.5486 - val_acc: 0.3606\n",
      "Epoch 160/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2553 - acc: 0.4687 - val_loss: 1.5178 - val_acc: 0.3545\n",
      "Epoch 161/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2172 - acc: 0.4553 - val_loss: 1.6239 - val_acc: 0.3273\n",
      "Epoch 162/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2541 - acc: 0.4479 - val_loss: 1.5669 - val_acc: 0.3303\n",
      "Epoch 163/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2259 - acc: 0.4479 - val_loss: 1.4588 - val_acc: 0.4030\n",
      "Epoch 164/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2049 - acc: 0.4818 - val_loss: 1.5357 - val_acc: 0.3697\n",
      "Epoch 165/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2529 - acc: 0.4649 - val_loss: 1.4874 - val_acc: 0.3727\n",
      "Epoch 166/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1998 - acc: 0.4853 - val_loss: 1.7309 - val_acc: 0.3182\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3075 - acc: 0.4101 - val_loss: 1.4656 - val_acc: 0.3697\n",
      "Epoch 168/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2165 - acc: 0.4627 - val_loss: 1.4704 - val_acc: 0.3818\n",
      "Epoch 169/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2141 - acc: 0.4815 - val_loss: 1.5566 - val_acc: 0.3576\n",
      "Epoch 170/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2349 - acc: 0.4744 - val_loss: 1.5898 - val_acc: 0.3303\n",
      "Epoch 171/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2581 - acc: 0.4264 - val_loss: 1.6005 - val_acc: 0.3061\n",
      "Epoch 172/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2388 - acc: 0.4267 - val_loss: 1.4974 - val_acc: 0.4000\n",
      "Epoch 173/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2047 - acc: 0.4673 - val_loss: 1.4521 - val_acc: 0.4273\n",
      "Epoch 174/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1910 - acc: 0.4913 - val_loss: 1.4208 - val_acc: 0.4485\n",
      "Epoch 175/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2415 - acc: 0.4737 - val_loss: 1.3656 - val_acc: 0.4576\n",
      "Epoch 176/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2606 - acc: 0.4832 - val_loss: 1.4266 - val_acc: 0.3879\n",
      "Epoch 177/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1708 - acc: 0.4952 - val_loss: 1.4451 - val_acc: 0.3879\n",
      "Epoch 178/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1754 - acc: 0.4882 - val_loss: 1.3379 - val_acc: 0.4455\n",
      "Epoch 179/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2489 - acc: 0.4659 - val_loss: 1.3431 - val_acc: 0.4515\n",
      "Epoch 180/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2255 - acc: 0.4871 - val_loss: 1.4014 - val_acc: 0.4242\n",
      "Epoch 181/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1991 - acc: 0.4896 - val_loss: 1.3595 - val_acc: 0.5000\n",
      "Epoch 182/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2632 - acc: 0.4988 - val_loss: 1.4220 - val_acc: 0.3848\n",
      "Epoch 183/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1614 - acc: 0.4790 - val_loss: 1.3862 - val_acc: 0.4455\n",
      "Epoch 184/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1881 - acc: 0.4747 - val_loss: 1.3234 - val_acc: 0.4515\n",
      "Epoch 185/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2617 - acc: 0.4769 - val_loss: 1.4119 - val_acc: 0.4212\n",
      "Epoch 186/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1912 - acc: 0.4800 - val_loss: 1.4418 - val_acc: 0.4030\n",
      "Epoch 187/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1973 - acc: 0.4853 - val_loss: 1.5346 - val_acc: 0.4030\n",
      "Epoch 188/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2121 - acc: 0.4772 - val_loss: 1.5556 - val_acc: 0.3364\n",
      "Epoch 189/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1899 - acc: 0.4588 - val_loss: 1.5518 - val_acc: 0.3455\n",
      "Epoch 190/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2111 - acc: 0.4486 - val_loss: 1.5665 - val_acc: 0.3636\n",
      "Epoch 191/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2131 - acc: 0.4504 - val_loss: 1.4627 - val_acc: 0.4182\n",
      "Epoch 192/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1706 - acc: 0.4956 - val_loss: 1.7439 - val_acc: 0.3394\n",
      "Epoch 193/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3085 - acc: 0.4316 - val_loss: 1.4390 - val_acc: 0.3697\n",
      "Epoch 194/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1731 - acc: 0.4769 - val_loss: 1.4133 - val_acc: 0.4152\n",
      "Epoch 195/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1568 - acc: 0.4878 - val_loss: 1.4577 - val_acc: 0.4333\n",
      "Epoch 196/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1890 - acc: 0.4829 - val_loss: 1.5564 - val_acc: 0.3818\n",
      "Epoch 197/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2397 - acc: 0.4497 - val_loss: 1.5080 - val_acc: 0.3758\n",
      "Epoch 198/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1851 - acc: 0.4730 - val_loss: 1.7056 - val_acc: 0.3424\n",
      "Epoch 199/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2982 - acc: 0.4366 - val_loss: 1.4755 - val_acc: 0.3576\n",
      "Epoch 200/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1627 - acc: 0.4790 - val_loss: 1.4145 - val_acc: 0.4212\n",
      "Epoch 201/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1399 - acc: 0.4974 - val_loss: 1.4950 - val_acc: 0.3848\n",
      "Epoch 202/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1812 - acc: 0.4956 - val_loss: 1.6459 - val_acc: 0.3455\n",
      "Epoch 203/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2560 - acc: 0.4020 - val_loss: 1.5092 - val_acc: 0.3848\n",
      "Epoch 204/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1983 - acc: 0.4772 - val_loss: 1.5337 - val_acc: 0.3606\n",
      "Epoch 205/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1804 - acc: 0.4694 - val_loss: 1.4824 - val_acc: 0.3848\n",
      "Epoch 206/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1468 - acc: 0.4864 - val_loss: 1.4593 - val_acc: 0.4030\n",
      "Epoch 207/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1617 - acc: 0.4843 - val_loss: 1.4539 - val_acc: 0.4000\n",
      "Epoch 208/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1551 - acc: 0.4800 - val_loss: 1.4621 - val_acc: 0.4030\n",
      "Epoch 209/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1703 - acc: 0.4772 - val_loss: 1.7066 - val_acc: 0.2970\n",
      "Epoch 210/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2682 - acc: 0.4059 - val_loss: 1.4535 - val_acc: 0.4273\n",
      "Epoch 211/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1805 - acc: 0.4963 - val_loss: 1.4553 - val_acc: 0.4364\n",
      "Epoch 212/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2032 - acc: 0.4673 - val_loss: 1.3939 - val_acc: 0.4606\n",
      "Epoch 213/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1552 - acc: 0.5072 - val_loss: 1.4433 - val_acc: 0.4212\n",
      "Epoch 214/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1395 - acc: 0.4899 - val_loss: 1.4540 - val_acc: 0.4455\n",
      "Epoch 215/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1597 - acc: 0.4956 - val_loss: 1.5690 - val_acc: 0.3727\n",
      "Epoch 216/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2070 - acc: 0.4490 - val_loss: 1.5818 - val_acc: 0.3606\n",
      "Epoch 217/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1858 - acc: 0.4680 - val_loss: 1.4667 - val_acc: 0.3788\n",
      "Epoch 218/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1614 - acc: 0.4860 - val_loss: 1.5089 - val_acc: 0.3788\n",
      "Epoch 219/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1703 - acc: 0.4737 - val_loss: 1.5863 - val_acc: 0.3364\n",
      "Epoch 220/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1803 - acc: 0.4419 - val_loss: 1.4187 - val_acc: 0.4091\n",
      "Epoch 221/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1166 - acc: 0.5009 - val_loss: 1.5273 - val_acc: 0.3848\n",
      "Epoch 222/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.1800 - acc: 0.4793 - val_loss: 1.5252 - val_acc: 0.3576\n",
      "Epoch 223/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1468 - acc: 0.4793 - val_loss: 1.4367 - val_acc: 0.4000\n",
      "Epoch 224/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1576 - acc: 0.4949 - val_loss: 1.4646 - val_acc: 0.4091\n",
      "Epoch 225/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1529 - acc: 0.4843 - val_loss: 1.4536 - val_acc: 0.4061\n",
      "Epoch 226/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1404 - acc: 0.4832 - val_loss: 1.6094 - val_acc: 0.3606\n",
      "Epoch 227/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.2089 - acc: 0.4387 - val_loss: 1.3875 - val_acc: 0.4515\n",
      "Epoch 228/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1256 - acc: 0.5200 - val_loss: 1.3964 - val_acc: 0.4455\n",
      "Epoch 229/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1488 - acc: 0.5132 - val_loss: 1.3539 - val_acc: 0.4788\n",
      "Epoch 230/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.2037 - acc: 0.4790 - val_loss: 1.3372 - val_acc: 0.4182\n",
      "Epoch 231/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1799 - acc: 0.4702 - val_loss: 1.3492 - val_acc: 0.4515\n",
      "Epoch 232/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1151 - acc: 0.5147 - val_loss: 1.3727 - val_acc: 0.4606\n",
      "Epoch 233/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1032 - acc: 0.4998 - val_loss: 1.3576 - val_acc: 0.4273\n",
      "Epoch 234/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1125 - acc: 0.4991 - val_loss: 1.3794 - val_acc: 0.4576\n",
      "Epoch 235/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1801 - acc: 0.4938 - val_loss: 1.4109 - val_acc: 0.4455\n",
      "Epoch 236/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.1475 - acc: 0.4970 - val_loss: 1.6911 - val_acc: 0.3424\n",
      "Epoch 237/300\n",
      "2831/2831 [==============================] - 0s 79us/step - loss: 1.2601 - acc: 0.4239 - val_loss: 1.4237 - val_acc: 0.4182\n",
      "Epoch 238/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1343 - acc: 0.5147 - val_loss: 1.4039 - val_acc: 0.4545\n",
      "Epoch 239/300\n",
      "2831/2831 [==============================] - 0s 89us/step - loss: 1.1104 - acc: 0.5083 - val_loss: 1.4535 - val_acc: 0.4394\n",
      "Epoch 240/300\n",
      "2831/2831 [==============================] - 0s 82us/step - loss: 1.1326 - acc: 0.4998 - val_loss: 1.5412 - val_acc: 0.3939\n",
      "Epoch 241/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1733 - acc: 0.4557 - val_loss: 1.5007 - val_acc: 0.3879\n",
      "Epoch 242/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1733 - acc: 0.4638 - val_loss: 1.3972 - val_acc: 0.4121\n",
      "Epoch 243/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1160 - acc: 0.4928 - val_loss: 1.4047 - val_acc: 0.4273\n",
      "Epoch 244/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.0927 - acc: 0.5129 - val_loss: 1.4156 - val_acc: 0.4394\n",
      "Epoch 245/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1185 - acc: 0.5087 - val_loss: 1.6893 - val_acc: 0.3242\n",
      "Epoch 246/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.2562 - acc: 0.4069 - val_loss: 1.4188 - val_acc: 0.4091\n",
      "Epoch 247/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1424 - acc: 0.5044 - val_loss: 1.3503 - val_acc: 0.4667\n",
      "Epoch 248/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.1145 - acc: 0.5214 - val_loss: 1.3469 - val_acc: 0.4636\n",
      "Epoch 249/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1409 - acc: 0.5193 - val_loss: 1.3139 - val_acc: 0.4485\n",
      "Epoch 250/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.1345 - acc: 0.5118 - val_loss: 1.3290 - val_acc: 0.4576\n",
      "Epoch 251/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1188 - acc: 0.5118 - val_loss: 1.3145 - val_acc: 0.4606\n",
      "Epoch 252/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1492 - acc: 0.5072 - val_loss: 1.4032 - val_acc: 0.4545\n",
      "Epoch 253/300\n",
      "2831/2831 [==============================] - 0s 81us/step - loss: 1.1462 - acc: 0.5097 - val_loss: 1.6486 - val_acc: 0.3636\n",
      "Epoch 254/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2234 - acc: 0.4694 - val_loss: 1.4042 - val_acc: 0.4242\n",
      "Epoch 255/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.0919 - acc: 0.4938 - val_loss: 1.3354 - val_acc: 0.4515\n",
      "Epoch 256/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1111 - acc: 0.5009 - val_loss: 1.2962 - val_acc: 0.4758\n",
      "Epoch 257/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1405 - acc: 0.5214 - val_loss: 1.3557 - val_acc: 0.4364\n",
      "Epoch 258/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.0992 - acc: 0.5168 - val_loss: 1.3405 - val_acc: 0.4212\n",
      "Epoch 259/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1066 - acc: 0.5157 - val_loss: 1.3078 - val_acc: 0.4667\n",
      "Epoch 260/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.1269 - acc: 0.5200 - val_loss: 1.4105 - val_acc: 0.4455\n",
      "Epoch 261/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1654 - acc: 0.5037 - val_loss: 1.3727 - val_acc: 0.4636\n",
      "Epoch 262/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0899 - acc: 0.5295 - val_loss: 1.3465 - val_acc: 0.4606\n",
      "Epoch 263/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.0733 - acc: 0.5253 - val_loss: 1.4080 - val_acc: 0.4424\n",
      "Epoch 264/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1190 - acc: 0.5097 - val_loss: 1.6304 - val_acc: 0.3788\n",
      "Epoch 265/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2459 - acc: 0.4080 - val_loss: 1.4581 - val_acc: 0.4152\n",
      "Epoch 266/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1465 - acc: 0.5005 - val_loss: 1.3953 - val_acc: 0.4333\n",
      "Epoch 267/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.0787 - acc: 0.5129 - val_loss: 1.4559 - val_acc: 0.3848\n",
      "Epoch 268/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0809 - acc: 0.5051 - val_loss: 1.5319 - val_acc: 0.3909\n",
      "Epoch 269/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.1587 - acc: 0.4769 - val_loss: 1.4839 - val_acc: 0.4000\n",
      "Epoch 270/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.1051 - acc: 0.4889 - val_loss: 1.3881 - val_acc: 0.4515\n",
      "Epoch 271/300\n",
      "2831/2831 [==============================] - 0s 79us/step - loss: 1.0893 - acc: 0.5196 - val_loss: 1.5237 - val_acc: 0.4030\n",
      "Epoch 272/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1544 - acc: 0.4779 - val_loss: 1.4531 - val_acc: 0.3909\n",
      "Epoch 273/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.0924 - acc: 0.4882 - val_loss: 1.4712 - val_acc: 0.4152\n",
      "Epoch 274/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.0922 - acc: 0.4935 - val_loss: 1.4759 - val_acc: 0.4212\n",
      "Epoch 275/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0917 - acc: 0.5048 - val_loss: 1.4142 - val_acc: 0.4394\n",
      "Epoch 276/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.0654 - acc: 0.5203 - val_loss: 1.3929 - val_acc: 0.4333\n",
      "Epoch 277/300\n",
      "2831/2831 [==============================] - 0s 79us/step - loss: 1.0875 - acc: 0.5150 - val_loss: 1.5826 - val_acc: 0.3515\n",
      "Epoch 278/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1937 - acc: 0.4348 - val_loss: 1.3769 - val_acc: 0.4424\n",
      "Epoch 279/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0559 - acc: 0.5291 - val_loss: 1.4812 - val_acc: 0.4091\n",
      "Epoch 280/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1468 - acc: 0.4868 - val_loss: 1.5434 - val_acc: 0.3758\n",
      "Epoch 281/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1290 - acc: 0.4825 - val_loss: 1.3578 - val_acc: 0.4394\n",
      "Epoch 282/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.0558 - acc: 0.5348 - val_loss: 1.4173 - val_acc: 0.4455\n",
      "Epoch 283/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1251 - acc: 0.5079 - val_loss: 1.4604 - val_acc: 0.4152\n",
      "Epoch 284/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1189 - acc: 0.4896 - val_loss: 1.4651 - val_acc: 0.3939\n",
      "Epoch 285/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.1309 - acc: 0.4744 - val_loss: 1.5318 - val_acc: 0.4091\n",
      "Epoch 286/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1132 - acc: 0.4864 - val_loss: 1.3648 - val_acc: 0.4273\n",
      "Epoch 287/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.0441 - acc: 0.5341 - val_loss: 1.3835 - val_acc: 0.4667\n",
      "Epoch 288/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.0407 - acc: 0.5429 - val_loss: 1.5221 - val_acc: 0.3848\n",
      "Epoch 289/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1690 - acc: 0.4691 - val_loss: 1.4445 - val_acc: 0.3879\n",
      "Epoch 290/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.0912 - acc: 0.5009 - val_loss: 1.3757 - val_acc: 0.4333\n",
      "Epoch 291/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0849 - acc: 0.5037 - val_loss: 1.3694 - val_acc: 0.4576\n",
      "Epoch 292/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0599 - acc: 0.5263 - val_loss: 1.4023 - val_acc: 0.4364\n",
      "Epoch 293/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.0709 - acc: 0.5118 - val_loss: 1.5975 - val_acc: 0.3576\n",
      "Epoch 294/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.2058 - acc: 0.4504 - val_loss: 1.4400 - val_acc: 0.4424\n",
      "Epoch 295/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1047 - acc: 0.5362 - val_loss: 1.4015 - val_acc: 0.4424\n",
      "Epoch 296/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 1.0582 - acc: 0.5376 - val_loss: 1.4371 - val_acc: 0.4424\n",
      "Epoch 297/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.0664 - acc: 0.5302 - val_loss: 1.3408 - val_acc: 0.4818\n",
      "Epoch 298/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0883 - acc: 0.5313 - val_loss: 1.3004 - val_acc: 0.4515\n",
      "Epoch 299/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2271 - acc: 0.4942 - val_loss: 1.3758 - val_acc: 0.4152\n",
      "Epoch 300/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.0702 - acc: 0.5221 - val_loss: 1.4534 - val_acc: 0.3939\n",
      "3\n",
      "Train on 2831 samples, validate on 330 samples\n",
      "Epoch 1/300\n",
      "2831/2831 [==============================] - 2s 562us/step - loss: 2.9039 - acc: 0.3405 - val_loss: 2.3868 - val_acc: 0.3303\n",
      "Epoch 2/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 2.2604 - acc: 0.3628 - val_loss: 2.2500 - val_acc: 0.3303\n",
      "Epoch 3/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 2.1855 - acc: 0.3818 - val_loss: 2.2567 - val_acc: 0.3303\n",
      "Epoch 4/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 2.1532 - acc: 0.3889 - val_loss: 2.2513 - val_acc: 0.3303\n",
      "Epoch 5/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 2.1280 - acc: 0.3847 - val_loss: 2.2408 - val_acc: 0.3242\n",
      "Epoch 6/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 2.1121 - acc: 0.3868 - val_loss: 2.2362 - val_acc: 0.3303\n",
      "Epoch 7/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 2.1114 - acc: 0.3833 - val_loss: 2.1604 - val_acc: 0.3303\n",
      "Epoch 8/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 2.0679 - acc: 0.3818 - val_loss: 2.1555 - val_acc: 0.3303\n",
      "Epoch 9/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 2.0584 - acc: 0.3811 - val_loss: 2.1347 - val_acc: 0.3303\n",
      "Epoch 10/300\n",
      "2831/2831 [==============================] - 0s 77us/step - loss: 2.0162 - acc: 0.3833 - val_loss: 2.1193 - val_acc: 0.3273\n",
      "Epoch 11/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 2.0042 - acc: 0.3822 - val_loss: 2.1428 - val_acc: 0.3242\n",
      "Epoch 12/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.9987 - acc: 0.3780 - val_loss: 2.1092 - val_acc: 0.3182\n",
      "Epoch 13/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.9704 - acc: 0.3900 - val_loss: 2.1145 - val_acc: 0.3303\n",
      "Epoch 14/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.9409 - acc: 0.3808 - val_loss: 2.0531 - val_acc: 0.3303\n",
      "Epoch 15/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.9222 - acc: 0.3949 - val_loss: 2.0332 - val_acc: 0.3273\n",
      "Epoch 16/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.9165 - acc: 0.3843 - val_loss: 2.0373 - val_acc: 0.3273\n",
      "Epoch 17/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.9017 - acc: 0.3815 - val_loss: 2.0395 - val_acc: 0.3242\n",
      "Epoch 18/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.8812 - acc: 0.3857 - val_loss: 2.0365 - val_acc: 0.3273\n",
      "Epoch 19/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.8715 - acc: 0.3924 - val_loss: 2.1040 - val_acc: 0.3242\n",
      "Epoch 20/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8767 - acc: 0.3931 - val_loss: 1.9994 - val_acc: 0.3212\n",
      "Epoch 21/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8334 - acc: 0.3871 - val_loss: 1.9635 - val_acc: 0.3242\n",
      "Epoch 22/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.8332 - acc: 0.3928 - val_loss: 1.9664 - val_acc: 0.3242\n",
      "Epoch 23/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7931 - acc: 0.3924 - val_loss: 1.8997 - val_acc: 0.3242\n",
      "Epoch 24/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.7823 - acc: 0.3999 - val_loss: 1.8854 - val_acc: 0.3212\n",
      "Epoch 25/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.7795 - acc: 0.3850 - val_loss: 1.8836 - val_acc: 0.3152\n",
      "Epoch 26/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.7519 - acc: 0.3981 - val_loss: 1.8667 - val_acc: 0.3212\n",
      "Epoch 27/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.7802 - acc: 0.3886 - val_loss: 1.9022 - val_acc: 0.3182\n",
      "Epoch 28/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.7524 - acc: 0.3921 - val_loss: 1.9464 - val_acc: 0.3121\n",
      "Epoch 29/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.7250 - acc: 0.3702 - val_loss: 1.8887 - val_acc: 0.3152\n",
      "Epoch 30/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.7009 - acc: 0.4034 - val_loss: 1.8485 - val_acc: 0.3091\n",
      "Epoch 31/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.7061 - acc: 0.3946 - val_loss: 1.8361 - val_acc: 0.3061\n",
      "Epoch 32/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.7472 - acc: 0.3737 - val_loss: 1.9190 - val_acc: 0.2970\n",
      "Epoch 33/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.6963 - acc: 0.3656 - val_loss: 1.7589 - val_acc: 0.3182\n",
      "Epoch 34/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.6520 - acc: 0.4034 - val_loss: 1.7712 - val_acc: 0.3212\n",
      "Epoch 35/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.6460 - acc: 0.4069 - val_loss: 1.8319 - val_acc: 0.3212\n",
      "Epoch 36/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.6577 - acc: 0.3882 - val_loss: 1.8901 - val_acc: 0.3212\n",
      "Epoch 37/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.6446 - acc: 0.4129 - val_loss: 1.9262 - val_acc: 0.3091\n",
      "Epoch 38/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.6416 - acc: 0.3663 - val_loss: 1.7854 - val_acc: 0.3152\n",
      "Epoch 39/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.6149 - acc: 0.3900 - val_loss: 1.8412 - val_acc: 0.3212\n",
      "Epoch 40/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.6425 - acc: 0.4016 - val_loss: 1.7708 - val_acc: 0.3242\n",
      "Epoch 41/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.6124 - acc: 0.3935 - val_loss: 1.7082 - val_acc: 0.3212\n",
      "Epoch 42/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5908 - acc: 0.3988 - val_loss: 1.7065 - val_acc: 0.3212\n",
      "Epoch 43/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.6074 - acc: 0.4143 - val_loss: 1.7080 - val_acc: 0.3091\n",
      "Epoch 44/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.5862 - acc: 0.4002 - val_loss: 1.7798 - val_acc: 0.3121\n",
      "Epoch 45/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.5839 - acc: 0.3737 - val_loss: 1.9107 - val_acc: 0.2970\n",
      "Epoch 46/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.5752 - acc: 0.3674 - val_loss: 1.7839 - val_acc: 0.3152\n",
      "Epoch 47/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5509 - acc: 0.4143 - val_loss: 1.8086 - val_acc: 0.3121\n",
      "Epoch 48/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5410 - acc: 0.4062 - val_loss: 1.7846 - val_acc: 0.3091\n",
      "Epoch 49/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.5466 - acc: 0.3974 - val_loss: 1.9239 - val_acc: 0.3152\n",
      "Epoch 50/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5705 - acc: 0.3931 - val_loss: 1.8285 - val_acc: 0.3121\n",
      "Epoch 51/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5242 - acc: 0.3977 - val_loss: 1.8062 - val_acc: 0.3152\n",
      "Epoch 52/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5219 - acc: 0.3981 - val_loss: 1.8110 - val_acc: 0.3091\n",
      "Epoch 53/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5141 - acc: 0.4002 - val_loss: 1.8325 - val_acc: 0.2970\n",
      "Epoch 54/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5188 - acc: 0.3797 - val_loss: 1.8278 - val_acc: 0.3152\n",
      "Epoch 55/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5048 - acc: 0.4013 - val_loss: 1.7064 - val_acc: 0.3121\n",
      "Epoch 56/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4754 - acc: 0.4161 - val_loss: 1.7720 - val_acc: 0.2091\n",
      "Epoch 57/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.5432 - acc: 0.3592 - val_loss: 1.7619 - val_acc: 0.2697\n",
      "Epoch 58/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.4855 - acc: 0.3871 - val_loss: 1.7123 - val_acc: 0.3091\n",
      "Epoch 59/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4593 - acc: 0.4260 - val_loss: 1.9959 - val_acc: 0.2576\n",
      "Epoch 60/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5202 - acc: 0.3780 - val_loss: 1.7171 - val_acc: 0.3182\n",
      "Epoch 61/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4538 - acc: 0.4158 - val_loss: 1.7121 - val_acc: 0.3152\n",
      "Epoch 62/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4487 - acc: 0.4027 - val_loss: 1.6598 - val_acc: 0.3273\n",
      "Epoch 63/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.5134 - acc: 0.3808 - val_loss: 1.7095 - val_acc: 0.3303\n",
      "Epoch 64/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4643 - acc: 0.4256 - val_loss: 1.6204 - val_acc: 0.3333\n",
      "Epoch 65/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4443 - acc: 0.4264 - val_loss: 1.5881 - val_acc: 0.3424\n",
      "Epoch 66/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4831 - acc: 0.4369 - val_loss: 1.7054 - val_acc: 0.2727\n",
      "Epoch 67/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4515 - acc: 0.3910 - val_loss: 1.8541 - val_acc: 0.2515\n",
      "Epoch 68/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4726 - acc: 0.3663 - val_loss: 1.7357 - val_acc: 0.3061\n",
      "Epoch 69/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4357 - acc: 0.4253 - val_loss: 1.8180 - val_acc: 0.3091\n",
      "Epoch 70/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4655 - acc: 0.4030 - val_loss: 1.6158 - val_acc: 0.3182\n",
      "Epoch 71/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4401 - acc: 0.3977 - val_loss: 1.5705 - val_acc: 0.3394\n",
      "Epoch 72/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.4415 - acc: 0.4313 - val_loss: 1.5955 - val_acc: 0.3212\n",
      "Epoch 73/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4120 - acc: 0.4362 - val_loss: 1.5417 - val_acc: 0.3394\n",
      "Epoch 74/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4437 - acc: 0.4211 - val_loss: 1.5610 - val_acc: 0.3152\n",
      "Epoch 75/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.4069 - acc: 0.4292 - val_loss: 1.6764 - val_acc: 0.2788\n",
      "Epoch 76/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.4177 - acc: 0.4154 - val_loss: 1.6613 - val_acc: 0.3121\n",
      "Epoch 77/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.4129 - acc: 0.4211 - val_loss: 1.6524 - val_acc: 0.2848\n",
      "Epoch 78/300\n",
      "2831/2831 [==============================] - 0s 79us/step - loss: 1.3825 - acc: 0.4285 - val_loss: 1.8090 - val_acc: 0.2303\n",
      "Epoch 79/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.4355 - acc: 0.3840 - val_loss: 1.8333 - val_acc: 0.3121\n",
      "Epoch 80/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4319 - acc: 0.4172 - val_loss: 1.8066 - val_acc: 0.3121\n",
      "Epoch 81/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.4339 - acc: 0.4175 - val_loss: 1.6609 - val_acc: 0.2879\n",
      "Epoch 82/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3694 - acc: 0.4334 - val_loss: 1.7259 - val_acc: 0.2727\n",
      "Epoch 83/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3633 - acc: 0.4299 - val_loss: 1.7322 - val_acc: 0.2879\n",
      "Epoch 84/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3770 - acc: 0.4165 - val_loss: 1.7182 - val_acc: 0.2879\n",
      "Epoch 85/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3765 - acc: 0.4345 - val_loss: 1.7741 - val_acc: 0.3030\n",
      "Epoch 86/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3798 - acc: 0.4246 - val_loss: 1.7470 - val_acc: 0.3121\n",
      "Epoch 87/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3826 - acc: 0.4369 - val_loss: 1.6925 - val_acc: 0.3242\n",
      "Epoch 88/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4176 - acc: 0.4002 - val_loss: 1.6015 - val_acc: 0.3364\n",
      "Epoch 89/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.3532 - acc: 0.4352 - val_loss: 1.4646 - val_acc: 0.4121\n",
      "Epoch 90/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.4070 - acc: 0.4377 - val_loss: 1.5245 - val_acc: 0.3424\n",
      "Epoch 91/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.3927 - acc: 0.4454 - val_loss: 1.6587 - val_acc: 0.2818\n",
      "Epoch 92/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.3504 - acc: 0.4147 - val_loss: 1.6253 - val_acc: 0.3424\n",
      "Epoch 93/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.3426 - acc: 0.4412 - val_loss: 1.6411 - val_acc: 0.3273\n",
      "Epoch 94/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.3518 - acc: 0.4285 - val_loss: 1.6239 - val_acc: 0.3333\n",
      "Epoch 95/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.3576 - acc: 0.4341 - val_loss: 1.4633 - val_acc: 0.4242\n",
      "Epoch 96/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.4011 - acc: 0.4468 - val_loss: 1.5050 - val_acc: 0.3667\n",
      "Epoch 97/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.3819 - acc: 0.4525 - val_loss: 1.5568 - val_acc: 0.3515\n",
      "Epoch 98/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.3198 - acc: 0.4355 - val_loss: 1.4622 - val_acc: 0.4303\n",
      "Epoch 99/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.3341 - acc: 0.4733 - val_loss: 1.4793 - val_acc: 0.3576\n",
      "Epoch 100/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.3336 - acc: 0.4550 - val_loss: 1.4956 - val_acc: 0.3515\n",
      "Epoch 101/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.3529 - acc: 0.4341 - val_loss: 1.5554 - val_acc: 0.3545\n",
      "Epoch 102/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3363 - acc: 0.4444 - val_loss: 1.6615 - val_acc: 0.3424\n",
      "Epoch 103/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.3432 - acc: 0.4394 - val_loss: 1.6668 - val_acc: 0.3303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.3383 - acc: 0.4430 - val_loss: 1.7867 - val_acc: 0.2636\n",
      "Epoch 105/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.3333 - acc: 0.4129 - val_loss: 1.6755 - val_acc: 0.2939\n",
      "Epoch 106/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.3154 - acc: 0.4147 - val_loss: 1.6271 - val_acc: 0.3303\n",
      "Epoch 107/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.3094 - acc: 0.4362 - val_loss: 1.6907 - val_acc: 0.3000\n",
      "Epoch 108/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.3169 - acc: 0.4239 - val_loss: 1.7101 - val_acc: 0.2909\n",
      "Epoch 109/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.3265 - acc: 0.4175 - val_loss: 1.6834 - val_acc: 0.3394\n",
      "Epoch 110/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3010 - acc: 0.4535 - val_loss: 1.8691 - val_acc: 0.2879\n",
      "Epoch 111/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3685 - acc: 0.4186 - val_loss: 1.5486 - val_acc: 0.3394\n",
      "Epoch 112/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.3326 - acc: 0.4447 - val_loss: 1.5656 - val_acc: 0.3667\n",
      "Epoch 113/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.2709 - acc: 0.4645 - val_loss: 1.6142 - val_acc: 0.3424\n",
      "Epoch 114/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2958 - acc: 0.4504 - val_loss: 1.9289 - val_acc: 0.2909\n",
      "Epoch 115/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3811 - acc: 0.4108 - val_loss: 1.5581 - val_acc: 0.3303\n",
      "Epoch 116/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2872 - acc: 0.4451 - val_loss: 1.5343 - val_acc: 0.3667\n",
      "Epoch 117/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3108 - acc: 0.4348 - val_loss: 1.5185 - val_acc: 0.3697\n",
      "Epoch 118/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3255 - acc: 0.4451 - val_loss: 1.5008 - val_acc: 0.3909\n",
      "Epoch 119/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2655 - acc: 0.4769 - val_loss: 1.4700 - val_acc: 0.4061\n",
      "Epoch 120/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2659 - acc: 0.4663 - val_loss: 1.3939 - val_acc: 0.4485\n",
      "Epoch 121/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3291 - acc: 0.4592 - val_loss: 1.4833 - val_acc: 0.3818\n",
      "Epoch 122/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2808 - acc: 0.4571 - val_loss: 1.5094 - val_acc: 0.4182\n",
      "Epoch 123/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2680 - acc: 0.4691 - val_loss: 1.5005 - val_acc: 0.4273\n",
      "Epoch 124/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3289 - acc: 0.4306 - val_loss: 1.3819 - val_acc: 0.4242\n",
      "Epoch 125/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.3692 - acc: 0.4723 - val_loss: 1.5883 - val_acc: 0.3364\n",
      "Epoch 126/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2738 - acc: 0.4518 - val_loss: 1.5230 - val_acc: 0.3758\n",
      "Epoch 127/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2804 - acc: 0.4535 - val_loss: 1.6414 - val_acc: 0.3485\n",
      "Epoch 128/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2752 - acc: 0.4535 - val_loss: 1.5683 - val_acc: 0.3636\n",
      "Epoch 129/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2386 - acc: 0.4687 - val_loss: 1.6480 - val_acc: 0.3212\n",
      "Epoch 130/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.3285 - acc: 0.4638 - val_loss: 1.7832 - val_acc: 0.3121\n",
      "Epoch 131/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3225 - acc: 0.4097 - val_loss: 1.5828 - val_acc: 0.3424\n",
      "Epoch 132/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2456 - acc: 0.4557 - val_loss: 1.6779 - val_acc: 0.3121\n",
      "Epoch 133/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2758 - acc: 0.4384 - val_loss: 1.6160 - val_acc: 0.3182\n",
      "Epoch 134/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2472 - acc: 0.4444 - val_loss: 1.5657 - val_acc: 0.3424\n",
      "Epoch 135/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2420 - acc: 0.4490 - val_loss: 1.5640 - val_acc: 0.3727\n",
      "Epoch 136/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3161 - acc: 0.4430 - val_loss: 1.4521 - val_acc: 0.4212\n",
      "Epoch 137/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2380 - acc: 0.4765 - val_loss: 1.4052 - val_acc: 0.4545\n",
      "Epoch 138/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2819 - acc: 0.4638 - val_loss: 1.3991 - val_acc: 0.4333\n",
      "Epoch 139/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2838 - acc: 0.4698 - val_loss: 1.4583 - val_acc: 0.4030\n",
      "Epoch 140/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2113 - acc: 0.4694 - val_loss: 1.5028 - val_acc: 0.3758\n",
      "Epoch 141/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2694 - acc: 0.4447 - val_loss: 1.4164 - val_acc: 0.4212\n",
      "Epoch 142/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2384 - acc: 0.4687 - val_loss: 1.3980 - val_acc: 0.4667\n",
      "Epoch 143/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2609 - acc: 0.4751 - val_loss: 1.4658 - val_acc: 0.4424\n",
      "Epoch 144/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2718 - acc: 0.4716 - val_loss: 1.4657 - val_acc: 0.4333\n",
      "Epoch 145/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2261 - acc: 0.4751 - val_loss: 1.4542 - val_acc: 0.4394\n",
      "Epoch 146/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.2069 - acc: 0.4769 - val_loss: 1.4912 - val_acc: 0.4242\n",
      "Epoch 147/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2733 - acc: 0.4560 - val_loss: 1.4844 - val_acc: 0.4242\n",
      "Epoch 148/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2333 - acc: 0.4620 - val_loss: 1.4574 - val_acc: 0.3970\n",
      "Epoch 149/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2010 - acc: 0.4807 - val_loss: 1.5137 - val_acc: 0.4121\n",
      "Epoch 150/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2222 - acc: 0.4807 - val_loss: 2.0094 - val_acc: 0.1818\n",
      "Epoch 151/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.4446 - acc: 0.3278 - val_loss: 1.5698 - val_acc: 0.3455\n",
      "Epoch 152/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2653 - acc: 0.4543 - val_loss: 1.5255 - val_acc: 0.3606\n",
      "Epoch 153/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.2062 - acc: 0.4659 - val_loss: 1.5895 - val_acc: 0.3697\n",
      "Epoch 154/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.2319 - acc: 0.4560 - val_loss: 1.8477 - val_acc: 0.2788\n",
      "Epoch 155/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3183 - acc: 0.4006 - val_loss: 1.5000 - val_acc: 0.3636\n",
      "Epoch 156/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2161 - acc: 0.4486 - val_loss: 1.4335 - val_acc: 0.4273\n",
      "Epoch 157/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2173 - acc: 0.4680 - val_loss: 1.4353 - val_acc: 0.4091\n",
      "Epoch 158/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1820 - acc: 0.4786 - val_loss: 1.3858 - val_acc: 0.4242\n",
      "Epoch 159/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2099 - acc: 0.4712 - val_loss: 1.3586 - val_acc: 0.4455\n",
      "Epoch 160/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2652 - acc: 0.4783 - val_loss: 1.4253 - val_acc: 0.4424\n",
      "Epoch 161/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2525 - acc: 0.4687 - val_loss: 1.4844 - val_acc: 0.4212\n",
      "Epoch 162/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2282 - acc: 0.4687 - val_loss: 1.5354 - val_acc: 0.3939\n",
      "Epoch 163/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2142 - acc: 0.4624 - val_loss: 1.5906 - val_acc: 0.3636\n",
      "Epoch 164/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2387 - acc: 0.4507 - val_loss: 1.7562 - val_acc: 0.3303\n",
      "Epoch 165/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2772 - acc: 0.4408 - val_loss: 1.5316 - val_acc: 0.3727\n",
      "Epoch 166/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2159 - acc: 0.4571 - val_loss: 1.4697 - val_acc: 0.4061\n",
      "Epoch 167/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1771 - acc: 0.4839 - val_loss: 1.4576 - val_acc: 0.4212\n",
      "Epoch 168/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1777 - acc: 0.4921 - val_loss: 1.5145 - val_acc: 0.3970\n",
      "Epoch 169/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2227 - acc: 0.4652 - val_loss: 1.8003 - val_acc: 0.2939\n",
      "Epoch 170/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.3185 - acc: 0.4140 - val_loss: 1.5526 - val_acc: 0.3758\n",
      "Epoch 171/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2297 - acc: 0.4574 - val_loss: 1.4824 - val_acc: 0.4152\n",
      "Epoch 172/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2111 - acc: 0.4666 - val_loss: 1.4572 - val_acc: 0.4212\n",
      "Epoch 173/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1895 - acc: 0.4765 - val_loss: 1.4010 - val_acc: 0.4485\n",
      "Epoch 174/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1865 - acc: 0.4868 - val_loss: 1.4327 - val_acc: 0.4455\n",
      "Epoch 175/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.2348 - acc: 0.4705 - val_loss: 1.3725 - val_acc: 0.4606\n",
      "Epoch 176/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2298 - acc: 0.4868 - val_loss: 1.4092 - val_acc: 0.3939\n",
      "Epoch 177/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2321 - acc: 0.4458 - val_loss: 1.3457 - val_acc: 0.4394\n",
      "Epoch 178/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2333 - acc: 0.4836 - val_loss: 1.4361 - val_acc: 0.4121\n",
      "Epoch 179/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1588 - acc: 0.4797 - val_loss: 1.4414 - val_acc: 0.4394\n",
      "Epoch 180/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1474 - acc: 0.4959 - val_loss: 1.4366 - val_acc: 0.4333\n",
      "Epoch 181/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.2080 - acc: 0.4514 - val_loss: 1.3744 - val_acc: 0.4455\n",
      "Epoch 182/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1957 - acc: 0.4811 - val_loss: 1.3497 - val_acc: 0.4788\n",
      "Epoch 183/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2457 - acc: 0.4790 - val_loss: 1.3847 - val_acc: 0.4152\n",
      "Epoch 184/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2270 - acc: 0.4807 - val_loss: 1.5149 - val_acc: 0.4061\n",
      "Epoch 185/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1995 - acc: 0.4483 - val_loss: 1.6133 - val_acc: 0.3667\n",
      "Epoch 186/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.2239 - acc: 0.4412 - val_loss: 1.6263 - val_acc: 0.3727\n",
      "Epoch 187/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.2113 - acc: 0.4613 - val_loss: 1.6030 - val_acc: 0.3485\n",
      "Epoch 188/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2512 - acc: 0.4479 - val_loss: 1.4953 - val_acc: 0.3848\n",
      "Epoch 189/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1483 - acc: 0.4783 - val_loss: 1.5376 - val_acc: 0.3848\n",
      "Epoch 190/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1828 - acc: 0.4631 - val_loss: 1.6833 - val_acc: 0.3303\n",
      "Epoch 191/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2628 - acc: 0.4105 - val_loss: 1.5521 - val_acc: 0.3606\n",
      "Epoch 192/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1946 - acc: 0.4528 - val_loss: 1.4243 - val_acc: 0.4212\n",
      "Epoch 193/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1319 - acc: 0.4807 - val_loss: 1.4285 - val_acc: 0.4333\n",
      "Epoch 194/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1508 - acc: 0.4783 - val_loss: 1.3605 - val_acc: 0.4515\n",
      "Epoch 195/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2472 - acc: 0.4680 - val_loss: 1.4463 - val_acc: 0.4364\n",
      "Epoch 196/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1840 - acc: 0.4822 - val_loss: 1.3729 - val_acc: 0.4636\n",
      "Epoch 197/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.2194 - acc: 0.4822 - val_loss: 1.3524 - val_acc: 0.4818\n",
      "Epoch 198/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2144 - acc: 0.4910 - val_loss: 1.3747 - val_acc: 0.4394\n",
      "Epoch 199/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1564 - acc: 0.4974 - val_loss: 1.3954 - val_acc: 0.4455\n",
      "Epoch 200/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1332 - acc: 0.4963 - val_loss: 1.4041 - val_acc: 0.4545\n",
      "Epoch 201/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1277 - acc: 0.4928 - val_loss: 1.3264 - val_acc: 0.4970\n",
      "Epoch 202/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1956 - acc: 0.4878 - val_loss: 1.3532 - val_acc: 0.4606\n",
      "Epoch 203/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2580 - acc: 0.4740 - val_loss: 1.4397 - val_acc: 0.4424\n",
      "Epoch 204/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1621 - acc: 0.4889 - val_loss: 1.5459 - val_acc: 0.4030\n",
      "Epoch 205/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1936 - acc: 0.4525 - val_loss: 1.6098 - val_acc: 0.3636\n",
      "Epoch 206/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1789 - acc: 0.4606 - val_loss: 1.4698 - val_acc: 0.4333\n",
      "Epoch 207/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1518 - acc: 0.4733 - val_loss: 1.4132 - val_acc: 0.4333\n",
      "Epoch 208/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1635 - acc: 0.4924 - val_loss: 1.4312 - val_acc: 0.4242\n",
      "Epoch 209/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1433 - acc: 0.4924 - val_loss: 1.5077 - val_acc: 0.4152\n",
      "Epoch 210/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1975 - acc: 0.4776 - val_loss: 1.6312 - val_acc: 0.3606\n",
      "Epoch 211/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1938 - acc: 0.4497 - val_loss: 1.4865 - val_acc: 0.4091\n",
      "Epoch 212/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1381 - acc: 0.4843 - val_loss: 1.5449 - val_acc: 0.3697\n",
      "Epoch 213/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1905 - acc: 0.4507 - val_loss: 1.5860 - val_acc: 0.3818\n",
      "Epoch 214/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1624 - acc: 0.4507 - val_loss: 1.4020 - val_acc: 0.4394\n",
      "Epoch 215/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1034 - acc: 0.5058 - val_loss: 1.3369 - val_acc: 0.4727\n",
      "Epoch 216/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1755 - acc: 0.4832 - val_loss: 1.3289 - val_acc: 0.4636\n",
      "Epoch 217/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.2246 - acc: 0.4846 - val_loss: 1.4346 - val_acc: 0.4545\n",
      "Epoch 218/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1449 - acc: 0.5016 - val_loss: 1.4717 - val_acc: 0.4273\n",
      "Epoch 219/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1351 - acc: 0.4938 - val_loss: 1.3638 - val_acc: 0.4879\n",
      "Epoch 220/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1429 - acc: 0.4875 - val_loss: 1.3297 - val_acc: 0.4758\n",
      "Epoch 221/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1978 - acc: 0.4998 - val_loss: 1.4183 - val_acc: 0.4242\n",
      "Epoch 222/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1429 - acc: 0.4949 - val_loss: 1.4045 - val_acc: 0.4545\n",
      "Epoch 223/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1707 - acc: 0.5005 - val_loss: 1.4384 - val_acc: 0.4545\n",
      "Epoch 224/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1320 - acc: 0.4970 - val_loss: 1.3957 - val_acc: 0.4424\n",
      "Epoch 225/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1132 - acc: 0.4942 - val_loss: 1.3564 - val_acc: 0.4576\n",
      "Epoch 226/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.1728 - acc: 0.4917 - val_loss: 1.2888 - val_acc: 0.4939\n",
      "Epoch 227/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.2507 - acc: 0.4935 - val_loss: 1.4485 - val_acc: 0.4303\n",
      "Epoch 228/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1247 - acc: 0.4815 - val_loss: 1.3908 - val_acc: 0.4424\n",
      "Epoch 229/300\n",
      "2831/2831 [==============================] - 0s 63us/step - loss: 1.1035 - acc: 0.5026 - val_loss: 1.4060 - val_acc: 0.4576\n",
      "Epoch 230/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1791 - acc: 0.4680 - val_loss: 1.3735 - val_acc: 0.4576\n",
      "Epoch 231/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.0982 - acc: 0.5087 - val_loss: 1.3176 - val_acc: 0.4939\n",
      "Epoch 232/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.1873 - acc: 0.4832 - val_loss: 1.3900 - val_acc: 0.4455\n",
      "Epoch 233/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.2441 - acc: 0.4730 - val_loss: 1.4033 - val_acc: 0.4727\n",
      "Epoch 234/300\n",
      "2831/2831 [==============================] - 0s 61us/step - loss: 1.1067 - acc: 0.5161 - val_loss: 1.4030 - val_acc: 0.4545\n",
      "Epoch 235/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.0887 - acc: 0.5108 - val_loss: 1.3827 - val_acc: 0.4515\n",
      "Epoch 236/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1769 - acc: 0.4935 - val_loss: 1.3350 - val_acc: 0.4970\n",
      "Epoch 237/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1517 - acc: 0.5009 - val_loss: 1.3542 - val_acc: 0.4667\n",
      "Epoch 238/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0925 - acc: 0.5235 - val_loss: 1.3395 - val_acc: 0.4879\n",
      "Epoch 239/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1577 - acc: 0.4815 - val_loss: 1.4072 - val_acc: 0.4182\n",
      "Epoch 240/300\n",
      "2831/2831 [==============================] - 0s 98us/step - loss: 1.1359 - acc: 0.4938 - val_loss: 1.5420 - val_acc: 0.3848\n",
      "Epoch 241/300\n",
      "2831/2831 [==============================] - 0s 81us/step - loss: 1.1686 - acc: 0.4772 - val_loss: 1.5565 - val_acc: 0.3545\n",
      "Epoch 242/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1497 - acc: 0.4497 - val_loss: 1.5333 - val_acc: 0.4030\n",
      "Epoch 243/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.1365 - acc: 0.4719 - val_loss: 1.4174 - val_acc: 0.4545\n",
      "Epoch 244/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.0697 - acc: 0.5122 - val_loss: 1.4177 - val_acc: 0.4364\n",
      "Epoch 245/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.0777 - acc: 0.5140 - val_loss: 1.4225 - val_acc: 0.4485\n",
      "Epoch 246/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.0998 - acc: 0.5097 - val_loss: 1.8034 - val_acc: 0.2848\n",
      "Epoch 247/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.3540 - acc: 0.3617 - val_loss: 1.5355 - val_acc: 0.3636\n",
      "Epoch 248/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.2135 - acc: 0.4574 - val_loss: 1.3704 - val_acc: 0.4697\n",
      "Epoch 249/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1065 - acc: 0.5009 - val_loss: 1.3512 - val_acc: 0.4788\n",
      "Epoch 250/300\n",
      "2831/2831 [==============================] - 0s 79us/step - loss: 1.1062 - acc: 0.5079 - val_loss: 1.3031 - val_acc: 0.5000\n",
      "Epoch 251/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1611 - acc: 0.5037 - val_loss: 1.3771 - val_acc: 0.4273\n",
      "Epoch 252/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1280 - acc: 0.5108 - val_loss: 1.4648 - val_acc: 0.4455\n",
      "Epoch 253/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1180 - acc: 0.4998 - val_loss: 1.3945 - val_acc: 0.4606\n",
      "Epoch 254/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0793 - acc: 0.5185 - val_loss: 1.3721 - val_acc: 0.4606\n",
      "Epoch 255/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0661 - acc: 0.5295 - val_loss: 1.4042 - val_acc: 0.4515\n",
      "Epoch 256/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0691 - acc: 0.5161 - val_loss: 1.5610 - val_acc: 0.4061\n",
      "Epoch 257/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1627 - acc: 0.4585 - val_loss: 1.6407 - val_acc: 0.3727\n",
      "Epoch 258/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.2210 - acc: 0.4606 - val_loss: 1.4173 - val_acc: 0.4303\n",
      "Epoch 259/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1252 - acc: 0.5012 - val_loss: 1.4049 - val_acc: 0.4545\n",
      "Epoch 260/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0734 - acc: 0.5108 - val_loss: 1.3552 - val_acc: 0.4697\n",
      "Epoch 261/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0678 - acc: 0.5094 - val_loss: 1.2975 - val_acc: 0.5061\n",
      "Epoch 262/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1785 - acc: 0.4853 - val_loss: 1.3812 - val_acc: 0.4242\n",
      "Epoch 263/300\n",
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1751 - acc: 0.4921 - val_loss: 1.4790 - val_acc: 0.4303\n",
      "Epoch 264/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1148 - acc: 0.4931 - val_loss: 1.4031 - val_acc: 0.4455\n",
      "Epoch 265/300\n",
      "2831/2831 [==============================] - 0s 70us/step - loss: 1.0730 - acc: 0.5228 - val_loss: 1.3842 - val_acc: 0.4636\n",
      "Epoch 266/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.0736 - acc: 0.5224 - val_loss: 1.3791 - val_acc: 0.4333\n",
      "Epoch 267/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1349 - acc: 0.5034 - val_loss: 1.3233 - val_acc: 0.4939\n",
      "Epoch 268/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1948 - acc: 0.5203 - val_loss: 1.4153 - val_acc: 0.4152\n",
      "Epoch 269/300\n",
      "2831/2831 [==============================] - 0s 67us/step - loss: 1.0587 - acc: 0.5122 - val_loss: 1.3784 - val_acc: 0.4424\n",
      "Epoch 270/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0445 - acc: 0.5316 - val_loss: 1.3250 - val_acc: 0.5061\n",
      "Epoch 271/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.1269 - acc: 0.4949 - val_loss: 1.3297 - val_acc: 0.4576\n",
      "Epoch 272/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.1290 - acc: 0.4974 - val_loss: 1.3444 - val_acc: 0.4576\n",
      "Epoch 273/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1221 - acc: 0.5157 - val_loss: 1.3698 - val_acc: 0.4545\n",
      "Epoch 274/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1713 - acc: 0.5023 - val_loss: 1.4262 - val_acc: 0.4515\n",
      "Epoch 275/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.0878 - acc: 0.5200 - val_loss: 1.4159 - val_acc: 0.4333\n",
      "Epoch 276/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0964 - acc: 0.5313 - val_loss: 1.4316 - val_acc: 0.4424\n",
      "Epoch 277/300\n",
      "2831/2831 [==============================] - 0s 66us/step - loss: 1.1306 - acc: 0.5026 - val_loss: 1.3261 - val_acc: 0.4758\n",
      "Epoch 278/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1274 - acc: 0.5157 - val_loss: 1.3323 - val_acc: 0.4182\n",
      "Epoch 279/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.1205 - acc: 0.5065 - val_loss: 1.3908 - val_acc: 0.4606\n",
      "Epoch 280/300\n",
      "2831/2831 [==============================] - 0s 62us/step - loss: 1.0668 - acc: 0.5217 - val_loss: 1.5166 - val_acc: 0.4212\n",
      "Epoch 281/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2831/2831 [==============================] - 0s 76us/step - loss: 1.1108 - acc: 0.4776 - val_loss: 1.5629 - val_acc: 0.3970\n",
      "Epoch 282/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1134 - acc: 0.4815 - val_loss: 1.5025 - val_acc: 0.3970\n",
      "Epoch 283/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0923 - acc: 0.4928 - val_loss: 1.5788 - val_acc: 0.3727\n",
      "Epoch 284/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.1404 - acc: 0.4793 - val_loss: 1.4452 - val_acc: 0.4212\n",
      "Epoch 285/300\n",
      "2831/2831 [==============================] - 0s 71us/step - loss: 1.1110 - acc: 0.4949 - val_loss: 1.4184 - val_acc: 0.4485\n",
      "Epoch 286/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0853 - acc: 0.5076 - val_loss: 1.5361 - val_acc: 0.3758\n",
      "Epoch 287/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1376 - acc: 0.4786 - val_loss: 1.5547 - val_acc: 0.3970\n",
      "Epoch 288/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0906 - acc: 0.4832 - val_loss: 1.3738 - val_acc: 0.4303\n",
      "Epoch 289/300\n",
      "2831/2831 [==============================] - 0s 74us/step - loss: 1.0869 - acc: 0.4959 - val_loss: 1.3597 - val_acc: 0.4909\n",
      "Epoch 290/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0985 - acc: 0.5037 - val_loss: 1.3480 - val_acc: 0.4515\n",
      "Epoch 291/300\n",
      "2831/2831 [==============================] - 0s 68us/step - loss: 1.0765 - acc: 0.5090 - val_loss: 1.3096 - val_acc: 0.4667\n",
      "Epoch 292/300\n",
      "2831/2831 [==============================] - 0s 65us/step - loss: 1.1771 - acc: 0.5143 - val_loss: 1.4409 - val_acc: 0.4000\n",
      "Epoch 293/300\n",
      "2831/2831 [==============================] - 0s 78us/step - loss: 1.1253 - acc: 0.4878 - val_loss: 1.4354 - val_acc: 0.4455\n",
      "Epoch 294/300\n",
      "2831/2831 [==============================] - 0s 69us/step - loss: 1.0535 - acc: 0.5267 - val_loss: 1.4256 - val_acc: 0.4364\n",
      "Epoch 295/300\n",
      "2831/2831 [==============================] - 0s 73us/step - loss: 1.0521 - acc: 0.5288 - val_loss: 1.5839 - val_acc: 0.3939\n",
      "Epoch 296/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.1321 - acc: 0.4666 - val_loss: 1.4463 - val_acc: 0.4152\n",
      "Epoch 297/300\n",
      "2831/2831 [==============================] - 0s 60us/step - loss: 1.0665 - acc: 0.5256 - val_loss: 1.6404 - val_acc: 0.3576\n",
      "Epoch 298/300\n",
      "2831/2831 [==============================] - 0s 64us/step - loss: 1.1928 - acc: 0.4458 - val_loss: 1.4521 - val_acc: 0.4000\n",
      "Epoch 299/300\n",
      "2831/2831 [==============================] - 0s 75us/step - loss: 1.0629 - acc: 0.5122 - val_loss: 1.4007 - val_acc: 0.4576\n",
      "Epoch 300/300\n",
      "2831/2831 [==============================] - 0s 72us/step - loss: 1.0428 - acc: 0.5253 - val_loss: 1.4951 - val_acc: 0.3970\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176.0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.585227</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.055814</td>\n",
       "      <td>0.427907</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>1.433414</td>\n",
       "      <td>0.387879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.081731</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.057851</td>\n",
       "      <td>0.442149</td>\n",
       "      <td>0.855372</td>\n",
       "      <td>1.581616</td>\n",
       "      <td>0.390909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>1.316483</td>\n",
       "      <td>0.451515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185.0</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.929730</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.057269</td>\n",
       "      <td>0.444934</td>\n",
       "      <td>0.845815</td>\n",
       "      <td>1.453355</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164.0</td>\n",
       "      <td>0.091463</td>\n",
       "      <td>0.603659</td>\n",
       "      <td>0.920732</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.466019</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>1.495068</td>\n",
       "      <td>0.396970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       176.0  0.090909  0.585227   0.931818               0.22   \n",
       "1       208.0  0.081731  0.519231   0.894231               0.24   \n",
       "2        77.0  0.181818  0.701299   0.948052               0.26   \n",
       "3       185.0  0.086486  0.567568   0.929730               0.22   \n",
       "4       164.0  0.091463  0.603659   0.920732               0.24   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0              0.80                0.96             0.055814   \n",
       "1              0.82                0.98             0.057851   \n",
       "2              0.82                0.96             0.071429   \n",
       "3              0.82                0.96             0.057269   \n",
       "4              0.80                0.96             0.077670   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.427907              0.860465  1.433414  0.387879  \n",
       "1            0.442149              0.855372  1.581616  0.390909  \n",
       "2            0.482143              0.869048  1.316483  0.451515  \n",
       "3            0.444934              0.845815  1.453355  0.393939  \n",
       "4            0.466019              0.844660  1.495068  0.396970  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cluster_models = []\n",
    "df_w2v_analysis = pd.DataFrame()\n",
    "#, kernel_regularizer=regularizers.l1(0.0001)\n",
    "for num in range(5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_shape=(631,), activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "    model.add(Dropout(.35))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    ada = keras.optimizers.Adagrad()\n",
    "    adam = keras.optimizers.Adam(lr=0.001)\n",
    "    rms = keras.optimizers.RMSprop(lr=0.001)\n",
    "    sgd = keras.optimizers.SGD(lr=0.75)\n",
    "    model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    w2v_model_2 = model.fit(x=X_train_w2v, y=y_cat_train_w2v, \n",
    "          batch_size=2000, \n",
    "          epochs=300, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_w2v, y_cat_test_w2v),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)\n",
    "    \n",
    "    predictions = model.predict(X_test_w2v)\n",
    "    loss = w2v_model_2.history['val_loss'][-1]\n",
    "    acc = w2v_model_2.history['val_acc'][-1]\n",
    "    gain_pred, gain_true_large, gain_true, gain_stay_true, four_five_split_large, four_five_split, four_five_split_mean, predicted_classes_gain_true, predicted_classes_gain_large_true, predicted_classes_gain_mean = model_metrics(predictions, y_cat_test_w2v)\n",
    "    df_w2v_analysis.loc[num,'pred_count'] = gain_pred\n",
    "    df_w2v_analysis.loc[num,'gain_10%'] = gain_true_large\n",
    "    df_w2v_analysis.loc[num,'gain_3%'] = gain_true\n",
    "    df_w2v_analysis.loc[num,'gain_mean'] = gain_stay_true\n",
    "    df_w2v_analysis.loc[num,'top_pred_prob_10%'] = four_five_split_large\n",
    "    df_w2v_analysis.loc[num,'top_pred_prob_3%'] = four_five_split\n",
    "    df_w2v_analysis.loc[num,'top_pred_prob_mean'] = four_five_split_mean\n",
    "    df_w2v_analysis.loc[num,'model_pred_prob_10%'] = predicted_classes_gain_large_true\n",
    "    df_w2v_analysis.loc[num,'model_pred_prob_3%'] = predicted_classes_gain_true\n",
    "    df_w2v_analysis.loc[num,'model_pred_prob_mean'] = predicted_classes_gain_mean\n",
    "    df_w2v_analysis.loc[num,'loss'] = loss\n",
    "    df_w2v_analysis.loc[num,'acc'] = acc\n",
    "    print(num)\n",
    "df_300_w = df_w2v_analysis\n",
    "df_w2v_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.087302</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>1.414496</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.948905</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.069519</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.866310</td>\n",
       "      <td>1.475380</td>\n",
       "      <td>0.416918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.539130</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>1.399635</td>\n",
       "      <td>0.459215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.086331</td>\n",
       "      <td>0.496403</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>1.491990</td>\n",
       "      <td>0.438066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.684685</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.072993</td>\n",
       "      <td>0.481752</td>\n",
       "      <td>0.854015</td>\n",
       "      <td>1.385046</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        87.0  0.094972  0.758621   0.977011           0.303030   \n",
       "1       137.0  0.094972  0.671533   0.948905           0.303030   \n",
       "2        54.0  0.094972  0.777778   0.962963           0.242424   \n",
       "3       100.0  0.094972  0.720000   0.960000           0.272727   \n",
       "4       111.0  0.094972  0.684685   0.945946           0.303030   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.939394             0.087302   \n",
       "1          0.848485            0.939394             0.069519   \n",
       "2          0.818182            0.939394             0.069565   \n",
       "3          0.787879            0.939394             0.086331   \n",
       "4          0.787879            0.939394             0.072993   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.539683              0.904762  1.414496  0.432024  \n",
       "1            0.491979              0.866310  1.475380  0.416918  \n",
       "2            0.539130              0.886957  1.399635  0.459215  \n",
       "3            0.496403              0.848921  1.491990  0.438066  \n",
       "4            0.481752              0.854015  1.385046  0.471299  "
      ]
     },
     "execution_count": 1121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_400_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.877049</td>\n",
       "      <td>1.543366</td>\n",
       "      <td>0.450151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.095652</td>\n",
       "      <td>0.486957</td>\n",
       "      <td>0.878261</td>\n",
       "      <td>1.527017</td>\n",
       "      <td>0.456193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>1.605956</td>\n",
       "      <td>0.456193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.634043</td>\n",
       "      <td>0.444109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>1.477137</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        52.0  0.094972  0.788462   0.961538           0.272727   \n",
       "1        45.0  0.094972  0.755556   0.955556           0.272727   \n",
       "2        44.0  0.094972  0.772727   0.954545           0.272727   \n",
       "3        47.0  0.094972  0.872340   0.978723           0.242424   \n",
       "4        70.0  0.094972  0.771429   0.971429           0.212121   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.969697             0.090164   \n",
       "1          0.787879            0.939394             0.095652   \n",
       "2          0.757576            0.939394             0.105263   \n",
       "3          0.787879            0.969697             0.064103   \n",
       "4          0.787879            0.969697             0.085714   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.557377              0.877049  1.543366  0.450151  \n",
       "1            0.486957              0.878261  1.527017  0.456193  \n",
       "2            0.526316              0.921053  1.605956  0.456193  \n",
       "3            0.615385              0.923077  1.634043  0.444109  \n",
       "4            0.528571              0.878571  1.477137  0.471299  "
      ]
     },
     "execution_count": 1119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without weights, 400 epochs\n",
    "df_400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.502242</td>\n",
       "      <td>0.878924</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>0.851711</td>\n",
       "      <td>1.546759</td>\n",
       "      <td>0.410876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.095588</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>1.335251</td>\n",
       "      <td>0.483384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.945205</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.100917</td>\n",
       "      <td>0.559633</td>\n",
       "      <td>0.889908</td>\n",
       "      <td>1.317518</td>\n",
       "      <td>0.462236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.086614</td>\n",
       "      <td>0.496063</td>\n",
       "      <td>0.842520</td>\n",
       "      <td>1.512711</td>\n",
       "      <td>0.422961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.527397</td>\n",
       "      <td>0.897260</td>\n",
       "      <td>1.338329</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       223.0  0.094972  0.502242   0.878924           0.333333   \n",
       "1       110.0  0.094972  0.690909   0.963636           0.272727   \n",
       "2        73.0  0.094972  0.726027   0.945205           0.333333   \n",
       "3        93.0  0.094972  0.731183   0.935484           0.242424   \n",
       "4        54.0  0.094972  0.833333   0.962963           0.303030   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.878788            0.969697             0.057034   \n",
       "1          0.818182            0.939394             0.095588   \n",
       "2          0.848485            0.939394             0.100917   \n",
       "3          0.787879            0.939394             0.086614   \n",
       "4          0.848485            0.939394             0.095890   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.418251              0.851711  1.546759  0.410876  \n",
       "1            0.485294              0.845588  1.335251  0.483384  \n",
       "2            0.559633              0.889908  1.317518  0.462236  \n",
       "3            0.496063              0.842520  1.512711  0.422961  \n",
       "4            0.527397              0.897260  1.338329  0.432024  "
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best\n",
    "df_300_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.672566</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.491018</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>1.431859</td>\n",
       "      <td>0.410876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>1.336695</td>\n",
       "      <td>0.474320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.753425</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.093458</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>1.354104</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.075145</td>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.838150</td>\n",
       "      <td>1.313191</td>\n",
       "      <td>0.444109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.065476</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.851190</td>\n",
       "      <td>1.275235</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       113.0  0.094972  0.672566   0.929204           0.303030   \n",
       "1        46.0  0.094972  0.760870   0.956522           0.272727   \n",
       "2        73.0  0.094972  0.753425   0.958904           0.272727   \n",
       "3        51.0  0.094972  0.784314   0.960784           0.242424   \n",
       "4        75.0  0.094972  0.733333   0.946667           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.787879            0.939394             0.083832   \n",
       "1          0.787879            0.969697             0.131579   \n",
       "2          0.757576            0.939394             0.093458   \n",
       "3          0.848485            0.939394             0.075145   \n",
       "4          0.787879            0.969697             0.065476   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.491018              0.880240  1.431859  0.410876  \n",
       "1            0.491228              0.842105  1.336695  0.474320  \n",
       "2            0.532710              0.897196  1.354104  0.465257  \n",
       "3            0.450867              0.838150  1.313191  0.444109  \n",
       "4            0.488095              0.851190  1.275235  0.471299  "
      ]
     },
     "execution_count": 1129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.081522</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>1.230999</td>\n",
       "      <td>0.447130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.674797</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.086294</td>\n",
       "      <td>0.477157</td>\n",
       "      <td>0.832487</td>\n",
       "      <td>1.229504</td>\n",
       "      <td>0.447130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.857798</td>\n",
       "      <td>1.281605</td>\n",
       "      <td>0.398792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.268600</td>\n",
       "      <td>0.416918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>1.282214</td>\n",
       "      <td>0.456193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        85.0  0.094972  0.705882   0.941176           0.242424   \n",
       "1       123.0  0.094972  0.674797   0.934959           0.272727   \n",
       "2       112.0  0.094972  0.678571   0.928571           0.272727   \n",
       "3        61.0  0.094972  0.819672   0.967213           0.242424   \n",
       "4        60.0  0.094972  0.783333   0.966667           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.757576            0.939394             0.081522   \n",
       "1          0.727273            0.939394             0.086294   \n",
       "2          0.848485            0.969697             0.064220   \n",
       "3          0.787879            0.939394             0.092308   \n",
       "4          0.757576            0.939394             0.090909   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.434783              0.847826  1.230999  0.447130  \n",
       "1            0.477157              0.832487  1.229504  0.447130  \n",
       "2            0.440367              0.857798  1.281605  0.398792  \n",
       "3            0.507692              0.846154  1.268600  0.416918  \n",
       "4            0.484848              0.863636  1.282214  0.456193  "
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without weights, 225 epochs\n",
    "df_225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.570681</td>\n",
       "      <td>0.926702</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.088372</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.879070</td>\n",
       "      <td>1.429407</td>\n",
       "      <td>0.410876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.732394</td>\n",
       "      <td>0.957746</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>1.344073</td>\n",
       "      <td>0.471299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>1.387137</td>\n",
       "      <td>0.429003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.496503</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>1.544066</td>\n",
       "      <td>0.380665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.640845</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.357028</td>\n",
       "      <td>0.465257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       191.0  0.094972  0.570681   0.926702           0.272727   \n",
       "1        71.0  0.094972  0.732394   0.957746           0.303030   \n",
       "2        59.0  0.094972  0.796610   0.966102           0.303030   \n",
       "3       119.0  0.094972  0.663866   0.941176           0.303030   \n",
       "4       142.0  0.094972  0.640845   0.929577           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.757576            0.909091             0.088372   \n",
       "1          0.818182            0.939394             0.071429   \n",
       "2          0.818182            0.939394             0.125000   \n",
       "3          0.818182            0.969697             0.083916   \n",
       "4          0.787879            0.969697             0.078125   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.488372              0.879070  1.429407  0.410876  \n",
       "1            0.428571              0.842857  1.344073  0.471299  \n",
       "2            0.556818              0.840909  1.387137  0.429003  \n",
       "3            0.496503              0.860140  1.544066  0.380665  \n",
       "4            0.447917              0.843750  1.357028  0.465257  "
      ]
     },
     "execution_count": 1123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights, 225 epochs\n",
    "df_225_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.061404</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.850877</td>\n",
       "      <td>1.198510</td>\n",
       "      <td>0.407855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.068293</td>\n",
       "      <td>0.448780</td>\n",
       "      <td>0.863415</td>\n",
       "      <td>1.237291</td>\n",
       "      <td>0.429003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>249.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.469880</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.066176</td>\n",
       "      <td>0.408088</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>1.319757</td>\n",
       "      <td>0.386707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.584699</td>\n",
       "      <td>0.923497</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.073733</td>\n",
       "      <td>0.433180</td>\n",
       "      <td>0.843318</td>\n",
       "      <td>1.329758</td>\n",
       "      <td>0.371601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.524038</td>\n",
       "      <td>0.913462</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.065041</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.331992</td>\n",
       "      <td>0.419940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0       128.0  0.094972  0.664062   0.937500           0.242424   \n",
       "1       154.0  0.094972  0.597403   0.909091           0.272727   \n",
       "2       249.0  0.094972  0.469880   0.867470           0.303030   \n",
       "3       183.0  0.094972  0.584699   0.923497           0.242424   \n",
       "4       208.0  0.094972  0.524038   0.913462           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.757576            0.939394             0.061404   \n",
       "1          0.727273            0.939394             0.068293   \n",
       "2          0.787879            0.969697             0.066176   \n",
       "3          0.818182            0.969697             0.073733   \n",
       "4          0.757576            0.909091             0.065041   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.421053              0.850877  1.198510  0.407855  \n",
       "1            0.448780              0.863415  1.237291  0.429003  \n",
       "2            0.408088              0.845588  1.319757  0.386707  \n",
       "3            0.433180              0.843318  1.329758  0.371601  \n",
       "4            0.439024              0.833333  1.331992  0.419940  "
      ]
     },
     "execution_count": 1127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_count</th>\n",
       "      <th>gain_10%</th>\n",
       "      <th>gain_3%</th>\n",
       "      <th>gain_mean</th>\n",
       "      <th>top_pred_prob_10%</th>\n",
       "      <th>top_pred_prob_3%</th>\n",
       "      <th>top_pred_prob_mean</th>\n",
       "      <th>model_pred_prob_10%</th>\n",
       "      <th>model_pred_prob_3%</th>\n",
       "      <th>model_pred_prob_mean</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.059113</td>\n",
       "      <td>0.394089</td>\n",
       "      <td>0.832512</td>\n",
       "      <td>1.342970</td>\n",
       "      <td>0.456193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.569832</td>\n",
       "      <td>0.927374</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.837104</td>\n",
       "      <td>1.422866</td>\n",
       "      <td>0.432024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.575269</td>\n",
       "      <td>0.930108</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.073913</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.843478</td>\n",
       "      <td>1.432149</td>\n",
       "      <td>0.404834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>271.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.435424</td>\n",
       "      <td>0.837638</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.060403</td>\n",
       "      <td>0.402685</td>\n",
       "      <td>0.828859</td>\n",
       "      <td>1.555367</td>\n",
       "      <td>0.344411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211.0</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.530806</td>\n",
       "      <td>0.914692</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.082251</td>\n",
       "      <td>0.471861</td>\n",
       "      <td>0.852814</td>\n",
       "      <td>1.608255</td>\n",
       "      <td>0.350453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_count  gain_10%   gain_3%  gain_mean  top_pred_prob_10%  \\\n",
       "0        62.0  0.094972  0.741935   0.951613           0.272727   \n",
       "1       179.0  0.094972  0.569832   0.927374           0.272727   \n",
       "2       186.0  0.094972  0.575269   0.930108           0.272727   \n",
       "3       271.0  0.094972  0.435424   0.837638           0.272727   \n",
       "4       211.0  0.094972  0.530806   0.914692           0.272727   \n",
       "\n",
       "   top_pred_prob_3%  top_pred_prob_mean  model_pred_prob_10%  \\\n",
       "0          0.848485            0.969697             0.059113   \n",
       "1          0.727273            0.939394             0.063348   \n",
       "2          0.727273            0.878788             0.073913   \n",
       "3          0.818182            0.969697             0.060403   \n",
       "4          0.818182            0.969697             0.082251   \n",
       "\n",
       "   model_pred_prob_3%  model_pred_prob_mean      loss       acc  \n",
       "0            0.394089              0.832512  1.342970  0.456193  \n",
       "1            0.434389              0.837104  1.422866  0.432024  \n",
       "2            0.426087              0.843478  1.432149  0.404834  \n",
       "3            0.402685              0.828859  1.555367  0.344411  \n",
       "4            0.471861              0.852814  1.608255  0.350453  "
      ]
     },
     "execution_count": 1125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_175_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "0.8571428571428571\n",
      "0.9523809523809523\n",
      "21\n",
      "0.7142857142857143\n",
      "0.8571428571428571\n",
      "0.8181818181818182\n",
      "0.23809523809523808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  3,  1,  0,  0],\n",
       "       [ 4, 39, 66, 23,  5],\n",
       "       [ 1,  7, 61, 38,  2],\n",
       "       [ 0,  1, 14, 48,  9],\n",
       "       [ 0,  2,  2,  1,  4]])"
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_w2v)\n",
    "#\n",
    "model_metrics(predictions, y_cat_test_w2v)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test_w2v.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13aeea710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w2v_model_2.history['loss'])\n",
    "plt.plot(w2v_model_2.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1500b4b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(zillow_model_2.history['val_loss'])\n",
    "plt.plot(business_model_2.history['val_loss'])\n",
    "plt.plot(w2v_model_2.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.xlim(0, 400)\n",
    "plt.ylim(1, 2.5)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['zillow', 'business', 'w2v'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_shape=(631,), activation='relu', kernel_regularizer=regularizers.l1(0.0001)))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.000005)))\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.000005)))\n",
    "model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l1(0.000005)))\n",
    "model.add(Dropout(.35))\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.000005)))\n",
    "model.add(Dense(15, activation='relu', kernel_regularizer=regularizers.l1(0.000005)))\n",
    "model.add(Dense(25, activation='relu', kernel_regularizer=regularizers.l1(0.000005)))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "ada = keras.optimizers.Adagrad()\n",
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "rms = keras.optimizers.RMSprop(lr=0.002)\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer=rms,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2839 samples, validate on 331 samples\n",
      "Epoch 1/400\n",
      "2839/2839 [==============================] - 7s 3ms/step - loss: 2.2779 - acc: 0.3209 - val_loss: 2.0955 - val_acc: 0.4350\n",
      "Epoch 2/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 2.0493 - acc: 0.3639 - val_loss: 1.9858 - val_acc: 0.3323\n",
      "Epoch 3/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.9301 - acc: 0.3656 - val_loss: 1.9579 - val_acc: 0.3323\n",
      "Epoch 4/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.8248 - acc: 0.3706 - val_loss: 1.8833 - val_acc: 0.3323\n",
      "Epoch 5/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.7600 - acc: 0.3695 - val_loss: 1.8576 - val_acc: 0.3323\n",
      "Epoch 6/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.7005 - acc: 0.3706 - val_loss: 1.8073 - val_acc: 0.3323\n",
      "Epoch 7/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.6577 - acc: 0.3677 - val_loss: 1.7993 - val_acc: 0.3323\n",
      "Epoch 8/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.6434 - acc: 0.3727 - val_loss: 1.7618 - val_acc: 0.3323\n",
      "Epoch 9/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.5999 - acc: 0.3727 - val_loss: 1.7110 - val_acc: 0.3323\n",
      "Epoch 10/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.5545 - acc: 0.3790 - val_loss: 1.6790 - val_acc: 0.3323\n",
      "Epoch 11/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.5245 - acc: 0.3653 - val_loss: 1.6516 - val_acc: 0.3323\n",
      "Epoch 12/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.4765 - acc: 0.3787 - val_loss: 1.6239 - val_acc: 0.3323\n",
      "Epoch 13/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.4629 - acc: 0.3741 - val_loss: 1.6192 - val_acc: 0.3323\n",
      "Epoch 14/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.4620 - acc: 0.3808 - val_loss: 1.6052 - val_acc: 0.3323\n",
      "Epoch 15/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.4405 - acc: 0.3790 - val_loss: 1.5727 - val_acc: 0.3323\n",
      "Epoch 16/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.4182 - acc: 0.3751 - val_loss: 1.5686 - val_acc: 0.3323\n",
      "Epoch 17/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.3991 - acc: 0.3850 - val_loss: 1.5382 - val_acc: 0.3323\n",
      "Epoch 18/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3714 - acc: 0.3942 - val_loss: 1.5498 - val_acc: 0.3323\n",
      "Epoch 19/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3846 - acc: 0.3875 - val_loss: 1.5796 - val_acc: 0.3323\n",
      "Epoch 20/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3839 - acc: 0.3903 - val_loss: 1.5211 - val_acc: 0.3323\n",
      "Epoch 21/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.3488 - acc: 0.3878 - val_loss: 1.5161 - val_acc: 0.3323\n",
      "Epoch 22/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3436 - acc: 0.3864 - val_loss: 1.5300 - val_acc: 0.3323\n",
      "Epoch 23/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3755 - acc: 0.3913 - val_loss: 1.5548 - val_acc: 0.3323\n",
      "Epoch 24/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3799 - acc: 0.3917 - val_loss: 1.4914 - val_acc: 0.3323\n",
      "Epoch 25/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3321 - acc: 0.3998 - val_loss: 1.4922 - val_acc: 0.3323\n",
      "Epoch 26/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.3333 - acc: 0.3959 - val_loss: 1.4960 - val_acc: 0.3323\n",
      "Epoch 27/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3472 - acc: 0.3949 - val_loss: 1.4872 - val_acc: 0.3323\n",
      "Epoch 28/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3187 - acc: 0.3973 - val_loss: 1.4741 - val_acc: 0.3323\n",
      "Epoch 29/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3173 - acc: 0.3956 - val_loss: 1.5239 - val_acc: 0.3323\n",
      "Epoch 30/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.3788 - acc: 0.3952 - val_loss: 1.4756 - val_acc: 0.3323\n",
      "Epoch 31/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3157 - acc: 0.3998 - val_loss: 1.4729 - val_acc: 0.3323\n",
      "Epoch 32/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3143 - acc: 0.3980 - val_loss: 1.4751 - val_acc: 0.3323\n",
      "Epoch 33/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3151 - acc: 0.3994 - val_loss: 1.4896 - val_acc: 0.3323\n",
      "Epoch 34/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3268 - acc: 0.3966 - val_loss: 1.4748 - val_acc: 0.3323\n",
      "Epoch 35/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.3213 - acc: 0.4005 - val_loss: 1.4770 - val_acc: 0.3323\n",
      "Epoch 36/400\n",
      "2839/2839 [==============================] - 0s 79us/step - loss: 1.3106 - acc: 0.4001 - val_loss: 1.4687 - val_acc: 0.3323\n",
      "Epoch 37/400\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 1.2913 - acc: 0.3991 - val_loss: 1.4518 - val_acc: 0.3323\n",
      "Epoch 38/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2901 - acc: 0.4019 - val_loss: 1.4884 - val_acc: 0.3323\n",
      "Epoch 39/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3349 - acc: 0.4005 - val_loss: 1.4740 - val_acc: 0.3323\n",
      "Epoch 40/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.3020 - acc: 0.4008 - val_loss: 1.4439 - val_acc: 0.3323\n",
      "Epoch 41/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2932 - acc: 0.4008 - val_loss: 1.4972 - val_acc: 0.3323\n",
      "Epoch 42/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.3370 - acc: 0.4005 - val_loss: 1.4684 - val_acc: 0.3323\n",
      "Epoch 43/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2947 - acc: 0.4005 - val_loss: 1.4446 - val_acc: 0.3323\n",
      "Epoch 44/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2786 - acc: 0.4030 - val_loss: 1.4453 - val_acc: 0.3323\n",
      "Epoch 45/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.2720 - acc: 0.4023 - val_loss: 1.4484 - val_acc: 0.3323\n",
      "Epoch 46/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.2954 - acc: 0.4026 - val_loss: 1.5046 - val_acc: 0.3323\n",
      "Epoch 47/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.3250 - acc: 0.4015 - val_loss: 1.4420 - val_acc: 0.3323\n",
      "Epoch 48/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2707 - acc: 0.4026 - val_loss: 1.4265 - val_acc: 0.3323\n",
      "Epoch 49/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2711 - acc: 0.4001 - val_loss: 1.4683 - val_acc: 0.3323\n",
      "Epoch 50/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3080 - acc: 0.4023 - val_loss: 1.4702 - val_acc: 0.3293\n",
      "Epoch 51/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.2974 - acc: 0.3991 - val_loss: 1.4282 - val_acc: 0.3323\n",
      "Epoch 52/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2667 - acc: 0.4012 - val_loss: 1.4303 - val_acc: 0.3323\n",
      "Epoch 53/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2570 - acc: 0.4012 - val_loss: 1.4453 - val_acc: 0.3323\n",
      "Epoch 54/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.3051 - acc: 0.3956 - val_loss: 1.4554 - val_acc: 0.3323\n",
      "Epoch 55/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2753 - acc: 0.4023 - val_loss: 1.4379 - val_acc: 0.3293\n",
      "Epoch 56/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.2704 - acc: 0.4026 - val_loss: 1.4851 - val_acc: 0.3323\n",
      "Epoch 57/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3185 - acc: 0.3966 - val_loss: 1.4492 - val_acc: 0.3293\n",
      "Epoch 58/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2710 - acc: 0.4030 - val_loss: 1.4325 - val_acc: 0.3293\n",
      "Epoch 59/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2503 - acc: 0.4033 - val_loss: 1.4338 - val_acc: 0.3263\n",
      "Epoch 60/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2711 - acc: 0.4015 - val_loss: 1.5444 - val_acc: 0.2749\n",
      "Epoch 61/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3406 - acc: 0.3561 - val_loss: 1.4486 - val_acc: 0.3233\n",
      "Epoch 62/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2600 - acc: 0.4044 - val_loss: 1.4416 - val_acc: 0.3263\n",
      "Epoch 63/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2639 - acc: 0.4008 - val_loss: 1.4976 - val_acc: 0.2991\n",
      "Epoch 64/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2979 - acc: 0.3892 - val_loss: 1.4435 - val_acc: 0.3233\n",
      "Epoch 65/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2571 - acc: 0.4008 - val_loss: 1.4538 - val_acc: 0.3112\n",
      "Epoch 66/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.2753 - acc: 0.4008 - val_loss: 1.5024 - val_acc: 0.3051\n",
      "Epoch 67/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3083 - acc: 0.3808 - val_loss: 1.4454 - val_acc: 0.3293\n",
      "Epoch 68/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2539 - acc: 0.4033 - val_loss: 1.4344 - val_acc: 0.3202\n",
      "Epoch 69/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2442 - acc: 0.4001 - val_loss: 1.4410 - val_acc: 0.3142\n",
      "Epoch 70/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2608 - acc: 0.4015 - val_loss: 1.5294 - val_acc: 0.2689\n",
      "Epoch 71/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.3178 - acc: 0.3653 - val_loss: 1.4413 - val_acc: 0.3293\n",
      "Epoch 72/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.2451 - acc: 0.4044 - val_loss: 1.4306 - val_acc: 0.3172\n",
      "Epoch 73/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.2513 - acc: 0.3980 - val_loss: 1.5307 - val_acc: 0.2659\n",
      "Epoch 74/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3143 - acc: 0.3480 - val_loss: 1.4455 - val_acc: 0.3263\n",
      "Epoch 75/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2399 - acc: 0.4015 - val_loss: 1.4359 - val_acc: 0.3051\n",
      "Epoch 76/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2561 - acc: 0.3966 - val_loss: 1.5064 - val_acc: 0.2719\n",
      "Epoch 77/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3023 - acc: 0.3540 - val_loss: 1.4306 - val_acc: 0.3202\n",
      "Epoch 78/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2349 - acc: 0.4030 - val_loss: 1.4212 - val_acc: 0.3172\n",
      "Epoch 79/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2266 - acc: 0.4008 - val_loss: 1.4323 - val_acc: 0.3021\n",
      "Epoch 80/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2468 - acc: 0.3945 - val_loss: 1.5654 - val_acc: 0.2145\n",
      "Epoch 81/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3304 - acc: 0.2934 - val_loss: 1.4427 - val_acc: 0.3082\n",
      "Epoch 82/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2463 - acc: 0.3952 - val_loss: 1.5029 - val_acc: 0.2628\n",
      "Epoch 83/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2942 - acc: 0.3512 - val_loss: 1.4378 - val_acc: 0.3051\n",
      "Epoch 84/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2460 - acc: 0.3875 - val_loss: 1.4493 - val_acc: 0.3021\n",
      "Epoch 85/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2506 - acc: 0.3875 - val_loss: 1.4710 - val_acc: 0.2779\n",
      "Epoch 86/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2580 - acc: 0.3713 - val_loss: 1.4633 - val_acc: 0.3021\n",
      "Epoch 87/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2563 - acc: 0.3875 - val_loss: 1.4242 - val_acc: 0.3082\n",
      "Epoch 88/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2364 - acc: 0.3906 - val_loss: 1.4607 - val_acc: 0.2810\n",
      "Epoch 89/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2594 - acc: 0.3716 - val_loss: 1.4787 - val_acc: 0.2719\n",
      "Epoch 90/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2768 - acc: 0.3600 - val_loss: 1.4272 - val_acc: 0.3082\n",
      "Epoch 91/400\n",
      "2839/2839 [==============================] - 0s 83us/step - loss: 1.2456 - acc: 0.3991 - val_loss: 1.4944 - val_acc: 0.2508\n",
      "Epoch 92/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2811 - acc: 0.3427 - val_loss: 1.4604 - val_acc: 0.2900\n",
      "Epoch 93/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2418 - acc: 0.3815 - val_loss: 1.4478 - val_acc: 0.2810\n",
      "Epoch 94/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2271 - acc: 0.3843 - val_loss: 1.3947 - val_acc: 0.3172\n",
      "Epoch 95/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2132 - acc: 0.3959 - val_loss: 1.4295 - val_acc: 0.2991\n",
      "Epoch 96/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2306 - acc: 0.3942 - val_loss: 1.4990 - val_acc: 0.2387\n",
      "Epoch 97/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2643 - acc: 0.3209 - val_loss: 1.4610 - val_acc: 0.2961\n",
      "Epoch 98/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2703 - acc: 0.3846 - val_loss: 1.4626 - val_acc: 0.2840\n",
      "Epoch 99/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2356 - acc: 0.3832 - val_loss: 1.4883 - val_acc: 0.2659\n",
      "Epoch 100/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2472 - acc: 0.3575 - val_loss: 1.4719 - val_acc: 0.2810\n",
      "Epoch 101/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2582 - acc: 0.3706 - val_loss: 1.3778 - val_acc: 0.3172\n",
      "Epoch 102/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.2319 - acc: 0.4061 - val_loss: 1.4143 - val_acc: 0.3202\n",
      "Epoch 103/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.2291 - acc: 0.3927 - val_loss: 1.4344 - val_acc: 0.3172\n",
      "Epoch 104/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2570 - acc: 0.3818 - val_loss: 1.4237 - val_acc: 0.3051\n",
      "Epoch 105/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2081 - acc: 0.3899 - val_loss: 1.4312 - val_acc: 0.3082\n",
      "Epoch 106/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2091 - acc: 0.3822 - val_loss: 1.3737 - val_acc: 0.3233\n",
      "Epoch 107/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2381 - acc: 0.4023 - val_loss: 1.5076 - val_acc: 0.3082\n",
      "Epoch 108/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.2345 - acc: 0.3987 - val_loss: 1.4239 - val_acc: 0.3051\n",
      "Epoch 109/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.1829 - acc: 0.4001 - val_loss: 1.4462 - val_acc: 0.2900\n",
      "Epoch 110/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1921 - acc: 0.3868 - val_loss: 1.3952 - val_acc: 0.3172\n",
      "Epoch 111/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1968 - acc: 0.3843 - val_loss: 1.3824 - val_acc: 0.3233\n",
      "Epoch 112/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.3222 - acc: 0.3783 - val_loss: 1.3784 - val_acc: 0.3202\n",
      "Epoch 113/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2153 - acc: 0.4019 - val_loss: 1.4148 - val_acc: 0.3202\n",
      "Epoch 114/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1956 - acc: 0.3973 - val_loss: 1.4400 - val_acc: 0.3233\n",
      "Epoch 115/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3018 - acc: 0.3730 - val_loss: 1.5449 - val_acc: 0.2779\n",
      "Epoch 116/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2167 - acc: 0.3811 - val_loss: 1.3921 - val_acc: 0.3202\n",
      "Epoch 117/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1960 - acc: 0.4089 - val_loss: 1.4404 - val_acc: 0.2840\n",
      "Epoch 118/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1819 - acc: 0.3973 - val_loss: 1.8275 - val_acc: 0.1027\n",
      "Epoch 119/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.3254 - acc: 0.2666 - val_loss: 1.3763 - val_acc: 0.3293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.2208 - acc: 0.4030 - val_loss: 1.3995 - val_acc: 0.3172\n",
      "Epoch 121/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2213 - acc: 0.4089 - val_loss: 1.4711 - val_acc: 0.2719\n",
      "Epoch 122/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2466 - acc: 0.3853 - val_loss: 1.4465 - val_acc: 0.2840\n",
      "Epoch 123/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2147 - acc: 0.3853 - val_loss: 1.4343 - val_acc: 0.2931\n",
      "Epoch 124/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1943 - acc: 0.4023 - val_loss: 1.4449 - val_acc: 0.2689\n",
      "Epoch 125/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2005 - acc: 0.3818 - val_loss: 1.6182 - val_acc: 0.1722\n",
      "Epoch 126/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2619 - acc: 0.2871 - val_loss: 1.3902 - val_acc: 0.3233\n",
      "Epoch 127/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1811 - acc: 0.4100 - val_loss: 1.4135 - val_acc: 0.3021\n",
      "Epoch 128/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1607 - acc: 0.3977 - val_loss: 1.3613 - val_acc: 0.3112\n",
      "Epoch 129/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1831 - acc: 0.4015 - val_loss: 1.3541 - val_acc: 0.3263\n",
      "Epoch 130/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1948 - acc: 0.4107 - val_loss: 1.5382 - val_acc: 0.2961\n",
      "Epoch 131/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2297 - acc: 0.4026 - val_loss: 1.3803 - val_acc: 0.3172\n",
      "Epoch 132/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2079 - acc: 0.3741 - val_loss: 1.3624 - val_acc: 0.3353\n",
      "Epoch 133/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2410 - acc: 0.4037 - val_loss: 1.4610 - val_acc: 0.3233\n",
      "Epoch 134/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1814 - acc: 0.4114 - val_loss: 1.5077 - val_acc: 0.3051\n",
      "Epoch 135/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1890 - acc: 0.4015 - val_loss: 1.4111 - val_acc: 0.3021\n",
      "Epoch 136/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1466 - acc: 0.4044 - val_loss: 1.3971 - val_acc: 0.3051\n",
      "Epoch 137/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1400 - acc: 0.3973 - val_loss: 1.3543 - val_acc: 0.3293\n",
      "Epoch 138/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1624 - acc: 0.4051 - val_loss: 1.3432 - val_acc: 0.3293\n",
      "Epoch 139/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2001 - acc: 0.3910 - val_loss: 1.5682 - val_acc: 0.3293\n",
      "Epoch 140/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2618 - acc: 0.4058 - val_loss: 1.4451 - val_acc: 0.2961\n",
      "Epoch 141/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1680 - acc: 0.3998 - val_loss: 1.3886 - val_acc: 0.3172\n",
      "Epoch 142/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1589 - acc: 0.3927 - val_loss: 1.3470 - val_acc: 0.3353\n",
      "Epoch 143/400\n",
      "2839/2839 [==============================] - 0s 85us/step - loss: 1.2469 - acc: 0.4128 - val_loss: 1.5216 - val_acc: 0.3172\n",
      "Epoch 144/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.1941 - acc: 0.4135 - val_loss: 1.4429 - val_acc: 0.3082\n",
      "Epoch 145/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.1474 - acc: 0.4054 - val_loss: 1.4586 - val_acc: 0.3051\n",
      "Epoch 146/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1344 - acc: 0.4132 - val_loss: 1.5876 - val_acc: 0.2810\n",
      "Epoch 147/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1960 - acc: 0.3980 - val_loss: 1.4199 - val_acc: 0.3293\n",
      "Epoch 148/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.2218 - acc: 0.4185 - val_loss: 1.4814 - val_acc: 0.2598\n",
      "Epoch 149/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1757 - acc: 0.3776 - val_loss: 1.5544 - val_acc: 0.2538\n",
      "Epoch 150/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1978 - acc: 0.3508 - val_loss: 1.4265 - val_acc: 0.3202\n",
      "Epoch 151/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1619 - acc: 0.4065 - val_loss: 1.3876 - val_acc: 0.3263\n",
      "Epoch 152/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1691 - acc: 0.4167 - val_loss: 1.4178 - val_acc: 0.2961\n",
      "Epoch 153/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1584 - acc: 0.4104 - val_loss: 1.3734 - val_acc: 0.3233\n",
      "Epoch 154/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1477 - acc: 0.4181 - val_loss: 1.3325 - val_acc: 0.3233\n",
      "Epoch 155/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1624 - acc: 0.4107 - val_loss: 1.3312 - val_acc: 0.3384\n",
      "Epoch 156/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1985 - acc: 0.3998 - val_loss: 1.4406 - val_acc: 0.3293\n",
      "Epoch 157/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1529 - acc: 0.4181 - val_loss: 1.5058 - val_acc: 0.2991\n",
      "Epoch 158/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1511 - acc: 0.4093 - val_loss: 1.3963 - val_acc: 0.3142\n",
      "Epoch 159/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1258 - acc: 0.4178 - val_loss: 1.3944 - val_acc: 0.3535\n",
      "Epoch 160/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1653 - acc: 0.4199 - val_loss: 1.4313 - val_acc: 0.3776\n",
      "Epoch 161/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1918 - acc: 0.4315 - val_loss: 1.8943 - val_acc: 0.1541\n",
      "Epoch 162/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3294 - acc: 0.2860 - val_loss: 1.4371 - val_acc: 0.3776\n",
      "Epoch 163/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2458 - acc: 0.4153 - val_loss: 1.4013 - val_acc: 0.3263\n",
      "Epoch 164/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1516 - acc: 0.4269 - val_loss: 1.3905 - val_acc: 0.3565\n",
      "Epoch 165/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1253 - acc: 0.4280 - val_loss: 1.3767 - val_acc: 0.3656\n",
      "Epoch 166/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1287 - acc: 0.4251 - val_loss: 1.3909 - val_acc: 0.3656\n",
      "Epoch 167/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1507 - acc: 0.4188 - val_loss: 1.4768 - val_acc: 0.3082\n",
      "Epoch 168/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1603 - acc: 0.3896 - val_loss: 1.5228 - val_acc: 0.3021\n",
      "Epoch 169/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1635 - acc: 0.3772 - val_loss: 1.5693 - val_acc: 0.3051\n",
      "Epoch 170/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1595 - acc: 0.4008 - val_loss: 1.4039 - val_acc: 0.3414\n",
      "Epoch 171/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0961 - acc: 0.4269 - val_loss: 1.4021 - val_acc: 0.3625\n",
      "Epoch 172/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1194 - acc: 0.4269 - val_loss: 1.4373 - val_acc: 0.3686\n",
      "Epoch 173/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1991 - acc: 0.4044 - val_loss: 1.5505 - val_acc: 0.2991\n",
      "Epoch 174/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1650 - acc: 0.3751 - val_loss: 1.4675 - val_acc: 0.3414\n",
      "Epoch 175/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1140 - acc: 0.4283 - val_loss: 1.5099 - val_acc: 0.3414\n",
      "Epoch 176/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1322 - acc: 0.4167 - val_loss: 1.4236 - val_acc: 0.3293\n",
      "Epoch 177/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1125 - acc: 0.4276 - val_loss: 1.4333 - val_acc: 0.3384\n",
      "Epoch 178/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1199 - acc: 0.4199 - val_loss: 1.4942 - val_acc: 0.3202\n",
      "Epoch 179/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1389 - acc: 0.4209 - val_loss: 1.3583 - val_acc: 0.3444\n",
      "Epoch 180/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1064 - acc: 0.4075 - val_loss: 1.3091 - val_acc: 0.3565\n",
      "Epoch 181/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.3252 - acc: 0.4072 - val_loss: 1.6340 - val_acc: 0.3353\n",
      "Epoch 182/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1995 - acc: 0.4251 - val_loss: 1.3496 - val_acc: 0.3505\n",
      "Epoch 183/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0962 - acc: 0.4259 - val_loss: 1.3107 - val_acc: 0.3565\n",
      "Epoch 184/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1074 - acc: 0.4350 - val_loss: 1.3714 - val_acc: 0.3625\n",
      "Epoch 185/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0819 - acc: 0.4340 - val_loss: 1.3531 - val_acc: 0.3746\n",
      "Epoch 186/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0826 - acc: 0.4375 - val_loss: 1.3670 - val_acc: 0.3716\n",
      "Epoch 187/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1473 - acc: 0.4340 - val_loss: 1.3058 - val_acc: 0.3505\n",
      "Epoch 188/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1259 - acc: 0.4223 - val_loss: 1.2920 - val_acc: 0.3505\n",
      "Epoch 189/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2454 - acc: 0.4075 - val_loss: 1.5204 - val_acc: 0.3444\n",
      "Epoch 190/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1517 - acc: 0.4251 - val_loss: 1.3991 - val_acc: 0.3353\n",
      "Epoch 191/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0813 - acc: 0.4283 - val_loss: 1.3691 - val_acc: 0.3656\n",
      "Epoch 192/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0624 - acc: 0.4361 - val_loss: 1.2969 - val_acc: 0.3807\n",
      "Epoch 193/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1607 - acc: 0.4192 - val_loss: 1.2861 - val_acc: 0.3625\n",
      "Epoch 194/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1702 - acc: 0.4333 - val_loss: 1.4367 - val_acc: 0.3414\n",
      "Epoch 195/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0935 - acc: 0.4280 - val_loss: 1.3234 - val_acc: 0.3656\n",
      "Epoch 196/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1179 - acc: 0.4181 - val_loss: 1.2857 - val_acc: 0.3505\n",
      "Epoch 197/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1797 - acc: 0.4251 - val_loss: 1.4720 - val_acc: 0.3474\n",
      "Epoch 198/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1328 - acc: 0.4273 - val_loss: 1.3302 - val_acc: 0.3565\n",
      "Epoch 199/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0736 - acc: 0.4364 - val_loss: 1.2742 - val_acc: 0.3686\n",
      "Epoch 200/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.1560 - acc: 0.4068 - val_loss: 1.2962 - val_acc: 0.3837\n",
      "Epoch 201/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1231 - acc: 0.4333 - val_loss: 1.4603 - val_acc: 0.3353\n",
      "Epoch 202/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1159 - acc: 0.4280 - val_loss: 1.4375 - val_acc: 0.3323\n",
      "Epoch 203/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1140 - acc: 0.4318 - val_loss: 1.4264 - val_acc: 0.3233\n",
      "Epoch 204/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0968 - acc: 0.4287 - val_loss: 1.4454 - val_acc: 0.3172\n",
      "Epoch 205/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0990 - acc: 0.4280 - val_loss: 1.4516 - val_acc: 0.3384\n",
      "Epoch 206/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0722 - acc: 0.4347 - val_loss: 1.5224 - val_acc: 0.3323\n",
      "Epoch 207/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1289 - acc: 0.4061 - val_loss: 1.4357 - val_acc: 0.3746\n",
      "Epoch 208/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0884 - acc: 0.4477 - val_loss: 1.3442 - val_acc: 0.3776\n",
      "Epoch 209/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1091 - acc: 0.4290 - val_loss: 1.2857 - val_acc: 0.3776\n",
      "Epoch 210/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1557 - acc: 0.4318 - val_loss: 1.3656 - val_acc: 0.3595\n",
      "Epoch 211/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1125 - acc: 0.4118 - val_loss: 1.3325 - val_acc: 0.3444\n",
      "Epoch 212/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1690 - acc: 0.4132 - val_loss: 1.3124 - val_acc: 0.3746\n",
      "Epoch 213/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0701 - acc: 0.4382 - val_loss: 1.3118 - val_acc: 0.3897\n",
      "Epoch 214/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1339 - acc: 0.4392 - val_loss: 1.3011 - val_acc: 0.3837\n",
      "Epoch 215/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1196 - acc: 0.4378 - val_loss: 1.3820 - val_acc: 0.3807\n",
      "Epoch 216/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1043 - acc: 0.4266 - val_loss: 1.3935 - val_acc: 0.3807\n",
      "Epoch 217/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0751 - acc: 0.4311 - val_loss: 1.5582 - val_acc: 0.3112\n",
      "Epoch 218/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1574 - acc: 0.3864 - val_loss: 1.4556 - val_acc: 0.3837\n",
      "Epoch 219/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1043 - acc: 0.4466 - val_loss: 1.3895 - val_acc: 0.3837\n",
      "Epoch 220/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0888 - acc: 0.4333 - val_loss: 1.3507 - val_acc: 0.3807\n",
      "Epoch 221/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0820 - acc: 0.4470 - val_loss: 1.3129 - val_acc: 0.3807\n",
      "Epoch 222/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0910 - acc: 0.4315 - val_loss: 1.2649 - val_acc: 0.3656\n",
      "Epoch 223/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1680 - acc: 0.4343 - val_loss: 1.3479 - val_acc: 0.3686\n",
      "Epoch 224/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1202 - acc: 0.4213 - val_loss: 1.4254 - val_acc: 0.3444\n",
      "Epoch 225/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0672 - acc: 0.4414 - val_loss: 1.3688 - val_acc: 0.3716\n",
      "Epoch 226/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0477 - acc: 0.4530 - val_loss: 1.8140 - val_acc: 0.2659\n",
      "Epoch 227/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2819 - acc: 0.3512 - val_loss: 1.3651 - val_acc: 0.3897\n",
      "Epoch 228/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1211 - acc: 0.4406 - val_loss: 1.3732 - val_acc: 0.3867\n",
      "Epoch 229/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0826 - acc: 0.4547 - val_loss: 1.3529 - val_acc: 0.3837\n",
      "Epoch 230/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0505 - acc: 0.4509 - val_loss: 1.3086 - val_acc: 0.3776\n",
      "Epoch 231/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0897 - acc: 0.4477 - val_loss: 1.2629 - val_acc: 0.3837\n",
      "Epoch 232/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1660 - acc: 0.4431 - val_loss: 1.3442 - val_acc: 0.3746\n",
      "Epoch 233/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0845 - acc: 0.4357 - val_loss: 1.2948 - val_acc: 0.3716\n",
      "Epoch 234/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0892 - acc: 0.4336 - val_loss: 1.2780 - val_acc: 0.3807\n",
      "Epoch 235/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0694 - acc: 0.4406 - val_loss: 1.3199 - val_acc: 0.3776\n",
      "Epoch 236/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0610 - acc: 0.4449 - val_loss: 1.3935 - val_acc: 0.3474\n",
      "Epoch 237/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0913 - acc: 0.4343 - val_loss: 1.4530 - val_acc: 0.3323\n",
      "Epoch 238/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0996 - acc: 0.4329 - val_loss: 1.5903 - val_acc: 0.3233\n",
      "Epoch 239/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1116 - acc: 0.4065 - val_loss: 1.3445 - val_acc: 0.3776\n",
      "Epoch 240/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0402 - acc: 0.4519 - val_loss: 1.3045 - val_acc: 0.3897\n",
      "Epoch 241/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0598 - acc: 0.4329 - val_loss: 1.2538 - val_acc: 0.3686\n",
      "Epoch 242/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1617 - acc: 0.4283 - val_loss: 1.3395 - val_acc: 0.3927\n",
      "Epoch 243/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0391 - acc: 0.4621 - val_loss: 1.3051 - val_acc: 0.3837\n",
      "Epoch 244/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1019 - acc: 0.4438 - val_loss: 1.2923 - val_acc: 0.3807\n",
      "Epoch 245/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1464 - acc: 0.4414 - val_loss: 1.3767 - val_acc: 0.3595\n",
      "Epoch 246/400\n",
      "2839/2839 [==============================] - 0s 77us/step - loss: 1.0531 - acc: 0.4417 - val_loss: 1.4229 - val_acc: 0.3897\n",
      "Epoch 247/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.1326 - acc: 0.4142 - val_loss: 1.3198 - val_acc: 0.3837\n",
      "Epoch 248/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.0498 - acc: 0.4512 - val_loss: 1.2522 - val_acc: 0.3837\n",
      "Epoch 249/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0922 - acc: 0.4505 - val_loss: 1.2762 - val_acc: 0.3837\n",
      "Epoch 250/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0894 - acc: 0.4502 - val_loss: 1.2993 - val_acc: 0.3897\n",
      "Epoch 251/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0351 - acc: 0.4547 - val_loss: 1.2715 - val_acc: 0.3897\n",
      "Epoch 252/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0731 - acc: 0.4449 - val_loss: 1.2501 - val_acc: 0.3897\n",
      "Epoch 253/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0942 - acc: 0.4547 - val_loss: 1.3478 - val_acc: 0.3746\n",
      "Epoch 254/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0225 - acc: 0.4653 - val_loss: 1.4458 - val_acc: 0.3686\n",
      "Epoch 255/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0803 - acc: 0.4244 - val_loss: 1.4110 - val_acc: 0.3867\n",
      "Epoch 256/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0440 - acc: 0.4621 - val_loss: 1.3814 - val_acc: 0.3988\n",
      "Epoch 257/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0359 - acc: 0.4491 - val_loss: 1.5335 - val_acc: 0.3746\n",
      "Epoch 258/400\n",
      "2839/2839 [==============================] - 0s 84us/step - loss: 1.1114 - acc: 0.4234 - val_loss: 1.5367 - val_acc: 0.3353\n",
      "Epoch 259/400\n",
      "2839/2839 [==============================] - 0s 63us/step - loss: 1.1532 - acc: 0.4361 - val_loss: 1.3749 - val_acc: 0.3444\n",
      "Epoch 260/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0627 - acc: 0.4410 - val_loss: 1.2729 - val_acc: 0.3656\n",
      "Epoch 261/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0581 - acc: 0.4378 - val_loss: 1.2507 - val_acc: 0.3867\n",
      "Epoch 262/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0712 - acc: 0.4480 - val_loss: 1.3205 - val_acc: 0.3716\n",
      "Epoch 263/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0231 - acc: 0.4632 - val_loss: 1.5338 - val_acc: 0.3323\n",
      "Epoch 264/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1466 - acc: 0.4283 - val_loss: 1.3914 - val_acc: 0.3716\n",
      "Epoch 265/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0400 - acc: 0.4452 - val_loss: 1.3259 - val_acc: 0.3746\n",
      "Epoch 266/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0208 - acc: 0.4502 - val_loss: 1.2379 - val_acc: 0.3746\n",
      "Epoch 267/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1140 - acc: 0.4340 - val_loss: 1.2650 - val_acc: 0.3988\n",
      "Epoch 268/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0276 - acc: 0.4604 - val_loss: 1.2863 - val_acc: 0.4018\n",
      "Epoch 269/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0020 - acc: 0.4625 - val_loss: 1.2376 - val_acc: 0.4079\n",
      "Epoch 270/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0926 - acc: 0.4600 - val_loss: 1.2970 - val_acc: 0.4079\n",
      "Epoch 271/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1366 - acc: 0.4561 - val_loss: 1.3355 - val_acc: 0.3746\n",
      "Epoch 272/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0368 - acc: 0.4523 - val_loss: 1.3526 - val_acc: 0.3927\n",
      "Epoch 273/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0307 - acc: 0.4428 - val_loss: 1.4229 - val_acc: 0.3746\n",
      "Epoch 274/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0525 - acc: 0.4368 - val_loss: 1.5152 - val_acc: 0.3716\n",
      "Epoch 275/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0894 - acc: 0.4216 - val_loss: 1.4348 - val_acc: 0.4079\n",
      "Epoch 276/400\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 1.0720 - acc: 0.435 - 0s 70us/step - loss: 1.0439 - acc: 0.4491 - val_loss: 1.3889 - val_acc: 0.3776\n",
      "Epoch 277/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0432 - acc: 0.4625 - val_loss: 1.4459 - val_acc: 0.3474\n",
      "Epoch 278/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0860 - acc: 0.4445 - val_loss: 1.4203 - val_acc: 0.3807\n",
      "Epoch 279/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0250 - acc: 0.4618 - val_loss: 1.3659 - val_acc: 0.3837\n",
      "Epoch 280/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9995 - acc: 0.4664 - val_loss: 1.3856 - val_acc: 0.3716\n",
      "Epoch 281/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0391 - acc: 0.4569 - val_loss: 1.4764 - val_acc: 0.3474\n",
      "Epoch 282/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0625 - acc: 0.4449 - val_loss: 1.3571 - val_acc: 0.3746\n",
      "Epoch 283/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0465 - acc: 0.4382 - val_loss: 1.4099 - val_acc: 0.3535\n",
      "Epoch 284/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2075 - acc: 0.4040 - val_loss: 1.1985 - val_acc: 0.4683\n",
      "Epoch 285/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1129 - acc: 0.4657 - val_loss: 1.2614 - val_acc: 0.4290\n",
      "Epoch 286/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0268 - acc: 0.4702 - val_loss: 1.2401 - val_acc: 0.4260\n",
      "Epoch 287/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0368 - acc: 0.4657 - val_loss: 1.2209 - val_acc: 0.4532\n",
      "Epoch 288/400\n",
      "2839/2839 [==============================] - ETA: 0s - loss: 1.0537 - acc: 0.487 - 0s 70us/step - loss: 1.0517 - acc: 0.4604 - val_loss: 1.2530 - val_acc: 0.4230\n",
      "Epoch 289/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0214 - acc: 0.4688 - val_loss: 1.2617 - val_acc: 0.4502\n",
      "Epoch 290/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0096 - acc: 0.4759 - val_loss: 1.2749 - val_acc: 0.4199\n",
      "Epoch 291/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0357 - acc: 0.4653 - val_loss: 1.2282 - val_acc: 0.5015\n",
      "Epoch 292/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1494 - acc: 0.4547 - val_loss: 1.2591 - val_acc: 0.3927\n",
      "Epoch 293/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0926 - acc: 0.4530 - val_loss: 1.3340 - val_acc: 0.3776\n",
      "Epoch 294/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0377 - acc: 0.4611 - val_loss: 1.3461 - val_acc: 0.3776\n",
      "Epoch 295/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0156 - acc: 0.4526 - val_loss: 1.2260 - val_acc: 0.4230\n",
      "Epoch 296/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0316 - acc: 0.4635 - val_loss: 1.2458 - val_acc: 0.4381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0127 - acc: 0.4727 - val_loss: 1.2372 - val_acc: 0.4411\n",
      "Epoch 298/400\n",
      "2839/2839 [==============================] - 0s 82us/step - loss: 1.0248 - acc: 0.4783 - val_loss: 1.2470 - val_acc: 0.4441\n",
      "Epoch 299/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 1.0148 - acc: 0.4783 - val_loss: 1.3682 - val_acc: 0.3927\n",
      "Epoch 300/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0706 - acc: 0.4579 - val_loss: 1.3652 - val_acc: 0.3746\n",
      "Epoch 301/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0169 - acc: 0.4540 - val_loss: 1.3153 - val_acc: 0.4199\n",
      "Epoch 302/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9808 - acc: 0.4836 - val_loss: 1.2939 - val_acc: 0.4411\n",
      "Epoch 303/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9713 - acc: 0.4731 - val_loss: 1.2486 - val_acc: 0.4502\n",
      "Epoch 304/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0746 - acc: 0.4336 - val_loss: 1.2024 - val_acc: 0.4743\n",
      "Epoch 305/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1425 - acc: 0.4491 - val_loss: 1.2937 - val_acc: 0.4350\n",
      "Epoch 306/400\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 1.0986 - acc: 0.4720 - val_loss: 1.2767 - val_acc: 0.4411\n",
      "Epoch 307/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0364 - acc: 0.4826 - val_loss: 1.2737 - val_acc: 0.4260\n",
      "Epoch 308/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.9824 - acc: 0.4678 - val_loss: 1.2716 - val_acc: 0.4471\n",
      "Epoch 309/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9733 - acc: 0.4741 - val_loss: 1.2397 - val_acc: 0.4441\n",
      "Epoch 310/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0192 - acc: 0.4741 - val_loss: 1.2201 - val_acc: 0.4955\n",
      "Epoch 311/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.2003 - acc: 0.4579 - val_loss: 1.4630 - val_acc: 0.3776\n",
      "Epoch 312/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1050 - acc: 0.4216 - val_loss: 1.3039 - val_acc: 0.4290\n",
      "Epoch 313/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.9893 - acc: 0.4720 - val_loss: 1.3830 - val_acc: 0.4169\n",
      "Epoch 314/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0405 - acc: 0.4533 - val_loss: 1.4618 - val_acc: 0.3988\n",
      "Epoch 315/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0372 - acc: 0.4333 - val_loss: 1.3406 - val_acc: 0.4381\n",
      "Epoch 316/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9733 - acc: 0.4762 - val_loss: 1.3102 - val_acc: 0.4230\n",
      "Epoch 317/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9595 - acc: 0.4854 - val_loss: 1.2948 - val_acc: 0.4441\n",
      "Epoch 318/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 0.9783 - acc: 0.4900 - val_loss: 1.4112 - val_acc: 0.4018\n",
      "Epoch 319/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0686 - acc: 0.4607 - val_loss: 1.4273 - val_acc: 0.3837\n",
      "Epoch 320/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0375 - acc: 0.4533 - val_loss: 1.2860 - val_acc: 0.4411\n",
      "Epoch 321/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9795 - acc: 0.4692 - val_loss: 1.2751 - val_acc: 0.4471\n",
      "Epoch 322/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9540 - acc: 0.4900 - val_loss: 1.2696 - val_acc: 0.4411\n",
      "Epoch 323/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.0278 - acc: 0.4554 - val_loss: 1.2424 - val_acc: 0.4532\n",
      "Epoch 324/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1186 - acc: 0.4357 - val_loss: 1.2143 - val_acc: 0.4622\n",
      "Epoch 325/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9697 - acc: 0.4921 - val_loss: 1.3156 - val_acc: 0.4471\n",
      "Epoch 326/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9512 - acc: 0.4963 - val_loss: 1.4289 - val_acc: 0.4199\n",
      "Epoch 327/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1146 - acc: 0.4498 - val_loss: 1.5370 - val_acc: 0.3837\n",
      "Epoch 328/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.1222 - acc: 0.4487 - val_loss: 1.3808 - val_acc: 0.3746\n",
      "Epoch 329/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0379 - acc: 0.4523 - val_loss: 1.3183 - val_acc: 0.4199\n",
      "Epoch 330/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9630 - acc: 0.4776 - val_loss: 1.2822 - val_acc: 0.4350\n",
      "Epoch 331/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9670 - acc: 0.4829 - val_loss: 1.2122 - val_acc: 0.4864\n",
      "Epoch 332/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0409 - acc: 0.4797 - val_loss: 1.2365 - val_acc: 0.4381\n",
      "Epoch 333/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0489 - acc: 0.4797 - val_loss: 1.2851 - val_acc: 0.4260\n",
      "Epoch 334/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9566 - acc: 0.4974 - val_loss: 1.2082 - val_acc: 0.4804\n",
      "Epoch 335/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0329 - acc: 0.4731 - val_loss: 1.2230 - val_acc: 0.4562\n",
      "Epoch 336/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0031 - acc: 0.4914 - val_loss: 1.2313 - val_acc: 0.4773\n",
      "Epoch 337/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9858 - acc: 0.4741 - val_loss: 1.1978 - val_acc: 0.4864\n",
      "Epoch 338/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0481 - acc: 0.4755 - val_loss: 1.2360 - val_acc: 0.4924\n",
      "Epoch 339/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 0.9819 - acc: 0.4981 - val_loss: 1.4963 - val_acc: 0.3927\n",
      "Epoch 340/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.1267 - acc: 0.4653 - val_loss: 1.4438 - val_acc: 0.3867\n",
      "Epoch 341/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0133 - acc: 0.4551 - val_loss: 1.3056 - val_acc: 0.4230\n",
      "Epoch 342/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9464 - acc: 0.4776 - val_loss: 1.3333 - val_acc: 0.4411\n",
      "Epoch 343/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9505 - acc: 0.4864 - val_loss: 1.5516 - val_acc: 0.3837\n",
      "Epoch 344/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.1528 - acc: 0.4044 - val_loss: 1.4739 - val_acc: 0.3716\n",
      "Epoch 345/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0655 - acc: 0.4526 - val_loss: 1.2978 - val_acc: 0.4320\n",
      "Epoch 346/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9989 - acc: 0.4755 - val_loss: 1.2226 - val_acc: 0.4834\n",
      "Epoch 347/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9523 - acc: 0.5033 - val_loss: 1.2703 - val_acc: 0.4743\n",
      "Epoch 348/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9424 - acc: 0.4963 - val_loss: 1.2956 - val_acc: 0.4441\n",
      "Epoch 349/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9724 - acc: 0.4840 - val_loss: 1.3623 - val_acc: 0.4199\n",
      "Epoch 350/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0498 - acc: 0.4797 - val_loss: 1.6230 - val_acc: 0.3474\n",
      "Epoch 351/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.2157 - acc: 0.3720 - val_loss: 1.3500 - val_acc: 0.4079\n",
      "Epoch 352/400\n",
      "2839/2839 [==============================] - 0s 75us/step - loss: 1.0142 - acc: 0.4762 - val_loss: 1.2566 - val_acc: 0.4381\n",
      "Epoch 353/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9760 - acc: 0.4974 - val_loss: 1.3121 - val_acc: 0.4441\n",
      "Epoch 354/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 0.9834 - acc: 0.4977 - val_loss: 1.2407 - val_acc: 0.5076\n",
      "Epoch 355/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0474 - acc: 0.5058 - val_loss: 1.2617 - val_acc: 0.4532\n",
      "Epoch 356/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9966 - acc: 0.4928 - val_loss: 1.2744 - val_acc: 0.4773\n",
      "Epoch 357/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9541 - acc: 0.5002 - val_loss: 1.3497 - val_acc: 0.4532\n",
      "Epoch 358/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9727 - acc: 0.4868 - val_loss: 1.3083 - val_acc: 0.4622\n",
      "Epoch 359/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0135 - acc: 0.4995 - val_loss: 1.2319 - val_acc: 0.4894\n",
      "Epoch 360/400\n",
      "2839/2839 [==============================] - 0s 76us/step - loss: 1.0260 - acc: 0.4886 - val_loss: 1.2281 - val_acc: 0.4532\n",
      "Epoch 361/400\n",
      "2839/2839 [==============================] - 0s 73us/step - loss: 1.0076 - acc: 0.4706 - val_loss: 1.2318 - val_acc: 0.4773\n",
      "Epoch 362/400\n",
      "2839/2839 [==============================] - 0s 65us/step - loss: 1.0854 - acc: 0.4639 - val_loss: 1.2273 - val_acc: 0.4955\n",
      "Epoch 363/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9807 - acc: 0.5076 - val_loss: 1.2824 - val_acc: 0.4683\n",
      "Epoch 364/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9326 - acc: 0.5037 - val_loss: 1.3265 - val_acc: 0.4471\n",
      "Epoch 365/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 0.9769 - acc: 0.4864 - val_loss: 1.5001 - val_acc: 0.3988\n",
      "Epoch 366/400\n",
      "2839/2839 [==============================] - 0s 64us/step - loss: 1.0642 - acc: 0.4406 - val_loss: 1.4470 - val_acc: 0.4048\n",
      "Epoch 367/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0154 - acc: 0.4850 - val_loss: 1.3616 - val_acc: 0.4199\n",
      "Epoch 368/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0156 - acc: 0.4748 - val_loss: 1.3240 - val_acc: 0.4109\n",
      "Epoch 369/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0107 - acc: 0.4632 - val_loss: 1.2094 - val_acc: 0.4985\n",
      "Epoch 370/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9770 - acc: 0.5097 - val_loss: 1.1854 - val_acc: 0.5045\n",
      "Epoch 371/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0172 - acc: 0.4868 - val_loss: 1.2425 - val_acc: 0.4683\n",
      "Epoch 372/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9543 - acc: 0.5185 - val_loss: 1.2288 - val_acc: 0.4864\n",
      "Epoch 373/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9804 - acc: 0.5041 - val_loss: 1.2081 - val_acc: 0.4864\n",
      "Epoch 374/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 1.0637 - acc: 0.5030 - val_loss: 1.3558 - val_acc: 0.3927\n",
      "Epoch 375/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9731 - acc: 0.4889 - val_loss: 1.4315 - val_acc: 0.3958\n",
      "Epoch 376/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9955 - acc: 0.4716 - val_loss: 1.4786 - val_acc: 0.3595\n",
      "Epoch 377/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0059 - acc: 0.4780 - val_loss: 1.3283 - val_acc: 0.4562\n",
      "Epoch 378/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9383 - acc: 0.5041 - val_loss: 1.3459 - val_acc: 0.4381\n",
      "Epoch 379/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9586 - acc: 0.4995 - val_loss: 1.5207 - val_acc: 0.3656\n",
      "Epoch 380/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0882 - acc: 0.4572 - val_loss: 1.3793 - val_acc: 0.3746\n",
      "Epoch 381/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0776 - acc: 0.4459 - val_loss: 1.1955 - val_acc: 0.4894\n",
      "Epoch 382/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0242 - acc: 0.4794 - val_loss: 1.2386 - val_acc: 0.5015\n",
      "Epoch 383/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 0.9356 - acc: 0.5227 - val_loss: 1.2706 - val_acc: 0.4713\n",
      "Epoch 384/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9159 - acc: 0.5224 - val_loss: 1.2225 - val_acc: 0.5166\n",
      "Epoch 385/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0132 - acc: 0.4893 - val_loss: 1.2093 - val_acc: 0.5015\n",
      "Epoch 386/400\n",
      "2839/2839 [==============================] - 0s 70us/step - loss: 1.1028 - acc: 0.4924 - val_loss: 1.3806 - val_acc: 0.3716\n",
      "Epoch 387/400\n",
      "2839/2839 [==============================] - 0s 72us/step - loss: 1.0206 - acc: 0.4734 - val_loss: 1.3533 - val_acc: 0.4109\n",
      "Epoch 388/400\n",
      "2839/2839 [==============================] - 0s 74us/step - loss: 0.9381 - acc: 0.4917 - val_loss: 1.3194 - val_acc: 0.4290\n",
      "Epoch 389/400\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 0.9327 - acc: 0.5107 - val_loss: 1.3821 - val_acc: 0.4350\n",
      "Epoch 390/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 0.9477 - acc: 0.5210 - val_loss: 1.3948 - val_acc: 0.4169\n",
      "Epoch 391/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9791 - acc: 0.4840 - val_loss: 1.4832 - val_acc: 0.3746\n",
      "Epoch 392/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 1.0197 - acc: 0.4459 - val_loss: 1.5331 - val_acc: 0.3776\n",
      "Epoch 393/400\n",
      "2839/2839 [==============================] - 0s 67us/step - loss: 1.0236 - acc: 0.4660 - val_loss: 1.3301 - val_acc: 0.4653\n",
      "Epoch 394/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9176 - acc: 0.5269 - val_loss: 1.4300 - val_acc: 0.4018\n",
      "Epoch 395/400\n",
      "2839/2839 [==============================] - 0s 71us/step - loss: 1.0452 - acc: 0.4773 - val_loss: 1.4433 - val_acc: 0.3565\n",
      "Epoch 396/400\n",
      "2839/2839 [==============================] - 0s 80us/step - loss: 1.0225 - acc: 0.4625 - val_loss: 1.3544 - val_acc: 0.4532\n",
      "Epoch 397/400\n",
      "2839/2839 [==============================] - 0s 66us/step - loss: 1.0189 - acc: 0.4741 - val_loss: 1.2644 - val_acc: 0.4713\n",
      "Epoch 398/400\n",
      "2839/2839 [==============================] - 0s 68us/step - loss: 0.9242 - acc: 0.5188 - val_loss: 1.2313 - val_acc: 0.5045\n",
      "Epoch 399/400\n",
      "2839/2839 [==============================] - 0s 69us/step - loss: 0.9663 - acc: 0.5157 - val_loss: 1.2589 - val_acc: 0.4653\n",
      "Epoch 400/400\n",
      "2839/2839 [==============================] - 0s 78us/step - loss: 1.0105 - acc: 0.5157 - val_loss: 1.2636 - val_acc: 0.4381\n"
     ]
    }
   ],
   "source": [
    "w2v_model = model.fit(x=X_train_w2v, y=y_cat_train_w2v, \n",
    "          batch_size=2000, \n",
    "          epochs=400, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_w2v, y_cat_test_w2v),\n",
    "          callbacks=None,\n",
    "          class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "0.6761904761904762\n",
      "0.9619047619047619\n",
      "43\n",
      "0.6046511627906976\n",
      "0.9302325581395349\n",
      "0.7575757575757576\n",
      "0.13333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  1,  1,  0],\n",
       "       [ 4, 35, 48, 19,  3],\n",
       "       [ 0,  4, 27,  5,  0],\n",
       "       [ 1,  6, 61, 82, 16],\n",
       "       [ 0,  3,  7,  3,  1]])"
      ]
     },
     "execution_count": 1080,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_w2v)\n",
    "model_metrics(predictions, y_cat_test_w2v)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "true_classes = y_cat_test_w2v.values.argmax(axis=1)\n",
    "matrix = confusion_matrix(predicted_classes, true_classes)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXeYXFXZwH9nZ3tPdje9AwmdEBICgkKkCKKUD0XpgorY\nPhRUwILwWT70U0RUQEREBFGkS5NQQk0CIQRSCOllk2yy2c32nd2ZnfP9ce6Ze+bOndnZMjtbzu95\n9pnZuXfuPXNn7vuetx4hpcRisVgsFoCsTA/AYrFYLIMHqxQsFovFEsUqBYvFYrFEsUrBYrFYLFGs\nUrBYLBZLFKsULBaLxRLFKgXLiEIIca8Q4qcp7rtFCHFyusdksQwmrFKwWCwWSxSrFCyWIYgQIjvT\nY7AMT6xSsAw6HLfNd4UQ7wshWoUQfxZCjBVCPCuEaBZCvCCEGGXsf6YQYrUQokEIsUgIcZCx7Ugh\nxHLnff8E8j3n+pQQYoXz3jeFEIenOMYzhBDvCiGahBDbhRA3erYf7xyvwdn+Bef1AiHEr4UQW4UQ\njUKI153XThRCVPtch5Od5zcKIR4WQtwvhGgCviCEOFoIsdg5xy4hxO+FELnG+w8RQiwUQtQLIXYL\nIb4vhBgnhGgTQlQY+80RQtQKIXJS+eyW4Y1VCpbByrnAKcBM4NPAs8D3gSrU7/a/AYQQM4EHgW85\n254B/i2EyHUE5OPA34DRwL+c4+K890jgHuArQAXwR+BJIUReCuNrBS4ByoEzgK8KIc52jjvVGe/v\nnDHNBlY47/sVcBTwEWdM3wMiKV6Ts4CHnXM+AHQB3wYqgWOBk4CvOWMoAV4AngMmAPsDL0opa4BF\nwHnGcS8G/iGlDKU4DsswxioFy2Dld1LK3VLKHcBrwFIp5btSyiDwGHCks9/ngKellAsdofYroAAl\ndI8BcoBbpZQhKeXDwNvGOa4A/iilXCql7JJS/hXocN6XFCnlIinlSillREr5PkoxneBsvgB4QUr5\noHPeOinlCiFEFnA5cJWUcodzzjellB0pXpPFUsrHnXO2SynfkVIukVKGpZRbUEpNj+FTQI2U8tdS\nyqCUsllKudTZ9lfgIgAhRAA4H6U4LRarFCyDlt3G83af/4ud5xOArXqDlDICbAcmOtt2yNiuj1uN\n51OBaxz3S4MQogGY7LwvKUKI+UKIlx23SyNwJWrGjnOMjT5vq0S5r/y2pcJ2zxhmCiGeEkLUOC6l\nn6cwBoAngIOFENNR1lijlPKtXo7JMsywSsEy1NmJEu4ACCEESiDuAHYBE53XNFOM59uBn0kpy42/\nQinlgymc9+/Ak8BkKWUZcCegz7Md2M/nPXuBYIJtrUCh8TkCKNeTibel8R3AWuAAKWUpyr1mjmGG\n38Ada+shlLVwMdZKsBhYpWAZ6jwEnCGEOMkJlF6DcgG9CSwGwsB/CyFyhBD/BRxtvPdPwJXOrF8I\nIYqcAHJJCuctAeqllEEhxNEol5HmAeBkIcR5QohsIUSFEGK2Y8XcA9wihJgghAgIIY51YhjrgHzn\n/DnAD4HuYhslQBPQIoQ4EPiqse0pYLwQ4ltCiDwhRIkQYr6x/T7gC8CZWKVgMbBKwTKkkVJ+iJrx\n/g41E/808GkpZaeUshP4L5Twq0fFHx413rsM+DLwe2AfsMHZNxW+BvyPEKIZuAGlnPRxtwGfRCmo\nelSQ+Qhn83eAlajYRj3wCyBLStnoHPNulJXTCsRkI/nwHZQyakYpuH8aY2hGuYY+DdQA64EFxvY3\nUAHu5VJK06VmGeEIu8iOxTIyEUK8BPxdSnl3psdiGTxYpWCxjECEEPOAhaiYSHOmx2MZPFj3kcUy\nwhBC/BVVw/AtqxAsXqylYLFYLJYo1lKwWCwWS5Qh11SrsrJSTps2LdPDsFgsliHFO++8s1dK6a19\niWPIKYVp06axbNmyTA/DYrFYhhRCiJRSj637yGKxWCxRrFKwWCwWSxSrFCwWi8USZcjFFPwIhUJU\nV1cTDAYzPZS0k5+fz6RJk8jJseuhWCyW/mdYKIXq6mpKSkqYNm0asQ0xhxdSSurq6qiurmb69OmZ\nHo7FYhmGDAv3UTAYpKKiYlgrBAAhBBUVFSPCIrJYLJlhWCgFYNgrBM1I+ZwWiyUzDBul0B3BUBc1\njUFCXakuh2uxWCwjjxGjFDpCXexpDhKO9H+vp4aGBm6//fYev++Tn/wkDQ0N/T4ei8Vi6S0jRilE\n3S5paACYSCmEw+Gk73vmmWcoLy/v9/FYLBZLbxkW2UepoHVCGgwFrrvuOjZu3Mjs2bPJyckhPz+f\nUaNGsXbtWtatW8fZZ5/N9u3bCQaDXHXVVVxxxRWA27KjpaWF008/neOPP54333yTiRMn8sQTT1BQ\nUND/g7VYLJYkDDulcNO/V7NmZ1Pc610RSTDURX5ugEAPg7UHTyjlx58+JOH2m2++mVWrVrFixQoW\nLVrEGWecwapVq6Jpo/fccw+jR4+mvb2defPmce6551JRURFzjPXr1/Pggw/ypz/9ifPOO49HHnmE\niy66qEfjtFgslr4y7JRCIqJ6QAJpTuA5+uijY+oIbrvtNh577DEAtm/fzvr16+OUwvTp05k9ezYA\nRx11FFu2bEnvIC0Wi8WHYacUEs3o2zrDbNjTwrSKIkoL0lsNXFRUFH2+aNEiXnjhBRYvXkxhYSEn\nnniib51BXl5e9HkgEKC9vT2tY7RYLBY/Rk6g2XlMxzpzJSUlNDf7r2rY2NjIqFGjKCwsZO3atSxZ\nsiQNI7BYLJb+YdhZConQ2UfpWH60oqKC4447jkMPPZSCggLGjh0b3Xbaaadx5513ctBBBzFr1iyO\nOeaYfj+/xWKx9BdpW6NZCDEZuA8Yi5qg3yWl/K1nnwuBa1ET+Wbgq1LK95Idd+7cudK7yM4HH3zA\nQQcdlHQ8HaEuPtzdzOTRhYwqzO3pxxlUpPJ5LRaLxUQI8Y6Ucm53+6XTUggD10gplwshSoB3hBAL\npZRrjH02AydIKfcJIU4H7gLmp2MwaSxTsFgslmFD2pSClHIXsMt53iyE+ACYCKwx9nnTeMsSYFK6\nxpNO95HFYrEMFwYk0CyEmAYcCSxNstsXgWfTNgbn0aoEi8ViSUzaA81CiGLgEeBbUsr4qjK1zwKU\nUjg+wfYrgCsApkyZ0ttxANZ9ZLFYLMlIq6UghMhBKYQHpJSPJtjncOBu4CwpZZ3fPlLKu6SUc6WU\nc6uqqno3FvdYvXq/xWKxjATSphSEmpr/GfhASnlLgn2mAI8CF0sp16VrLOpc6tGqBIvFYklMOi2F\n44CLgY8LIVY4f58UQlwphLjS2ecGoAK43dm+LOHR+ogQAoFIi6XQ29bZALfeeittbW39PCKLxWLp\nHWlTClLK16WUQkp5uJRytvP3jJTyTinlnc4+X5JSjjK2d5tD2xeESI+lYJWCxWIZLoyYimZwlEKa\nW2efcsopjBkzhoceeoiOjg7OOeccbrrpJlpbWznvvPOorq6mq6uLH/3oR+zevZudO3eyYMECKisr\nefnll/t/cBaLxdIDhp9SePY6qFnpu2laZ5jsLAHZgZ4dc9xhcPrNCTebrbOff/55Hn74Yd566y2k\nlJx55pm8+uqr1NbWMmHCBJ5++mlA9UQqKyvjlltu4eWXX6aysrJnY7JYLJY0MGIa4g0Uzz//PM8/\n/zxHHnkkc+bMYe3ataxfv57DDjuMhQsXcu211/Laa69RVlaW6aFaLBZLHMPPUkgyo99e00RRbjaT\nRxem7fRSSq6//nq+8pWvxG1bvnw5zzzzDD/84Q856aSTuOGGG9I2DovFYukNI8pSSFf2kdk6+xOf\n+AT33HMPLS0tAOzYsYM9e/awc+dOCgsLueiii/jud7/L8uXL495rsVgsmWb4WQpJSFf2kdk6+/TT\nT+eCCy7g2GOPBaC4uJj777+fDRs28N3vfpesrCxycnK44447ALjiiis47bTTmDBhgg00WyyWjJO2\n1tnporetswHW724mJ5DFtMqibvcdzNjW2RaLpaek2jp7ZLmPhCAyxJSgxWKxDCQjRylISRYRbKML\ni8ViScywUQrdusHa9zEjvJHsSGhgBpQmhpq7z2KxDC2GhVLIz8+nrq4uucDMUgVrgsgAjar/kVJS\nV1dHfn5+podisViGKcMi+2jSpElUV1dTW1ubeKdwEFr2sE+EaKlPst8gJz8/n0mT0rZAncViGeEM\nC6WQk5PD9OnTk++0Yzk8ch4/LPgBP732ewMzMIvFYhliDAv3UUrklQCQ09We4YFYLBbL4GXkKIVc\nVZuQH7Ftqi0WiyURI04p5EaspWCxWCyJGEFKoVg9WKVgsVgsCRk5SiErQCgrj5yuNpvrb7FYLAkY\nOUoBCAUKKZTtdHYN3VoFi8ViSScjSil0ZRdSKDpo6+jK9FAsFotlUJI2pSCEmCyEeFkIsUYIsVoI\ncZXPPkIIcZsQYoMQ4n0hxJx0jQegK7uIIoK0dobTeRqLxWIZsqSzeC0MXCOlXC6EKAHeEUIslFKu\nMfY5HTjA+ZsP3OE8poVIThGFBGnrtJaCxWKx+JE2S0FKuUtKudx53gx8AEz07HYWcJ9ULAHKhRDj\n0zUmcosoEkFaO6ylYLFYLH4MSExBCDENOBJY6tk0Edhu/F9NvOLoP3KLKbKWgsVisSQk7UpBCFEM\nPAJ8S0rZ1MtjXCGEWCaEWJa06V13x8krtpaCxWKxJCGtSkEIkYNSCA9IKR/12WUHMNn4f5LzWgxS\nyruklHOllHOrqqp6PZ6s/FJKaaXNKgWLxWLxJZ3ZRwL4M/CBlPKWBLs9CVziZCEdAzRKKXelbUxV\nB1Aq2ok070zXKSwWi2VIk87so+OAi4GVQogVzmvfB6YASCnvBJ4BPglsANqAy9I4HrInHA5AYd0H\nwNHpPJXFYrEMSdKmFKSUrwOim30k8PV0jcFL3kSlFEoaPxioU1osFsuQYkRVNAcKStkqxzK6eV2m\nh2KxWCyDkhGlFAD2igryOuoyPQyLxWIZlIw4pdAaKCE/3KvMWIvFYhn2jDil0B4opcAqBYvFYvFl\n5CmF7FKKuqxSsFgsFj9GnFLozCkll04I2RXYLBaLxcsIVApl6kn7vswOxGKxWAYhI04pdOWVqydW\nKVgsFkscI08p5FulYLFYLIkYcUqB/FHq0SqF4Unzbvj756C9IdMjsViGJCNPKRSMBqCrtT7DA7Gk\nhV0rYN1zUPthpkdisQxJRpxSyCquACDUmLZmrJZMEnEWUJJ2ISWLpTeMOKWQW1DClshY2LUy00Ox\npAMZUY8RqxQslt4w4pRCUV6AlXI6gZp3Mz0USzqQ1lKwWPrCiFMKhbnZvBfZj5yWHdDS+6U9LYOU\nqPsoktlxWCxDlBGnFIpyA3wgp6h/atdmdjCW/ifqPrJKwWLpDSNOKRTmZVMjVQYSzTbYPOywgWaL\npU+MOKVQlBtwlUKTXat52KGVgQ00Wyy9YsQphcK8bFopIBQotJZCOggFQcrMnd9aChZLnxhxSqEo\nNwBAa94YqxT6m7Z6+NlYeP03mRuDtRQslj6RNqUghLhHCLFHCLEqwfYyIcS/hRDvCSFWCyEuS9dY\nTIrzsgFoyqmEJqsU+pWW3erx/X9mbgzDxVIIBaGzLdOjsIxA0mkp3AuclmT714E1UsojgBOBXwsh\nctM4HgCyA1mMKsyhPqvCWgr9TSbdRtExDJPso2e+Aw9dkulRWEYgaVMKUspXgWQNhiRQIoQQQLGz\nbzhd4zEZXZTLTqpUoLlu40CccoQhMnfq4WIpNO+C5ppMj8IyAslkTOH3wEHATmAlcJWUA1NxVFGc\nxxM5p0NeMbxw40CccoQwmCyFIa4UIl1DX7FZhiSZVAqfAFYAE4DZwO+FEKV+OwohrhBCLBNCLKut\n7XsVcmVxLhvai2H/k6Hm/T4fz+JBZNBSGC5tLmTX0FdsliFJJpXCZcCjUrEB2Awc6LejlPIuKeVc\nKeXcqqqqPp94dFEu9a2dMHoGNGyDcGefj2lhcMQUIsMk+ygSGfqKzTIkyaRS2AacBCCEGAvMAjYN\nxIkrivLY1xaiq3y6cjc0bh+I044AtFKwlkKfsZaCJUNkp+vAQogHUVlFlUKIauDHQA6AlPJO4CfA\nvUKIlSgpcq2Ucm+6xmNSWaySnJoKJjEKoH4TVOw3EKce3gyGJnSRYZJ9ZGMKlgyRNqUgpTy/m+07\ngVPTdf5kjC7KA2Bv3kRHKWzOxDCGHxEneczGFPqO7Br6is0yJBlxFc0AFY6lsKerDLLzoWFrhkc0\nTIgKsUGQkjrUXS/WUrBkiJGpFIqUUqhrC0HJeFvE1l9ELYUMjmFYWQpD/DNYhiQjUykUK/dRXUuH\nUgq23UX/MBgE8XBZZMdmH1kyxIhUCuUFOWQJqGvphFJrKfQbkQEpSE/OcGmIZy0FS4ZISSkIIR4V\nQpwhhBgWSiQrSzC6KJe61k7XfTQYcuyHOlEhlslAs/M9DnlLoWvofwbLkCRVIX87cAGwXghxsxBi\nVhrHNCBUFOW57qNwEIINmR7S0GcwzGyHS6DZWgqWDJGSUpBSviClvBCYA2wBXhBCvCmEuEwIkZPO\nAaaLimKnqrl0vHrBxhX6jnbd2JTUvmOzjywZImV3kBCiAvgC8CXgXeC3KCWxMC0jSzNR91HpRPVC\nY3VmBzQcGAwz22FjKUSG/mewDElSjSk8BrwGFAKfllKeKaX8p5Tym6i210OOyuI89rZ0QOVM9ULt\nB5kd0HAgGmi2lkKfsZaCJUOkWtF8m5TyZb8NUsq5/TieAaOqJI/mYJi27FIKS8bD7tWZHtLQZzAI\nsWFjKTiBZikz646zjDhSdR8dLIQo1/8IIUYJIb6WpjENCJNGFQCwY187jD0Edq/J8IiGAZHBEFOI\nxD4OVYZLvYVlyJGqUviylDKaniOl3Ad8OT1DGhi0Uqje1w5jDoa9H0JXKMOjGuIMhtn5cLIUYOh/\nDsuQI1WlEHCWzQRACBEA0r6ecjqZNKoQgOoGx1Lo6rRLc/YVG1PoP3QfqaH+OSxDjlSVwnPAP4UQ\nJwkhTgIedF4bslQV55EbyKJ6X5tSCgC7V2V2UEOdwSDArKVgsfSJVAPN1wJfAb7q/L8QuDstIxog\nsrIE48vzVUyh8hAQAdhj4wp9YlC0zh4mM+zIMLF4LEOOlJSClDIC3OH8DRsOGFPMu9saiGTlklV5\ngA0295XB0P8/OsMeBGPpC9ZSsGSIVOsUDhBCPCyEWCOE2KT/0j24dPPpIyawo6GdpZvrnQwk6z7q\nE4MhpjBcfPE2+8iSIVKNKfwFZSWEgQXAfcD96RrUQHHqwePICQheWVcL4w5XazW31Wd6WEOXwSCI\nh8sMe7h8DsuQI1WlUCClfBEQUsqtUsobgTPSN6yBoSA3wITyAnY0tMP4w9WLNSszO6ihzGCIKfTF\nF98VhhvL4LVf9++Yeorp+hoMitYyokhVKXQ4bbPXCyG+IYQ4hyHa3sLLhLICdja0K0sBYMeyzA5o\nKDMYZrWyD26XcLt6fDXDSsFUBIPhmo401j4NHS2ZHkXGSFUpXIXqe/TfwFHARcClyd4ghLhHCLFH\nCJHQUS+EOFEIsUIIsVoI8Uqqg+5PJpQ7SqGoEibNg5f/F3a8k4mhDH0SCbB374d3/jqwY+iNMB0s\na2qYY7eWwsCyew384wJ46tuZHknG6FYpOIVqn5NStkgpq6WUl0kpz5VSLunmrfcCpyU5bjlqnYYz\npZSHAJ/twbj7jYnl+exuChLqisAFD0F2Prz950wMZeiTSIA98XX4938P0Bj60OYiGijPsHKwlkLm\n6HQshH2bMzuODNKtUpBSdgHH9/TAUspXgWRR2wuAR6WU25z99/T0HP3BxFEFRCTUNAahcDQccjas\nfhxC7ZkYztBGC9VMCrK+WArReESGlUKMpWCzjwaUTH/3g4BU3UfvCiGeFEJcLIT4L/3Xx3PPBEYJ\nIRYJId4RQlzSx+P1ignlqgfSzgZHCex/MoRaofbDTAxnaDMYCq760uYiontfWUvBMnJJtaI5H6gD\nPm68JoFH+3juo4CTgAJgsRBiiZRynXdHIcQVwBUAU6ZM6cMp44kqhUZHKVQdqB73roMJs/v1XMOe\nqKXQz7Pb5t1KUJZOSGEMfbEUwt3vMxCkmn20/W3Y8ip89Jr0j2nEMXLblada0XxZGs5dDdRJKVuB\nViHEq8ARQJxSkFLeBdwFMHfu3H6dxk0sN1poA4yeoVpe1K5V/3eFVLO83KL+PO3wJF0tJn7tLIR0\nY2N6xzBYlEKqlsKfT1aPVilY+pGUlIIQ4i/42NRSysv7cO4ngN8LIbJRHVfnA7/pw/F6RX5OgIqi\nXHY0BNUL2blKMWj30f3nwuZXUhNII53B0IyuL0VfgzKmYN1HA4q93inHFJ4Cnnb+XgRKgaSJvEKI\nB4HFwCwhRLUQ4otCiCuFEFcCSCk/QHVafR94C7hbSpmRPhPRtFTN2ENg0yuw/S2lEGDo99IZCPRM\nuy831urH+1ZVHkmQfRTuhIU3JD/2oMw+SuF3Z+MO/YddUyVl99Ej5v+OwH+9m/ecn8Jx/w/4v1TG\nkE4mlOezqbbVfeGUm2DrG/DGb93Xgg0qO2moEGyCmyfDqT+Dj3xjYM7Z19YMzTXwr0th6vFw2dP9\nO4Yd76jvc8KRcMg5/u8dLO6jnloKXSHICqRvPCOJiFUKqVoKXg4AxvTnQDLJxPJCdjS0I7XbYNQ0\nmHIMrH/e3allD+xdD6se8T3GoKO1Vj2+/aeBO6efpdATC0sLw7oN/tvb6pUlkcoxvMK0aYd6DHck\nfq+eJQ6U++jp76i2Gl56mn1kBVnv8X7X+jcwgtfFTrVLarMQokn/Af9GrbEwLJg8uoC2zi5qWwyB\nMeZgFWDWtO6B38+Fhy/PvM85FfQYB2KsUiphqxWAqQi6kgjhuOPo9ycQcg9doiyJ5t1JjpHAUmjc\nrh7DQWhvgJ9PUi5Ck4F2wyRS2D3tfWRdHr3j9d/ATeXQaXgJ7LVMTSlIKUuklKXG30yvS2koM2ts\nCQDraowwyZiDYndqMWrrggMUdF6/UM0k63tRXRlq6//xJOKdv8BPx0DDNvW/KcjMmblWUKF21XzO\ni1bCftsA9nwQu58fiSyFRsNS2L0KOpth0c2e92bIfeS1XsxxpKKorCDrHcv+oh5bjElGdEJiLYWk\nCCHOEUKUGf+XCyHOTt+wBpZZ45RSWFvT5L449jD1eMCp6tF0abTVDczA3ntQPfamF1O0InsALIX3\n/qke6501riMJlIIWXj8bBw9dHH8cvd20FEwFEXYyxJJVmydaZEe7j0LtKuUY4pVApgLN3uZrsqcx\nhSRKMhFv/BbuPB5a96r/m3fDmid6fpzBxL+vgjs/mvr+Os08xlIYBHGlLa9ntCFfqjGFH0spo9Nj\nKWUD8OP0DGngqSjOo7I4jw9rmt0XK/eHy5+Hzz2ghMjap9xt+kZKN8L5ehK1Olj6R3jpZ6rZ3I1l\n0L7P3aYthYGQb17hGmMpBI39DGH/4TPxx4laCsZ+psWj+9Iks4ISLbLTWO2Mp8MdRyKlMNDuwY6m\n2P8jPc0+6qGl0NGsMrFqVqo4GcAD5yr33FDuDvrOvVDzfur7a6VgWv6Zjs80bIN7z4Cnr87YEFJV\nCn77pVoNPSSYNa6YdXs8N8SU+apuoagqdp0FHcSVEm6bA2+lKZjbnVJY+xSsedw9f8N2d9tAWgrR\n2blP76MYS6GbGa1pKUgJz/8Qtr4Zv18ypZAophANNAfdmWGcUtDv8blmqx+HPWuTDj+Ohy5Nbfbd\n2VdLoYeCzJwZa6W9z3H99cbqGKrkFKrH9gb3tXS64pb9xVXCidD38L4t6RtHN6SqFJYJIW4RQuzn\n/N0CDKv+0pPKC9nVkMAtsb9TOTpjgXpscyyFYINymex8Nz2D0koh0Y0abFJ/OlMi4jPDHohZrxau\nnfqcCQLNXaHk5rn5OcMd8Obv4MHPxe+XVCn4WAqhdtflF+4wlIJH4CaLKfzrUrh9fuLtXiJdSmE/\nlEJLL+/s3LQO0hFTMN1vWmlnZcVvizlHGP71BdjVg5n4YCdqKRhKIV1xpUgEnvoW3LUg+X76d1pY\nkZ5xpECqSuGbQCfwT+AfQBD4eroGlQnGleVT29KhWmh7+fSt8MlfwZm3qf+1+6hpl/O4o2cnq9+c\nWhdWrRQSmfTBRvWn9ws2wvL71MwnHZbC7jXw0k/jFY0WXCEfYeu1FLzZSItvhw0vuNuj+ybJWur0\nKIVwJ3zw79hzm4qpaaexb9Cdme9ZDTdPNT5HP84Szdlnd/TGUjC/g57O7k2Xnn6e5Rj+iX6Xe9fB\n6sfgkS/17FyDmailYLhd02Up6eN2Niffr9VJaBnsSkFK2SqlvE5KOVdKOU9K+X2nZ9GwYXxZPlLC\nnmYfYRTIgaO/DOVTILdYaXMpodkRNqbQ8SPY6M5Ou0Jw22x49IrUB9fRrMzKcKe6KR/5svN6k1ot\nTAuOTYvgyW8qf6S+uZt2wP+k+APbthTqNyXe/rez4dX/i51Z6c9kkiim0NUZn2nzn+tVKxHvcZLV\nE3gF10v/A/+8SAXo/NxHOp6gj2u6T/xmif1hXbWnUJWtBXGHR1DExBQSKAXzWvV0duunFHTwPZxA\nKfRlnYqBJtXU4uw89dgX99HWN1NzEZrXPBl6opnvU78yQKSafbTQWRRH/z9KCPGf9A1r4BlXlg9A\nTWM3M/iCUbDkdvi//dwfg/4iu0L+gcGbp8AfHNeDjkesTaFiV/+QmnfBrYcqYb/yX7DyISW4gk6A\nUgs5rZyaa2JdLJGwUihe3vsHvHaL+/89p8JtRyYejxY+ce6ORL55PEohFCvsvcIwxn2U5CYKeeYj\n2tff0eLf5kJbclnZjqXgsTQaq5WijY7HoxR6oyRSadWhlUJvLAXz++3p7DbkZyk4SiGRpaCvzVDo\nDZRsQrHoZpWUEYm4CiBmYtDD4rW/nJ6aizDZmEyaHVmSwdYlqbqPKp2MIwCklPsYRhXNAOPLVLfU\nXY3daPSqWeqxrU65akCZhG318Iej4aWfOK+1wl0nqtk7uMVTOidaJLj0nW3w4bPuMcBN9VzzpLtf\nOOj+gLUZ/ZAOAAAgAElEQVSQbq5xNor4m9svjfaxr8CLN8W//u4D8M+L44VhIFc9eus0vIIiUZ2C\n133U6HG7xSiFJILO+9lMF4ivpeCcp3xqrPtI8+Q3laLVLigvvQk+pmQp5KjHpJZCgpm5eQ16Or6w\nT0xBpKoUBqmlYP5Wk00oFv9BPbbVub+3GPdRL63F7vZPtYhTT+x6UvTZz6SqFCJCiOhCBkKIaWS8\na1j/4loK3SiFzz8I126Fs++MfX3F35XrZekfVabS7tUqAH3fWe4+wSa3CC5Rr5rnfwgPfl69VyuF\nOkcpmKmLppDXMx1d5CZEfDDW3L8rDMvuMbbVx86en/gafPCkWyym0ULMqxS8sxoZcW8Sr1Iw//fG\nYmLcR0m+h7iYgq5faPMvXmvcDoWVyiQ3s4+i251xaKXnpTd+5lQshYB2H/XCUjAFe0/dR6aloJVA\nd4Fm/dvzKoXOtvjvIxOYv6tk8Trdv6ylxv29tftYCj29pqZiMekKqcljqis56oldBrPAUlUKPwBe\nF0L8TQhxP/AKcH36hjXwlOZnU5KXzbrd3QSCsnOhoDy+qdrCH6nHUKsqCvJbqH7PmlhLQUqoWRUr\nFLRPv7HandFGhaehh80qZ/0DanJ858LHUmjdA2ufUQrh7btjFyav2+A/s/3gydj/A4mUgs8NtHuV\nUmbe4jU/peCXZZXsJvIqPK0UOlv8LYXmGigdDzkF8TEF6D4mlOwG3bQIdq6Ifz16PZO4IfQYvcHH\nVLKPYiyFngaak1gKiWIKUaXgmQv+cgb8YlrPzp8OTOsv2YRCB3Cba9zZeNAnptBTpdCSoPXKa7co\nS1QXonaHHksySznNpBpofg6YC3wIPAhcAwyrRYyFEJxy8FgeX7GT3U0pBIVy8uG8v8F598GCH8CM\nE+H8f8LHHeWw4v7499SsjFUKT30b7jwOXjf8+tnKYmHflnjhZVKXJN85EokXqu/8Ff5xPjx3LWx+\nNXbb3nX+M9sdy2P/T+Q+8hNKdx4Pv5sTH1Mw99Uz9Nzi+ON4g9kmXqWgZ76drf6WQrABCkarwKKf\n+0gLZW8RmTnuRNx3Ftx1Qvzr5vVM5ALS16Y3lkJf3Ee9iSkEE1gK4fb0uTra6pPfAyYpK4VK9di8\ny8gIMuMzvVQKOhbgZa+zLkuqloK+zhl0H6W6yM6XgKuAScAK4BjUWgkfT/a+ocb586fw6Ls7OP4X\nL/H2D06mvDCBO0Fz8Jnxr806TbkpnvlO/LbqZZBfqp53tsK2xep5rfHD0YKpbkPyG2Jvgk6ioISg\nV3Dqiuy371YC0mTPB1A6Mfa1wsp4IRm1FDwCO5n7IKaFQGdsAM8sKINY4ZbI/RLIjb8uerYbYykY\nwivYCJVj1Y3eWqveP2o6jD9C1RJovL59c9yp8Not6vwf+65hKUg1Lv29ayJdhlDyBu5TyD4K+Qiy\nVOlNSqr+LaQaMO0PfjldZfzNv1J1J/7yS4n37eippbDbvW5+VfepXtNArvoeEzVp1G7brBREbVfY\nTaLIYD+rVN1HVwHzgK1SygXAkUAPErGHBvOmjebGTx9MqEuyZFMfFnqZdXr8axPnQvXbhpkp3SU/\nG7Ypwfq/k9U6DqBcL8laDiSzFNr3xd/c5szH6ypa/ZibFTX3i3DhIzB5fryQ9HMfdYWSz2p2vRe7\nrylUtB9WxxpM4ZsoUFs6Mf6zaaFqjjfGUmhUQjk7z3UflU9R62aYJLreqdYvvHiTquOA2BiOnwVi\nXoegZ3tPLYWe1ldoIRjI9UlJTSBQ9bUNNg5sG5CGbapgbvfq5PuZE4VQEqWgJyWmpeDXnyvVa5pf\n7h7PD/07SKWJpvk7GUjl6yFVpRCUUgYBhBB5Usq1wKz0DStzXDB/Kvk5WSzZ1Iemd2WTYPaF8Klb\n3ddmna6yiPxymhu2qTbK5g9x92ro8Pkh5Tn5y3vjlrJ2aW+IT9sE2M/HsJt9kZqx63UiTrwODjgZ\n8krihZm+iRb9r+rgColn1xrTVdXVGSv4zeN3NHuUghG4K5ngPs8t9qTbdrn7msLVnGEHm9TNm53v\nBppziyDPM3tPaCmYAfAUrIaVD6sMMu0K9Ap9iBW+3iBlKtlHMbn1KVoyjTvgiW+4780v96loTmD1\n6c8QCcGbt6l05oFSDsEGJ9suSZpmjPsoiatGf5fNNUavLUMA68lTqo3xtNstUUyhTf82U1AKMZOt\nTjUx3Lc1tXH0I6kqhWqnTuFxYKEQ4glg4Ec7AORmZzF36ui+KQWAs2+HuZcp07ewEg48A8qmqNiD\nqSwmHa1+9G/c5r6WU+TOlOd4cqA/ew9U7B/fG6V4rPs81OpfUbvgB/Gvaaum+m31WDBKPeaVKCEp\npXsjmbPTBz6jHrtTCi017nNv9lGrZzZtCt9m430nXuc+zy2MFVztDUQD8KaS0TPsrpC6HvllhqXQ\n0kOlYAbAHWW78eXErpZHr1DX75w/xo9LY16HNk+DxVQshVajlXuqroZF/wvv/s0JegplPenPEG1r\n3o2lAKqZ3mNfSb0gq6/o33Iyd2qMUkgyy9YTr7a9hvuoD5aCvgaJso+ilkIKjhXzdxJqVzG5O49P\nbRz9SKqB5nOklA1SyhuBHwF/BoZN62wvx8wYzdqaZupb+yED4PRfwPc2qvUZvr0SLnkCZl/gbp/m\nfOlte+GU/4EJc+DCf7nbxx0OX1sKV70PP9qr+jAd9y217bDPwtTj1PN5nvYDjdUw6wy4Zh185JtK\nOU08yt2ug7uVB6hU07Y6ZYVoF1F+qZodvnEr/KQKXrgptn249s3qeIgfk+apx6O/oh697qPWWjd4\n7bUUdF3Hla/D4ee5r+cUxMYwzBtJz2Zzi9WxQkH3tbxSyC5Qr7XVKeUX8Ph5TesqZqEgY1ydrVC7\nTlV3P+2JG2nFLLuUVVY2KXZcJlqY5JXG15Ckkn2k3X2QulLQVbKN29V1zC5wvw8/xW/ip9hMxZ1O\nH7gWqMn6ecW4j1KwFEJt7mfvS0xBW45+lkAk4lotKVkKzjXOLoAdy9TzRMkPaaTHy3FKKV+RUj4p\npRy27RSP3U8JvLc2p2ndhOw8d0aum+0BzLkUrngZph0Hp90M87+qUl/HHAijproCe/aFcPFjcNbt\nyuoomQAzT4Nrt8CFD6t92uthwmwoGQun/lQpJzPIO/lo9VhY4QqzIqMdRl6JEm4v3AjI2AwpUJ1j\nAXYuBwSUOgLQDFgv+L7KztJrRHuL11proWS8eu5VCrpbZMl4COS5r+cUxQoAc4ao21mMO1w97tvs\nuuC0pdDRqG600fuRlJg1HYznnW3ujbp7pec9hsthyrGuJZLMUiidoARGtLq2ER41FHwiS6Gl1v0N\neWe1wSa3aNLEbJ2QnedmY4HhX0+iFPT5NFuMZdpTzRLqDV5L4Y6PwD8u9IwvRUvBVH5mplGkK3Z7\nTy0FP6FvWgcxtRAJvlP9O9H3FqilgQeY3q7R3C1CiHuEEHuEEKu62W+eECIshPhMusbSUw6bWE5B\nTqBvwebu+MqrcObvlQL48ktwfbWqf9Ac81U4/WYoqox/b1aWmolm50LVTLjmAxh/uLpp9z/Z/SF5\nrQeAE65VlsNn/gIX/Esdv8RRCqagzCtJPn5dhLfjHag60BX2ZZPdfYrGwMFnub71uN5HUglFcJRC\nSM3yRcC1FLLzXH93bokaa8M2+M8P1EzVFEZaSB9winqs2+DerPll7jhArZcByto68qLYGxFiFYHX\nfaTrKryullAQjvqCSkuefYGbceTnOtDCVytR7X5Y8ffY/RJaCnvc93pjCv/6gkqV9WZwmdcqu8CJ\nsaRoKQQb1RK1Jlte8z92f2NaCq11qt7HXN8EUo8paGEfCsZ35QX3OqQSU+gKu0rbzxo0XUrmb6C7\ntF/znk81lbUfSZtSAO4FTku2gxAiAPwCeD6N4+gxudlZzJ02qu9xhWSUT4E5zupjE4/qXginihDw\npZeUy0lXb5os+L6yHArKYaazqpy2FCoPcPcz/e0XPw7jZ6vnJ14PC36oLJFwhyrcmnCkK4DKDaWg\nZ5bawnnmO24arkYrhWCTukmz85WAjmbIOFbC+f+Ar72pUkk7m2Hx72Hhj11hYM5io6vlbTSUQqnb\nAA1UXAbg3LvhrD8od4qJOeuPcR+1udvMG1ZKNebCCvjYd2JjFr7uI61EHcGuO+96UxcTxhRq3Wvn\nFWB6Bu9NdTXjAjn56k8LUK3UEwmhtn3xS9TuWeM+X/z7/i248uuf1dnmvzgTxH7WZNlHpvvIrytv\npAeWgul28rMUTOvA3J4ww8tHKfj9dtJM2pSClPJVoLup9jeBR4A93ew34Bwzo6L/4goDTVGFcjml\nii5I0oISYpXC2ENcpZVTACXj1PO969SMtfIAV3iZloJWSmb7CN3XSVN1oHIJbX1d3aSBXCjWbbWE\nq1Bmna4U6ejp7nsjYXeGqhVbIFcJr6IxjqXg3FReS8EcJ7htlM1ja7wrwWnBaQa8Q063WvMcOQVK\nyPu6jxzBoGf70Xx2T/uTRNlHLbXqM2Zlx1sKWsB5U2xNpSCyPJaCdh/5CKyukHK7ea0pc8GYJbfD\nUk/rl77gl1EVanXTuM3fKqjfQbaj2JMFwE2Fri1T8LEUUlAKeowikMB9lCD4nDDDyzlGoaEUwu2p\njaUfSaelkBQhxETgHOCOTI0hGcfMSHNcYTChhUe5sbaAabkUVblKIqdQtYwAd1W0CsPtZFoKevZt\nKgWvO6SoCg45G1Y9poRnIMcV8Nn58d0qTR+r7HLHrt9TOlEJ1qpZanw6iyu/TAW+swuUv98rfL2W\nQiL3UUez/1rR2lVgKgUh1HVLZino2X40SyVJzUL0NaksheIqlSSQaFYbV7lt/L9vqyemYMygvejP\nZvb4L5sSL3z7c4EaP6XQ2eaOz2sNdLQ4kxCRXClEM46cSmz9O9fvMdfp7q5TqX5P8RhlvXr3T7Sm\nRiJLJtio7i+9+E/09YG1FjKmFIBbgWul7L7tohDiCiHEMiHEstra2u527xcOnVhKdpZg5Y4UsgaG\nOgd9Sj1WGaUnplIQAvKcGVUgV7lwAN5/SD2asQg98zKDmqZLxFvolleirIDOZlWkFMg1lIJPRbmp\nFBq2u5bC9I+pR51hNfdyVRfy7gPOeUph8jz4/g74go8LItvrPkoQaK7f5G8paJ+xqRRAXQffzB2n\n2Knc6TOZKHXRTzAFG9X4iqqcitoESsGbYtvR7M5CIyE3GwtcIeznPtKuQdNFN+7Q+P383JW9xc+n\nH2pzv2+v8upsUb89XYuS8LiGsomEDUvBZ33w7mboplKA+O9ZK9O8stjXE1kKe9e536mJX71SGsmk\nUpgL/EMIsQX4DHC7EMI3zVVKeZezwM/cqqoqv136nbzsAPuPKeYPL2/k9N++xuPvqpYMHeEugqEh\n0FO+J8y/UmUumbN8b4wjevMElWUwaZ6bNjd6Bnxtier9pHX8xLnue4WIr7eInqfUtVD2bVY3hA58\n+7UGMMfVuN2d/R77DfjOend1vIPPUsetfktlRmkllRVwA9cmSWMKhnDYs8YQOkZqZNRSMOIW4Kb2\nmkgJb90NVQfBlI8oV45WEt21JQfX+imsUGm13uyo6HOv+6hJtfbQaEtBSldY+vrG9flGw0evUd+Z\nTrc18SrEvuBrKbS4SsEr+HXtSU5+8piC15qJsxR6sHiRtvaKHKXgvXb6N6HdrdH3eWIRTbtU8sT6\nhSr9Wk+GtDIZKZaClHK6lHKalHIa8DDwNSnl4928bUCZNU79YD7Y1cQ/397O2pomZv3wOa5/1E1F\n3NXYzryfvdB9d9XBjBDx6YbaXaStAn3z6NnQcVe5++YWKj/+rNPcSmtdo6A583fubD7mPMXubBmU\n+0j/n6gg6Hub4aPfUVWkbXXKp5udp2Zs2vTOCqisIoAjPtf9oilx7iOfQPO4w9SSpH6zaT/3Eajr\n6J1BNu9SmVJHXaoEQNkkt+ut1+XQuCM+L1+fq2CU23tHY7Yjj4sptKgEg5wimH6CG1OI6Tnl4y6N\nWgqj4aQb4Prt7uxYGG64/mz37OcS6/RYCuZ16WhRv6WcwuSZUN7Zf1QpdMRv7y7YrN9TnEQp5BbH\nT7BMS+FPJ8EtB8KmVwAJh3/OTa7QbtoBrlVIZ0rqg6imebOEENVCiC8KIa4UQlyZrnP2N1XF6svJ\nzc5i094W/vL6FgBeWOOWtH9Y00xtcwerdw4zN1NRBZz6M1VsB26Rna4BOOjTcOm/Vb2EyZxLYMYC\nONpnuVFdk6AfQd0wBeWuEgrkukohkWexcLSbKVW71klj9RH6R31BFfDNvTzpRwXiA83P/xAeu1Jl\nV2lhN3626jnld5MmtBTK1ExPSleIacGvr8PoGcpKgnj30cqH4luYtxlKISsndkZrFrX5ZR/llaj0\n50ueUIow1Oa69LILlAKI840blkL0cznp06by74lSWL8wfr0OEz/XTagtVqCa6c2drep3UDjaX7Fp\nIiFi2plrgd3loxS6S0tNRSkUjIqfcJhZWrqHmX4snegmV+jA/gBbCil1Se0NUsrze7DvF9I1jr7w\njY/vz8RRBbQEw/x64Treq1Y3bCDg/qjqWjpjHocVuugMVO7/t9e4KZTgP/MfNQ0uSWDwaTM6O89Z\nTyLizuzLp6g1GAK5sQHvROjsoT0fuPEOL8Vj4Py/+2/z4r1x1zurzb73oHLxgMrTj4TdRY9MtKD3\nHievVAmLm5w1OD57ryv4tUtr1HS3J5Y+TulEd9a/7j/KHRY9l2kp5HhajhuCyS+mkFfius/yipV7\nSgud0vEqZtK+LzYtUgtZs7uuDjofei5sX6Kep5qSGu5026R84x23ZsTEqxSyspXgj6lcblPuIlAx\nqdxiJUi9bUNijht2XHrOddKTEV/3UaoxBcfd6RXe7Q1qwuO1Hk33kb4PNr6kLLjcQqJuSa0Uhoul\nMBwoL8zlsuOms/8YJXTW1qibrKEtRHunmk3VtarZwpBMXe0ppkLoDbr4ad8WuPw/sP8pbiW0FvKB\nHH9/tRdtTTTtiM/W6A1eS8Gk2VmER8c6/DpiJosp6Jt6tWNVacGvixVHT1ez8fYGJawOPReuXgMX\nPQJjD42v7YgqhdGOUjCEl6kUTEuhK6wybsxUYx0n0kK/xJMJpWmrV8ravM6HnAOXPRtbIJmqpWCO\nq+Z9/33MY+WWuI0QTaVgClfd5LCw0q358CMSig385nkCzZ1tbiyr20CzJ4PM2xQvaik4SiGnKP6z\nVTmp4zUr3Y4CeiwZshSsUkiBGVXuTHTOFHUjr9nVyOvr91LnKIMRoRT6ymHnqZtg5mmqzcZFD7tB\nNV0Y1bInXrD6UTLe9Wf3h1LwqxzXaCGjZ4RNPkohUfaRX0whWlDnKAUdt9m3WR1Hv77/yXDE+Wr2\n3mKU8phptgGP+yjGUjCEr15IyPRv6+f6eNqH7RWqjdVKQJkuOiFg6keU1XHazeq1ZEK0ucZVZqYL\nKFHLaXOWXlDuxAocpaCFqxnb0TGFoqrkSqGrM3ZtCzPQLKVSiNqtZ17XpX+Et//sOZajFMqnqjiA\ndgFGx9SsFJrObNPnNd1euYaVq5WAPq7+TVpLYfAxvdIVOh89QH1x596xmIv+vJSte9UPfG+a3Ec7\nGtp5Y0OSH/lQIisLvrpYrXPtRfv9dXFSYaUbv/AjkO1aLrkJ3Ec9wbvIEKjGg1k57sxWZ5l415aG\n5DEFk2BjvPtIF+TVb1bWgvkeXQOy6324sUwtqdq+z2lemO2uidDZqpoTaqVQMiF2Rq5dSea1iloK\njlLQwjBmPe8QbHzR31WomX8lIBKvq9EVgl/PgvvPVf+bGVKJlkI1FUx+udMdt1UpFD2j1sol0qWs\noNwStS3Umnjhp65wrLVkBpqDjcqdpi1V0yp59nvw9NWxx9LCPadQfYf1HqUQblfuRG0paPeb3yJH\n4KYLa0shp1D9pdJMrx+xSiEFcrOzWHL9SVxzykw+fcT4mG1vbFQCu741/obY29LBL59bS2tHmHk/\ne4FnVyaYFSXhtFtf5cK7l/Zu4IORrCz/lNDyyXD2HXDxo+r/76xX/aGSUea4kPrDUigdH//amINd\n10Ag1w20+i0A1J7AUjALvkBVAbd7YwrT1OPuVUoomT2wtIDa9LJ6fPWXjlvC2adkvLJcHrsS/nC0\ncmHkFKntZkyhw89S8LiPooV0xiRk2xIllGZ9Mv4za4RQyjCR+0jHS3a8ox7NbrSJLIUuj6VQMFpV\ncXe2usIzugyro/xyi9zZdqK4QiQUaylEl4LtcK+Dtty6awuvBXp2nkoW8CqFUFApBH2c/Z31TPx6\nLoFrGUQD/3n+lmaasUohRcaV5fPNkw5g0qhCcgPuZWsOKhOzvrWTSETy1Ps7aWxTP+hfPLuW2xdt\n5G9LtlLb3MGPn3RXj0ppHWjj+KGubmv84li3u5mP/vIlapsHbhUnKSWb9/ayOdrsC9R6E6AUR3dp\npFUz1aNnedFX19VyzUPv+bwhCeZCPpriKlfIBHJdt44fWlF4lUKxp66m5n0lZPNK3arqvBJlhex8\nV/2f76MU9PoZnW1KKWgFVT4VGra6cYcdy5XQyy32txRMpZCr3UeefHrTUti1Qj3q7LNEJCui0x1b\nJ893P4PGzxUHsYIzv0wVVu5cDkhXeGpLQbvJ8opdhZHIhdQVirUU9LHCplKY5hzXRymYKcNRpZCv\nFEn9ptg02XC7ch3tcuImB53lnst7DHCzvvT4Ckb517mkGasUekh+ToD/fPtjLP3+SZQX5kRfr2vp\n5JlVu/jG39/lzD+8Tke4i46wEuQrq5X5p38uL63dzfyfv8ir6/yrs5uDoTil0djuf8O1doQTKow1\nO5vYXt/eeyHdDY8ur+bWF2JXgLt/yVYW/GoRy7clqDHoT079mWqU9/HYxYNeW1/LI8ure6ZI/SyF\nIlMp5Ch3jbc6VaPdIF6loF1Oms2vxsYNNKOnq/RXiLUU8suVgN/tNBsOtSt3j64rGTVVCUfduXXn\nciVE84pjYwpRwenjOtHCMM8J6JrdVes3q+N1V60cyE3csjoaS/BUgo+a5gbxvZj+/IJyZbXp90Xd\nLNpScH7fOvsI/JWClMoSMy2FCXPcY+nroN150XWpzZbu29znUZec4z4Kt8cGm0PtylL41G9Uu3td\nbR+jFDrc34K+TideB6f/Eg75L2spDBWmVxYxtjSfIyapLzM3kEVzR5irndnp1ro2HnlnR3Si+9p6\nJfz1JOLlter/D2v8zdP/+8+HfP6uJTGvNSVQCof8+D984+/LWbKpjkeXV8e+JxhK+t6+cvVD73Hr\nC7FrRb+7Tc2kNu5Jsr50f5FbqFpkeLKVgiGlDLSVlRJ+XWqLqtyZpG49UOBjLeQWu/UB3phCsaEU\nJs1TGUg7V0CBR7mMmh4bQNYIoT6fthRCrepc2i2lZ7Wm/zu/LF6Y6Odm+q5+rgVZdoFbV6HZt8V1\npyTDW0RnomfXWqjrsVbsrywFv6Z/MZZCeWyHVm9ModOIl+jvy295TG3JmN+1jiWFO+PdR7tXqQC/\nWTvSYCw42bjDudYlrpLW/n/dNTe7AGacoFZizMpyrpPHUtBW2GFOmm5OAcz/itrfWgpDi9vOP5I7\nL5rDA1+ez8dmVnHQ+FLu/+J8jphczt2vb6KmUc1kmhzhFHZ+/Gam0i+eWxtVGpv3ttLYFmJTbStb\n6lpjZrraUpBScsMTq1iyqY6OsDI3/7N6N5+/a0lUKWm0MtDKYSDIylKaMDKQi7t7aHfakDT38nO/\nFXHSBAsr3JlnlmMVRt02RhW22XE1LqZgZDUd8Xn1WPsBMQVUENvTyWtFmEqvo1kFuvX5dU2H6SrS\nM3tzxu/rPnKUgp79FlY4dRWGENy3ObYzbSK89RImegas3UZamE+YowSkTi4w8cYUzLUc4mIK2lIo\nUtcqK1u5cuKO6YzPtJYC2SqLzc9SeOO38OsDYyvrzTWTG7e7372+lt42HN66lUBerOXR1anOd0N9\n7AqDGmspDC3KCnI47dDxzJs2mvsuP5onvn4cxx9QyTmzJ7CptpWlm2MDkg1tIYKhLqoblBm9tb6V\nOxZt5OI/v0W4K8KCXy3ijN+9xq7GdqQkJhbQ2B5iw54W6ls7uW/xVi655y3fjKfl2/bxsV++TFMw\nFFUk6bIUNB/sauK8Py6mrTNMwDGPehEC6TdcpdCzrp0N5zzIZztu4PLO78AVr8S23GhyrDAdvxg9\nw32jWb/htRTMJT/nXKrcXeC6g/yO4bVGTKXQ1alcK1GlMIU48kqUgNe1D+//K7lS0IKucHRsA7+u\nsFIYqVgKyQLNUaXQGvuo173Y+kb8e0ylUDLerRGB5DGFQI5SlPU+BYY6zVUrbp3xlp3vBpoDeera\n6VoF2RWrFMyK8cZq97vRyQ7RNhyOq8yrFLJzXYURtSby47v2aqylMDz46Ew3uGims4JyLa13+iQt\n2+L+2BY6rTOq97Wzy7EwzFjA3xZv5eRbXuHeN7cAkBfIYq9PAPknT61hW30b729vpKldCcU7X9nE\nY+9Wx+3bX9zwxCre2lzPO1v3Rau9uxKtAzAABJ3Cwp5aSC1TTuRteSAtFKqlTEG18zDxupMAise5\nryW6uUEJrFmnwwnXqZXvTEqNQHcyS0GjLYTcQtd1oes2dr6rBJuMwOu/Uct76uaFZkpqVpbKVNKZ\nOt7AZvNOpYBSshSSBJqjMQWPUhhzkHLfbHldVYm//HPXlaQVzHl/U/UtoOpbwBXqfjEFUG6pOj9L\nwZkkBHLUzPwMZ4nZ7DwlxFv3KsUoROwSsKZSMJ/HWAqFsWMxg9AmWgGBurYykrwuJ1GX3TRilUIa\nmFFZxNQK9SO58oQZnHnEBD43V/147n1zM22O0FprxBReWusWJ+nta3a6P4YXne3PrlKLpZcW5Phm\nFe12FEqXlFGhWNMU5L7FW+P27S+0Dz8iIdtxH4W6Muc+CoZ7Zyn4jrmoUq0sN+FI9f+hjt9340vu\nPvIHnxkAACAASURBVDpuYLYeT8aC69UaEiZmnYTp3oD4BYEg1kLQmVNzLlYz3GO/7sYc9KJGW95Q\nCsCrtHRcIa9UCUvtrti2FN6+2zm+TxDeS6JAc6jdqSEoVkIw3OnO8HMK1XWtXQsv/Bhe+QVscdKQ\n9ax+0jy3wPHsO+CYr7sWhl9MAVRtR/1GNRPf9IprSehjZmWr66CDfgXlKhawe5XbU8vMCtKKIJDn\nPg82qfdELQXn3KFuLIVArus+8q4u6EdemdOfagBdwAN2phGEEIInvn4cf738aM6aPZHbzj+SX3zm\ncGaNLeHBt9TawwePd2/88sIcXvHJRPpgV/wMYYMTwA1kCWpb4m/CGidrqaGtM2amrOMbmkhEsqcp\nyA1PrOKZXtRPmOhW4uGuCFnOjdba0Y8LrvQQ3YKkp26zznAC6+ZLL8GXnTqBA05RHUZPu1kVt43e\nz52pe9cw1sy+yFUqiTAtBaeO4+O/WsQdiza6CmPMIe4+pvWgXStFVXBDnWo9oWMfez9Uj617/IPp\nWpjpz6D7At1zqvKpQ/Jqb02iQLMOMuvxhpz+RTmF6nOWTVJuGG1tbXhRPWohaFpkhaPhtJ+rz1Ey\nXmVyQWxMAZRSCLXB9rfgvjPhqW97julmDUY/e3MN7F7tZiOZLct1bGb0dDfe0lgd+7m87qOEloKx\nsJFWoslajkfX+R44a8EqhTRRXpjLCTOryM9xZ2ZnH+nOBudOUzdhRVEuR00ZxR6fWf8aH6WgqW3u\n8LUUIs5kt761M+o+AtjT3EFXxJ0J/+TpNRz98xe5b/FWvvbA8ujrXRGZMCsqEXpm3tIRjp6jpTOD\nSqE32UckqQUxayaEgEufhKO/DKfcBP+93BWGiZTC2X+AKxYlP7nXOgA27W3lF8+tdQVPwSg46EyV\n0WK6HPR7zawlb9Ec+CsF/ZreP78svveRN63WD28PJinhvrPhFacFhlZsevU03WuqdKIKkmsXyVt3\nwXPfdwPnAZ+enULAMV9TSmHNk64loAWzVjC7nRb3OpCt01yzfJTCltfUdp02arJvi0r5LZ/iWgq6\n6E4r87iYQoJAs4691G10W5ckcx/p73YAF9pJW5dUSzxf+uh0lm6uY+rowmg/pcMmlTFzXEnUPXTg\nuBLW1jRTWZzH+iRpne2hLrbUJa4/2Ncaayl0RSR7WzoYW6pmJX95Y4vv+77417dZ9GEtL11zQkzP\np2Ro91FTeyhqNbR1ZG4homAvA82dvY2Oz/uSmkH6tQtPlWSFeqUTAKHcHJ/9a3xLcS2QzMpuUylM\nnKtiCtrvHXPsiapATVsWPsopJUshOy/W377jHbcKG9xAeqfTgkKPRSu83WuUG2X/k2HJH9xAvncV\nMs28L6lK6UevgNnnO5aHMwHTn0W35tYKSCtvr6IxYzg6lmTyzr3qGhZWwB5HwWiBrvth6X5MWpmF\nHfeR1woIOPGL381xkxZStRQ+fE4pLW9BZD9jLYUBJCeQxb2XHc1NZx3KxcdM5bXvLeCui+fysQPc\nL/mZ//4oi6//OKceMpauiKSyOI8sj7w4x7E4ViVZKnRfWyjOffLO1n3cv2RrNNDtZXt9G4s+dGso\nHn6nmj1NQXY0tLOrMXZhGdPV4gZ2w9HMH9N9JKVk2nVPc8vC2EI3P5ZsqmPadU+zpQ8Fd8FepqSG\nErmPuqOgHE79qb/Q7QkzTlRCEXXNomTnqSK1kvFOrrtHqGmhZ6Y6aoGTWwzzvqieN2yPP+cUp8pY\nz6LzfZRCKg0KA7lKETzmLJey5PbY7aWG+yhkNLXTSmHvh1A5UwWWcwrdlNJESiG3ED56tRK+29+K\nDaDrz16zyt0X/F1SEBuo9+uBFQmpa5hf7iq+FhXbi1k6NivHSLvVloLnN5Gd5362aBV8kuurlfvi\nP8CDn4dFP0+8bz9hLYUMkZUlmDxa/WCO3a+CyaMLyBKCrCzB+LICjp1Rwd+XbuOYGaP52MwqGttC\n7G3p4I+vbuLTR4znsXd3sG53C6X52dE6CJP61s641003kZeGtk7W73GVxQNLt/H6hr18Y8H+/P7l\nDQBsufmM6HbttwdodhSAshSUYG0xlEKrs+9tL67n6lNmJr0uD72tBNdbW+qZVtm7nka9TUnNZHAc\ncBc0AsIRz1guedJfYIOxCpoxx8stUrPSiXPcZnZ+PZv0WhG1jsJO1sojGVrQrvyXWj1s1SOx23Vj\nv842f0tBdy/NylLKYdcKQCTP5hp3mHrcvTq2zkNbCjWO+0hfl2ig2cd9BErA6/NN+6hyKWkOPEOl\n7nY2K+XSskcpNrMYMLfIiCnoQLPXUshVWUsmySyFCmetiZUPqaVUT/1Z4n37CasUBgkvXH1CTGHn\nR/aroCg3wCkHj+Ws2Wr2EolIvnXyzGgRHMDnj57CXa/Gpt9NLC9g+762mBiCZnplkW/bi231bWzc\no17Py87idacz67vb/dtVtIXiBW5TMBwtqGsOhvnG35dzybHTGFuawkzTQbcGycvO4qv3v8OF86dy\n/AEpuC8MtMJq7uihpZDEfXT/kq0cOrGM2ZN7KTR7SNxYRiVZeOgj31QzfXMdbCHgyAth6nGu4NVZ\nOyZ6zeZjnBm+dh/lFMU2rusOrRQiYXjpp0oQH/9teO3XKgaiU2gfvlwJSp1RVTxWZQNFjO6lVQcq\npZDIStCUTVZKLNgQK5y1kNfj11XG0ZRUj9jT+5vHuPBh2PWeCrhDfNVyc01s7QQ4/aZaHeXhWAzZ\n3piCjwLITvI5Cyvc72L/k/tujaaAVQqDhLzs2BlRRXEeb//wZAqMQHVWlqAgNwAE+Pk5h3HHKxu4\n/LjpLN1cz9WnzOTSe94C4OAJpdG6h68v2I+87AC7Gtv52AFVVJXk8Zk7VfO0X557OK9t2Mu/39vJ\nhzXNbKxtoaIol5ljS1i8qY6CnABvbHCDjtvr23hm5S4uPGZqjKWgaQq6iw+t2N5Ae6iLpZvrufMi\nFbwLePxg1zz0Hiu27+PFa06MvqaVQlMwzLOrahhfVtAjpRCJyOgxemopdCRxH/3wceWKMK2ldBIK\n98BqySmABd+Pf/1Tv3Gff3+nv5DNzoUfN7gxDe1GmjQXNr+S+hjMY+9YpmIClU6Kbn6pG+/Qbpcj\nL1aPWQHlFmvc7lpCOrU3UStujRDKWtjyWqz7KJCjUjl1cFZnQHVnKZhB4Zx8d5auEwh0UWH7PmUp\nFHuVQqH6HL893C1+81oKfgogmaUghJsJpS2jNGOVwiCmMDfx13PB/ClcMF/lqj/x9eMA+L/PHM7U\niiIee9ft93/lCftRku/eBHoGesmxUzlv3mQ+c9Qklm/dx7/f30V7Z5j9qoo5eEIpq3Y08pUTZvCr\n5904wG8WruPRd3ewbOs+rjrpgLgxNbWHoplI2oUTEII6J3U221AKb2zYyyPL4wvqdLB3p1P1nWo3\nWY0p2HuakmrOzkNdEXKcbrgyAy07Qv1d/JesvbgZ5J48X/n/T7kJnvymWuM6Fbzt0KsOdDNz8kpj\nZ7hjDoFjvur+XzZJCVOdCeWXAZSI8UfEKwWAwlGGUtin3Fnb31b/e4WwFvY5nmtUVKFqI/Y/xdnP\nUR6v3wpbX1eZYCa5RbDXuV+0cvVaCn41Cd3FbHQKa7L1RfoRqxSGEZ91CuQKcwM8+NY2cgIiRiGA\nCnav+Z9PRC2TrCzBf82ZyB9e3kBudhbnzZ3M1afM5PLjpyOA3764Puprf9Xp0bRmZ1NU6Js0BcPR\nmIImHJHR1em0kF2+bV/MGhGd4Qi52VnOc3VcrRRqDKVQva+NieUFiCSZOua4+pKS2tbZRVmBGlMy\nCyJd9KZVer9QVAFXOy3er3w99ffpNhrF45Q1UDnTVQoHnOIK3MIK+OobsYpIB3e1+0i32E4FLSi9\nSs9cqKZtr3JbaeJagzhj8aaPgmrnrpkwRxUKrrhf/e9dQCm3OH59iDhLwVEARVVGE8UklgLABf+C\nt/+UWruRfsBmHw1DDp1Yxh8umMODXz7Gd3thbnaMK+ecIycSkSq1dMGsMRTlZTOxvIAJ5QWsvPET\n3H3JXMBdXW5PczBadW3S3B6KcyvtbelgU61K08t2WmBsr49dFctsEKgD1LsalDLQRXdLNtVx/C9e\n5qn3kxfaaaWQnSV8A/DJMAVx0FAufp813fTIfTQY0MVVhzqrq409RAWXr1gEp/xEpbWecB186YX4\n9FudrqrdR15BmgztUsnzWAq6JbeTzRUlu8BdN0Kjq5gP+yyg0rff9Wv9XuQotMuehfPugxO+F7vd\nzxrzWgpmIDu6Tzefd+apcOG//BenSgNpO4sQ4h4hxB4hxKoE2y8UQrwvhFgphHhTCHFEusYyEjnj\n8PHMndZND3yHGVXFHDmlnIKcAMfuF1v0lJ8T4KAJbtbL1IpCQl0yOpM32dnYTktHmFMOHsuvPnsE\nPzvnUABeW6+C1lpheAPde43K7HpH8ex0UmD3NAeJRCRvOoHv1TuTV3bqc1SV5PU4JbXTyD4yFUFb\nBgrxel0zkSGkVgqzToeLHoFDzlH/TzjS6UQqVHsPs5GgRgedzRqJr78FFz/e/YkrZ6rFggoTxJ0m\nzon9P7cwXilV7AfX71D1DsCtL6zjnNvf5P3qBuIoHK3Wpj74rHiLw5t+CvGCXBcG7rfAfa27gPoA\nk07Vcy9wWpLtm4ETpJSHAT8B7krjWCzd8LOzD+P2C+fEVGBrxpTkRVebO+1QNcvSdQT7VanZ0ezJ\n5QRDERrbQ0weVchnjprEpw6fQJZwezx1hCMEQ11sqvVXClJK9jpWg1Y6oS5JfVtntLNsd+gZ/piS\nPDrCkcStK3ww9zUVgV9QPd2EM9hQsDc0Nyph15lXrmbn3lYSyfC6jwCqZtE1/UQiPhl0MQSy4Yv/\ngeOuin39sufgk79yldAxX1ePkQQK3rA0Vjr1Pz1esVDXFGQXwNhD/ffRRW9lk+GkHzvvS23yNlCk\nTSlIKV8FfBKjo9vflFJqG20J4NMK0jJQHDyhlAUH+rczyAlksfDqj/HmdR/n1INVxoWutj5ufzVD\nmzy60O0vlqt+VmUFOUytiDWpm4IhNu1t4bj9K/jzpbFuqeaOcFQwm7KgpjHI6h1qJnr3a5v48+vu\nWrjBUBeX3vNWdHU7rRSqSpTv1s9aaAqGOOonC6PWh8Z0H7V3WvdRT+iQajLRJHtRWzLuUDXLroqt\nYdnv+89w/p+WJHiTwdhD4gXr1GNVK5KDzoSr3ncD2925anAXw+puNdg4dH+rcDt8cSFc/UH8Pnrx\nn9KJqvjuhvr+WWO8HxksMYUvAs8m2iiEuEIIsUwIsay21n8JS0t6mVpRxITyAsaUqJvqpbV7mDSq\ngEs/Mg2As2dPYNZYlT2Sb6TXaiVy/tHKRbCtro11u1s4aFwp82eomdVf3thMUzAUF2vQ7GoMstGJ\nS4Qjkp88tSa6bfXORl5ZV8v1j6l1cNui7iM1Tr9g88Y9LdS1dvKTp2NvWrOiudVQBK2G1dDtzLWf\nGGruo8dn3szNoc/TkN2LFgzlU+AHu9yaCQPvmiQ9RghV41E2CeZf6a5nkQT9DQvvQkjdMWme+zy3\nMLbJoUav4VDuuMySFedliIwrBSHEApRSuDbRPlLKu6SUc6WUc6uq0tv3w5KcMUYh2oNfPob9qorZ\n/L+f5KSDxkZdS9sM4f7tU2byyndP5BOHqG13LNpIZzjCefMmU5SrbojVO5v4+9JtbK1T7xtfFjub\nW7e7Oa7CV1sAO52AtM6m0vGKg8crBeW3poK2Ruo8XWZNS8Fs0xFjNXiyrsJdEd8iwb5ijiUdx+9v\ndgcmcGfXmdHq9kGHEHD6L+JjDD7oFOQeWwq6riEZH/mmqgvxy3QaJGRUKQghDgfuBs6SUtZ1t78l\n8+RlBzj5oLH85KxDom06dIro5+epwNtJB7luqPycAFMriqgsVsrkxbV7mDdtFDPHliCEiC5CVN/a\nGW3wd8gElepXVpBDllDWgJcte9ucR/WeHCezaeWORiqL89h/jFIKfpaCXpFur0cpmIFms02H6T5q\n8RzvsBuf57/ueDPuHH3FVAq6Snww09s1LAYj2n3UY1WclaUqob/yWvL9eqxtBpaMKQUhxBTgUeBi\nKWX3ndIsg4a7L53LxcdOi3t9XFk+m//3k5x2aPyiLLPGuW2bj53hZjg9e9VHmVpRyI597Wyra6Oy\nOJdxZUqBjC7KpbI4L5px9MXj3TztHz2xivrWzqhlsK9VCfpVOxo5dGIppQWqBMcvpqCVQkTGFqal\nYik0B0O0dYa59uH32VjbQnuoi/e2+2Sp+NDYFuK+xVuSFsO9vaWe+tZOwoaC6nBqP4KhrowU0qVC\nRy/blQ9GpKMOepKkEOWAU2D8wBSZpYt0pqQ+CCwGZgkhqoUQXxRCXCmEcJqscANQAdwuhFghhFiW\nrrFYBo5EhWU5gazobN5Mlc3PCTBldCHV+9rYUtfK1Ioixpcp03pbfRvjy/KjbqXPzp3E2p+ohLYV\n2xv46dNr2OxYFzsb2gmGuli/p4VDJ5RR6hTt6VqFpZvq+NJfl/Ha+tqoUgBi1rHoDEeiLq3WmIZ+\n7vPmjjC/+s86/rlsO4/6VGQn47sPv8cNT6xm1Q7/tNquiOTCPy3lntc3x8QUguEudjW2c+CPnuOB\npdt837u7Kchzzqp8mcBtLdI/K4QNVOzGD613M1ZAmGHSmX10vpRyvJQyR0o5SUr5ZynlnVL+f3vn\nHR9Xeeb73zO9qhdblmxZuINxibGNbYwXY4ohtMtCaEu9JAGSEG82oYQNIYSQbMJm9y4JYQmXDtlQ\nAgESYzsE04wbuFfcZatLoxlJ09/7xznvO+dMkWRfSyPw8/189NHMnDNHz7yS3uc8XTymH79FCFEs\nhJiqf80YKFmYocG3z9KKhKaNNDeVqy5242B7D7YdCWJshQ/Xztaap5V4HWr+g3zusluxYLwWV/rz\nhsMqKykYiWPL4U4kkgJjKnzwu6SlEEd9Rw+ue3I1lm9rxM1Pr8Wh9lR6q7HeIpZIwuu0wW4lhCLZ\ns49C4TiWbmnIeP3TA+3alLQc/GHNAbyj96NKGu727/vTZjzyzg5d1hiiiSTqO3rM7qNYEnv1NN43\nNhzOev2rHl+Fbzy37qg2snAsgUPt2YP7R8uxzrDIxWAF2ruj8Yy4k/z9HJOl8CUg74Fm5sThjrPG\nYOsD52a03qgu9qCtK4pATwwzaktQ6LZj2Xfn449fPx3DDEHnEo9W5PPUjTPx4V1nAdA2j9v/QWvL\n/L90335NiQc+p6YUAt1RPKq3/r7r/AmIxpOmFuGNnQZLIaG12/A6bcpSCPTEsGJbozonGI6ru2Fj\nvcWlv/kIP//r9qx3ykII/OCVTeq5sRXHs6v24z//psnX0a3P1A6E02IKSSRk8DPj6hp7dDdadzSB\nHQ3BfrmZbn56Deb9/F11bnMwkjE3o7/kshQaAmHc96fNR73BGluLDOTmfOa//R2n3v+O6TW5dKwU\nGGaAIaKsTf5OMkx4O00fUzq20o/aMi9qDXUONmvqz3VEkRs3zKnFV0YVY8mi8WrwEACMLPHAZrVg\ndJkXOxqDeHd7ExZNrMTkEVoAe09zFzy6m0g23IvGk2gORuCwWuB1pJTC/W9swfoDqZhBe3dUBaFl\nmqyR9MI8ACZ3FZB7frU8r7EzbJrtEI4lVIC7rxjl0i0NOPfXK/HK+vreTwRUB1y5AZ/20+U4/Wd/\n6/N92VCWQtpnW7mrGc+u2o+dOQY7SXY1BvHAn7cqt5FxQx7Ied/ZCtTYfcQweeacSZWYPrIII4rc\nGFlibhXw1SlZcr117r1gEl755hxYLYRrZqVaDpT5NIvilBGFWLqlEUcCYcyqK0GJV3v9SCCM2lIv\nbBbC9oZOhCJx3Pb8ery/qwU2K8HntKmNvylo7tLaEAirwjqjG0qyO8sI1SMB8zXe3dGE3U0h0928\nEAIdJqVgthTksb5y51d9rm306/a39bvDbLa0XcnGQx249Dcf9lnVnatduXxfX9XB33rxUzz54V4V\nIzJmXIUGOc1Vuo/y0QhxKMBKgck7Fgvhj9+Yg2VL5mcEqsv9Tlw5owY3zq3t9RoyjRVIBbsnj0i1\nTTittgSl3lSPmWKvHeV+J15cfRCX//YjLNddRK2hKLxOqwoul/tSdRkehxUH+/DB7zZYD93ROP75\nfzZk9NB5btUBfP/lDaaOsp3huLIUuqIJtBuaBIZjCXWsL0tBdpV9cfVBzHpoRe8n6/QWB7j3tc34\n9EAHth7pvedULveRXMf+toyQ5xktheOd0bS7KZQxktYY2JaPvmgFhMcLbp3NDAmsluyuJQD4+eV9\np/i5dXfQ2IqUK0pmOZ1cVYDxlX7llwe0Ggh5Byp7MwFAa1cUk6oKVNaSvEN32S0o9jiyVl1bKNWW\nY/3+dgghQERYuqUBr6w/lHVuxPoDHdhsqL9oDoZNbiajFRKJJ1W8oS+XRn+tA2Nvp942XSlT+pzw\ndCI5As3desC+OdS7UpC/v3r9cxvv0vuyFJo6wyhw2+GwWvDgW9swq65EFUtm4+xHtOFBxoFJXdG4\ninVJS+GL1mrkeMGWAvOlYfOPz8Ubd8xTz6ePLMbyJfPxxh3zYLEQ7FaLykoq9TpzboY+Q6C5rSuK\n2XUlWH/fIhR57GqzrtB7KxV77BhmyJD6ZG8b7vzDZ+iOxvt09Ty3ar963NQZQaA7ZR0YlUJ7VxSB\nHu1YZ49Z5nY9QC85mMWllQ3j9dML8po6w3hWl61Dl6krcmzuo/5aCnLCoMwGi5qUQu9prjMfWoEb\n/u9qvLTmIJ78cC/+bemOXs/PhlFuWSMSTQz9osGBgC0F5kuDzDgyIiubJfKff0pNEU6uKsDL6w5h\n3YF23Dx3NJ7QG+15nTbsbgrhr5sb0BqKYszoEngcNhR7HKqQbkSxG03BCCYMK0BXNI7DgTBumFOL\naCKJFz45gNl1paaZDNl4/bNUemljmqUg3VQWAva3daljxnOSSYFpP1mmek4BmRkzxglyRuoNqbjp\nLp9bnlmLjYcCOHNsubKY+tqYUymp5vOUpdCHUpC9pqRckX66j2QLkFV72lTMaETR0beQMP4MGc84\nUbOPWCkwJyTTRxahrtyHr80ciVAkDp/Tho/3tKK2zKuyT77x3Dp4HFa12RR5Uqm0sm3H+GF+lQ5a\n5LHjOwvHYuXOZjz09jbMNlRuL5xQgUevmY4J9/0VALBgfDn+viPV3HFfSzc6umMo8tjR0a01B7Ra\nCNXFbuxv7VbuI2NQ+D19Et6OXjJ7guG4kt9Ih8EqSd90ZesQc7B3YC0FGUOpz2Ip9KYUjK4lGaPp\nLXCeC+N75GcxZoANFE+8vwfbjgTxqyuGzjgZdh8xJySy5xKQsjDe+vYZePTq6eiJmfselejZTMWe\n1OYa0DfpunKvmjVR6LaDiLBoUiWC4TiWbU3VN5x+UqlpVoXR513qdWDplgYEemIYVuBCgcuGpNCm\nx40s8eBAW7eyELqjCcQS2pyIh9/eDiCzgaCRP31aj6agls10/xtbsF/P7pFKBtA2xHjaKFLAPAo1\n1MdGG8nR+0heq6+YQnvaHA2j66a3zKegaTPXzktPAe4PpuvoymUwso9W7WnD+7uGVudnVgrMCcXS\nO+fjhVtm9Trn+b4LJ+FqQ4qrLJorNlgK3100DlNqivDVU6vg1OdLS+Vy58JxmDjcMDAGWvZTrudX\nzxqJ7Q1BvLO1EeV+pyrYc1gtGFXqMVkKAHDZbz7C37Y3YUdjEFWFLtM403QeeHMrZv50Bd7aeARP\nfbRP+duN1wuG46bur7IjrTG9tiuawMeft6L2rrdQ39GDP284rNJ1E0mBWELAabMgmkimjTLt21KI\nJZKqvkHO1ogYMrO6epl8Z3L7SEuhn0rBmHFkjNX0x320em9bRrpyX7R1RXHd7z9R42kBzS13LJbN\nQMJKgTmhGD/Mjzljcoxu1Ble6MYPzpugnktXUZHBUpg2sgiv3z4XxV4HHDbzv1Ghx447zx6rnn//\nvPGqcE5SaWhBLmdSAMBN80ar1h52mwUjSzwI9MRQ39GDCXpTwU31AbyztQEWAi6eNqJfd7RvbdJm\nW+9p7sLjKz9HoCcGv8sGr8OKYDie9W68vt0Yd4jjqY+0mMuyLQ341ouf4hvPrgOQ2kTlOhk3amkp\nhCLxnGNN23VXVoXfiUBPDJF4wpQO2rulYNzMtfcEemL9qug2VpYbLQXphsqV6SWEwBW/+xiX/ebo\nuuNurg/g/V0tuPLx1OCgUCSOcCw5pDrhslJgmCwUuu342WWTcc/iCZg3VlMi4wwBXadBEcjHxpkP\npxiUwG0LxsCSltNpDIqX+ZxYvuRM/OLyU/EP4ytSSsFKpmrvm+aOxgu3zAIAvLq+HrWlXlPmU29I\nV9bWI5146O3taO2Koshjh99lR2c4ZnJ1SQ4bWl50GabiyVoM6f+Xm6gsGjRusMZq5Jag2aLZ3qAF\n7WWHW7m+raFovy0FGQC3W0lZKLGEMG34RozWgfG6MqAuhOjTUpCfN1vxYm/IEavNwYhSkFKppWeV\n5RMONDNMDq6aaR7MPkNvwQGYu8EuOWccgpG4qfq6qtCFYQUu3DrfPKi+rsyLPS1dICIsXzJf1UiM\nqfBhjF5jITd6CxHGGrKnRpZ6MGt0CSoLnGjsjGBcpd8U/D4attQHUOR2IBxL4OV1h/DyusxaCqOl\n0KXf0QLA2n3aFF2ZRirdNXIEavosCilvUzCMkaVaxfpfNh3BN59fj99eMx1l+vvqyr34YHcLWkIR\nRPS7dKfNojKYJF9/di3CsSSevmmm2lTtVovJYgr0xLLWvURM7TNS15UunHhSqJqTXMVrHT1m5baz\nMYhSrwOlhkJHQKuGt1ssKNYD/T3R1PUaOyMYXWZT6cCd4Zhav3zDlgLD9BNjoNhIhd+FR6+ebrr7\nJyKsumchbjLMgACA1++Yi1V3LwSgpcteeGpmGw85e+JIIIzq4lR65ahSD4gIt87XGgCW+hymRlJd\n6gAAFMJJREFU4LfEnyU1N509LV0o8thR6st8v0RaAn6XDcFIHIc6tDRZqchcdiviiSQW/PLvALK7\nj7oicTWn2xhX2KZXSG870qmUiDyvJRRRd+nFHkfGjOylWxrx3k4tONtpUgoJ9dlzBZuN8Q5jBpZM\nHOhPI770a5/z7ytx7q9XZpw386cr8JUHl6nnRutFFhnKtTqW4PhAwZYCwxwFH911lvKBHwt+lz2j\nS2w6c05KpbIa3U6V+tzpG+bUIhJP4KIpVcr1YqTIa+/XWMwCtx23nlGH//3MWtNcCYkM+lYVutHR\nHTVZDoDmczdWg0ulsK+1C3tbunDNrJHojiZQW+rB6r1t5gwk3dISSLmYanUroiUYVS6cYq8jp/so\nGI4pV5XdSojEkygvcCLYHFebfCgSh9dhVZadcWPuMGzE8ncaMRzPZSkEDEF6GbuQa5WOcSxEj+Fz\nNAUjpthJf4PjgwFbCgxzFFQVuU19lgaCdDfEvDFlsFlIKQirhXDbgjGoLvZkdR898U+n4QY9eP2V\nUSmX18lV5oyoIrcdU2qK8Mk9C3uVp6LAiTX72pE+96atK4r1B9rVczmx7N7XNuOHf9qMrUc6EU8K\nVBd7YCGzpSDv2HuiCaUUlEVhsBSK3PYMS0GyqylkskrCsYSqNO8Mx7HxUAdO+dFSVSS48VAH5jyc\n6gJr3NylcjUqjWg8iZ2NQcx4cJmppbjxrv5o0laN127qDJsqyTuz1GK8u70JH+5u6ff1jxesFBhm\nCPL67XPx0q2zAQBP3zQT2/SJc+lIf7Ux8D1+mB/3X3Qy1t+3CM/rgWkAGR1obbqS6S091+e0ocCd\nUjw3GDKl2rtjWL23DR6HFdfMGokrZtSY3vuZPqbU77Kh1Oc0KYUW/XFTMKIK48p8DvicNi2mEE/C\nZiH4XDas29+Of3zsI2w93GnKKtrVGFQbq5bBk1RB+kBPDD95cyuA1IzvNzceMclntPjaujPbiETj\nSTzz8T60hKJYaphqJy0Mm4Vy3uFnmxwnlZvdSmgORkwKLZv76Man1uCaJz7Jev2BhJUCwwxBptQU\nqYpoq963KRs+pw33XTgJf/nOGXj4ssn48UUnq2NyUp2saJbzKK6drQXQq4s9GdezZcmSqja0jfjR\nVyfhvX9ZgHsXTwSgFV/NrivFTy+dnNFe4pM9balrFLvx6YEO/OqdHQjHEspd1RQMK0vB67ShzOdA\nc1CzFJw2i5p7sWZfOx5ZtsN0Z763pVu5j0KROIRI9aQK9MRwQG9e+P6uFtzz2qaMNFVZq+GyW1St\nhwwiF3vsiCWSsFm0NTNWN0tFYLWQ6Q7fmHIbyuLy6okl4LRZUFngQmNn2BSQ7819NNjxBo4pMMwX\nnJv1YHadIX3VyNI756MzHMMv9cK1WaNL8fX5J5lGnUpOP6kU7+9KuSz8LhvuOGsMVu1pxRljy0FE\nGFXqVQHwllBEWSDGIUi1pR41OnT8MD9mjCrGf7+/Fzsag9hwKIAPdLdIUzCCrkgcTpsFdqsFZT4n\nWkIRFHnscNgspgyipmDEVOjVEopkVFDLuEagJ6Y2/e0NQWxvCOKKGdWmc2WgeViBCwfaupFIClMm\nlbRWgJTr54NdLXjwrW0AtLCIMf22IRBWvwPjJi+75oajCbgdVlT4nRmfxfi4vSuKj/e0quefHmjH\ngvEVGCzYUmCYLznlfidOKvfhzHHabOvxw/yoKfGYiu6+ddYYAMCFpw4HABTo3WS1zrJ2vH7HPHzv\n3PHqfKOVYXRL/frKqfjLd87AJD1+YSFdKRgquFfuTLV1aO6MqN5TAHSlEEU0ro9GdaQyvlpDUZMS\naAlFTMFiQJt54XfZ0NQZzvD3bzgYMD2X760scCEptI08YFAK0XhS3c3LbKFrf59y54RjSdPPf1sv\nEPyfNQexvzXVYl02++uOJuC2W1Hmc6I1FDXHFHpk9lMC036yDLc9v14du+fVTab5GgPNgFkKRPQk\ngAsBNAkhTslynAD8B4DFALoB3CCEWJ9+HsMwx4crT6vB2ZMq1d20kSWLxuG7Z48DkVa4l0gCt7+w\nPudQoXHDUlZJjUEpXKKPRb146gi8vakBSQE4bdaMNh+SYCSOhkAYXqkU/A6s2htBW1cUxR6Hch8B\nWmA7XSmkVzs7bVYUuu3Y15o5FjW9cWC7bknItiJt3VFlXVT6Xdh6uBOt+mbc2BlRxXZGGg1T9X75\nzk5sawjirY1HTAkAwXAMPqcNPTHNUvC77HrmlPZZHFaLimUYlQkA1JS4cbCtB5vqA1i5sxmz60px\n9qTKzIU8jgyk++gpAP8F4Jkcx88HMFb/mgXgt/p3hmEGACLKqhDkMRlvPu+U4WgImPPo03HaUpt1\negAbAM6eWIlyvxOXTdeURInXgdX3LkRTZwTr9rdjzkml2Hw4gO/+YQO2HO5UAfNynwsd3THsb+1G\nTYlHZTw5bRb0xBJo0u/YR5d50RKMIp5Mwm4l5fN32i0odNtxoLX3CXkA1LWkG03OprBZCHXlXrz6\naUyl4TZ2hrGjIbMbrRy1+vwts/DYe5/jLT2Ybewt9dRH+7CzIQgLEdx2zZIJhuPKCqkqcin3UfoQ\np1OqCnGwrQft3VH8/sO98DptA64UBsx9JIRYCaCtl1MuBvCM0FgFoIiIhg+UPAzD9B9593xeLxPM\nZOV1TUnm/AKrhbD6noW4+/yJ6rUKvwunjCjE9XNqMbbSr1J7GzrD8Dk1JVPm15TDrqYQhhe6lM++\nVk9XlXfSo8u8aAlF0N4dM8VGpKVwWN+scxXy+Zw2FeyWn6NVVwqFbrtKj5VjSBs7w2rDXr7kTHzv\nnHGa7PrPmVpThEU5NuvfvbcH7+5oRlt3FB6HFQUuG0LRuHIZVRW5ldsqXSnIwPnOxiCEgKmYcaDI\nZ0xhBICDhueH9NcyIKJbiWgtEa1tbh5abWYZ5svK1gfOxX9dPS3n8Ze/eTr+86ppOceo9pbqCmgW\nhjzFa4gpSKqK3MpSGaUXtkm3UF2ZF/GkQCIpTK3DpaUgqc5ixQBau3GZcTRC32ibOsPoUErB/L7m\nUAR7WrpQ5nNgTIVPdcE9HOjRR8laTX2qsnG4owcuuxUFbjuE0JShw6YF16WCSJ+cV6ErLGmljPiS\nK4V+I4R4XAgxQwgxo7y8PN/iMMwJgcdhM2UUpVNd7MFFUzLbdPQXl92KqkJtk8uuFFy4XM8Ykn2l\nlKVQnpqHYbYUzEphRJF27JKpZjmHG9JnqwrdsFsJhwNhdPbEUOixY1SJ13ANN4QA1uxrUwF2qQgb\nAmH4nDYQUZ9KobEzotxHgKYkClw2FLhtKrX1YFu3ScnJJoPbjmhKoboou5I7nuRTKdQDMFa7VOuv\nMQxzgiCbADp15WO8Q68qdGPOSWXY9/AFqvX4Xn0q3OjS1KZt7BTrsltNSqFAbykypaYI/+eqabhU\nD4QPM7Qud9m12oEjHT3KfVRoCBTLjKyDbT0qqC439vqOHhS4tcfGdui58OiBZgA43KEplAKXXbX7\nPtjeY+rG63Pa4XFYUd/RA6KUW28gyadSeAPAP5HGbAABIcSRvt7EMMyXB9kwsFEfWFPmc2LBeM0b\nMNKgIGQn1fqOHoyt8KmmgYB5o3TaLKYKbFn057RZ8dUpVXjkiinY9/AFKPGmNnCHzYKqQjcOB8Jo\n64oqpXLFjGpcO3skrpk1Sp1bo7tvKnQF0B1NqCFMRIQli8apu/tTRmguJqOycDvMloLfZdezvQSC\nkTj2NIcwtsJnON+i0nUr/a6M2R0DwUCmpL4IYAGAMiI6BOBHAOwAIIR4DMDb0NJRd0NLSb1xoGRh\nGGZocua4cvzrhZNwuqEJ4OPXzcDmwwEML0y5eHxOG06tLsTGQwHMH1du6g81YViqp5PTZlVKwee0\nIalXMctCbRnnKPGaFcewQhc+3tOK1lAEl03XXFa/uFybm2zslipTa8u8TtgshHhSmPz83144Fomk\nwH+s2IVpNcV48JLJKPc7MVfvueSypyyFoF6fIeXdUt+JSDxpshTcdptWdxGMDEo8ARhApSCEuKqP\n4wLA7QP18xmG+WKQ3l7cYbNg+sjijPMumlKFjYcCpi6yADCuMnVn7bJbcMaYMpw9sQLfWTgOz3y8\nDwAyYiPplsLwIpfqzTTBYIXI4xLZesRiIVQWuFDf0aPiIhLZU2lEsRtTa4qQMPRB8hgsBQDwuWzK\nxbVuv5asOdbweTwOK3z68cHIPAK4zQXDMF8Qbpo7GlNrilTn11vn1+HxlXtQ5HFgbIUPu5pCcNqs\nqC524onrTwMA/Mt545EUwAWTzdnu6ZaCsb/T+DSlAADjK/2IJZNwG4rpZBPC9Dv4hRMr8czH+3H2\nRK01hdXQT2rS8EKlBAAtNiHdVWv04UVjjZaCw6rSatN7Sw0UrBQYhvlCYLGQqV3G3edPwJJF4+Cw\nWfDSrbOxbGtjxvSyCr8Lv7piSsa1ZMHd7LoSFLhsOH/ycNz3+hYAqZoII299ex7S+54mdNdUVdpm\nfea4cuz92eKsKbmz60pUphWg1VHIQPV7O5tRW+oxDWty263q+RfefcQwDDOQEJGahlfqc+JraeNT\ne2NMhR+r7l6IygKnqvR+6NLJ2NsSMt3ZS7Kl5sp4RbY7+Fw1GjIW4rBaEE0k4XfZTdPzFqdZNG6H\nFT4XWwoMwzADTnp659Wz+q9UAODcScPwxAd7TXUFuVjxz2eanpf5HDgcCMPn0tqKL548DG9valAp\nsxJjDIJjCgzDMEOYu86fgBvnjc6YlJeN9MK2U6uLcDjQAJfNAiLCo1dPR0somuH+ctmsKHI7YKFM\nN9VA8YWoaGYYhhlq2KyWY3bpzKjVguWyUysRmRSC7CdlsRCumT0ST904M2c7keMNWwoMwzCDzLWz\nR6EpGMH1hvGmRl67ba5qjlfmc2L+uMFr78NKgWEYZpBx2a24Z/HEnMfLfM6cbc4HGnYfMQzDMApW\nCgzDMIyClQLDMAyjYKXAMAzDKFgpMAzDMApWCgzDMIyClQLDMAyjYKXAMAzDKEiI9IawQxsiagaw\n/xjfXgag5TiKczwZqrKxXEcHy3V0sFxHz7HKNkoI0Wdp9BdOKfz/QERrhRAz8i1HNoaqbCzX0cFy\nHR0s19Ez0LKx+4hhGIZRsFJgGIZhFCeaUng83wL0wlCVjeU6Oliuo4PlOnoGVLYTKqbAMAzD9M6J\nZikwDMMwvcBKgWEYhlGcMEqBiM4joh1EtJuI7sqzLPuIaBMRfUZEa/XXSohoGRHt0r8XD4IcTxJR\nExFtNryWUw4iultfvx1EdO4gy3U/EdXra/YZES3Og1w1RPQuEW0loi1E9B399byuWS9y5XXNiMhF\nRKuJaIMu14/114fC31gu2YbC35mViD4lojf154O7XkKIL/0XACuAzwHUAXAA2ABgUh7l2QegLO21\nXwC4S398F4CfD4Ic8wFMB7C5LzkATNLXzQlgtL6e1kGU634A38ty7mDKNRzAdP2xH8BO/efndc16\nkSuvawaAAPj0x3YAnwCYne/16kO2ofB3tgTACwDe1J8P6nqdKJbCTAC7hRB7hBBRAC8BuDjPMqVz\nMYCn9cdPA7hkoH+gEGIlgLZ+ynExgJeEEBEhxF4Au6Gt62DJlYvBlOuIEGK9/jgIYBuAEcjzmvUi\nVy4GSy4hhAjpT+36l8DQ+BvLJVsuBkU2IqoGcAGAJ9J+9qCt14miFEYAOGh4fgi9/9MMNALAciJa\nR0S36q9VCiGO6I8bAFTmR7SccgyFNfwWEW3U3UvShM6LXERUC2AatDvMIbNmaXIBeV4z3RXyGYAm\nAMuEEENmvXLIBuR3zX4N4PsAkobXBnW9ThSlMNSYJ4SYCuB8ALcT0XzjQaHZhnnPFR4qcuj8Fpr7\nbyqAIwB+lS9BiMgH4BUAdwohOo3H8rlmWeTK+5oJIRL633o1gJlEdEra8bytVw7Z8rZmRHQhgCYh\nxLpc5wzGep0oSqEeQI3hebX+Wl4QQtTr35sAvAbN5GskouEAoH9vypN4ueTI6xoKIRr1f+IkgP9G\nykweVLmIyA5t431eCPGq/nLe1yybXENlzXRZOgC8C+A8DIH1yiVbntdsLoCLiGgfNBf3WUT0HAZ5\nvU4UpbAGwFgiGk1EDgBfA/BGPgQhIi8R+eVjAOcA2KzLc71+2vUAXs+HfL3I8QaArxGRk4hGAxgL\nYPVgCSX/KXQuhbZmgyoXERGA3wPYJoR4xHAor2uWS658rxkRlRNRkf7YDWARgO0YAn9juWTL55oJ\nIe4WQlQLIWqh7VF/E0Jci8Fer4GIng/FLwCLoWVlfA7g3jzKUQctY2ADgC1SFgClAFYA2AVgOYCS\nQZDlRWgmcgyaP/Lm3uQAcK++fjsAnD/Icj0LYBOAjfo/w/A8yDUPmum+EcBn+tfifK9ZL3Lldc0A\nnArgU/3nbwbwr339rQ/i7zKXbHn/O9N/1gKkso8Gdb24zQXDMAyjOFHcRwzDMEw/YKXAMAzDKFgp\nMAzDMApWCgzDMIyClQLDMAyjYKXAMIMIES2Q3S8ZZijCSoFhGIZRsFJgmCwQ0bV6v/3PiOh3evO0\nEBH9u95/fwURlevnTiWiVXoTtddkEzUiGkNEy/We/euJ6CT98j4iepmIthPR83pFMsMMCVgpMEwa\nRDQRwJUA5gqtYVoCwDUAvADWCiFOBvAegB/pb3kGwA+EEKdCq4aVrz8P4FEhxBQAc6BVaQNaF9M7\nofXDr4PW84ZhhgS2fAvAMEOQhQC+AmCNfhPvhtaELAngD/o5zwF4lYgKARQJId7TX38awB/1/lYj\nhBCvAYAQIgwA+vVWCyEO6c8/A1AL4IOB/1gM0zesFBgmEwLwtBDibtOLRPelnXesPWIihscJ8P8h\nM4Rg9xHDZLICwOVEVAGoGbmjoP2/XK6fczWAD4QQAQDtRHSG/vp1AN4T2gS0Q0R0iX4NJxF5BvVT\nMMwxwHcoDJOGEGIrEf0QwDtEZIHWrfV2AF3QhrH8EJo76Ur9LdcDeEzf9PcAuFF//ToAvyOiB/Rr\n/OMgfgyGOSa4SyrD9BMiCgkhfPmWg2EGEnYfMQzDMAq2FBiGYRgFWwoMwzCMgpUCwzAMo2ClwDAM\nwyhYKTAMwzAKVgoMwzCM4v8BL3Le3QEKP9cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x150a6cac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(w2v_model.history['loss'])\n",
    "plt.plot(w2v_model.history['val_loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
