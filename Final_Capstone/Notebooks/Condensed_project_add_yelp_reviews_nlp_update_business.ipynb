{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import keras\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import json\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean notebook to be only new \n",
    "- Be able to merge train features and train targets\n",
    "- same with test\n",
    "- be able to then split and run program\n",
    "- test for overfit\n",
    "- create additional features (business/extra features from swing/ extra features from bugs)\n",
    "- run reviews for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp_business = []\n",
    "with open('./yelp_dataset/business.json') as data_file:    \n",
    "    for line in data_file:\n",
    "        data = json.loads(line)\n",
    "        yelp_business.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_yelp_business = pd.DataFrame(yelp_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4855 E Warner Rd, Ste B9</td>\n",
       "      <td>{'AcceptsInsurance': True, 'ByAppointmentOnly'...</td>\n",
       "      <td>FYWN1wneV18bWNgQjJ2GNg</td>\n",
       "      <td>[Dentists, General Dentistry, Health &amp; Medical...</td>\n",
       "      <td>Ahwatukee</td>\n",
       "      <td>{'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.330690</td>\n",
       "      <td>-111.978599</td>\n",
       "      <td>Dental by Design</td>\n",
       "      <td></td>\n",
       "      <td>85044</td>\n",
       "      <td>22</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3101 Washington Rd</td>\n",
       "      <td>{'BusinessParking': {'garage': False, 'street'...</td>\n",
       "      <td>He-G7vWjzVUysIKrfNbPUQ</td>\n",
       "      <td>[Hair Stylists, Hair Salons, Men's Hair Salons...</td>\n",
       "      <td>McMurray</td>\n",
       "      <td>{'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.291685</td>\n",
       "      <td>-80.104900</td>\n",
       "      <td>Stephen Szabo Salon</td>\n",
       "      <td></td>\n",
       "      <td>15317</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6025 N 27th Ave, Ste 1</td>\n",
       "      <td>{}</td>\n",
       "      <td>KQPW8lFf1y5BT2MxiSZ3QA</td>\n",
       "      <td>[Departments of Motor Vehicles, Public Service...</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "      <td>33.524903</td>\n",
       "      <td>-112.115310</td>\n",
       "      <td>Western Motor Vehicle</td>\n",
       "      <td></td>\n",
       "      <td>85017</td>\n",
       "      <td>18</td>\n",
       "      <td>1.5</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000 Arizona Mills Cr, Ste 435</td>\n",
       "      <td>{'BusinessAcceptsCreditCards': True, 'Restaura...</td>\n",
       "      <td>8DShNS-LuFqpEWIp0HxijA</td>\n",
       "      <td>[Sporting Goods, Shopping]</td>\n",
       "      <td>Tempe</td>\n",
       "      <td>{'Monday': '10:00-21:00', 'Tuesday': '10:00-21...</td>\n",
       "      <td>0</td>\n",
       "      <td>33.383147</td>\n",
       "      <td>-111.964725</td>\n",
       "      <td>Sports Authority</td>\n",
       "      <td></td>\n",
       "      <td>85282</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>581 Howe Ave</td>\n",
       "      <td>{'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...</td>\n",
       "      <td>PfOCPjBrlQAnz__NXj9h_w</td>\n",
       "      <td>[American (New), Nightlife, Bars, Sandwiches, ...</td>\n",
       "      <td>Cuyahoga Falls</td>\n",
       "      <td>{'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.119535</td>\n",
       "      <td>-81.475690</td>\n",
       "      <td>Brick House Tavern + Tap</td>\n",
       "      <td></td>\n",
       "      <td>44221</td>\n",
       "      <td>116</td>\n",
       "      <td>3.5</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          address  \\\n",
       "0        4855 E Warner Rd, Ste B9   \n",
       "1              3101 Washington Rd   \n",
       "2          6025 N 27th Ave, Ste 1   \n",
       "3  5000 Arizona Mills Cr, Ste 435   \n",
       "4                    581 Howe Ave   \n",
       "\n",
       "                                          attributes             business_id  \\\n",
       "0  {'AcceptsInsurance': True, 'ByAppointmentOnly'...  FYWN1wneV18bWNgQjJ2GNg   \n",
       "1  {'BusinessParking': {'garage': False, 'street'...  He-G7vWjzVUysIKrfNbPUQ   \n",
       "2                                                 {}  KQPW8lFf1y5BT2MxiSZ3QA   \n",
       "3  {'BusinessAcceptsCreditCards': True, 'Restaura...  8DShNS-LuFqpEWIp0HxijA   \n",
       "4  {'Alcohol': 'full_bar', 'HasTV': True, 'NoiseL...  PfOCPjBrlQAnz__NXj9h_w   \n",
       "\n",
       "                                          categories            city  \\\n",
       "0  [Dentists, General Dentistry, Health & Medical...       Ahwatukee   \n",
       "1  [Hair Stylists, Hair Salons, Men's Hair Salons...        McMurray   \n",
       "2  [Departments of Motor Vehicles, Public Service...         Phoenix   \n",
       "3                         [Sporting Goods, Shopping]           Tempe   \n",
       "4  [American (New), Nightlife, Bars, Sandwiches, ...  Cuyahoga Falls   \n",
       "\n",
       "                                               hours  is_open   latitude  \\\n",
       "0  {'Friday': '7:30-17:00', 'Tuesday': '7:30-17:0...        1  33.330690   \n",
       "1  {'Monday': '9:00-20:00', 'Tuesday': '9:00-20:0...        1  40.291685   \n",
       "2                                                 {}        1  33.524903   \n",
       "3  {'Monday': '10:00-21:00', 'Tuesday': '10:00-21...        0  33.383147   \n",
       "4  {'Monday': '11:00-1:00', 'Tuesday': '11:00-1:0...        1  41.119535   \n",
       "\n",
       "    longitude                      name neighborhood postal_code  \\\n",
       "0 -111.978599          Dental by Design                    85044   \n",
       "1  -80.104900       Stephen Szabo Salon                    15317   \n",
       "2 -112.115310     Western Motor Vehicle                    85017   \n",
       "3 -111.964725          Sports Authority                    85282   \n",
       "4  -81.475690  Brick House Tavern + Tap                    44221   \n",
       "\n",
       "   review_count  stars state  \n",
       "0            22    4.0    AZ  \n",
       "1            11    3.0    PA  \n",
       "2            18    1.5    AZ  \n",
       "3             9    3.0    AZ  \n",
       "4           116    3.5    OH  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_yelp_business_less = df_yelp_business[['business_id', 'postal_code', 'review_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['average_review_count'] = df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['business_count'] = df_yelp_business_less.groupby('postal_code')['business_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['zip_review_count'] = df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['average_review_std'] = df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['average_review_max'] = df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['average_review_min'] = df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kylehoward/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_yelp_business_less['average_review_variance'] = df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.max) - df_yelp_business_less.groupby('postal_code')['review_count'].transform(np.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp_business_less = df_yelp_business_less.sort_values('postal_code',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_yelp_business_less = df_yelp_business_less.set_index('postal_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_count</th>\n",
       "      <th>average_review_count</th>\n",
       "      <th>business_count</th>\n",
       "      <th>zip_review_count</th>\n",
       "      <th>average_review_std</th>\n",
       "      <th>average_review_max</th>\n",
       "      <th>average_review_min</th>\n",
       "      <th>average_review_variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postal_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98145</th>\n",
       "      <td>eyYtakIp6Zu5-ZrXQNs4PQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95966</th>\n",
       "      <td>bujMV3UPks8INgBnjV9gcw</td>\n",
       "      <td>110</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95308</th>\n",
       "      <td>yHdYGeVbM8XFdLRS9-LJzw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95224</th>\n",
       "      <td>jTN3ZnlS6val3UtOtcuKUA</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94901</th>\n",
       "      <td>9kk5C_BW7rdBwtCJiSiBtw</td>\n",
       "      <td>214</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94565</th>\n",
       "      <td>Xp2Owe_MZcRiCFZJuyarVg</td>\n",
       "      <td>8</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94066</th>\n",
       "      <td>AZ4JC4-YOIWhOOOSo0AcMw</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93612</th>\n",
       "      <td>YkbV9Bn9Ohl4vlP-wesvgQ</td>\n",
       "      <td>51</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93449</th>\n",
       "      <td>BhzrquydjzzND2wE4l5PcA</td>\n",
       "      <td>9</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93013</th>\n",
       "      <td>Z_2Ma7FLYKM8G9UG6zufIg</td>\n",
       "      <td>7</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92832</th>\n",
       "      <td>gWeldWpI2crZVAFe5WCeTA</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92585</th>\n",
       "      <td>0R_7iZNQKSGMawKb9dU0oQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92328</th>\n",
       "      <td>FaUk138U3lXk5EaTpI_37w</td>\n",
       "      <td>7</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92008</th>\n",
       "      <td>OMzS4wNgWnviyGZv1UtlEw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91950</th>\n",
       "      <td>VYaXjNs2Nxp-TKJRH1L25A</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91205</th>\n",
       "      <td>SPaOlp1kVYjw7EhQX3YD6Q</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91020</th>\n",
       "      <td>sMxs2yhapicPlc0Xgy446A</td>\n",
       "      <td>15</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90720</th>\n",
       "      <td>8d8QHAktYg8Q2C3ZpAhwvQ</td>\n",
       "      <td>14</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90210</th>\n",
       "      <td>dZbNHpA9jMUxu4vJzCvBNA</td>\n",
       "      <td>4</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90210</th>\n",
       "      <td>UgpuzhYqPCfyM1b4jLtdZg</td>\n",
       "      <td>3</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90001</th>\n",
       "      <td>_FP4CBz5D8V4_3dtex9_iw</td>\n",
       "      <td>53</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>_LgoiM_BtUcZ4zBLdDEJEQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>097NLPKwDFERZJ-xIUTJPQ</td>\n",
       "      <td>4</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>9QDkZHq5vVCPZlsmzP3KGg</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89434</th>\n",
       "      <td>iHPCNNfU3lbDBHAua_uZGw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89410</th>\n",
       "      <td>UofOiG8hwABQ07fZDZu2uw</td>\n",
       "      <td>20</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>cyuSqCubTraFvBXHd_1aLQ</td>\n",
       "      <td>76</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>5kXRiKjCiaCaI4TjuQu88w</td>\n",
       "      <td>3</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>ro5z1tILXj0GjK19Ts1fhQ</td>\n",
       "      <td>59</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>3cg4pKBL8DTFXCtZ5y9V4g</td>\n",
       "      <td>134</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>nepzpPC8cdnp5Bv0yoYu6g</td>\n",
       "      <td>12</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>NqBPd6I5zZ6uUVCKEJAwGw</td>\n",
       "      <td>22</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>Zdl-pmteQRZv9FLYWCkqPg</td>\n",
       "      <td>3</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>pqIQslkOOF0s7FGYgwnJKA</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>CukWZHDpnTAXRALZbLWvxw</td>\n",
       "      <td>16</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>ts5Vjynp92Uvzyouj0NGFQ</td>\n",
       "      <td>13</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>1HjCLZi90uSp_ZstLIVq_w</td>\n",
       "      <td>33</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>RG77CJxElM5-kkz86c4_RA</td>\n",
       "      <td>6</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>-XKG6GamlSBwy5AcXQGAfA</td>\n",
       "      <td>6</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>UQOB6eHrMeXQlx1sSKoPGg</td>\n",
       "      <td>35</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>-nClpn9Z4uFk6h5f39yaDg</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>9FuAvqFdVyF5m9-vSVjSqg</td>\n",
       "      <td>28</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>9Xuz9KDfSZciPZ5cr4jvnA</td>\n",
       "      <td>8</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>K3QncV5RW5E13MKZKScv5g</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>UDs3bbFKFA5TfeFXJuPXxA</td>\n",
       "      <td>55</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>9C2sU7YvSr3omPWpoCEetQ</td>\n",
       "      <td>11</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>wf77MjHDjFiT_VQXl6Qyhg</td>\n",
       "      <td>4</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>4s58KPF0N-pCpct4RDjxdw</td>\n",
       "      <td>5</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>mLNigY641R0Exka68bc_gA</td>\n",
       "      <td>5</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>eoPg6wP1JpV4rTig8RT0pw</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>fTlSJ-oz87jJjQOf49TzcA</td>\n",
       "      <td>33</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>eECmaHr3YLDqo0Xp7TQgoQ</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>TfeFpeiNbaYNg4Paf3YJow</td>\n",
       "      <td>7</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>Dw_tBpjY9N285E5Euw_H4w</td>\n",
       "      <td>8</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>oSICacIUlLb6NSeM1DNKzQ</td>\n",
       "      <td>5</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>WaJTEhzS08m1-F2yyleMug</td>\n",
       "      <td>10</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>NwUyGwrVOJERJwLpT4o-OA</td>\n",
       "      <td>4</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>3DEG0nmT_oTAcmBGvQzJEA</td>\n",
       "      <td>5</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>AnBhAU1AicYCeBw5JEX7Vg</td>\n",
       "      <td>6</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>s8Hum8LKEVGpsBQ_KfZbwA</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        business_id  review_count  average_review_count  \\\n",
       "postal_code                                                               \n",
       "98145        eyYtakIp6Zu5-ZrXQNs4PQ             3              3.000000   \n",
       "95966        bujMV3UPks8INgBnjV9gcw           110            110.000000   \n",
       "95308        yHdYGeVbM8XFdLRS9-LJzw             3              3.000000   \n",
       "95224        jTN3ZnlS6val3UtOtcuKUA             3              3.000000   \n",
       "94901        9kk5C_BW7rdBwtCJiSiBtw           214            214.000000   \n",
       "94565        Xp2Owe_MZcRiCFZJuyarVg             8              8.000000   \n",
       "94066        AZ4JC4-YOIWhOOOSo0AcMw             6              6.000000   \n",
       "93612        YkbV9Bn9Ohl4vlP-wesvgQ            51             51.000000   \n",
       "93449        BhzrquydjzzND2wE4l5PcA             9              9.000000   \n",
       "93013        Z_2Ma7FLYKM8G9UG6zufIg             7              7.000000   \n",
       "92832        gWeldWpI2crZVAFe5WCeTA            11             11.000000   \n",
       "92585        0R_7iZNQKSGMawKb9dU0oQ             3              3.000000   \n",
       "92328        FaUk138U3lXk5EaTpI_37w             7              7.000000   \n",
       "92008        OMzS4wNgWnviyGZv1UtlEw             3              3.000000   \n",
       "91950        VYaXjNs2Nxp-TKJRH1L25A             5              5.000000   \n",
       "91205        SPaOlp1kVYjw7EhQX3YD6Q            11             11.000000   \n",
       "91020        sMxs2yhapicPlc0Xgy446A            15             15.000000   \n",
       "90720        8d8QHAktYg8Q2C3ZpAhwvQ            14             14.000000   \n",
       "90210        dZbNHpA9jMUxu4vJzCvBNA             4              3.500000   \n",
       "90210        UgpuzhYqPCfyM1b4jLtdZg             3              3.500000   \n",
       "90001        _FP4CBz5D8V4_3dtex9_iw            53             53.000000   \n",
       "8967         _LgoiM_BtUcZ4zBLdDEJEQ             3              3.333333   \n",
       "8967         097NLPKwDFERZJ-xIUTJPQ             4              3.333333   \n",
       "8967         9QDkZHq5vVCPZlsmzP3KGg             3              3.333333   \n",
       "89434        iHPCNNfU3lbDBHAua_uZGw             3              3.000000   \n",
       "89410        UofOiG8hwABQ07fZDZu2uw            20             20.000000   \n",
       "89199        cyuSqCubTraFvBXHd_1aLQ            76             68.000000   \n",
       "89199        5kXRiKjCiaCaI4TjuQu88w             3             68.000000   \n",
       "89199        ro5z1tILXj0GjK19Ts1fhQ            59             68.000000   \n",
       "89199        3cg4pKBL8DTFXCtZ5y9V4g           134             68.000000   \n",
       "...                             ...           ...                   ...   \n",
       "89193        nepzpPC8cdnp5Bv0yoYu6g            12             23.333333   \n",
       "89193        NqBPd6I5zZ6uUVCKEJAwGw            22             23.333333   \n",
       "89193        Zdl-pmteQRZv9FLYWCkqPg             3             23.333333   \n",
       "89191        pqIQslkOOF0s7FGYgwnJKA             3             11.433333   \n",
       "89191        CukWZHDpnTAXRALZbLWvxw            16             11.433333   \n",
       "89191        ts5Vjynp92Uvzyouj0NGFQ            13             11.433333   \n",
       "89191        1HjCLZi90uSp_ZstLIVq_w            33             11.433333   \n",
       "89191        RG77CJxElM5-kkz86c4_RA             6             11.433333   \n",
       "89191        -XKG6GamlSBwy5AcXQGAfA             6             11.433333   \n",
       "89191        UQOB6eHrMeXQlx1sSKoPGg            35             11.433333   \n",
       "89191        -nClpn9Z4uFk6h5f39yaDg             3             11.433333   \n",
       "89191        9FuAvqFdVyF5m9-vSVjSqg            28             11.433333   \n",
       "89191        9Xuz9KDfSZciPZ5cr4jvnA             8             11.433333   \n",
       "89191        K3QncV5RW5E13MKZKScv5g             3             11.433333   \n",
       "89191        UDs3bbFKFA5TfeFXJuPXxA            55             11.433333   \n",
       "89191        9C2sU7YvSr3omPWpoCEetQ            11             11.433333   \n",
       "89191        wf77MjHDjFiT_VQXl6Qyhg             4             11.433333   \n",
       "89191        4s58KPF0N-pCpct4RDjxdw             5             11.433333   \n",
       "89191        mLNigY641R0Exka68bc_gA             5             11.433333   \n",
       "89191        eoPg6wP1JpV4rTig8RT0pw             3             11.433333   \n",
       "89191        fTlSJ-oz87jJjQOf49TzcA            33             11.433333   \n",
       "89191        eECmaHr3YLDqo0Xp7TQgoQ             3             11.433333   \n",
       "89191        TfeFpeiNbaYNg4Paf3YJow             7             11.433333   \n",
       "89191        Dw_tBpjY9N285E5Euw_H4w             8             11.433333   \n",
       "89191        oSICacIUlLb6NSeM1DNKzQ             5             11.433333   \n",
       "89191        WaJTEhzS08m1-F2yyleMug            10             11.433333   \n",
       "89191        NwUyGwrVOJERJwLpT4o-OA             4             11.433333   \n",
       "89191        3DEG0nmT_oTAcmBGvQzJEA             5             11.433333   \n",
       "89191        AnBhAU1AicYCeBw5JEX7Vg             6             11.433333   \n",
       "89191        s8Hum8LKEVGpsBQ_KfZbwA             3             11.433333   \n",
       "\n",
       "             business_count  zip_review_count  average_review_std  \\\n",
       "postal_code                                                         \n",
       "98145                     1                 3                 NaN   \n",
       "95966                     1               110                 NaN   \n",
       "95308                     1                 3                 NaN   \n",
       "95224                     1                 3                 NaN   \n",
       "94901                     1               214                 NaN   \n",
       "94565                     1                 8                 NaN   \n",
       "94066                     1                 6                 NaN   \n",
       "93612                     1                51                 NaN   \n",
       "93449                     1                 9                 NaN   \n",
       "93013                     1                 7                 NaN   \n",
       "92832                     1                11                 NaN   \n",
       "92585                     1                 3                 NaN   \n",
       "92328                     1                 7                 NaN   \n",
       "92008                     1                 3                 NaN   \n",
       "91950                     1                 5                 NaN   \n",
       "91205                     1                11                 NaN   \n",
       "91020                     1                15                 NaN   \n",
       "90720                     1                14                 NaN   \n",
       "90210                     2                 7            0.707107   \n",
       "90210                     2                 7            0.707107   \n",
       "90001                     1                53                 NaN   \n",
       "8967                      3                10            0.577350   \n",
       "8967                      3                10            0.577350   \n",
       "8967                      3                10            0.577350   \n",
       "89434                     1                 3                 NaN   \n",
       "89410                     1                20                 NaN   \n",
       "89199                     4               272           53.932056   \n",
       "89199                     4               272           53.932056   \n",
       "89199                     4               272           53.932056   \n",
       "89199                     4               272           53.932056   \n",
       "...                     ...               ...                 ...   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "\n",
       "             average_review_max  average_review_min  average_review_variance  \n",
       "postal_code                                                                   \n",
       "98145                         3                   3                        0  \n",
       "95966                       110                 110                        0  \n",
       "95308                         3                   3                        0  \n",
       "95224                         3                   3                        0  \n",
       "94901                       214                 214                        0  \n",
       "94565                         8                   8                        0  \n",
       "94066                         6                   6                        0  \n",
       "93612                        51                  51                        0  \n",
       "93449                         9                   9                        0  \n",
       "93013                         7                   7                        0  \n",
       "92832                        11                  11                        0  \n",
       "92585                         3                   3                        0  \n",
       "92328                         7                   7                        0  \n",
       "92008                         3                   3                        0  \n",
       "91950                         5                   5                        0  \n",
       "91205                        11                  11                        0  \n",
       "91020                        15                  15                        0  \n",
       "90720                        14                  14                        0  \n",
       "90210                         4                   3                        1  \n",
       "90210                         4                   3                        1  \n",
       "90001                        53                  53                        0  \n",
       "8967                          4                   3                        1  \n",
       "8967                          4                   3                        1  \n",
       "8967                          4                   3                        1  \n",
       "89434                         3                   3                        0  \n",
       "89410                        20                  20                        0  \n",
       "89199                       134                   3                      131  \n",
       "89199                       134                   3                      131  \n",
       "89199                       134                   3                      131  \n",
       "89199                       134                   3                      131  \n",
       "...                         ...                 ...                      ...  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "\n",
       "[67 rows x 9 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business_less[42883:42950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_yelp_business_less = df_yelp_business_less.iloc[42883:-623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_count</th>\n",
       "      <th>average_review_count</th>\n",
       "      <th>business_count</th>\n",
       "      <th>zip_review_count</th>\n",
       "      <th>average_review_std</th>\n",
       "      <th>average_review_max</th>\n",
       "      <th>average_review_min</th>\n",
       "      <th>average_review_variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postal_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98145</th>\n",
       "      <td>eyYtakIp6Zu5-ZrXQNs4PQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95966</th>\n",
       "      <td>bujMV3UPks8INgBnjV9gcw</td>\n",
       "      <td>110</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95308</th>\n",
       "      <td>yHdYGeVbM8XFdLRS9-LJzw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95224</th>\n",
       "      <td>jTN3ZnlS6val3UtOtcuKUA</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94901</th>\n",
       "      <td>9kk5C_BW7rdBwtCJiSiBtw</td>\n",
       "      <td>214</td>\n",
       "      <td>214.0</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        business_id  review_count  average_review_count  \\\n",
       "postal_code                                                               \n",
       "98145        eyYtakIp6Zu5-ZrXQNs4PQ             3                   3.0   \n",
       "95966        bujMV3UPks8INgBnjV9gcw           110                 110.0   \n",
       "95308        yHdYGeVbM8XFdLRS9-LJzw             3                   3.0   \n",
       "95224        jTN3ZnlS6val3UtOtcuKUA             3                   3.0   \n",
       "94901        9kk5C_BW7rdBwtCJiSiBtw           214                 214.0   \n",
       "\n",
       "             business_count  zip_review_count  average_review_std  \\\n",
       "postal_code                                                         \n",
       "98145                     1                 3                 NaN   \n",
       "95966                     1               110                 NaN   \n",
       "95308                     1                 3                 NaN   \n",
       "95224                     1                 3                 NaN   \n",
       "94901                     1               214                 NaN   \n",
       "\n",
       "             average_review_max  average_review_min  average_review_variance  \n",
       "postal_code                                                                   \n",
       "98145                         3                   3                        0  \n",
       "95966                       110                 110                        0  \n",
       "95308                         3                   3                        0  \n",
       "95224                         3                   3                        0  \n",
       "94901                       214                 214                        0  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business_less.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = df_yelp_business_less['business_count'].max()\n",
    "mn = df_yelp_business_less['business_count'].min()\n",
    "diff = mx - mn\n",
    "bus_mean = df_yelp_business_less['business_count'].mean()\n",
    "std = df_yelp_business_less['business_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx_review = df_yelp_business_less['zip_review_count'].max()\n",
    "mn_review = df_yelp_business_less['zip_review_count'].min()\n",
    "diff_review = mx_review - mn_review\n",
    "mean_review = df_yelp_business_less['zip_review_count'].mean()\n",
    "std_review = df_yelp_business_less['zip_review_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 36295.,  41781.,  20352.,  11727.,   9182.,   6715.,   2044.,\n",
       "             0.,      0.,   2965.]),\n",
       " array([  1.00000000e+00,   2.97400000e+02,   5.93800000e+02,\n",
       "          8.90200000e+02,   1.18660000e+03,   1.48300000e+03,\n",
       "          1.77940000e+03,   2.07580000e+03,   2.37220000e+03,\n",
       "          2.66860000e+03,   2.96500000e+03]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14cd46080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_yelp_business_less['business_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x155af8cf8>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAELCAYAAAA/cjqaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXtL7/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155bbaeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_yelp_business_less['business_count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 107871.,   16481.,    3744.,       0.,       0.,       0.,\n",
       "              0.,       0.,       0.,    2965.]),\n",
       " array([  3.00000000e+00,   4.89371000e+04,   9.78712000e+04,\n",
       "          1.46805300e+05,   1.95739400e+05,   2.44673500e+05,\n",
       "          2.93607600e+05,   3.42541700e+05,   3.91475800e+05,\n",
       "          4.40409900e+05,   4.89344000e+05]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkrowUCRJXfx/O7xwLmBSInsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1504a4898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'm guessing the vegas strip is the outlier\n",
    "plt.hist(df_yelp_business_less['zip_review_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x152f64f60>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAELCAYAAAAlTtoUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbSCQGsQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1504b5160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_yelp_business_less['zip_review_count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2965\n",
      "1\n",
      "2964\n",
      "665.06783101\n",
      "585.719523461\n"
     ]
    }
   ],
   "source": [
    "print(mx)\n",
    "print(mn)\n",
    "print(diff)\n",
    "print(bus_mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "489344\n",
      "3\n",
      "489341\n",
      "33598.9776516\n",
      "73612.081861\n"
     ]
    }
   ],
   "source": [
    "print(mx_review)\n",
    "print(mn_review)\n",
    "print(diff_review)\n",
    "print(mean_review)\n",
    "print(std_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>review_count</th>\n",
       "      <th>average_review_count</th>\n",
       "      <th>business_count</th>\n",
       "      <th>zip_review_count</th>\n",
       "      <th>average_review_std</th>\n",
       "      <th>average_review_max</th>\n",
       "      <th>average_review_min</th>\n",
       "      <th>average_review_variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postal_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98145</th>\n",
       "      <td>eyYtakIp6Zu5-ZrXQNs4PQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95966</th>\n",
       "      <td>bujMV3UPks8INgBnjV9gcw</td>\n",
       "      <td>110</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95308</th>\n",
       "      <td>yHdYGeVbM8XFdLRS9-LJzw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95224</th>\n",
       "      <td>jTN3ZnlS6val3UtOtcuKUA</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94901</th>\n",
       "      <td>9kk5C_BW7rdBwtCJiSiBtw</td>\n",
       "      <td>214</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94565</th>\n",
       "      <td>Xp2Owe_MZcRiCFZJuyarVg</td>\n",
       "      <td>8</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94066</th>\n",
       "      <td>AZ4JC4-YOIWhOOOSo0AcMw</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93612</th>\n",
       "      <td>YkbV9Bn9Ohl4vlP-wesvgQ</td>\n",
       "      <td>51</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93449</th>\n",
       "      <td>BhzrquydjzzND2wE4l5PcA</td>\n",
       "      <td>9</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93013</th>\n",
       "      <td>Z_2Ma7FLYKM8G9UG6zufIg</td>\n",
       "      <td>7</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92832</th>\n",
       "      <td>gWeldWpI2crZVAFe5WCeTA</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92585</th>\n",
       "      <td>0R_7iZNQKSGMawKb9dU0oQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92328</th>\n",
       "      <td>FaUk138U3lXk5EaTpI_37w</td>\n",
       "      <td>7</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92008</th>\n",
       "      <td>OMzS4wNgWnviyGZv1UtlEw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91950</th>\n",
       "      <td>VYaXjNs2Nxp-TKJRH1L25A</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91205</th>\n",
       "      <td>SPaOlp1kVYjw7EhQX3YD6Q</td>\n",
       "      <td>11</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91020</th>\n",
       "      <td>sMxs2yhapicPlc0Xgy446A</td>\n",
       "      <td>15</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90720</th>\n",
       "      <td>8d8QHAktYg8Q2C3ZpAhwvQ</td>\n",
       "      <td>14</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90210</th>\n",
       "      <td>dZbNHpA9jMUxu4vJzCvBNA</td>\n",
       "      <td>4</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90210</th>\n",
       "      <td>UgpuzhYqPCfyM1b4jLtdZg</td>\n",
       "      <td>3</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90001</th>\n",
       "      <td>_FP4CBz5D8V4_3dtex9_iw</td>\n",
       "      <td>53</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>_LgoiM_BtUcZ4zBLdDEJEQ</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>097NLPKwDFERZJ-xIUTJPQ</td>\n",
       "      <td>4</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>9QDkZHq5vVCPZlsmzP3KGg</td>\n",
       "      <td>3</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89434</th>\n",
       "      <td>iHPCNNfU3lbDBHAua_uZGw</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89410</th>\n",
       "      <td>UofOiG8hwABQ07fZDZu2uw</td>\n",
       "      <td>20</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>cyuSqCubTraFvBXHd_1aLQ</td>\n",
       "      <td>76</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>5kXRiKjCiaCaI4TjuQu88w</td>\n",
       "      <td>3</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>ro5z1tILXj0GjK19Ts1fhQ</td>\n",
       "      <td>59</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89199</th>\n",
       "      <td>3cg4pKBL8DTFXCtZ5y9V4g</td>\n",
       "      <td>134</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>272</td>\n",
       "      <td>53.932056</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89195</th>\n",
       "      <td>bxZtj089f-8h195_Yp6L-Q</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>MQd__ZOAhDG_-rb1h1rXyA</td>\n",
       "      <td>98</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>kELI9g936jKsxad9hwPg0Q</td>\n",
       "      <td>9</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>fq6g5iGzB7RzxinV-0StFw</td>\n",
       "      <td>5</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>ZLoRKv34xolZs8dy6MkB3A</td>\n",
       "      <td>5</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>MTfaC6rsXwiU1JPssfNOeg</td>\n",
       "      <td>11</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>7maHW53GXG6Kfi8SeTaLQw</td>\n",
       "      <td>45</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>nepzpPC8cdnp5Bv0yoYu6g</td>\n",
       "      <td>12</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>NqBPd6I5zZ6uUVCKEJAwGw</td>\n",
       "      <td>22</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89193</th>\n",
       "      <td>Zdl-pmteQRZv9FLYWCkqPg</td>\n",
       "      <td>3</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>9</td>\n",
       "      <td>210</td>\n",
       "      <td>30.858548</td>\n",
       "      <td>98</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>pqIQslkOOF0s7FGYgwnJKA</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>CukWZHDpnTAXRALZbLWvxw</td>\n",
       "      <td>16</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>ts5Vjynp92Uvzyouj0NGFQ</td>\n",
       "      <td>13</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>1HjCLZi90uSp_ZstLIVq_w</td>\n",
       "      <td>33</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>RG77CJxElM5-kkz86c4_RA</td>\n",
       "      <td>6</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>-XKG6GamlSBwy5AcXQGAfA</td>\n",
       "      <td>6</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>UQOB6eHrMeXQlx1sSKoPGg</td>\n",
       "      <td>35</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>-nClpn9Z4uFk6h5f39yaDg</td>\n",
       "      <td>3</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>9FuAvqFdVyF5m9-vSVjSqg</td>\n",
       "      <td>28</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89191</th>\n",
       "      <td>9Xuz9KDfSZciPZ5cr4jvnA</td>\n",
       "      <td>8</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>30</td>\n",
       "      <td>343</td>\n",
       "      <td>12.672760</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        business_id  review_count  average_review_count  \\\n",
       "postal_code                                                               \n",
       "98145        eyYtakIp6Zu5-ZrXQNs4PQ             3              3.000000   \n",
       "95966        bujMV3UPks8INgBnjV9gcw           110            110.000000   \n",
       "95308        yHdYGeVbM8XFdLRS9-LJzw             3              3.000000   \n",
       "95224        jTN3ZnlS6val3UtOtcuKUA             3              3.000000   \n",
       "94901        9kk5C_BW7rdBwtCJiSiBtw           214            214.000000   \n",
       "94565        Xp2Owe_MZcRiCFZJuyarVg             8              8.000000   \n",
       "94066        AZ4JC4-YOIWhOOOSo0AcMw             6              6.000000   \n",
       "93612        YkbV9Bn9Ohl4vlP-wesvgQ            51             51.000000   \n",
       "93449        BhzrquydjzzND2wE4l5PcA             9              9.000000   \n",
       "93013        Z_2Ma7FLYKM8G9UG6zufIg             7              7.000000   \n",
       "92832        gWeldWpI2crZVAFe5WCeTA            11             11.000000   \n",
       "92585        0R_7iZNQKSGMawKb9dU0oQ             3              3.000000   \n",
       "92328        FaUk138U3lXk5EaTpI_37w             7              7.000000   \n",
       "92008        OMzS4wNgWnviyGZv1UtlEw             3              3.000000   \n",
       "91950        VYaXjNs2Nxp-TKJRH1L25A             5              5.000000   \n",
       "91205        SPaOlp1kVYjw7EhQX3YD6Q            11             11.000000   \n",
       "91020        sMxs2yhapicPlc0Xgy446A            15             15.000000   \n",
       "90720        8d8QHAktYg8Q2C3ZpAhwvQ            14             14.000000   \n",
       "90210        dZbNHpA9jMUxu4vJzCvBNA             4              3.500000   \n",
       "90210        UgpuzhYqPCfyM1b4jLtdZg             3              3.500000   \n",
       "90001        _FP4CBz5D8V4_3dtex9_iw            53             53.000000   \n",
       "8967         _LgoiM_BtUcZ4zBLdDEJEQ             3              3.333333   \n",
       "8967         097NLPKwDFERZJ-xIUTJPQ             4              3.333333   \n",
       "8967         9QDkZHq5vVCPZlsmzP3KGg             3              3.333333   \n",
       "89434        iHPCNNfU3lbDBHAua_uZGw             3              3.000000   \n",
       "89410        UofOiG8hwABQ07fZDZu2uw            20             20.000000   \n",
       "89199        cyuSqCubTraFvBXHd_1aLQ            76             68.000000   \n",
       "89199        5kXRiKjCiaCaI4TjuQu88w             3             68.000000   \n",
       "89199        ro5z1tILXj0GjK19Ts1fhQ            59             68.000000   \n",
       "89199        3cg4pKBL8DTFXCtZ5y9V4g           134             68.000000   \n",
       "89195        bxZtj089f-8h195_Yp6L-Q             4              4.000000   \n",
       "89193        MQd__ZOAhDG_-rb1h1rXyA            98             23.333333   \n",
       "89193        kELI9g936jKsxad9hwPg0Q             9             23.333333   \n",
       "89193        fq6g5iGzB7RzxinV-0StFw             5             23.333333   \n",
       "89193        ZLoRKv34xolZs8dy6MkB3A             5             23.333333   \n",
       "89193        MTfaC6rsXwiU1JPssfNOeg            11             23.333333   \n",
       "89193        7maHW53GXG6Kfi8SeTaLQw            45             23.333333   \n",
       "89193        nepzpPC8cdnp5Bv0yoYu6g            12             23.333333   \n",
       "89193        NqBPd6I5zZ6uUVCKEJAwGw            22             23.333333   \n",
       "89193        Zdl-pmteQRZv9FLYWCkqPg             3             23.333333   \n",
       "89191        pqIQslkOOF0s7FGYgwnJKA             3             11.433333   \n",
       "89191        CukWZHDpnTAXRALZbLWvxw            16             11.433333   \n",
       "89191        ts5Vjynp92Uvzyouj0NGFQ            13             11.433333   \n",
       "89191        1HjCLZi90uSp_ZstLIVq_w            33             11.433333   \n",
       "89191        RG77CJxElM5-kkz86c4_RA             6             11.433333   \n",
       "89191        -XKG6GamlSBwy5AcXQGAfA             6             11.433333   \n",
       "89191        UQOB6eHrMeXQlx1sSKoPGg            35             11.433333   \n",
       "89191        -nClpn9Z4uFk6h5f39yaDg             3             11.433333   \n",
       "89191        9FuAvqFdVyF5m9-vSVjSqg            28             11.433333   \n",
       "89191        9Xuz9KDfSZciPZ5cr4jvnA             8             11.433333   \n",
       "\n",
       "             business_count  zip_review_count  average_review_std  \\\n",
       "postal_code                                                         \n",
       "98145                     1                 3                 NaN   \n",
       "95966                     1               110                 NaN   \n",
       "95308                     1                 3                 NaN   \n",
       "95224                     1                 3                 NaN   \n",
       "94901                     1               214                 NaN   \n",
       "94565                     1                 8                 NaN   \n",
       "94066                     1                 6                 NaN   \n",
       "93612                     1                51                 NaN   \n",
       "93449                     1                 9                 NaN   \n",
       "93013                     1                 7                 NaN   \n",
       "92832                     1                11                 NaN   \n",
       "92585                     1                 3                 NaN   \n",
       "92328                     1                 7                 NaN   \n",
       "92008                     1                 3                 NaN   \n",
       "91950                     1                 5                 NaN   \n",
       "91205                     1                11                 NaN   \n",
       "91020                     1                15                 NaN   \n",
       "90720                     1                14                 NaN   \n",
       "90210                     2                 7            0.707107   \n",
       "90210                     2                 7            0.707107   \n",
       "90001                     1                53                 NaN   \n",
       "8967                      3                10            0.577350   \n",
       "8967                      3                10            0.577350   \n",
       "8967                      3                10            0.577350   \n",
       "89434                     1                 3                 NaN   \n",
       "89410                     1                20                 NaN   \n",
       "89199                     4               272           53.932056   \n",
       "89199                     4               272           53.932056   \n",
       "89199                     4               272           53.932056   \n",
       "89199                     4               272           53.932056   \n",
       "89195                     1                 4                 NaN   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89193                     9               210           30.858548   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "89191                    30               343           12.672760   \n",
       "\n",
       "             average_review_max  average_review_min  average_review_variance  \n",
       "postal_code                                                                   \n",
       "98145                         3                   3                        0  \n",
       "95966                       110                 110                        0  \n",
       "95308                         3                   3                        0  \n",
       "95224                         3                   3                        0  \n",
       "94901                       214                 214                        0  \n",
       "94565                         8                   8                        0  \n",
       "94066                         6                   6                        0  \n",
       "93612                        51                  51                        0  \n",
       "93449                         9                   9                        0  \n",
       "93013                         7                   7                        0  \n",
       "92832                        11                  11                        0  \n",
       "92585                         3                   3                        0  \n",
       "92328                         7                   7                        0  \n",
       "92008                         3                   3                        0  \n",
       "91950                         5                   5                        0  \n",
       "91205                        11                  11                        0  \n",
       "91020                        15                  15                        0  \n",
       "90720                        14                  14                        0  \n",
       "90210                         4                   3                        1  \n",
       "90210                         4                   3                        1  \n",
       "90001                        53                  53                        0  \n",
       "8967                          4                   3                        1  \n",
       "8967                          4                   3                        1  \n",
       "8967                          4                   3                        1  \n",
       "89434                         3                   3                        0  \n",
       "89410                        20                  20                        0  \n",
       "89199                       134                   3                      131  \n",
       "89199                       134                   3                      131  \n",
       "89199                       134                   3                      131  \n",
       "89199                       134                   3                      131  \n",
       "89195                         4                   4                        0  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89193                        98                   3                       95  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  \n",
       "89191                        55                   3                       52  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business_less.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_file_names = glob.glob(\"./zip_2/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_data_list_2 = []\n",
    "fail_list = []\n",
    "for i, data in enumerate(list_of_file_names):\n",
    "    try:\n",
    "        data = pd.read_csv(data)\n",
    "        #data = unicode(data, errors='replace')\n",
    "        read_data_list_2.append(data)\n",
    "    except UnicodeDecodeError:\n",
    "        fail_list.append(i)\n",
    "#read_data_list\n",
    "#fail_list\n",
    "list_of_file_names_org = list_of_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in fail_list:\n",
    "    del list_of_file_names[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./zip_2/Zip_Listings_PriceCut_SeasAdj_AllHomes.csv'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_file_names[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build loop for data we can work with. Only dates and zip\n",
    "def build_useful_df(data_list):\n",
    "    useful_df = []\n",
    "    for dataset in range(len(data_list)):\n",
    "        data = read_data_list_2[dataset]\n",
    "        region_name = data['RegionName']\n",
    "        region_name = region_name.astype(str)\n",
    "        new_df = data.select_dtypes(include=['float64'])\n",
    "        new_df.insert(loc=0, column='RegionName', value=region_name) \n",
    "        new_df = new_df.sort_values('RegionName',ascending=False)\n",
    "        new_df = new_df.set_index('RegionName')\n",
    "        useful_df.append(new_df)\n",
    "    return useful_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_df = build_useful_df(read_data_list_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sixteen_list = []\n",
    "for i, file in enumerate(useful_df):\n",
    "    if len(file) >= 10000:\n",
    "        sixteen_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_list = []\n",
    "for file in useful_df:\n",
    "    file_2 = len(file)\n",
    "    len_list.append(file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sixteen_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 19.,  11.,   4.,   5.,   0.,   0.,   3.,   1.,   0.,   5.,   2.,\n",
       "          0.,   8.,   4.,   0.,   0.,   3.,   2.,   2.,  14.]),\n",
       " array([  8.00000000e+00,   8.02550000e+02,   1.59710000e+03,\n",
       "          2.39165000e+03,   3.18620000e+03,   3.98075000e+03,\n",
       "          4.77530000e+03,   5.56985000e+03,   6.36440000e+03,\n",
       "          7.15895000e+03,   7.95350000e+03,   8.74805000e+03,\n",
       "          9.54260000e+03,   1.03371500e+04,   1.11317000e+04,\n",
       "          1.19262500e+04,   1.27208000e+04,   1.35153500e+04,\n",
       "          1.43099000e+04,   1.51044500e+04,   1.58990000e+04]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEidJREFUeJzt3XuMXGd9xvHv04RACVEDeAm5uRuqECkgCHSbhqvCtYkT\nkVKh1haUa2WggKBFRQ5ItP0vQGkrGhTjQgqUEO6BiJhLoKgBCQhOmotDYmKCITYhdkBNuElg+PWP\nOSbTZda7njPr2fj9fqTRnPOe95zz29ndZ868c+ZMqgpJUjt+Z9oFSJIOLoNfkhpj8EtSYwx+SWqM\nwS9JjTH4JakxBr8kNcbgl6TGGPyS1JjDp13AKKtWrarZ2dlplyFJ9xnXXHPNXVU1s5S+KzL4Z2dn\n2bJly7TLkKT7jCTfXWpfh3okqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4Jakx\nK/KTu33Mbrhi7HV3XHDOBCuRpJXJI35JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtS\nYwx+SWqMwS9JjTH4Jakxi16rJ8nFwLnA7qp6dNf2YeCUrsvRwP9W1Wkj1t0B/Bj4FbC3quYmVLck\naUxLuUjbe4ELgffva6iqv9g3neTtwN37Wf9pVXXXuAVKkiZr0eCvqquSzI5aliTAnwNPn2xZkqTl\n0neM/ynAnVV16wLLC/hCkmuSrO+5L0nSBPS9Hv864NL9LH9yVe1K8jDgyiS3VNVVozp2TwzrAVav\nXt2zLEnSQsY+4k9yOPBnwIcX6lNVu7r73cBlwOn76bupquaqam5mZmbcsiRJi+gz1PNM4Jaq2jlq\nYZIjkxy1bxp4NrC1x/4kSROwaPAnuRT4KnBKkp1JXtYtWsu8YZ4kxyXZ3M0eA3wlyfXA1cAVVfXZ\nyZUuSRrHUs7qWbdA+4tHtH0fWNNN3wY8tmd9kqQJ85O7ktQYg1+SGmPwS1JjDH5JaozBL0mNMfgl\nqTEGvyQ1xuCXpMYY/JLUGINfkhpj8EtSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5Ia\nY/BLUmOW8mXrFyfZnWTrUNs/JNmV5LrutmaBdc9Ksi3J9iQbJlm4JGk8Sznify9w1oj2f6mq07rb\n5vkLkxwGvBM4GzgVWJfk1D7FSpL6WzT4q+oq4EdjbPt0YHtV3VZVvwA+BJw3xnYkSRPUZ4z/NUlu\n6IaCHjxi+fHA7UPzO7u2kZKsT7IlyZY9e/b0KEuStD/jBv9FwCOA04A7gLf3LaSqNlXVXFXNzczM\n9N2cJGkBYwV/Vd1ZVb+qql8D/85gWGe+XcCJQ/MndG2SpCkaK/iTHDs0+1xg64hu3wBOTnJSkiOA\ntcDl4+xPkjQ5hy/WIcmlwJnAqiQ7gb8HzkxyGlDADuDlXd/jgHdX1Zqq2pvk1cDngMOAi6vqpmX5\nKSRJS7Zo8FfVuhHN71mg7/eBNUPzm4HfOtVTkjQ9fnJXkhpj8EtSYwx+SWqMwS9JjTH4Jakxi57V\nI0lautkNV4y97o4LzplgJQvziF+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINf\nkhpj8EtSYwx+SWqMwS9JjVk0+JNcnGR3kq1DbW9LckuSG5JcluToBdbdkeTGJNcl2TLJwiVJ41nK\nEf97gbPmtV0JPLqqHgN8Czh/P+s/rapOq6q58UqUJE3SosFfVVcBP5rX9vmq2tvNfg04YRlqkyQt\ng0mM8b8U+MwCywr4QpJrkqyfwL4kST31+iKWJG8C9gKXLNDlyVW1K8nDgCuT3NK9ghi1rfXAeoDV\nq1f3KUuStB9jH/EneTFwLvD8qqpRfapqV3e/G7gMOH2h7VXVpqqaq6q5mZmZccuSJC1irOBPchbw\nBuA5VfWzBfocmeSofdPAs4Gto/pKkg6epZzOeSnwVeCUJDuTvAy4EDiKwfDNdUk2dn2PS7K5W/UY\n4CtJrgeuBq6oqs8uy08hSVqyRcf4q2rdiOb3LND3+8Cabvo24LG9qpMkTZyf3JWkxhj8ktQYg1+S\nGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1xuCXpMYY/JLUGINfkhrT63r8h5rZDVeMve6OC86ZYCWS\ntHw84pekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNWcp37l6cZHeSrUNtD0lyZZJbu/sHL7DuWUm2\nJdmeZMMkC5ckjWcpR/zvBc6a17YB+GJVnQx8sZv/f5IcBrwTOBs4FViX5NRe1UqSels0+KvqKuBH\n85rPA97XTb8P+NMRq54ObK+q26rqF8CHuvUkSVM07hj/MVV1Rzf9A+CYEX2OB24fmt/ZtUmSpqj3\nm7tVVUD13U6S9Um2JNmyZ8+evpuTJC1g3OC/M8mxAN397hF9dgEnDs2f0LWNVFWbqmququZmZmbG\nLEuStJhxg/9y4EXd9IuAT43o8w3g5CQnJTkCWNutJ0maoqWcznkp8FXglCQ7k7wMuAB4VpJbgWd2\n8yQ5LslmgKraC7wa+BxwM/CRqrppeX4MSdJSLXpZ5qpat8CiZ4zo+31gzdD8ZmDz2NVJkibOT+5K\nUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mNMfglqTEGvyQ1\nxuCXpMYY/JLUGINfkhpj8EtSYxb9Bi5J0ze74Ype6++44JwJVaJDwdhH/ElOSXLd0O2eJK+b1+fM\nJHcP9Xlz/5IlSX2MfcRfVduA0wCSHAbsAi4b0fXLVXXuuPuRJE3WpMb4nwF8u6q+O6HtSZKWyaSC\nfy1w6QLLnpjkhiSfSfKoCe1PkjSm3sGf5AjgOcBHRyy+FlhdVY8B/g345H62sz7JliRb9uzZ07cs\nSdICJnHEfzZwbVXdOX9BVd1TVT/ppjcD90uyatRGqmpTVc1V1dzMzMwEypIkjTKJ4F/HAsM8SR6e\nJN306d3+fjiBfUqSxtTrPP4kRwLPAl4+1PYKgKraCDwPeGWSvcDPgbVVVX32KUnqp1fwV9VPgYfO\na9s4NH0hcGGffUiSJstLNkhSYwx+SWqMwS9JjTH4JakxBr8kNcbgl6TGGPyS1BiDX5IaY/BLUmMM\nfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaozBL0mN6RX8SXYkuTHJdUm2\njFieJO9Isj3JDUke32d/kqT+en3ZeudpVXXXAsvOBk7ubn8MXNTdS5KmZLmHes4D3l8DXwOOTnLs\nMu9TkrQffY/4C/hCkl8B76qqTfOWHw/cPjS/s2u7Y/6GkqwH1gOsXr26Z1n3LbMbrui1/o4LzplQ\nJW3o83j7WOtQ0PeI/8lVdRqDIZ1XJXnquBuqqk1VNVdVczMzMz3LkiQtpFfwV9Wu7n43cBlw+rwu\nu4ATh+ZP6NokSVMydvAnOTLJUfumgWcDW+d1uxx4YXd2zxnA3VX1W8M8kqSDp88Y/zHAZUn2beeD\nVfXZJK8AqKqNwGZgDbAd+Bnwkn7lSpL6Gjv4q+o24LEj2jcOTRfwqnH3IUmaPD+5K0mNMfglqTEG\nvyQ1xuCXpMYY/JLUmElcpE30v+yCtJy8TIWGecQvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPw\nS1JjDH5JaozBL0mNMfglqTFeskFj8zIAWk59L4Pi39jCPOKXpMb0+bL1E5N8Kck3k9yU5LUj+pyZ\n5O4k13W3N/crV5LUV5+hnr3A66vq2iRHAdckubKqvjmv35er6twe+5EkTdDYR/xVdUdVXdtN/xi4\nGTh+UoVJkpbHRMb4k8wCjwO+PmLxE5PckOQzSR41if1JksbX+6yeJA8CPg68rqrumbf4WmB1Vf0k\nyRrgk8DJC2xnPbAeYPXq1X3LkiQtoNcRf5L7MQj9S6rqE/OXV9U9VfWTbnozcL8kq0Ztq6o2VdVc\nVc3NzMz0KUuStB99zuoJ8B7g5qr65wX6PLzrR5LTu/39cNx9SpL66zPU8yTgL4Ebk1zXtb0RWA1Q\nVRuB5wGvTLIX+Dmwtqqqxz4lST2NHfxV9RUgi/S5ELhw3H1IkibPSzZIB8DLVNx39L3kw6HMSzZI\nUmMMfklqjMEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1JjDH5JaoyXbNBU9P04vZc/kMbn\nEb8kNcbgl6TGGPyS1BiDX5IaY/BLUmMMfklqTK/gT3JWkm1JtifZMGJ5kryjW35Dksf32Z8kqb+x\ngz/JYcA7gbOBU4F1SU6d1+1s4OTuth64aNz9SZImo88R/+nA9qq6rap+AXwIOG9en/OA99fA14Cj\nkxzbY5+SpJ76BP/xwO1D8zu7tgPtI0k6iFbMJRuSrGcwHATwkyTbxtzUKuCuyVQ1UctWV97Sa/WV\n+njBfmrr+TP3NdZjdhBqXpbf5QTqvk/+jU3D0GM9Tl2/v9SOfYJ/F3Di0PwJXduB9gGgqjYBm3rU\nA0CSLVU113c7k2ZdB26l1mZdB2al1gUrt7blrqvPUM83gJOTnJTkCGAtcPm8PpcDL+zO7jkDuLuq\n7uixT0lST2Mf8VfV3iSvBj4HHAZcXFU3JXlFt3wjsBlYA2wHfga8pH/JkqQ+eo3xV9VmBuE+3LZx\naLqAV/XZxxh6DxctE+s6cCu1Nus6MCu1Lli5tS1rXRlksySpFV6yQZIac8gE/2KXj1iG/Z2Y5EtJ\nvpnkpiSv7dofkuTKJLd29w8eWuf8rr5tSf5kqP0Pk9zYLXtHkkygvsOS/E+ST6+wuo5O8rEktyS5\nOckTVkJtSf6m+z1uTXJpkgdMo64kFyfZnWTrUNvE6khy/yQf7tq/nmS2Z21v636XNyS5LMnRB7u2\nUXUNLXt9kkqyaqXUleQ13WN2U5K3Huy6AKiq+/yNwZvL3wYeARwBXA+cusz7PBZ4fDd9FPAtBpeu\neCuwoWvfALylmz61q+v+wEldvYd1y64GzgACfAY4ewL1/S3wQeDT3fxKqet9wF9100cAR0+7NgYf\nKvwO8Lvd/EeAF0+jLuCpwOOBrUNtE6sD+GtgYze9Fvhwz9qeDRzeTb9lGrWNqqtrP5HBySffBVat\nhLqApwFfAO7fzT9sKr/Lvv/IK+EGPAH43ND8+cD5B7mGTwHPArYBx3ZtxwLbRtXU/UE+oetzy1D7\nOuBdPWs5Afgi8HTuDf6VUNfvMQjYzGufam3c+wnzhzA44eHTDAJtKnUBs/PCYmJ17OvTTR/O4ENC\nGbe2ecueC1wyjdpG1QV8DHgssIN7g3+qdTE4qHjmiH4Hta5DZahnqpeG6F5iPQ74OnBM3ftZhR8A\nx3TTC9V4fDc9v72PfwXeAPx6qG0l1HUSsAf4jwyGod6d5Mhp11ZVu4B/Ar4H3MHg8yafn3ZdQyZZ\nx2/Wqaq9wN3AQydQI8BLGRyRTr22JOcBu6rq+nmLpv2YPRJ4Sjc0899J/mgadR0qwT81SR4EfBx4\nXVXdM7ysBk/FB/W0qSTnArur6pqF+kyjrs7hDF76XlRVjwN+ymDoYqq1dWPm5zF4YjoOODLJC6Zd\n1ygrpY75krwJ2AtcsgJqeSDwRuDN065lhMMZvLI8A/g74CN9358ax6ES/Eu+NMQkJbkfg9C/pKo+\n0TXfme4KpN397kVq3NVNz28f15OA5yTZweCKqU9P8oEVUBcMjlZ2VtXXu/mPMXgimHZtzwS+U1V7\nquqXwCeAJ66AuvaZZB2/WSfJ4QyG337Yp7gkLwbOBZ7fPTFNu7Y/YPAkfn33f3ACcG2Sh0+5Lhj8\nD3yiBq5m8Kp81cGu61AJ/qVcPmKiumfp9wA3V9U/Dy26HHhRN/0iBmP/+9rXdu/En8TgOwqu7l7C\n35PkjG6bLxxa54BV1flVdUJVzTJ4HP6rql4w7bq62n4A3J7klK7pGcA3V0Bt3wPOSPLAbnvPAG5e\nAXXtM8k6hrf1PAZ/H2O/gkhyFoNhxedU1c/m1TyV2qrqxqp6WFXNdv8HOxmciPGDadbV+SSDN3hJ\n8kgGJzjcddDrWsobAfeFG4NLQ3yLwbvhbzoI+3syg5fcNwDXdbc1DMbYvgjcyuDd+4cMrfOmrr5t\nDJ3tAcwBW7tlF3IAb7YtUuOZ3Pvm7oqoCzgN2NI9bp8EHrwSagP+Ebil2+Z/Mji74qDXBVzK4H2G\nXzIIrJdNsg7gAcBHGVxG5WrgET1r285gnHnf/8DGg13bqLrmLd9B9+butOtiEPQf6PZzLfD0afwu\n/eSuJDXmUBnqkSQtkcEvSY0x+CWpMQa/JDXG4Jekxhj8ktQYg1+SGmPwS1Jj/g/ZvhAL4n2zngAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x153723b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(len_list, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_list(good_range, df_list):\n",
    "    year_list = ['2011', '2012', '2013', '2014', '2015', '2016']\n",
    "    feature_df_list = []\n",
    "    bugs = []\n",
    "    for num in good_range:\n",
    "        df = df_list[num]\n",
    "        try:\n",
    "            df = df.loc[:, '1990-01':'2016-12']\n",
    "            features = pd.DataFrame()\n",
    "            for i, year in enumerate(year_list):\n",
    "                mean = df.loc[:, year + '-01': year + '-12'].mean(axis=1)\n",
    "                features[year + '_mean'] = mean\n",
    "                std = df.loc[:, year + '-01': year + '-12'].std(axis=1)\n",
    "                features[year + '_std'] = std\n",
    "                mn = df.loc[:, year + '-01': year + '-12'].min(axis=1)\n",
    "                features[year + '_min'] = mn\n",
    "                mx = df.loc[:, year + '-01': year + '-12'].max(axis=1)\n",
    "                features[year + '_max'] = mx\n",
    "                features[year + '_swing'] = mx - mn\n",
    "                change = df[year + '-12'] - df[year + '-01']\n",
    "                features[year + '_change'] = change\n",
    "                if i > 0:\n",
    "                    yoy = features[year + '_mean'] / df.loc[:, year_list[i - 1] + '-01': year_list[i - 1] + '-12'].mean(axis=1)\n",
    "                    features[year + '_yoy'] = yoy\n",
    "                    features[year + '_gain'] = np.where(features[year + '_yoy']>1, 1, 0)\n",
    "                    \n",
    "                    #big swing and gain, big swing and loss, big swing and big gain, big swing and big loss\n",
    "            feature_df_list.append(features)\n",
    "        except:\n",
    "            bugs.append(num)\n",
    "    return feature_df_list, bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_list_add(good_range, df_list, pure_feature_list):\n",
    "    year_list = ['2011', '2012', '2013', '2014', '2015', '2016']\n",
    "    feature_df_list = []\n",
    "    bugs = []\n",
    "    for feature in pure_feature_list:\n",
    "        df = df_list[feature]\n",
    "        feature_df_list.append(df)\n",
    "    for num in good_range:\n",
    "        df = df_list[num]\n",
    "        try:\n",
    "            df = df.loc[:, '2011-01':'2016-12']\n",
    "            features = pd.DataFrame()\n",
    "            for i, year in enumerate(year_list):\n",
    "                mean = df.loc[:, year + '-01': year + '-12'].mean(axis=1)\n",
    "                features[year + '_mean'] = mean\n",
    "                std = df.loc[:, year + '-01': year + '-12'].std(axis=1)\n",
    "                features[year + '_std'] = std\n",
    "                mn = df.loc[:, year + '-01': year + '-12'].min(axis=1)\n",
    "                features[year + '_min'] = mn\n",
    "                mx = df.loc[:, year + '-01': year + '-12'].max(axis=1)\n",
    "                features[year + '_max'] = mx\n",
    "                features[year + '_swing'] = mx - mn\n",
    "                change = df[year + '-12'] - df[year + '-01']\n",
    "                features[year + '_change'] = change\n",
    "                if i > 0:\n",
    "                    yoy = features[year + '_mean'] / df.loc[:, year_list[i - 1] + '-01': year_list[i - 1] + '-12'].mean(axis=1)\n",
    "                    features[year + '_yoy'] = yoy\n",
    "                    features[year + '_gain'] = np.where(features[year + '_yoy']>1, 1, 0)\n",
    "                    \n",
    "            feature_df_list.append(features)\n",
    "        except:\n",
    "            bugs.append(num)\n",
    "    return feature_df_list, bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (<ipython-input-61-44fc68f0f1d7>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-44fc68f0f1d7>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    except:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "def build_feature_list_bugs(good_range, df_list):\n",
    "    year_list = ['2011', '2012', '2013', '2014', '2015', '2016']\n",
    "    feature_df_list = []\n",
    "    bugs = []\n",
    "    for num in good_range:\n",
    "        for year in year_list:\n",
    "            for month in month_list:\n",
    "                df = df_list[num]\n",
    "                df = df.loc[:, '2011-01':year+\"-\"+month]\n",
    "                features = pd.DataFrame()\n",
    "                for i, year in enumerate(year_list):\n",
    "                    try:\n",
    "                        mean = df.loc[:, year + '-01': year + '-12'].mean(axis=1)\n",
    "                        features[year + '_mean'] = mean\n",
    "                        std = df.loc[:, year + '-01': year + '-12'].std(axis=1)\n",
    "                        features[year + '_std'] = std\n",
    "                        mn = df.loc[:, year + '-01': year + '-12'].min(axis=1)\n",
    "                        features[year + '_min'] = mn\n",
    "                        mx = df.loc[:, year + '-01': year + '-12'].max(axis=1)\n",
    "                        features[year + '_max'] = mx\n",
    "                        features[year + '_swing'] = mx - mn\n",
    "                        change = df[year + '-12'] - df[year + '-01']\n",
    "                        features[year + '_change'] = change\n",
    "                        if i > 0:\n",
    "                            yoy = features[year + '_mean'] / df.loc[:, year_list[i - 1] + '-01': year_list[i - 1] + '-12'].mean(axis=1)\n",
    "                            features[year + '_yoy'] = yoy\n",
    "                            features[year + '_gain'] = np.where(features[year + '_yoy']>1, 1, 0)\n",
    "                    \n",
    "                        feature_df_list.append(features)\n",
    "            except:\n",
    "                mean = df.loc[:, year + '-01': year + '-12'].mean(axis=1)\n",
    "                features[year + '_mean'] = mean\n",
    "                std = df.loc[:, year + '-01': year + '-12'].std(axis=1)\n",
    "                features[year + '_std'] = std\n",
    "                mn = df.loc[:, year + '-01': year + '-12'].min(axis=1)\n",
    "                features[year + '_min'] = mn\n",
    "                mx = df.loc[:, year + '-01': year + '-12'].max(axis=1)\n",
    "                features[year + '_max'] = mx\n",
    "                features[year + '_swing'] = mx - mn\n",
    "                #change = df[year + '-12'] - df[year + '-01']\n",
    "                features[year + '_change'] = change\n",
    "                if i > 0:\n",
    "                    yoy = features[year + '_mean'] / df.loc[:, year_list[i - 1] + '-01': year_list[i - 1] + '-12'].mean(axis=1)\n",
    "                    features[year + '_yoy'] = yoy\n",
    "                    features[year + '_gain'] = np.where(features[year + '_yoy']>1, 1, 0)\n",
    "                \n",
    "                feature_df_list.append(features)\n",
    "        #    bugs.append(num)\n",
    "    return feature_df_list, bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(useful_df)\n",
    "for num in range(len(useful_df)):\n",
    "    useful_df[num].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sixteen_doc_features, bugs = build_feature_list(sixteen_list, useful_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 52, 60, 61, 62, 64, 65, 68, 70, 71, 72, 74, 75, 76, 77, 80, 81]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sixteen_good = []\n",
    "for six in sixteen_list:\n",
    "    if six not in bugs and six != 23:\n",
    "        sixteen_good.append(six)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 14, 26, 53, 54, 55, 58, 66, 69, 73, 82]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixteen_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_data = useful_df[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data_list = []\n",
    "for i, data in enumerate(useful_df):\n",
    "    if i in sixteen_good:\n",
    "        final_data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010-01</th>\n",
       "      <th>2010-02</th>\n",
       "      <th>2010-03</th>\n",
       "      <th>2010-04</th>\n",
       "      <th>2010-05</th>\n",
       "      <th>2010-06</th>\n",
       "      <th>2010-07</th>\n",
       "      <th>2010-08</th>\n",
       "      <th>2010-09</th>\n",
       "      <th>2010-10</th>\n",
       "      <th>...</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "      <th>2017-07</th>\n",
       "      <th>2017-08</th>\n",
       "      <th>2017-09</th>\n",
       "      <th>2017-10</th>\n",
       "      <th>2017-11</th>\n",
       "      <th>2017-12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99709</th>\n",
       "      <td>234950.0</td>\n",
       "      <td>240450.0</td>\n",
       "      <td>249950.0</td>\n",
       "      <td>252475.0</td>\n",
       "      <td>259450.0</td>\n",
       "      <td>254500.0</td>\n",
       "      <td>248500.0</td>\n",
       "      <td>249500.0</td>\n",
       "      <td>243000.0</td>\n",
       "      <td>246450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>218700.0</td>\n",
       "      <td>217450.0</td>\n",
       "      <td>214900.0</td>\n",
       "      <td>213450.0</td>\n",
       "      <td>214950.0</td>\n",
       "      <td>214900.0</td>\n",
       "      <td>196000.0</td>\n",
       "      <td>176650.0</td>\n",
       "      <td>191745.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99705</th>\n",
       "      <td>210000.0</td>\n",
       "      <td>225950.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>244250.0</td>\n",
       "      <td>244950.0</td>\n",
       "      <td>240500.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>234900.0</td>\n",
       "      <td>234900.0</td>\n",
       "      <td>234900.0</td>\n",
       "      <td>234950.0</td>\n",
       "      <td>239325.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>232000.0</td>\n",
       "      <td>232000.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>219900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99701</th>\n",
       "      <td>199900.0</td>\n",
       "      <td>179000.0</td>\n",
       "      <td>179500.0</td>\n",
       "      <td>182500.0</td>\n",
       "      <td>182500.0</td>\n",
       "      <td>184500.0</td>\n",
       "      <td>184950.0</td>\n",
       "      <td>198400.0</td>\n",
       "      <td>193500.0</td>\n",
       "      <td>191500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>179950.0</td>\n",
       "      <td>181250.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>169000.0</td>\n",
       "      <td>169000.0</td>\n",
       "      <td>168997.5</td>\n",
       "      <td>169000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>164500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99669</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>302500.0</td>\n",
       "      <td>320000.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>273900.0</td>\n",
       "      <td>259000.0</td>\n",
       "      <td>259000.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>237500.0</td>\n",
       "      <td>254900.0</td>\n",
       "      <td>268900.0</td>\n",
       "      <td>267750.0</td>\n",
       "      <td>267250.0</td>\n",
       "      <td>266000.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>259000.0</td>\n",
       "      <td>259450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99654</th>\n",
       "      <td>235000.0</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>236000.0</td>\n",
       "      <td>238450.0</td>\n",
       "      <td>238000.0</td>\n",
       "      <td>236000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>289900.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>298000.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>289900.0</td>\n",
       "      <td>289950.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>289450.0</td>\n",
       "      <td>293025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99645</th>\n",
       "      <td>237900.0</td>\n",
       "      <td>237500.0</td>\n",
       "      <td>237500.0</td>\n",
       "      <td>236250.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>238000.0</td>\n",
       "      <td>237575.0</td>\n",
       "      <td>237500.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>282500.0</td>\n",
       "      <td>279950.0</td>\n",
       "      <td>279900.0</td>\n",
       "      <td>286900.0</td>\n",
       "      <td>284950.0</td>\n",
       "      <td>289700.0</td>\n",
       "      <td>293000.0</td>\n",
       "      <td>289500.0</td>\n",
       "      <td>279900.0</td>\n",
       "      <td>283950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99611</th>\n",
       "      <td>215000.0</td>\n",
       "      <td>215000.0</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>229999.0</td>\n",
       "      <td>234500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>229000.0</td>\n",
       "      <td>229937.5</td>\n",
       "      <td>229950.0</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>237450.0</td>\n",
       "      <td>236185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99603</th>\n",
       "      <td>257000.0</td>\n",
       "      <td>259000.0</td>\n",
       "      <td>260000.0</td>\n",
       "      <td>266750.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>272450.0</td>\n",
       "      <td>268000.0</td>\n",
       "      <td>269000.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>290000.0</td>\n",
       "      <td>293750.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>299450.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>287500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99577</th>\n",
       "      <td>299000.0</td>\n",
       "      <td>299983.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>309450.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>324900.0</td>\n",
       "      <td>319400.0</td>\n",
       "      <td>313450.0</td>\n",
       "      <td>314950.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>361500.0</td>\n",
       "      <td>359950.0</td>\n",
       "      <td>369000.0</td>\n",
       "      <td>359850.0</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>365000.0</td>\n",
       "      <td>352900.0</td>\n",
       "      <td>367000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99516</th>\n",
       "      <td>498000.0</td>\n",
       "      <td>509900.0</td>\n",
       "      <td>489900.0</td>\n",
       "      <td>499900.0</td>\n",
       "      <td>499000.0</td>\n",
       "      <td>479900.0</td>\n",
       "      <td>484900.0</td>\n",
       "      <td>478950.0</td>\n",
       "      <td>474950.0</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>539000.0</td>\n",
       "      <td>539000.0</td>\n",
       "      <td>535000.0</td>\n",
       "      <td>522000.0</td>\n",
       "      <td>509900.0</td>\n",
       "      <td>504999.5</td>\n",
       "      <td>514950.0</td>\n",
       "      <td>499000.0</td>\n",
       "      <td>524950.0</td>\n",
       "      <td>521950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99515</th>\n",
       "      <td>298250.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>310400.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>314950.0</td>\n",
       "      <td>315750.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>345000.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>349000.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>349925.0</td>\n",
       "      <td>371950.0</td>\n",
       "      <td>355000.0</td>\n",
       "      <td>359000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99507</th>\n",
       "      <td>225000.0</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>229950.0</td>\n",
       "      <td>249000.0</td>\n",
       "      <td>254950.0</td>\n",
       "      <td>240000.0</td>\n",
       "      <td>251750.0</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>296000.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>296500.0</td>\n",
       "      <td>295750.0</td>\n",
       "      <td>292750.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>279900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99504</th>\n",
       "      <td>205450.0</td>\n",
       "      <td>214000.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>224950.0</td>\n",
       "      <td>224900.0</td>\n",
       "      <td>218900.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>209900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>246200.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249500.0</td>\n",
       "      <td>258500.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>242500.0</td>\n",
       "      <td>244500.0</td>\n",
       "      <td>239950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99502</th>\n",
       "      <td>261900.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>262400.0</td>\n",
       "      <td>278900.0</td>\n",
       "      <td>277700.0</td>\n",
       "      <td>277700.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>324000.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>324900.0</td>\n",
       "      <td>320000.0</td>\n",
       "      <td>315900.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>328000.0</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>316450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99501</th>\n",
       "      <td>256500.0</td>\n",
       "      <td>254000.0</td>\n",
       "      <td>255900.0</td>\n",
       "      <td>224900.0</td>\n",
       "      <td>242000.0</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>237000.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>234900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>259950.0</td>\n",
       "      <td>260000.0</td>\n",
       "      <td>274000.0</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>264950.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>260000.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>254250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99403</th>\n",
       "      <td>215000.0</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>199500.0</td>\n",
       "      <td>195500.0</td>\n",
       "      <td>194500.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>192200.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>234900.0</td>\n",
       "      <td>234750.0</td>\n",
       "      <td>236750.0</td>\n",
       "      <td>229000.0</td>\n",
       "      <td>229000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99362</th>\n",
       "      <td>219000.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>209500.0</td>\n",
       "      <td>209000.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>211350.0</td>\n",
       "      <td>214950.0</td>\n",
       "      <td>215000.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>229700.0</td>\n",
       "      <td>229350.0</td>\n",
       "      <td>254750.0</td>\n",
       "      <td>264450.0</td>\n",
       "      <td>259450.0</td>\n",
       "      <td>261500.0</td>\n",
       "      <td>269000.0</td>\n",
       "      <td>269000.0</td>\n",
       "      <td>261950.0</td>\n",
       "      <td>268450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99353</th>\n",
       "      <td>239900.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>241950.0</td>\n",
       "      <td>244500.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>238750.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>239200.0</td>\n",
       "      <td>226450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>334077.5</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>365000.0</td>\n",
       "      <td>365000.0</td>\n",
       "      <td>359950.0</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>354900.0</td>\n",
       "      <td>354900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99352</th>\n",
       "      <td>239900.0</td>\n",
       "      <td>231749.5</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>236250.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>248000.0</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>341445.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>356900.0</td>\n",
       "      <td>358900.0</td>\n",
       "      <td>355450.0</td>\n",
       "      <td>359000.0</td>\n",
       "      <td>360450.0</td>\n",
       "      <td>358400.0</td>\n",
       "      <td>349974.5</td>\n",
       "      <td>349850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99338</th>\n",
       "      <td>276750.0</td>\n",
       "      <td>264900.0</td>\n",
       "      <td>264900.0</td>\n",
       "      <td>257000.0</td>\n",
       "      <td>254900.0</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>274900.0</td>\n",
       "      <td>287500.0</td>\n",
       "      <td>291250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>370450.0</td>\n",
       "      <td>371000.0</td>\n",
       "      <td>372450.0</td>\n",
       "      <td>409950.0</td>\n",
       "      <td>406950.0</td>\n",
       "      <td>392023.0</td>\n",
       "      <td>391775.0</td>\n",
       "      <td>394950.0</td>\n",
       "      <td>382422.0</td>\n",
       "      <td>402659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99337</th>\n",
       "      <td>204900.0</td>\n",
       "      <td>199500.0</td>\n",
       "      <td>209900.0</td>\n",
       "      <td>199950.0</td>\n",
       "      <td>204950.0</td>\n",
       "      <td>209900.0</td>\n",
       "      <td>204950.0</td>\n",
       "      <td>209950.0</td>\n",
       "      <td>209950.0</td>\n",
       "      <td>199500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>267450.0</td>\n",
       "      <td>259950.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>272500.0</td>\n",
       "      <td>283000.0</td>\n",
       "      <td>279900.0</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>274900.0</td>\n",
       "      <td>270499.5</td>\n",
       "      <td>269999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99336</th>\n",
       "      <td>150365.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>153900.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>155450.0</td>\n",
       "      <td>164900.0</td>\n",
       "      <td>165000.0</td>\n",
       "      <td>168250.0</td>\n",
       "      <td>166900.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>166313.0</td>\n",
       "      <td>164900.0</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>177375.0</td>\n",
       "      <td>176816.5</td>\n",
       "      <td>175156.5</td>\n",
       "      <td>183283.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>185497.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99324</th>\n",
       "      <td>202300.0</td>\n",
       "      <td>204700.0</td>\n",
       "      <td>199000.0</td>\n",
       "      <td>188250.0</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>199450.0</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>196350.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>230900.0</td>\n",
       "      <td>237000.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>246750.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>259000.0</td>\n",
       "      <td>270743.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99301</th>\n",
       "      <td>175000.0</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>179700.0</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>176900.0</td>\n",
       "      <td>179800.0</td>\n",
       "      <td>177950.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>282120.0</td>\n",
       "      <td>291900.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>284900.0</td>\n",
       "      <td>275000.0</td>\n",
       "      <td>284900.0</td>\n",
       "      <td>289900.0</td>\n",
       "      <td>292336.5</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>299700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99223</th>\n",
       "      <td>240000.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>249000.0</td>\n",
       "      <td>246975.0</td>\n",
       "      <td>249995.0</td>\n",
       "      <td>249995.0</td>\n",
       "      <td>249500.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>239450.0</td>\n",
       "      <td>231500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>281475.0</td>\n",
       "      <td>282499.5</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>287200.0</td>\n",
       "      <td>285129.0</td>\n",
       "      <td>280000.0</td>\n",
       "      <td>279925.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>289900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99218</th>\n",
       "      <td>238250.0</td>\n",
       "      <td>239450.0</td>\n",
       "      <td>239495.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>234500.0</td>\n",
       "      <td>226475.0</td>\n",
       "      <td>222400.0</td>\n",
       "      <td>234900.0</td>\n",
       "      <td>239990.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>260000.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>269500.0</td>\n",
       "      <td>269949.5</td>\n",
       "      <td>255000.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>247250.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>269000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99217</th>\n",
       "      <td>169900.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>154950.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>156900.0</td>\n",
       "      <td>149700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>174925.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>182900.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>177000.0</td>\n",
       "      <td>179900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99216</th>\n",
       "      <td>189950.0</td>\n",
       "      <td>189600.0</td>\n",
       "      <td>189600.0</td>\n",
       "      <td>187950.0</td>\n",
       "      <td>187725.0</td>\n",
       "      <td>185000.0</td>\n",
       "      <td>183500.0</td>\n",
       "      <td>178325.0</td>\n",
       "      <td>174950.0</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>189950.0</td>\n",
       "      <td>193900.0</td>\n",
       "      <td>207500.0</td>\n",
       "      <td>209000.0</td>\n",
       "      <td>202000.0</td>\n",
       "      <td>193900.0</td>\n",
       "      <td>199500.0</td>\n",
       "      <td>214900.0</td>\n",
       "      <td>212000.0</td>\n",
       "      <td>208500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99212</th>\n",
       "      <td>159000.0</td>\n",
       "      <td>159000.0</td>\n",
       "      <td>154000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>153500.0</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>155000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>172000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>182250.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>186950.0</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>179000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99208</th>\n",
       "      <td>217650.0</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>219250.0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>210696.5</td>\n",
       "      <td>211745.0</td>\n",
       "      <td>217000.0</td>\n",
       "      <td>209900.0</td>\n",
       "      <td>214000.0</td>\n",
       "      <td>208995.0</td>\n",
       "      <td>...</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>264950.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>269250.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>266327.0</td>\n",
       "      <td>269750.0</td>\n",
       "      <td>276327.0</td>\n",
       "      <td>279332.0</td>\n",
       "      <td>280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>199400.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>194000.0</td>\n",
       "      <td>189850.0</td>\n",
       "      <td>179450.0</td>\n",
       "      <td>184800.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>183500.0</td>\n",
       "      <td>183900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>195000.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>205750.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>219450.0</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>219900.0</td>\n",
       "      <td>210449.5</td>\n",
       "      <td>218700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10804</th>\n",
       "      <td>749000.0</td>\n",
       "      <td>749000.0</td>\n",
       "      <td>749500.0</td>\n",
       "      <td>795900.0</td>\n",
       "      <td>779000.0</td>\n",
       "      <td>772000.0</td>\n",
       "      <td>759888.0</td>\n",
       "      <td>749000.0</td>\n",
       "      <td>708999.5</td>\n",
       "      <td>699000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>820000.0</td>\n",
       "      <td>849000.0</td>\n",
       "      <td>849999.0</td>\n",
       "      <td>880000.0</td>\n",
       "      <td>899000.0</td>\n",
       "      <td>862500.0</td>\n",
       "      <td>849500.0</td>\n",
       "      <td>850000.0</td>\n",
       "      <td>829000.0</td>\n",
       "      <td>827000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>229900.0</td>\n",
       "      <td>239000.0</td>\n",
       "      <td>244900.0</td>\n",
       "      <td>247500.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>229950.0</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>235900.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240900.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>229700.0</td>\n",
       "      <td>264900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>252443.5</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>253999.5</td>\n",
       "      <td>258000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>185000.0</td>\n",
       "      <td>189700.0</td>\n",
       "      <td>189700.0</td>\n",
       "      <td>184900.0</td>\n",
       "      <td>184900.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>166200.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>171500.0</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>182400.0</td>\n",
       "      <td>180700.0</td>\n",
       "      <td>184950.0</td>\n",
       "      <td>183700.0</td>\n",
       "      <td>187500.0</td>\n",
       "      <td>204900.0</td>\n",
       "      <td>212400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10607</th>\n",
       "      <td>519000.0</td>\n",
       "      <td>499900.0</td>\n",
       "      <td>489000.0</td>\n",
       "      <td>464500.0</td>\n",
       "      <td>493450.0</td>\n",
       "      <td>499000.0</td>\n",
       "      <td>489450.0</td>\n",
       "      <td>465000.0</td>\n",
       "      <td>484500.0</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>499011.0</td>\n",
       "      <td>499021.0</td>\n",
       "      <td>499510.0</td>\n",
       "      <td>509950.0</td>\n",
       "      <td>499949.5</td>\n",
       "      <td>514450.0</td>\n",
       "      <td>521500.0</td>\n",
       "      <td>499000.0</td>\n",
       "      <td>499250.0</td>\n",
       "      <td>499000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10598</th>\n",
       "      <td>439900.0</td>\n",
       "      <td>439900.0</td>\n",
       "      <td>439000.0</td>\n",
       "      <td>439900.0</td>\n",
       "      <td>446750.0</td>\n",
       "      <td>429900.0</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>429450.0</td>\n",
       "      <td>425000.0</td>\n",
       "      <td>424900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>469000.0</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>499000.0</td>\n",
       "      <td>471950.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>444500.0</td>\n",
       "      <td>429900.0</td>\n",
       "      <td>449000.0</td>\n",
       "      <td>409000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10579</th>\n",
       "      <td>439900.0</td>\n",
       "      <td>439950.0</td>\n",
       "      <td>444000.0</td>\n",
       "      <td>449900.0</td>\n",
       "      <td>444500.0</td>\n",
       "      <td>429500.0</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>419900.0</td>\n",
       "      <td>403950.0</td>\n",
       "      <td>399900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>399500.0</td>\n",
       "      <td>435000.0</td>\n",
       "      <td>432000.0</td>\n",
       "      <td>414000.0</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>415000.0</td>\n",
       "      <td>415000.0</td>\n",
       "      <td>417450.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>216950.0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>209900.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>209500.0</td>\n",
       "      <td>224900.0</td>\n",
       "      <td>224900.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>215000.0</td>\n",
       "      <td>215000.0</td>\n",
       "      <td>218450.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>264950.0</td>\n",
       "      <td>262450.0</td>\n",
       "      <td>256450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10562</th>\n",
       "      <td>394495.0</td>\n",
       "      <td>419500.0</td>\n",
       "      <td>389000.0</td>\n",
       "      <td>392111.0</td>\n",
       "      <td>384222.0</td>\n",
       "      <td>367000.0</td>\n",
       "      <td>359111.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>349900.0</td>\n",
       "      <td>342450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399999.0</td>\n",
       "      <td>399900.0</td>\n",
       "      <td>399222.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>382500.0</td>\n",
       "      <td>399999.0</td>\n",
       "      <td>375000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>237450.0</td>\n",
       "      <td>249000.0</td>\n",
       "      <td>239450.0</td>\n",
       "      <td>239450.0</td>\n",
       "      <td>239450.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>226000.0</td>\n",
       "      <td>224937.5</td>\n",
       "      <td>202450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>257000.0</td>\n",
       "      <td>257000.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>231700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>409450.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399900.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399900.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399900.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>349450.0</td>\n",
       "      <td>369000.0</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>429950.0</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>435000.0</td>\n",
       "      <td>425000.0</td>\n",
       "      <td>412200.0</td>\n",
       "      <td>419500.0</td>\n",
       "      <td>429900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10512</th>\n",
       "      <td>299900.0</td>\n",
       "      <td>309000.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>309000.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>299700.0</td>\n",
       "      <td>307000.0</td>\n",
       "      <td>311000.0</td>\n",
       "      <td>310500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>328900.0</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>339000.0</td>\n",
       "      <td>344990.0</td>\n",
       "      <td>339250.0</td>\n",
       "      <td>339500.0</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>336599.5</td>\n",
       "      <td>339900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10509</th>\n",
       "      <td>375000.0</td>\n",
       "      <td>365000.0</td>\n",
       "      <td>359499.5</td>\n",
       "      <td>359000.0</td>\n",
       "      <td>369900.0</td>\n",
       "      <td>359999.5</td>\n",
       "      <td>355000.0</td>\n",
       "      <td>354000.0</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>365000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>339000.0</td>\n",
       "      <td>333900.0</td>\n",
       "      <td>331450.0</td>\n",
       "      <td>324950.0</td>\n",
       "      <td>326950.0</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>324950.0</td>\n",
       "      <td>339500.0</td>\n",
       "      <td>344500.0</td>\n",
       "      <td>340000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10506</th>\n",
       "      <td>1125000.0</td>\n",
       "      <td>1125000.0</td>\n",
       "      <td>1170000.0</td>\n",
       "      <td>1125000.0</td>\n",
       "      <td>1174500.0</td>\n",
       "      <td>1197500.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1250000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1597000.0</td>\n",
       "      <td>1599000.0</td>\n",
       "      <td>1697000.0</td>\n",
       "      <td>1650000.0</td>\n",
       "      <td>1695000.0</td>\n",
       "      <td>1712500.0</td>\n",
       "      <td>1597000.0</td>\n",
       "      <td>1495000.0</td>\n",
       "      <td>1495000.0</td>\n",
       "      <td>1595000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10463</th>\n",
       "      <td>299990.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>299495.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>292200.0</td>\n",
       "      <td>295000.0</td>\n",
       "      <td>299000.0</td>\n",
       "      <td>296500.0</td>\n",
       "      <td>309000.0</td>\n",
       "      <td>299900.0</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>320000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>139900.0</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>149000.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>149200.0</td>\n",
       "      <td>154500.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>149700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>170749.5</td>\n",
       "      <td>179700.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>176900.0</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>179950.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>199900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>399900.0</td>\n",
       "      <td>409900.0</td>\n",
       "      <td>404000.0</td>\n",
       "      <td>399367.5</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>389000.0</td>\n",
       "      <td>392000.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>398750.0</td>\n",
       "      <td>398500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>539450.0</td>\n",
       "      <td>535000.0</td>\n",
       "      <td>549900.0</td>\n",
       "      <td>549900.0</td>\n",
       "      <td>554499.5</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>559000.0</td>\n",
       "      <td>569750.0</td>\n",
       "      <td>559000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10308</th>\n",
       "      <td>402450.0</td>\n",
       "      <td>419000.0</td>\n",
       "      <td>419000.0</td>\n",
       "      <td>414900.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399700.0</td>\n",
       "      <td>399900.0</td>\n",
       "      <td>396500.0</td>\n",
       "      <td>402499.5</td>\n",
       "      <td>394500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>524000.0</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>529000.0</td>\n",
       "      <td>529900.0</td>\n",
       "      <td>509000.0</td>\n",
       "      <td>544450.0</td>\n",
       "      <td>539397.0</td>\n",
       "      <td>529000.0</td>\n",
       "      <td>548944.0</td>\n",
       "      <td>539888.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10306</th>\n",
       "      <td>399900.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>410000.0</td>\n",
       "      <td>409900.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>394000.0</td>\n",
       "      <td>397000.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>399000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>529000.0</td>\n",
       "      <td>548500.0</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>568000.0</td>\n",
       "      <td>586500.0</td>\n",
       "      <td>574450.0</td>\n",
       "      <td>568000.0</td>\n",
       "      <td>577449.0</td>\n",
       "      <td>579998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10305</th>\n",
       "      <td>384450.0</td>\n",
       "      <td>382450.0</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>375000.0</td>\n",
       "      <td>375000.0</td>\n",
       "      <td>372450.0</td>\n",
       "      <td>369900.0</td>\n",
       "      <td>369000.0</td>\n",
       "      <td>376000.0</td>\n",
       "      <td>375000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>525000.0</td>\n",
       "      <td>525000.0</td>\n",
       "      <td>537000.0</td>\n",
       "      <td>549999.0</td>\n",
       "      <td>554450.0</td>\n",
       "      <td>549900.0</td>\n",
       "      <td>559500.0</td>\n",
       "      <td>549000.0</td>\n",
       "      <td>579000.0</td>\n",
       "      <td>589999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>254900.0</td>\n",
       "      <td>257000.0</td>\n",
       "      <td>254900.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>246450.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>239800.0</td>\n",
       "      <td>239700.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>329900.0</td>\n",
       "      <td>319900.0</td>\n",
       "      <td>309900.0</td>\n",
       "      <td>307400.0</td>\n",
       "      <td>292450.0</td>\n",
       "      <td>285241.0</td>\n",
       "      <td>279900.0</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>329900.0</td>\n",
       "      <td>326750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>235000.0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>248750.0</td>\n",
       "      <td>248000.0</td>\n",
       "      <td>248500.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>229000.0</td>\n",
       "      <td>229900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>249900.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>261500.0</td>\n",
       "      <td>254450.0</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>284900.0</td>\n",
       "      <td>269700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>155950.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>172000.0</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>164900.0</td>\n",
       "      <td>164900.0</td>\n",
       "      <td>164900.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>173850.0</td>\n",
       "      <td>179000.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>184650.0</td>\n",
       "      <td>184650.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>184900.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>187350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>149900.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>147450.0</td>\n",
       "      <td>149900.0</td>\n",
       "      <td>149950.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>159900.0</td>\n",
       "      <td>154000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>178500.0</td>\n",
       "      <td>179000.0</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>172443.5</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>171249.5</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>164900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>260850.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>254900.0</td>\n",
       "      <td>254900.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>250000.0</td>\n",
       "      <td>249700.0</td>\n",
       "      <td>248950.0</td>\n",
       "      <td>...</td>\n",
       "      <td>322400.0</td>\n",
       "      <td>319900.0</td>\n",
       "      <td>336000.0</td>\n",
       "      <td>324900.0</td>\n",
       "      <td>324900.0</td>\n",
       "      <td>319900.0</td>\n",
       "      <td>297200.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>291200.0</td>\n",
       "      <td>295000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10025</th>\n",
       "      <td>775000.0</td>\n",
       "      <td>725000.0</td>\n",
       "      <td>762500.0</td>\n",
       "      <td>725000.0</td>\n",
       "      <td>732500.0</td>\n",
       "      <td>745000.0</td>\n",
       "      <td>722000.0</td>\n",
       "      <td>745000.0</td>\n",
       "      <td>770500.0</td>\n",
       "      <td>750000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1450000.0</td>\n",
       "      <td>1497500.0</td>\n",
       "      <td>1467000.0</td>\n",
       "      <td>1400000.0</td>\n",
       "      <td>1332500.0</td>\n",
       "      <td>1325000.0</td>\n",
       "      <td>1295000.0</td>\n",
       "      <td>1397500.0</td>\n",
       "      <td>1485000.0</td>\n",
       "      <td>1537500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>272450.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>269900.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>289900.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>284900.0</td>\n",
       "      <td>289000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>329900.0</td>\n",
       "      <td>359000.0</td>\n",
       "      <td>387400.0</td>\n",
       "      <td>345875.0</td>\n",
       "      <td>369450.0</td>\n",
       "      <td>372200.0</td>\n",
       "      <td>379000.0</td>\n",
       "      <td>393250.0</td>\n",
       "      <td>337400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>757500.0</td>\n",
       "      <td>725000.0</td>\n",
       "      <td>699000.0</td>\n",
       "      <td>699000.0</td>\n",
       "      <td>649000.0</td>\n",
       "      <td>695000.0</td>\n",
       "      <td>699000.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1486500.0</td>\n",
       "      <td>1470000.0</td>\n",
       "      <td>1422500.0</td>\n",
       "      <td>1445000.0</td>\n",
       "      <td>1380000.0</td>\n",
       "      <td>1405000.0</td>\n",
       "      <td>1399499.5</td>\n",
       "      <td>1425000.0</td>\n",
       "      <td>1425000.0</td>\n",
       "      <td>1425000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>204900.0</td>\n",
       "      <td>194900.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>194900.0</td>\n",
       "      <td>184900.0</td>\n",
       "      <td>179750.0</td>\n",
       "      <td>178500.0</td>\n",
       "      <td>176750.0</td>\n",
       "      <td>179700.0</td>\n",
       "      <td>169900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>204450.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>209500.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>199900.0</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>204450.0</td>\n",
       "      <td>209000.0</td>\n",
       "      <td>209950.0</td>\n",
       "      <td>219000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>1154880.0</td>\n",
       "      <td>1257500.0</td>\n",
       "      <td>1161440.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>1215000.0</td>\n",
       "      <td>1050000.0</td>\n",
       "      <td>995000.0</td>\n",
       "      <td>995000.0</td>\n",
       "      <td>1070000.0</td>\n",
       "      <td>1050000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1475000.0</td>\n",
       "      <td>1499000.0</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>1460000.0</td>\n",
       "      <td>1440000.0</td>\n",
       "      <td>1380000.0</td>\n",
       "      <td>1355000.0</td>\n",
       "      <td>1347500.0</td>\n",
       "      <td>1355000.0</td>\n",
       "      <td>1375000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3382 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              2010-01    2010-02    2010-03    2010-04    2010-05    2010-06  \\\n",
       "RegionName                                                                     \n",
       "99709        234950.0   240450.0   249950.0   252475.0   259450.0   254500.0   \n",
       "99705        210000.0   225950.0   225000.0   239900.0   244250.0   244950.0   \n",
       "99701        199900.0   179000.0   179500.0   182500.0   182500.0   184500.0   \n",
       "99669        300000.0   302500.0   320000.0   310000.0   299000.0   279000.0   \n",
       "99654        235000.0   230000.0   235000.0   235000.0   236000.0   238450.0   \n",
       "99645        237900.0   237500.0   237500.0   236250.0   239900.0   238000.0   \n",
       "99611        215000.0   215000.0   230000.0   225000.0   225000.0   219000.0   \n",
       "99603        257000.0   259000.0   260000.0   266750.0   275000.0   275000.0   \n",
       "99577        299000.0   299983.0   295000.0   309450.0   310000.0   324900.0   \n",
       "99516        498000.0   509900.0   489900.0   499900.0   499000.0   479900.0   \n",
       "99515        298250.0   299000.0   295000.0   295000.0   315000.0   310400.0   \n",
       "99507        225000.0   230000.0   230000.0   229950.0   249000.0   254950.0   \n",
       "99504        205450.0   214000.0   219900.0   219000.0   224950.0   224900.0   \n",
       "99502        261900.0   265000.0   262400.0   278900.0   277700.0   277700.0   \n",
       "99501        256500.0   254000.0   255900.0   224900.0   242000.0   230000.0   \n",
       "99403        215000.0   205000.0   199900.0   199500.0   195500.0   194500.0   \n",
       "99362        219000.0   219000.0   209500.0   209000.0   219000.0   219000.0   \n",
       "99353        239900.0   239900.0   239900.0   241950.0   244500.0   239900.0   \n",
       "99352        239900.0   231749.5   239900.0   236250.0   245000.0   248000.0   \n",
       "99338        276750.0   264900.0   264900.0   257000.0   254900.0   257500.0   \n",
       "99337        204900.0   199500.0   209900.0   199950.0   204950.0   209900.0   \n",
       "99336        150365.0   149900.0   153900.0   149900.0   155450.0   164900.0   \n",
       "99324        202300.0   204700.0   199000.0   188250.0   205000.0   199450.0   \n",
       "99301        175000.0   175000.0   177000.0   179700.0   175000.0   176900.0   \n",
       "99223        240000.0   229900.0   249000.0   246975.0   249995.0   249995.0   \n",
       "99218        238250.0   239450.0   239495.0   229900.0   234500.0   226475.0   \n",
       "99217        169900.0   169900.0   149900.0   154950.0   150000.0   150000.0   \n",
       "99216        189950.0   189600.0   189600.0   187950.0   187725.0   185000.0   \n",
       "99212        159000.0   159000.0   154000.0   150000.0   150000.0   153500.0   \n",
       "99208        217650.0   210000.0   219250.0   216000.0   210696.5   211745.0   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "1089         195000.0   199400.0   189900.0   194000.0   189850.0   179450.0   \n",
       "10804        749000.0   749000.0   749500.0   795900.0   779000.0   772000.0   \n",
       "1075         229900.0   239000.0   244900.0   247500.0   249900.0   249900.0   \n",
       "1069         185000.0   189700.0   189700.0   184900.0   184900.0   169900.0   \n",
       "10607        519000.0   499900.0   489000.0   464500.0   493450.0   499000.0   \n",
       "10598        439900.0   439900.0   439000.0   439900.0   446750.0   429900.0   \n",
       "10579        439900.0   439950.0   444000.0   449900.0   444500.0   429500.0   \n",
       "1057         216950.0   202500.0   209900.0   229900.0   209500.0   224900.0   \n",
       "10562        394495.0   419500.0   389000.0   392111.0   384222.0   367000.0   \n",
       "1056         237450.0   249000.0   239450.0   239450.0   239450.0   235000.0   \n",
       "10541        409450.0   399000.0   399900.0   399000.0   399900.0   399000.0   \n",
       "10512        299900.0   309000.0   299900.0   305000.0   309000.0   300000.0   \n",
       "10509        375000.0   365000.0   359499.5   359000.0   369900.0   359999.5   \n",
       "10506       1125000.0  1125000.0  1170000.0  1125000.0  1174500.0  1197500.0   \n",
       "10463        299990.0   299000.0   299000.0   285000.0   285000.0   279000.0   \n",
       "1040         139900.0   140000.0   149000.0   149900.0   149200.0   154500.0   \n",
       "10312        399900.0   409900.0   404000.0   399367.5   395000.0   389000.0   \n",
       "10308        402450.0   419000.0   419000.0   414900.0   399000.0   399700.0   \n",
       "10306        399900.0   405000.0   410000.0   409900.0   399000.0   399000.0   \n",
       "10305        384450.0   382450.0   380000.0   375000.0   375000.0   372450.0   \n",
       "1028         254900.0   257000.0   254900.0   245000.0   246450.0   239900.0   \n",
       "1027         235000.0   235000.0   245000.0   248750.0   248000.0   248500.0   \n",
       "1020         155950.0   169900.0   169900.0   172000.0   174900.0   169900.0   \n",
       "1013         149900.0   149900.0   149900.0   149900.0   147450.0   149900.0   \n",
       "1007         260850.0   270000.0   265000.0   259900.0   254900.0   254900.0   \n",
       "10025        775000.0   725000.0   762500.0   725000.0   732500.0   745000.0   \n",
       "1002         272450.0   269900.0   269900.0   289000.0   289000.0   289900.0   \n",
       "10016        757500.0   725000.0   699000.0   699000.0   649000.0   695000.0   \n",
       "1001         204900.0   194900.0   199900.0   194900.0   184900.0   179750.0   \n",
       "10005       1154880.0  1257500.0  1161440.0  1200000.0  1215000.0  1050000.0   \n",
       "\n",
       "              2010-07    2010-08    2010-09    2010-10    ...        2017-03  \\\n",
       "RegionName                                                ...                  \n",
       "99709        248500.0   249500.0   243000.0   246450.0    ...       199900.0   \n",
       "99705        240500.0   239900.0   239900.0   235000.0    ...       234900.0   \n",
       "99701        184950.0   198400.0   193500.0   191500.0    ...       179950.0   \n",
       "99669        273900.0   259000.0   259000.0   245000.0    ...       237500.0   \n",
       "99654        238000.0   236000.0   235000.0   225000.0    ...       285000.0   \n",
       "99645        237575.0   237500.0   239000.0   235000.0    ...       282500.0   \n",
       "99611        235000.0   235000.0   229999.0   234500.0    ...       224000.0   \n",
       "99603        272450.0   268000.0   269000.0   275000.0    ...       275000.0   \n",
       "99577        319400.0   313450.0   314950.0   310000.0    ...       350000.0   \n",
       "99516        484900.0   478950.0   474950.0   475000.0    ...       539000.0   \n",
       "99515        299900.0   314950.0   315750.0   299900.0    ...       345000.0   \n",
       "99507        240000.0   251750.0   255000.0   250000.0    ...       297000.0   \n",
       "99504        218900.0   219000.0   219900.0   209900.0    ...       245000.0   \n",
       "99502        269900.0   269900.0   279000.0   275000.0    ...       324000.0   \n",
       "99501        246000.0   237000.0   239000.0   234900.0    ...       259950.0   \n",
       "99403        189900.0   192200.0   199900.0   189900.0    ...       239000.0   \n",
       "99362        211350.0   214950.0   215000.0   219000.0    ...       229700.0   \n",
       "99353        238750.0   239900.0   239200.0   226450.0    ...       334077.5   \n",
       "99352        255000.0   255000.0   259900.0   259900.0    ...       341445.0   \n",
       "99338        269900.0   274900.0   287500.0   291250.0    ...       370450.0   \n",
       "99337        204950.0   209950.0   209950.0   199500.0    ...       267450.0   \n",
       "99336        165000.0   168250.0   166900.0   160000.0    ...       166313.0   \n",
       "99324        205000.0   195000.0   196350.0   199900.0    ...       230900.0   \n",
       "99301        179800.0   177950.0   179900.0   179900.0    ...       282120.0   \n",
       "99223        249500.0   239000.0   239450.0   231500.0    ...       281475.0   \n",
       "99218        222400.0   234900.0   239990.0   245000.0    ...       260000.0   \n",
       "99217        149900.0   149900.0   156900.0   149700.0    ...       174925.0   \n",
       "99216        183500.0   178325.0   174950.0   174900.0    ...       189950.0   \n",
       "99212        153000.0   150000.0   159900.0   155000.0    ...       172000.0   \n",
       "99208        217000.0   209900.0   214000.0   208995.0    ...       250000.0   \n",
       "...               ...        ...        ...        ...    ...            ...   \n",
       "1089         184800.0   179900.0   183500.0   183900.0    ...       195000.0   \n",
       "10804        759888.0   749000.0   708999.5   699000.0    ...       820000.0   \n",
       "1075         229950.0   224000.0   235900.0   229900.0    ...       240900.0   \n",
       "1069         175000.0   166200.0   189900.0   189900.0    ...       171500.0   \n",
       "10607        489450.0   465000.0   484500.0   475000.0    ...       499011.0   \n",
       "10598        429000.0   429450.0   425000.0   424900.0    ...       469000.0   \n",
       "10579        429000.0   419900.0   403950.0   399900.0    ...       399500.0   \n",
       "1057         224900.0   229900.0   225000.0   225000.0    ...       215000.0   \n",
       "10562        359111.0   349900.0   349900.0   342450.0    ...       399000.0   \n",
       "1056         229900.0   226000.0   224937.5   202450.0    ...       249900.0   \n",
       "10541        399000.0   399000.0   399900.0   399000.0    ...       349450.0   \n",
       "10512        299700.0   307000.0   311000.0   310500.0    ...       315000.0   \n",
       "10509        355000.0   354000.0   350000.0   365000.0    ...       339000.0   \n",
       "10506       1200000.0  1200000.0  1250000.0  1200000.0    ...      1597000.0   \n",
       "10463        299000.0   299495.0   299000.0   299000.0    ...       289000.0   \n",
       "1040         159900.0   159900.0   159900.0   149700.0    ...       170749.5   \n",
       "10312        392000.0   399000.0   398750.0   398500.0    ...       539450.0   \n",
       "10308        399900.0   396500.0   402499.5   394500.0    ...       524000.0   \n",
       "10306        394000.0   397000.0   399000.0   399000.0    ...       529000.0   \n",
       "10305        369900.0   369000.0   376000.0   375000.0    ...       525000.0   \n",
       "1028         239900.0   239800.0   239700.0   239900.0    ...       329900.0   \n",
       "1027         229900.0   229900.0   229000.0   229900.0    ...       249900.0   \n",
       "1020         164900.0   164900.0   164900.0   159900.0    ...       173850.0   \n",
       "1013         149950.0   159900.0   159900.0   154000.0    ...       169900.0   \n",
       "1007         250000.0   250000.0   249700.0   248950.0    ...       322400.0   \n",
       "10025        722000.0   745000.0   770500.0   750000.0    ...      1450000.0   \n",
       "1002         289000.0   289000.0   284900.0   289000.0    ...       329000.0   \n",
       "10016        699000.0   720000.0   650000.0   650000.0    ...      1486500.0   \n",
       "1001         178500.0   176750.0   179700.0   169900.0    ...       204450.0   \n",
       "10005        995000.0   995000.0  1070000.0  1050000.0    ...      1475000.0   \n",
       "\n",
       "              2017-04    2017-05    2017-06    2017-07    2017-08    2017-09  \\\n",
       "RegionName                                                                     \n",
       "99709        218700.0   217450.0   214900.0   213450.0   214950.0   214900.0   \n",
       "99705        234900.0   234900.0   234950.0   239325.0   235000.0   232000.0   \n",
       "99701        181250.0   180000.0   169000.0   169000.0   168997.5   169000.0   \n",
       "99669        254900.0   268900.0   267750.0   267250.0   266000.0   270000.0   \n",
       "99654        289900.0   295000.0   298000.0   295000.0   289900.0   289950.0   \n",
       "99645        279950.0   279900.0   286900.0   284950.0   289700.0   293000.0   \n",
       "99611        229000.0   229937.5   229950.0   230000.0   239000.0   239000.0   \n",
       "99603        290000.0   293750.0   299000.0   299450.0   315000.0   310000.0   \n",
       "99577        350000.0   361500.0   359950.0   369000.0   359850.0   360000.0   \n",
       "99516        539000.0   535000.0   522000.0   509900.0   504999.5   514950.0   \n",
       "99515        350000.0   349900.0   349000.0   349900.0   349900.0   349925.0   \n",
       "99507        305000.0   297000.0   296000.0   299900.0   296500.0   295750.0   \n",
       "99504        246200.0   249900.0   249500.0   258500.0   249900.0   245000.0   \n",
       "99502        325000.0   324900.0   320000.0   315900.0   325000.0   325000.0   \n",
       "99501        260000.0   274000.0   255000.0   264950.0   249900.0   260000.0   \n",
       "99403        249900.0   249900.0   235000.0   235000.0   234900.0   234750.0   \n",
       "99362        229350.0   254750.0   264450.0   259450.0   261500.0   269000.0   \n",
       "99353        329000.0   349900.0   349900.0   365000.0   365000.0   359950.0   \n",
       "99352        349900.0   356900.0   358900.0   355450.0   359000.0   360450.0   \n",
       "99338        371000.0   372450.0   409950.0   406950.0   392023.0   391775.0   \n",
       "99337        259950.0   269900.0   272500.0   283000.0   279900.0   279000.0   \n",
       "99336        164900.0   170000.0   174900.0   177375.0   176816.5   175156.5   \n",
       "99324        237000.0   249900.0   249900.0   246750.0   239000.0   250000.0   \n",
       "99301        291900.0   299900.0   284900.0   275000.0   284900.0   289900.0   \n",
       "99223        282499.5   289000.0   295000.0   287200.0   285129.0   280000.0   \n",
       "99218        269900.0   265000.0   269500.0   269949.5   255000.0   245000.0   \n",
       "99217        189900.0   199900.0   219900.0   205000.0   199900.0   182900.0   \n",
       "99216        193900.0   207500.0   209000.0   202000.0   193900.0   199500.0   \n",
       "99212        180000.0   182250.0   180000.0   184000.0   186950.0   175000.0   \n",
       "99208        264950.0   269900.0   269250.0   269900.0   266327.0   269750.0   \n",
       "...               ...        ...        ...        ...        ...        ...   \n",
       "1089         219900.0   205750.0   219900.0   219450.0   200000.0   210000.0   \n",
       "10804        849000.0   849999.0   880000.0   899000.0   862500.0   849500.0   \n",
       "1075         269900.0   229700.0   264900.0   249900.0   259900.0   252443.5   \n",
       "1069         174900.0   179900.0   182400.0   180700.0   184950.0   183700.0   \n",
       "10607        499021.0   499510.0   509950.0   499949.5   514450.0   521500.0   \n",
       "10598        475000.0   499000.0   471950.0   450000.0   440000.0   444500.0   \n",
       "10579        435000.0   432000.0   414000.0   429000.0   415000.0   415000.0   \n",
       "1057         215000.0   218450.0   249900.0   265000.0   259900.0   259900.0   \n",
       "10562        399999.0   399900.0   399222.0   399000.0   395000.0   380000.0   \n",
       "1056         257000.0   257000.0   249900.0   235000.0   249900.0   249900.0   \n",
       "10541        369000.0   400000.0   429950.0   450000.0   435000.0   425000.0   \n",
       "10512        328900.0   335000.0   339000.0   344990.0   339250.0   339500.0   \n",
       "10509        333900.0   331450.0   324950.0   326950.0   315000.0   324950.0   \n",
       "10506       1599000.0  1697000.0  1650000.0  1695000.0  1712500.0  1597000.0   \n",
       "10463        292200.0   295000.0   299000.0   296500.0   309000.0   299900.0   \n",
       "1040         179700.0   189900.0   179900.0   176900.0   174900.0   179950.0   \n",
       "10312        535000.0   549900.0   549900.0   554499.5   549000.0   549000.0   \n",
       "10308        549000.0   529000.0   529900.0   509000.0   544450.0   539397.0   \n",
       "10306        548500.0   549000.0   549000.0   568000.0   586500.0   574450.0   \n",
       "10305        525000.0   537000.0   549999.0   554450.0   549900.0   559500.0   \n",
       "1028         319900.0   309900.0   307400.0   292450.0   285241.0   279900.0   \n",
       "1027         249900.0   259900.0   259900.0   261500.0   254450.0   279000.0   \n",
       "1020         179000.0   179900.0   184650.0   184650.0   179900.0   184900.0   \n",
       "1013         169900.0   178500.0   179000.0   175000.0   172443.5   174900.0   \n",
       "1007         319900.0   336000.0   324900.0   324900.0   319900.0   297200.0   \n",
       "10025       1497500.0  1467000.0  1400000.0  1332500.0  1325000.0  1295000.0   \n",
       "1002         329900.0   359000.0   387400.0   345875.0   369450.0   372200.0   \n",
       "10016       1470000.0  1422500.0  1445000.0  1380000.0  1405000.0  1399499.5   \n",
       "1001         199900.0   209500.0   199900.0   199900.0   210000.0   204450.0   \n",
       "10005       1499000.0  1500000.0  1460000.0  1440000.0  1380000.0  1355000.0   \n",
       "\n",
       "              2017-10    2017-11    2017-12  \n",
       "RegionName                                   \n",
       "99709        196000.0   176650.0   191745.5  \n",
       "99705        232000.0   219900.0   219900.0  \n",
       "99701        150000.0   150000.0   164500.0  \n",
       "99669        275000.0   259000.0   259450.0  \n",
       "99654        289000.0   289450.0   293025.0  \n",
       "99645        289500.0   279900.0   283950.0  \n",
       "99611        239900.0   237450.0   236185.0  \n",
       "99603        299000.0   299000.0   287500.0  \n",
       "99577        365000.0   352900.0   367000.0  \n",
       "99516        499000.0   524950.0   521950.0  \n",
       "99515        371950.0   355000.0   359000.0  \n",
       "99507        292750.0   289000.0   279900.0  \n",
       "99504        242500.0   244500.0   239950.0  \n",
       "99502        328000.0   325000.0   316450.0  \n",
       "99501        250000.0   250000.0   254250.0  \n",
       "99403        236750.0   229000.0   229000.0  \n",
       "99362        269000.0   261950.0   268450.0  \n",
       "99353        359900.0   354900.0   354900.0  \n",
       "99352        358400.0   349974.5   349850.0  \n",
       "99338        394950.0   382422.0   402659.0  \n",
       "99337        274900.0   270499.5   269999.0  \n",
       "99336        183283.0   189900.0   185497.5  \n",
       "99324        255000.0   259000.0   270743.5  \n",
       "99301        292336.5   295000.0   299700.0  \n",
       "99223        279925.0   299900.0   289900.0  \n",
       "99218        247250.0   245000.0   269000.0  \n",
       "99217        179900.0   177000.0   179900.0  \n",
       "99216        214900.0   212000.0   208500.0  \n",
       "99212        189900.0   189000.0   179000.0  \n",
       "99208        276327.0   279332.0   280000.0  \n",
       "...               ...        ...        ...  \n",
       "1089         219900.0   210449.5   218700.0  \n",
       "10804        850000.0   829000.0   827000.0  \n",
       "1075         249900.0   253999.5   258000.0  \n",
       "1069         187500.0   204900.0   212400.0  \n",
       "10607        499000.0   499250.0   499000.0  \n",
       "10598        429900.0   449000.0   409000.0  \n",
       "10579        417450.0   399000.0   399000.0  \n",
       "1057         264950.0   262450.0   256450.0  \n",
       "10562        382500.0   399999.0   375000.0  \n",
       "1056         249900.0   239900.0   231700.0  \n",
       "10541        412200.0   419500.0   429900.0  \n",
       "10512        329000.0   336599.5   339900.0  \n",
       "10509        339500.0   344500.0   340000.0  \n",
       "10506       1495000.0  1495000.0  1595000.0  \n",
       "10463        305000.0   310000.0   320000.0  \n",
       "1040         179900.0   189900.0   199900.0  \n",
       "10312        559000.0   569750.0   559000.0  \n",
       "10308        529000.0   548944.0   539888.0  \n",
       "10306        568000.0   577449.0   579998.0  \n",
       "10305        549000.0   579000.0   589999.0  \n",
       "1028         297000.0   329900.0   326750.0  \n",
       "1027         285000.0   284900.0   269700.0  \n",
       "1020         179900.0   179900.0   187350.0  \n",
       "1013         171249.5   169900.0   164900.0  \n",
       "1007         289000.0   291200.0   295000.0  \n",
       "10025       1397500.0  1485000.0  1537500.0  \n",
       "1002         379000.0   393250.0   337400.0  \n",
       "10016       1425000.0  1425000.0  1425000.0  \n",
       "1001         209000.0   209950.0   219000.0  \n",
       "10005       1347500.0  1355000.0  1375000.0  \n",
       "\n",
       "[3382 rows x 96 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_list[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sixteen_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./zip_2/Zip_Zri_AllHomesPlusMultifamily_Summary.csv'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_file_names[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Docs that don't need feature engineering. They are already aggregates\n",
    "pure_features = [0, 71, 75, 77, 79, 81]\n",
    "#sixteen_doc_features[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>1996-07</th>\n",
       "      <th>1996-08</th>\n",
       "      <th>1996-09</th>\n",
       "      <th>1996-10</th>\n",
       "      <th>1996-11</th>\n",
       "      <th>1996-12</th>\n",
       "      <th>1997-01</th>\n",
       "      <th>...</th>\n",
       "      <th>2013-04</th>\n",
       "      <th>2013-05</th>\n",
       "      <th>2013-06</th>\n",
       "      <th>2013-07</th>\n",
       "      <th>2013-08</th>\n",
       "      <th>2013-09</th>\n",
       "      <th>2013-10</th>\n",
       "      <th>2013-11</th>\n",
       "      <th>2013-12</th>\n",
       "      <th>2014-01</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99901</th>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99362</th>\n",
       "      <td>73.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99354</th>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99353</th>\n",
       "      <td>67.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99352</th>\n",
       "      <td>71.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1996-04  1996-05  1996-06  1996-07  1996-08  1996-09  1996-10  \\\n",
       "RegionName                                                                  \n",
       "99901         162.0    162.0    162.0    162.0    162.0    162.0    162.0   \n",
       "99362          73.0     74.0     74.0     75.0     76.0     76.0     76.0   \n",
       "99354          71.0     72.0     73.0     73.0     73.0     72.0     72.0   \n",
       "99353          67.0     67.0     66.0     66.0     66.0     66.0     65.0   \n",
       "99352          71.0     71.0     72.0     72.0     73.0     72.0     73.0   \n",
       "\n",
       "            1996-11  1996-12  1997-01   ...     2013-04  2013-05  2013-06  \\\n",
       "RegionName                              ...                                 \n",
       "99901         162.0    162.0    162.0   ...       161.0    162.0    162.0   \n",
       "99362          76.0     75.0     75.0   ...       127.0    127.0    127.0   \n",
       "99354          72.0     72.0     73.0   ...       114.0    114.0    114.0   \n",
       "99353          66.0     65.0     66.0   ...       112.0    112.0    112.0   \n",
       "99352          73.0     73.0     73.0   ...       114.0    115.0    115.0   \n",
       "\n",
       "            2013-07  2013-08  2013-09  2013-10  2013-11  2013-12  2014-01  \n",
       "RegionName                                                                 \n",
       "99901         162.0    162.0    162.0    162.0    161.0    161.0    161.0  \n",
       "99362         128.0    128.0    129.0    129.0    129.0    130.0    130.0  \n",
       "99354         114.0    115.0    115.0    115.0    115.0    115.0    115.0  \n",
       "99353         113.0    114.0    114.0    113.0    114.0    114.0    114.0  \n",
       "99352         115.0    115.0    115.0    115.0    116.0    116.0    116.0  \n",
       "\n",
       "[5 rows x 214 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_df[52].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(sixteen_doc_features)\n",
    "for num in range(len(sixteen_doc_features)):\n",
    "    sixteen_doc_features[num].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging all dfs into one. Merging on indez which is zipcode. I was hoping for inner join but it looks like\n",
    "# There are many zipcodes that only exist in certain dfs. Im hoping that reducing them to metro areas will fix this\n",
    "def merge_dataframes(feature_df_list):\n",
    "    df_1 = feature_df_list[0]\n",
    "    for df in feature_df_list[1:]:\n",
    "        df_1 = pd.merge(df_1, df, left_index=True, right_index=True, how='inner')\n",
    "    \n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_16 = merge_dataframes(sixteen_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_16.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2011_mean_x</th>\n",
       "      <th>2011_std_x</th>\n",
       "      <th>2011_min_x</th>\n",
       "      <th>2011_max_x</th>\n",
       "      <th>2011_swing_x</th>\n",
       "      <th>2011_change_x</th>\n",
       "      <th>2012_mean_x</th>\n",
       "      <th>2012_std_x</th>\n",
       "      <th>2012_min_x</th>\n",
       "      <th>2012_max_x</th>\n",
       "      <th>...</th>\n",
       "      <th>2015_yoy_y</th>\n",
       "      <th>2015_gain_y</th>\n",
       "      <th>2016_mean_y</th>\n",
       "      <th>2016_std_y</th>\n",
       "      <th>2016_min_y</th>\n",
       "      <th>2016_max_y</th>\n",
       "      <th>2016_swing_y</th>\n",
       "      <th>2016_change_y</th>\n",
       "      <th>2016_yoy_y</th>\n",
       "      <th>2016_gain_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99223</th>\n",
       "      <td>17.470575</td>\n",
       "      <td>2.151552</td>\n",
       "      <td>13.157744</td>\n",
       "      <td>19.884243</td>\n",
       "      <td>6.726499</td>\n",
       "      <td>0.772589</td>\n",
       "      <td>14.925278</td>\n",
       "      <td>2.807599</td>\n",
       "      <td>10.810474</td>\n",
       "      <td>18.942333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.052054</td>\n",
       "      <td>1</td>\n",
       "      <td>0.856167</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.050296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99218</th>\n",
       "      <td>17.800899</td>\n",
       "      <td>2.953572</td>\n",
       "      <td>11.325792</td>\n",
       "      <td>21.163550</td>\n",
       "      <td>9.837757</td>\n",
       "      <td>2.571792</td>\n",
       "      <td>13.344549</td>\n",
       "      <td>4.715423</td>\n",
       "      <td>2.777553</td>\n",
       "      <td>18.333317</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031048</td>\n",
       "      <td>1</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>0.015843</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.057409</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99217</th>\n",
       "      <td>16.730506</td>\n",
       "      <td>2.461923</td>\n",
       "      <td>11.315092</td>\n",
       "      <td>20.255591</td>\n",
       "      <td>8.940499</td>\n",
       "      <td>-2.257172</td>\n",
       "      <td>14.715073</td>\n",
       "      <td>2.405299</td>\n",
       "      <td>11.693415</td>\n",
       "      <td>20.439508</td>\n",
       "      <td>...</td>\n",
       "      <td>1.057155</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.011817</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>1.035182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99212</th>\n",
       "      <td>15.310737</td>\n",
       "      <td>2.952748</td>\n",
       "      <td>11.290826</td>\n",
       "      <td>20.327481</td>\n",
       "      <td>9.036655</td>\n",
       "      <td>0.120253</td>\n",
       "      <td>14.234445</td>\n",
       "      <td>3.086468</td>\n",
       "      <td>8.897363</td>\n",
       "      <td>21.014987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.048559</td>\n",
       "      <td>1</td>\n",
       "      <td>0.905833</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.044389</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99208</th>\n",
       "      <td>12.623406</td>\n",
       "      <td>1.774491</td>\n",
       "      <td>9.810553</td>\n",
       "      <td>15.617595</td>\n",
       "      <td>5.807042</td>\n",
       "      <td>1.675172</td>\n",
       "      <td>13.064704</td>\n",
       "      <td>1.348408</td>\n",
       "      <td>10.896670</td>\n",
       "      <td>15.656514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850667</td>\n",
       "      <td>0.014853</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.048049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            2011_mean_x  2011_std_x  2011_min_x  2011_max_x  2011_swing_x  \\\n",
       "RegionName                                                                  \n",
       "99223         17.470575    2.151552   13.157744   19.884243      6.726499   \n",
       "99218         17.800899    2.953572   11.325792   21.163550      9.837757   \n",
       "99217         16.730506    2.461923   11.315092   20.255591      8.940499   \n",
       "99212         15.310737    2.952748   11.290826   20.327481      9.036655   \n",
       "99208         12.623406    1.774491    9.810553   15.617595      5.807042   \n",
       "\n",
       "            2011_change_x  2012_mean_x  2012_std_x  2012_min_x  2012_max_x  \\\n",
       "RegionName                                                                   \n",
       "99223            0.772589    14.925278    2.807599   10.810474   18.942333   \n",
       "99218            2.571792    13.344549    4.715423    2.777553   18.333317   \n",
       "99217           -2.257172    14.715073    2.405299   11.693415   20.439508   \n",
       "99212            0.120253    14.234445    3.086468    8.897363   21.014987   \n",
       "99208            1.675172    13.064704    1.348408   10.896670   15.656514   \n",
       "\n",
       "               ...       2015_yoy_y  2015_gain_y  2016_mean_y  2016_std_y  \\\n",
       "RegionName     ...                                                          \n",
       "99223          ...         1.052054            1     0.856167    0.016656   \n",
       "99218          ...         1.031048            1     0.813500    0.015843   \n",
       "99217          ...         1.057155            1     0.868000    0.011817   \n",
       "99212          ...         1.048559            1     0.905833    0.013306   \n",
       "99208          ...         1.040598            1     0.850667    0.014853   \n",
       "\n",
       "            2016_min_y  2016_max_y  2016_swing_y  2016_change_y  2016_yoy_y  \\\n",
       "RegionName                                                                    \n",
       "99223            0.826       0.874         0.048          0.048    1.050296   \n",
       "99218            0.786       0.834         0.048          0.048    1.057409   \n",
       "99217            0.846       0.882         0.036          0.036    1.035182   \n",
       "99212            0.878       0.918         0.040          0.040    1.044389   \n",
       "99208            0.824       0.866         0.042          0.042    1.048049   \n",
       "\n",
       "            2016_gain_y  \n",
       "RegionName               \n",
       "99223                 1  \n",
       "99218                 1  \n",
       "99217                 1  \n",
       "99212                 1  \n",
       "99208                 1  \n",
       "\n",
       "[5 rows x 552 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_yelp_business' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-95b2cd11d622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_yelp_business\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_yelp_business\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'postal_code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_yelp_business\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_yelp_business\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'postal_code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_yelp_business' is not defined"
     ]
    }
   ],
   "source": [
    "df_yelp_business = df_yelp_business.sort_values('postal_code',ascending=False)\n",
    "df_yelp_business = df_yelp_business.set_index('postal_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_yelp_business_merge = pd.DataFrame()\n",
    "df_yelp_business_merge['review_count'] = df_yelp_business['review_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postal_code</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>YO22 5LY</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YO22 5AL</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YO22 4RG</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YO22 4NT</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YO22 4JT</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             review_count\n",
       "postal_code              \n",
       "YO22 5LY                4\n",
       "YO22 5AL                4\n",
       "YO22 4RG                3\n",
       "YO22 4NT                3\n",
       "YO22 4JT               15"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_count</th>\n",
       "      <th>zeroes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review_count, zeroes]\n",
       "Index: []"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp_business_merge.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['99223', '99218', '99217', '99212', '99208', '99207', '99206', '99205',\n",
       "       '99203', '99202',\n",
       "       ...\n",
       "       '1069', '1057', '1056', '1040', '1028', '1027', '1020', '1013', '1002',\n",
       "       '1001'],\n",
       "      dtype='object', name='RegionName', length=1048)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_yelp_business_merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-fa99d6b1b84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merging the Zillow data and the yelp zipcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_yelp_business_merge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_yelp_business_merge' is not defined"
     ]
    }
   ],
   "source": [
    "# Merging the Zillow data and the yelp zipcodes \n",
    "df_2 = pd.merge(X_16, df_yelp_business_merge, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-c1afe4c5c852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2' is not defined"
     ]
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_list_swing(good_range, df_list, year_list, month_list):\n",
    "    feature_df_list = []\n",
    "    bugs = []\n",
    "    for num in good_range:\n",
    "        for year in year_list:\n",
    "            for month in month_list:\n",
    "                future_time = month + timedelta(days=6*31)\n",
    "                df = df_list[num]\n",
    "                df = df.loc[:, '2011-01':year+\"-\"+month]\n",
    "                features = pd.DataFrame()\n",
    "                for i, year in enumerate(year_list):\n",
    "                    try:\n",
    "                        mean = df.loc[:, year + '-01': year + '-12'].mean(axis=1)\n",
    "                        features[year + '_mean'] = mean\n",
    "                        std = df.loc[:, year + '-01': year + '-12'].std(axis=1)\n",
    "                        features[year + '_std'] = std\n",
    "                        mn = df.loc[:, year + '-01': year + '-12'].min(axis=1)\n",
    "                        features[year + '_min'] = mn\n",
    "                        mx = df.loc[:, year + '-01': year + '-12'].max(axis=1)\n",
    "                        features[year + '_max'] = mx\n",
    "                        features[year + '_swing'] = mx - mn\n",
    "                        change = df[year + '-12'] - df[year + '-01']\n",
    "                        features[year + '_change'] = change\n",
    "                        yoy = features[year + '_mean'] / df.loc[:, year_list[i - 1] + '-01': year_list[i - 1] + '-12'].mean(axis=1)\n",
    "                        features[year + '_yoy'] = yoy\n",
    "                        features[year + '_gain'] = np.where(features[year + '_yoy']>1, 1, 0)\n",
    "                        mean_yoy = features[year + '_yoy'].mean()\n",
    "                        features[year + '_yoy_pos'] = np.where(features[year + '_yoy']>mean_yoy, 1, 0)\n",
    "                        big_yoy = features[year + '_yoy'].std() + mean_yoy\n",
    "                        features[year + '_yoy_big'] = np.where(features[year + '_yoy']>big_yoy, 1, 0)\n",
    "                        features[year + '_yoy_neg'] = np.where(features[year + '_yoy']<mean_yoy, 1, 0)\n",
    "                        big_loss = mean_yoy - features[year + '_yoy'].std() \n",
    "                        features[year + '_yoy_loss_big'] = np.where(features[year + '_yoy']<big_loss, 1, 0)\n",
    "                        mean_swing = features[year + '_swing'].mean()\n",
    "                        features[year + '_swing_pos'] = np.where(features[year + '_swing']>mean_swing, 1, 0)\n",
    "                        big_swing = features[year + '_swing'].std() + mean_swing\n",
    "                        features[year + '_swing_big'] = np.where(features[year + '_swing']>big_swing, 1, 0)\n",
    "                        features[year + '_swing_neg'] = np.where(features[year + '_swing']<mean_swing, 1, 0)\n",
    "                        swing_big_loss = mean_swing - features[year + '_swing'].std() \n",
    "                        features[year + '_swing_loss_big'] = np.where(features[year + '_swing']<swing_big_loss, 1, 0)\n",
    "                        feature_df_list.append(features)\n",
    "                    except:\n",
    "                        bugs.append(num)\n",
    "    return feature_df_list, bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(df, past_time_string, now_string):\n",
    "    #df.dropna(inplace=True)\n",
    "    features = pd.DataFrame()\n",
    "    mean = df.loc[:, past_time_string : now_string].mean(axis=1)\n",
    "    features['mean'] = mean\n",
    "    std = df.loc[:, past_time_string : now_string].std(axis=1)\n",
    "    features['std'] = std\n",
    "    mn = df.loc[:, past_time_string : now_string].min(axis=1)\n",
    "    features['min'] = mn\n",
    "    mx = df.loc[:, past_time_string : now_string].max(axis=1)\n",
    "    features['max'] = mx\n",
    "    features['swing'] = mx - mn\n",
    "    change = df[now_string] - df[past_time_string]\n",
    "    features['change'] = change\n",
    "    #features.dropna(inplace=True)\n",
    "    #features = features.set_index(df.index)\n",
    "    #print(features.iloc[1, :])\n",
    "    return features\n",
    "    \n",
    "# List of data frames only on one now_time\n",
    "def make_modeling_data(df_list, df_for_target, now_string):\n",
    "    df_one = pd.DataFrame()\n",
    "    now_time = pd.to_datetime(now_string)\n",
    "    now_value = df_for_target[now_string]\n",
    "    #future_time = now_time + timedelta(days=6*31)\n",
    "    #future_time_string= future_time.strftime(\"%Y-%m\")\n",
    "    #future_value = df_for_target[future_time_string]\n",
    "    #target = future_value/now_value\n",
    "    \n",
    "    past_time = now_time - timedelta(days=6*31)\n",
    "    past_time_string= past_time.strftime(\"%Y-%m\")\n",
    "    df_to_use_for_features_org= df_list[0].loc[:, :now_string]\n",
    "    features_org = make_features(df_to_use_for_features_org, past_time_string, now_string)\n",
    "    df_one = pd.merge(df_one, features_org, left_index=True, right_index=True, how = 'right')\n",
    "    for i, df in enumerate(df_list[1:]):\n",
    "        ind = str(i)\n",
    "        columns = df.columns\n",
    "        if '2014-01' in columns and '2015-01' in columns and '2016-01' in columns and '2017-01' in columns:\n",
    "            df_to_use_for_features= df.loc[:, :now_string]\n",
    "            features = make_features(df_to_use_for_features, past_time_string, now_string)\n",
    "            df_one = df_one.append(features)\n",
    "            \n",
    "    #now_time = pd.to_datetime(now_string)\n",
    "    #now_value = df_for_target[now_string]\n",
    "    future_time = now_time + timedelta(days=6*31)\n",
    "    future_time_string= future_time.strftime(\"%Y-%m\")\n",
    "    future_value = df_for_target[future_time_string]\n",
    "    target = future_value/now_value\n",
    "\n",
    "    \n",
    "    return df_one, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_features, test_targets = make_modeling_data(final_data_list, target_data, \"2017-01\")\n",
    "train_features, train_targets = make_modeling_data(final_data_list, target_data, \"2016-07\")\n",
    "#df = useful_df[num]\n",
    "#columns = df.columns\n",
    "for year in [\"2015\",\"2016\"]:\n",
    "    for month in [\"01\",\"07\"]:\n",
    "            new_time = year+\"-\"+month\n",
    "            #if '2014-01' in columns and '2015-01' in columns and '2016-01' in columns and '2017-01' in columns:\n",
    "                 #   extra_train_features, extra_train_targets = make_modeling_data(df, new_time)\n",
    "                  #  train_features = train_features.append(extra_train_features)\n",
    "                   # train_targets = train_targets.append(extra_train_targets)\n",
    "            extra_train_features, extra_train_targets = make_modeling_data(final_data_list, target_data, new_time)\n",
    "            extra_train_features.dropna(inplace=True)\n",
    "            train_features = train_features.append(extra_train_features)\n",
    "            train_targets = train_targets.append(extra_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>swing</th>\n",
       "      <th>change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RegionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98004</th>\n",
       "      <td>4.26875</td>\n",
       "      <td>1.606686</td>\n",
       "      <td>2.18</td>\n",
       "      <td>6.68</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98003</th>\n",
       "      <td>8.71375</td>\n",
       "      <td>6.302820</td>\n",
       "      <td>0.92</td>\n",
       "      <td>14.96</td>\n",
       "      <td>14.04</td>\n",
       "      <td>-9.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98002</th>\n",
       "      <td>2.52750</td>\n",
       "      <td>1.294524</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.29</td>\n",
       "      <td>3.64</td>\n",
       "      <td>-2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98001</th>\n",
       "      <td>1.15250</td>\n",
       "      <td>0.282628</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97760</th>\n",
       "      <td>13.54375</td>\n",
       "      <td>4.165254</td>\n",
       "      <td>6.89</td>\n",
       "      <td>18.71</td>\n",
       "      <td>11.82</td>\n",
       "      <td>-10.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97759</th>\n",
       "      <td>14.60500</td>\n",
       "      <td>3.318963</td>\n",
       "      <td>11.36</td>\n",
       "      <td>20.34</td>\n",
       "      <td>8.98</td>\n",
       "      <td>-3.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97756</th>\n",
       "      <td>2.45750</td>\n",
       "      <td>0.731413</td>\n",
       "      <td>1.49</td>\n",
       "      <td>3.56</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97739</th>\n",
       "      <td>6.02250</td>\n",
       "      <td>1.791940</td>\n",
       "      <td>3.43</td>\n",
       "      <td>8.51</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97707</th>\n",
       "      <td>10.20500</td>\n",
       "      <td>3.031963</td>\n",
       "      <td>6.47</td>\n",
       "      <td>14.38</td>\n",
       "      <td>7.91</td>\n",
       "      <td>7.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97702</th>\n",
       "      <td>3.63500</td>\n",
       "      <td>1.081533</td>\n",
       "      <td>2.16</td>\n",
       "      <td>4.78</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97701</th>\n",
       "      <td>4.31125</td>\n",
       "      <td>1.046620</td>\n",
       "      <td>3.59</td>\n",
       "      <td>6.36</td>\n",
       "      <td>2.77</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97544</th>\n",
       "      <td>20.34375</td>\n",
       "      <td>14.159076</td>\n",
       "      <td>4.30</td>\n",
       "      <td>38.54</td>\n",
       "      <td>34.24</td>\n",
       "      <td>-34.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97540</th>\n",
       "      <td>2.88375</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>2.13</td>\n",
       "      <td>3.57</td>\n",
       "      <td>1.44</td>\n",
       "      <td>-0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97538</th>\n",
       "      <td>8.00375</td>\n",
       "      <td>4.392955</td>\n",
       "      <td>3.49</td>\n",
       "      <td>15.17</td>\n",
       "      <td>11.68</td>\n",
       "      <td>-11.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97535</th>\n",
       "      <td>3.88750</td>\n",
       "      <td>0.899901</td>\n",
       "      <td>2.53</td>\n",
       "      <td>4.90</td>\n",
       "      <td>2.37</td>\n",
       "      <td>-1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97532</th>\n",
       "      <td>10.21625</td>\n",
       "      <td>1.359075</td>\n",
       "      <td>9.02</td>\n",
       "      <td>12.95</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97530</th>\n",
       "      <td>7.17625</td>\n",
       "      <td>0.767630</td>\n",
       "      <td>6.36</td>\n",
       "      <td>8.42</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97527</th>\n",
       "      <td>8.21375</td>\n",
       "      <td>1.784744</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.11</td>\n",
       "      <td>4.06</td>\n",
       "      <td>-3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97526</th>\n",
       "      <td>7.42000</td>\n",
       "      <td>0.895864</td>\n",
       "      <td>5.86</td>\n",
       "      <td>8.29</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97524</th>\n",
       "      <td>3.43500</td>\n",
       "      <td>0.327632</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.01</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97520</th>\n",
       "      <td>8.31250</td>\n",
       "      <td>1.685770</td>\n",
       "      <td>5.73</td>\n",
       "      <td>10.80</td>\n",
       "      <td>5.07</td>\n",
       "      <td>-5.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97504</th>\n",
       "      <td>4.25125</td>\n",
       "      <td>0.979248</td>\n",
       "      <td>3.04</td>\n",
       "      <td>5.70</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97503</th>\n",
       "      <td>1.26875</td>\n",
       "      <td>0.424514</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97502</th>\n",
       "      <td>2.65625</td>\n",
       "      <td>0.850982</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.25</td>\n",
       "      <td>-2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97501</th>\n",
       "      <td>3.16250</td>\n",
       "      <td>0.435029</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.03</td>\n",
       "      <td>1.46</td>\n",
       "      <td>-1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97496</th>\n",
       "      <td>11.11500</td>\n",
       "      <td>2.884783</td>\n",
       "      <td>7.08</td>\n",
       "      <td>15.14</td>\n",
       "      <td>8.06</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97487</th>\n",
       "      <td>4.51500</td>\n",
       "      <td>2.027461</td>\n",
       "      <td>2.04</td>\n",
       "      <td>6.79</td>\n",
       "      <td>4.75</td>\n",
       "      <td>-3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97479</th>\n",
       "      <td>12.70500</td>\n",
       "      <td>2.674349</td>\n",
       "      <td>9.38</td>\n",
       "      <td>16.76</td>\n",
       "      <td>7.38</td>\n",
       "      <td>-0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97478</th>\n",
       "      <td>4.10625</td>\n",
       "      <td>0.965711</td>\n",
       "      <td>2.92</td>\n",
       "      <td>5.72</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-2.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97477</th>\n",
       "      <td>3.35250</td>\n",
       "      <td>0.586070</td>\n",
       "      <td>2.15</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.93</td>\n",
       "      <td>-1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97358</th>\n",
       "      <td>8.61125</td>\n",
       "      <td>5.564537</td>\n",
       "      <td>3.03</td>\n",
       "      <td>18.52</td>\n",
       "      <td>15.49</td>\n",
       "      <td>-15.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97355</th>\n",
       "      <td>6.14000</td>\n",
       "      <td>0.448107</td>\n",
       "      <td>5.46</td>\n",
       "      <td>6.73</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97352</th>\n",
       "      <td>8.02625</td>\n",
       "      <td>2.805367</td>\n",
       "      <td>3.29</td>\n",
       "      <td>11.34</td>\n",
       "      <td>8.05</td>\n",
       "      <td>-7.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97351</th>\n",
       "      <td>8.03500</td>\n",
       "      <td>0.914674</td>\n",
       "      <td>6.99</td>\n",
       "      <td>9.87</td>\n",
       "      <td>2.88</td>\n",
       "      <td>-0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97341</th>\n",
       "      <td>18.88125</td>\n",
       "      <td>1.888601</td>\n",
       "      <td>15.54</td>\n",
       "      <td>21.65</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97338</th>\n",
       "      <td>4.56625</td>\n",
       "      <td>0.690402</td>\n",
       "      <td>3.69</td>\n",
       "      <td>5.78</td>\n",
       "      <td>2.09</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97333</th>\n",
       "      <td>4.19375</td>\n",
       "      <td>0.416960</td>\n",
       "      <td>3.39</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.46</td>\n",
       "      <td>-0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97330</th>\n",
       "      <td>3.64625</td>\n",
       "      <td>0.360671</td>\n",
       "      <td>3.06</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97327</th>\n",
       "      <td>5.08500</td>\n",
       "      <td>1.181706</td>\n",
       "      <td>3.31</td>\n",
       "      <td>6.93</td>\n",
       "      <td>3.62</td>\n",
       "      <td>-1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97325</th>\n",
       "      <td>5.98875</td>\n",
       "      <td>1.540329</td>\n",
       "      <td>3.90</td>\n",
       "      <td>8.55</td>\n",
       "      <td>4.65</td>\n",
       "      <td>-4.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97322</th>\n",
       "      <td>1.89000</td>\n",
       "      <td>0.391371</td>\n",
       "      <td>1.39</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97321</th>\n",
       "      <td>3.25000</td>\n",
       "      <td>0.924461</td>\n",
       "      <td>1.85</td>\n",
       "      <td>4.55</td>\n",
       "      <td>2.70</td>\n",
       "      <td>-2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97317</th>\n",
       "      <td>4.34500</td>\n",
       "      <td>2.409179</td>\n",
       "      <td>1.62</td>\n",
       "      <td>8.29</td>\n",
       "      <td>6.67</td>\n",
       "      <td>-6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97306</th>\n",
       "      <td>3.25000</td>\n",
       "      <td>1.198046</td>\n",
       "      <td>1.93</td>\n",
       "      <td>5.40</td>\n",
       "      <td>3.47</td>\n",
       "      <td>-3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97305</th>\n",
       "      <td>2.53000</td>\n",
       "      <td>1.700193</td>\n",
       "      <td>0.56</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.44</td>\n",
       "      <td>-4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97304</th>\n",
       "      <td>1.85500</td>\n",
       "      <td>0.554205</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.63</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97303</th>\n",
       "      <td>2.47875</td>\n",
       "      <td>1.226592</td>\n",
       "      <td>0.86</td>\n",
       "      <td>4.17</td>\n",
       "      <td>3.31</td>\n",
       "      <td>-3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97302</th>\n",
       "      <td>3.17000</td>\n",
       "      <td>1.931439</td>\n",
       "      <td>1.26</td>\n",
       "      <td>6.92</td>\n",
       "      <td>5.66</td>\n",
       "      <td>-5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97301</th>\n",
       "      <td>5.38500</td>\n",
       "      <td>3.427048</td>\n",
       "      <td>1.26</td>\n",
       "      <td>9.98</td>\n",
       "      <td>8.72</td>\n",
       "      <td>-8.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97267</th>\n",
       "      <td>1.17125</td>\n",
       "      <td>0.573297</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.37</td>\n",
       "      <td>-1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97266</th>\n",
       "      <td>1.34125</td>\n",
       "      <td>0.314254</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.98</td>\n",
       "      <td>-0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97239</th>\n",
       "      <td>2.43125</td>\n",
       "      <td>0.866873</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.92</td>\n",
       "      <td>-1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97236</th>\n",
       "      <td>1.77375</td>\n",
       "      <td>0.591679</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97233</th>\n",
       "      <td>1.40375</td>\n",
       "      <td>0.765860</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.98</td>\n",
       "      <td>-1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97232</th>\n",
       "      <td>2.88250</td>\n",
       "      <td>1.888391</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.99</td>\n",
       "      <td>4.33</td>\n",
       "      <td>-4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97231</th>\n",
       "      <td>5.83625</td>\n",
       "      <td>3.076812</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9.84</td>\n",
       "      <td>7.62</td>\n",
       "      <td>-7.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97230</th>\n",
       "      <td>1.35250</td>\n",
       "      <td>0.839145</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97229</th>\n",
       "      <td>1.47875</td>\n",
       "      <td>0.609225</td>\n",
       "      <td>0.69</td>\n",
       "      <td>2.09</td>\n",
       "      <td>1.40</td>\n",
       "      <td>-1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97227</th>\n",
       "      <td>0.84250</td>\n",
       "      <td>0.680184</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97225</th>\n",
       "      <td>1.22000</td>\n",
       "      <td>0.344051</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.06</td>\n",
       "      <td>-1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean        std    min    max  swing  change\n",
       "RegionName                                                  \n",
       "98004        4.26875   1.606686   2.18   6.68   4.50    2.19\n",
       "98003        8.71375   6.302820   0.92  14.96  14.04   -9.52\n",
       "98002        2.52750   1.294524   0.65   4.29   3.64   -2.54\n",
       "98001        1.15250   0.282628   0.78   1.75   0.97    0.04\n",
       "97760       13.54375   4.165254   6.89  18.71  11.82  -10.40\n",
       "97759       14.60500   3.318963  11.36  20.34   8.98   -3.18\n",
       "97756        2.45750   0.731413   1.49   3.56   2.07    1.30\n",
       "97739        6.02250   1.791940   3.43   8.51   5.08    5.08\n",
       "97707       10.20500   3.031963   6.47  14.38   7.91    7.24\n",
       "97702        3.63500   1.081533   2.16   4.78   2.62    2.49\n",
       "97701        4.31125   1.046620   3.59   6.36   2.77    0.23\n",
       "97544       20.34375  14.159076   4.30  38.54  34.24  -34.24\n",
       "97540        2.88375   0.499912   2.13   3.57   1.44   -0.62\n",
       "97538        8.00375   4.392955   3.49  15.17  11.68  -11.68\n",
       "97535        3.88750   0.899901   2.53   4.90   2.37   -1.96\n",
       "97532       10.21625   1.359075   9.02  12.95   3.93    0.11\n",
       "97530        7.17625   0.767630   6.36   8.42   2.06    0.80\n",
       "97527        8.21375   1.784744   6.05  10.11   4.06   -3.54\n",
       "97526        7.42000   0.895864   5.86   8.29   2.43    2.11\n",
       "97524        3.43500   0.327632   2.94   4.01   1.07   -1.07\n",
       "97520        8.31250   1.685770   5.73  10.80   5.07   -5.07\n",
       "97504        4.25125   0.979248   3.04   5.70   2.66   -2.50\n",
       "97503        1.26875   0.424514   0.69   1.89   1.20   -0.86\n",
       "97502        2.65625   0.850982   1.55   3.80   2.25   -2.25\n",
       "97501        3.16250   0.435029   2.57   4.03   1.46   -1.46\n",
       "97496       11.11500   2.884783   7.08  15.14   8.06   -1.50\n",
       "97487        4.51500   2.027461   2.04   6.79   4.75   -3.89\n",
       "97479       12.70500   2.674349   9.38  16.76   7.38   -0.59\n",
       "97478        4.10625   0.965711   2.92   5.72   2.80   -2.80\n",
       "97477        3.35250   0.586070   2.15   4.08   1.93   -1.31\n",
       "...              ...        ...    ...    ...    ...     ...\n",
       "97358        8.61125   5.564537   3.03  18.52  15.49  -15.49\n",
       "97355        6.14000   0.448107   5.46   6.73   1.27    0.74\n",
       "97352        8.02625   2.805367   3.29  11.34   8.05   -7.10\n",
       "97351        8.03500   0.914674   6.99   9.87   2.88   -0.66\n",
       "97341       18.88125   1.888601  15.54  21.65   6.11   -5.50\n",
       "97338        4.56625   0.690402   3.69   5.78   2.09   -0.50\n",
       "97333        4.19375   0.416960   3.39   4.85   1.46   -0.79\n",
       "97330        3.64625   0.360671   3.06   4.08   1.02   -0.54\n",
       "97327        5.08500   1.181706   3.31   6.93   3.62   -1.02\n",
       "97325        5.98875   1.540329   3.90   8.55   4.65   -4.65\n",
       "97322        1.89000   0.391371   1.39   2.31   0.92   -0.78\n",
       "97321        3.25000   0.924461   1.85   4.55   2.70   -2.70\n",
       "97317        4.34500   2.409179   1.62   8.29   6.67   -6.67\n",
       "97306        3.25000   1.198046   1.93   5.40   3.47   -3.47\n",
       "97305        2.53000   1.700193   0.56   5.00   4.44   -4.44\n",
       "97304        1.85500   0.554205   1.33   2.63   1.30    1.09\n",
       "97303        2.47875   1.226592   0.86   4.17   3.31   -3.31\n",
       "97302        3.17000   1.931439   1.26   6.92   5.66   -5.66\n",
       "97301        5.38500   3.427048   1.26   9.98   8.72   -8.72\n",
       "97267        1.17125   0.573297   0.48   1.85   1.37   -1.28\n",
       "97266        1.34125   0.314254   1.05   2.03   0.98   -0.98\n",
       "97239        2.43125   0.866873   1.35   3.27   1.92   -1.88\n",
       "97236        1.77375   0.591679   1.03   2.46   1.43   -1.43\n",
       "97233        1.40375   0.765860   0.47   2.45   1.98   -1.98\n",
       "97232        2.88250   1.888391   0.66   4.99   4.33   -4.33\n",
       "97231        5.83625   3.076812   2.22   9.84   7.62   -7.45\n",
       "97230        1.35250   0.839145   0.30   2.05   1.75   -1.64\n",
       "97229        1.47875   0.609225   0.69   2.09   1.40   -1.02\n",
       "97227        0.84250   0.680184   0.20   1.78   1.58   -1.58\n",
       "97225        1.22000   0.344051   0.84   1.90   1.06   -1.06\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape\n",
    "train_features[19800:19900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    features[\"my_first_feature\"] = df.iloc[:,-1]/df.iloc[:,-2]\n",
    "    features = features.set_index(df.index)\n",
    "    return features\n",
    "    \n",
    "# List of data frames only on one now_time\n",
    "def make_modeling_data(df_list, df_for_target, now_string):\n",
    "    df_one = pd.DataFrame()\n",
    "    for df in df_list:\n",
    "        columns = df.columns\n",
    "        if '2014-01' in columns and '2015-01' in columns and '2016-01' in columns and '2017-01' in columns:\n",
    "            df_to_use_for_features= df.loc[:, :now_string]\n",
    "            features = make_features(df_to_use_for_features)\n",
    "            df_one = pd.merge(df_one, features, left_index=True, right_index=True, how = 'right')\n",
    "    \n",
    "    now_time = pd.to_datetime(now_string)\n",
    "    now_value = df_for_target[now_string]\n",
    "    future_time = now_time + timedelta(days=6*31)\n",
    "    future_time_string= future_time.strftime(\"%Y-%m\")\n",
    "    future_value = df_for_target[future_time_string]\n",
    "    target = future_value/now_value\n",
    "\n",
    "    \n",
    "    return df_one, target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_features, test_targets = make_modeling_data(useful_df, useful_df[23], \"2017-01\")\n",
    "train_features, train_targets = make_modeling_data(useful_df, useful_df[23], \"2016-07\")\n",
    "df = useful_df[num]\n",
    "columns = df.columns\n",
    "for year in [\"2015\",\"2016\"]:\n",
    "    for month in [\"01\",\"07\"]:\n",
    "            new_time = year+\"-\"+month\n",
    "                #if '2014-01' in columns and '2015-01' in columns and '2016-01' in columns and '2017-01' in columns:\n",
    "                 #   extra_train_features, extra_train_targets = make_modeling_data(df, new_time)\n",
    "                  #  train_features = train_features.append(extra_train_features)\n",
    "                   # train_targets = train_targets.append(extra_train_targets)\n",
    "            extra_train_features, extra_targets = make_modeling_data(useful_df, useful_df[23], new_time)\n",
    "            train_features = train_features.append(extra_train_features)\n",
    "            train_targets = train_targets.append(extra_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74265, 58)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10521"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10521"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(windows[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1513.2575757575798",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2441\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1513.2575757575798",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8869b41fcb45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_y_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_data_list_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-24d3d65dee28>\u001b[0m in \u001b[0;36mcreate_y_ratio\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mwindow_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow_2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#y = y.sort_values('RegionName',ascending=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#y = y.set_index('RegionName')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2442\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2444\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5280)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5126)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20523)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20477)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1513.2575757575798"
     ]
    }
   ],
   "source": [
    "y = create_y_ratio(read_data_list_2[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10521"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_data_list_2[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RegionName</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10025</td>\n",
       "      <td>0.950185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10023</td>\n",
       "      <td>0.995899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77494</td>\n",
       "      <td>1.031827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75070</td>\n",
       "      <td>0.981885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RegionName         y\n",
       "0      10025  0.950185\n",
       "1      60657       NaN\n",
       "2      10023  0.995899\n",
       "3      77494  1.031827\n",
       "4      75070  0.981885"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-69c587d04836>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_16_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "X_16_df = pd.merge(X_16, y, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2_df = pd.merge(df_2, y, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge train fetaures, train targets, test features, test targets\n",
    "#X_new = pd.merge(df_2, y, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2011_mean_x</th>\n",
       "      <th>2011_std_x</th>\n",
       "      <th>2011_min_x</th>\n",
       "      <th>2011_max_x</th>\n",
       "      <th>2011_swing_x</th>\n",
       "      <th>2011_change_x</th>\n",
       "      <th>2011_swing_pos_x</th>\n",
       "      <th>2011_swing_big_x</th>\n",
       "      <th>2011_swing_neg_x</th>\n",
       "      <th>2011_swing_loss_big_x</th>\n",
       "      <th>...</th>\n",
       "      <th>2016_yoy_pos_y</th>\n",
       "      <th>2016_yoy_big_y</th>\n",
       "      <th>2016_yoy_neg_y</th>\n",
       "      <th>2016_yoy_loss_big_y</th>\n",
       "      <th>2016_swing_pos_y</th>\n",
       "      <th>2016_swing_big_y</th>\n",
       "      <th>2016_swing_neg_y</th>\n",
       "      <th>2016_swing_loss_big_y</th>\n",
       "      <th>review_count</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>13.663362</td>\n",
       "      <td>3.136821</td>\n",
       "      <td>10.19523</td>\n",
       "      <td>18.892134</td>\n",
       "      <td>8.696905</td>\n",
       "      <td>-2.455884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>13.663362</td>\n",
       "      <td>3.136821</td>\n",
       "      <td>10.19523</td>\n",
       "      <td>18.892134</td>\n",
       "      <td>8.696905</td>\n",
       "      <td>-2.455884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>13.663362</td>\n",
       "      <td>3.136821</td>\n",
       "      <td>10.19523</td>\n",
       "      <td>18.892134</td>\n",
       "      <td>8.696905</td>\n",
       "      <td>-2.455884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>13.663362</td>\n",
       "      <td>3.136821</td>\n",
       "      <td>10.19523</td>\n",
       "      <td>18.892134</td>\n",
       "      <td>8.696905</td>\n",
       "      <td>-2.455884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>13.663362</td>\n",
       "      <td>3.136821</td>\n",
       "      <td>10.19523</td>\n",
       "      <td>18.892134</td>\n",
       "      <td>8.696905</td>\n",
       "      <td>-2.455884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.004622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 542 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2011_mean_x  2011_std_x  2011_min_x  2011_max_x  2011_swing_x  \\\n",
       "15003    13.663362    3.136821    10.19523   18.892134      8.696905   \n",
       "15003    13.663362    3.136821    10.19523   18.892134      8.696905   \n",
       "15003    13.663362    3.136821    10.19523   18.892134      8.696905   \n",
       "15003    13.663362    3.136821    10.19523   18.892134      8.696905   \n",
       "15003    13.663362    3.136821    10.19523   18.892134      8.696905   \n",
       "\n",
       "       2011_change_x  2011_swing_pos_x  2011_swing_big_x  2011_swing_neg_x  \\\n",
       "15003      -2.455884                 0                 0                 1   \n",
       "15003      -2.455884                 0                 0                 1   \n",
       "15003      -2.455884                 0                 0                 1   \n",
       "15003      -2.455884                 0                 0                 1   \n",
       "15003      -2.455884                 0                 0                 1   \n",
       "\n",
       "       2011_swing_loss_big_x    ...     2016_yoy_pos_y  2016_yoy_big_y  \\\n",
       "15003                      0    ...                  0               0   \n",
       "15003                      0    ...                  0               0   \n",
       "15003                      0    ...                  0               0   \n",
       "15003                      0    ...                  0               0   \n",
       "15003                      0    ...                  0               0   \n",
       "\n",
       "       2016_yoy_neg_y  2016_yoy_loss_big_y  2016_swing_pos_y  \\\n",
       "15003               1                    0                 0   \n",
       "15003               1                    0                 0   \n",
       "15003               1                    0                 0   \n",
       "15003               1                    0                 0   \n",
       "15003               1                    0                 0   \n",
       "\n",
       "       2016_swing_big_y  2016_swing_neg_y  2016_swing_loss_big_y  \\\n",
       "15003                 0                 1                      0   \n",
       "15003                 0                 1                      0   \n",
       "15003                 0                 1                      0   \n",
       "15003                 0                 1                      0   \n",
       "15003                 0                 1                      0   \n",
       "\n",
       "       review_count         y  \n",
       "15003             4  1.004622  \n",
       "15003             8  1.004622  \n",
       "15003            39  1.004622  \n",
       "15003             3  1.004622  \n",
       "15003             8  1.004622  \n",
       "\n",
       "[5 rows x 542 columns]"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dropna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-948-1c31721c04ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dropna'"
     ]
    }
   ],
   "source": [
    "X_2.dropna(inplace=True)\n",
    "X_2.isnull().sum().sum()\n",
    "X_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9347079037800687"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2_df['2016_gain'].sum() / len(X_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2011_mean_x</th>\n",
       "      <th>2011_std_x</th>\n",
       "      <th>2011_min_x</th>\n",
       "      <th>2011_max_x</th>\n",
       "      <th>2011_swing_x</th>\n",
       "      <th>2011_change_x</th>\n",
       "      <th>2011_swing_pos_x</th>\n",
       "      <th>2011_swing_big_x</th>\n",
       "      <th>2011_swing_neg_x</th>\n",
       "      <th>2011_swing_loss_big_x</th>\n",
       "      <th>...</th>\n",
       "      <th>2016_gain_y</th>\n",
       "      <th>2016_yoy_pos_y</th>\n",
       "      <th>2016_yoy_big_y</th>\n",
       "      <th>2016_yoy_neg_y</th>\n",
       "      <th>2016_yoy_loss_big_y</th>\n",
       "      <th>2016_swing_pos_y</th>\n",
       "      <th>2016_swing_big_y</th>\n",
       "      <th>2016_swing_neg_y</th>\n",
       "      <th>2016_swing_loss_big_y</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "      <td>5129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.281579</td>\n",
       "      <td>2.873584</td>\n",
       "      <td>9.687841</td>\n",
       "      <td>19.192768</td>\n",
       "      <td>9.504927</td>\n",
       "      <td>-1.164982</td>\n",
       "      <td>0.428154</td>\n",
       "      <td>0.149152</td>\n",
       "      <td>0.571846</td>\n",
       "      <td>0.149152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872685</td>\n",
       "      <td>0.551764</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.448236</td>\n",
       "      <td>0.049912</td>\n",
       "      <td>0.360889</td>\n",
       "      <td>0.098265</td>\n",
       "      <td>0.639111</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>1.067475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.122251</td>\n",
       "      <td>1.019149</td>\n",
       "      <td>3.194033</td>\n",
       "      <td>4.072836</td>\n",
       "      <td>3.459762</td>\n",
       "      <td>4.325728</td>\n",
       "      <td>0.494859</td>\n",
       "      <td>0.356273</td>\n",
       "      <td>0.494859</td>\n",
       "      <td>0.356273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333358</td>\n",
       "      <td>0.497362</td>\n",
       "      <td>0.363422</td>\n",
       "      <td>0.497362</td>\n",
       "      <td>0.217785</td>\n",
       "      <td>0.480305</td>\n",
       "      <td>0.297701</td>\n",
       "      <td>0.480305</td>\n",
       "      <td>0.144236</td>\n",
       "      <td>0.072273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.149246</td>\n",
       "      <td>0.717761</td>\n",
       "      <td>-1.717095</td>\n",
       "      <td>7.217994</td>\n",
       "      <td>2.280413</td>\n",
       "      <td>-23.062877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.791820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.116049</td>\n",
       "      <td>2.138989</td>\n",
       "      <td>7.456067</td>\n",
       "      <td>16.480570</td>\n",
       "      <td>7.036975</td>\n",
       "      <td>-3.732543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.023399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.286639</td>\n",
       "      <td>2.739566</td>\n",
       "      <td>9.738339</td>\n",
       "      <td>19.012437</td>\n",
       "      <td>9.001742</td>\n",
       "      <td>-0.989435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.061971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.355682</td>\n",
       "      <td>3.455925</td>\n",
       "      <td>11.863632</td>\n",
       "      <td>21.755320</td>\n",
       "      <td>11.443071</td>\n",
       "      <td>1.592072</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.106317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.309110</td>\n",
       "      <td>9.054726</td>\n",
       "      <td>21.888196</td>\n",
       "      <td>38.354467</td>\n",
       "      <td>32.033667</td>\n",
       "      <td>17.008037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.425089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 541 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       2011_mean_x   2011_std_x   2011_min_x   2011_max_x  2011_swing_x  \\\n",
       "count  5129.000000  5129.000000  5129.000000  5129.000000   5129.000000   \n",
       "mean     14.281579     2.873584     9.687841    19.192768      9.504927   \n",
       "std       3.122251     1.019149     3.194033     4.072836      3.459762   \n",
       "min       5.149246     0.717761    -1.717095     7.217994      2.280413   \n",
       "25%      12.116049     2.138989     7.456067    16.480570      7.036975   \n",
       "50%      14.286639     2.739566     9.738339    19.012437      9.001742   \n",
       "75%      16.355682     3.455925    11.863632    21.755320     11.443071   \n",
       "max      25.309110     9.054726    21.888196    38.354467     32.033667   \n",
       "\n",
       "       2011_change_x  2011_swing_pos_x  2011_swing_big_x  2011_swing_neg_x  \\\n",
       "count    5129.000000       5129.000000       5129.000000       5129.000000   \n",
       "mean       -1.164982          0.428154          0.149152          0.571846   \n",
       "std         4.325728          0.494859          0.356273          0.494859   \n",
       "min       -23.062877          0.000000          0.000000          0.000000   \n",
       "25%        -3.732543          0.000000          0.000000          0.000000   \n",
       "50%        -0.989435          0.000000          0.000000          1.000000   \n",
       "75%         1.592072          1.000000          0.000000          1.000000   \n",
       "max        17.008037          1.000000          1.000000          1.000000   \n",
       "\n",
       "       2011_swing_loss_big_x     ...       2016_gain_y  2016_yoy_pos_y  \\\n",
       "count            5129.000000     ...       5129.000000     5129.000000   \n",
       "mean                0.149152     ...          0.872685        0.551764   \n",
       "std                 0.356273     ...          0.333358        0.497362   \n",
       "min                 0.000000     ...          0.000000        0.000000   \n",
       "25%                 0.000000     ...          1.000000        0.000000   \n",
       "50%                 0.000000     ...          1.000000        1.000000   \n",
       "75%                 0.000000     ...          1.000000        1.000000   \n",
       "max                 1.000000     ...          1.000000        1.000000   \n",
       "\n",
       "       2016_yoy_big_y  2016_yoy_neg_y  2016_yoy_loss_big_y  2016_swing_pos_y  \\\n",
       "count     5129.000000     5129.000000          5129.000000       5129.000000   \n",
       "mean         0.156561        0.448236             0.049912          0.360889   \n",
       "std          0.363422        0.497362             0.217785          0.480305   \n",
       "min          0.000000        0.000000             0.000000          0.000000   \n",
       "25%          0.000000        0.000000             0.000000          0.000000   \n",
       "50%          0.000000        0.000000             0.000000          0.000000   \n",
       "75%          0.000000        1.000000             0.000000          1.000000   \n",
       "max          1.000000        1.000000             1.000000          1.000000   \n",
       "\n",
       "       2016_swing_big_y  2016_swing_neg_y  2016_swing_loss_big_y            y  \n",
       "count       5129.000000       5129.000000            5129.000000  5129.000000  \n",
       "mean           0.098265          0.639111               0.021252     1.067475  \n",
       "std            0.297701          0.480305               0.144236     0.072273  \n",
       "min            0.000000          0.000000               0.000000     0.791820  \n",
       "25%            0.000000          0.000000               0.000000     1.023399  \n",
       "50%            0.000000          1.000000               0.000000     1.061971  \n",
       "75%            0.000000          1.000000               0.000000     1.106317  \n",
       "max            1.000000          1.000000               1.000000     1.425089  \n",
       "\n",
       "[8 rows x 541 columns]"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16_df.dropna(inplace=True)\n",
    "X_16_df.isnull().sum().sum()\n",
    "X_16_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2 = X_2_df.iloc[:, :-1].values\n",
    "y_2 = X_2_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_16 = X_16_df.iloc[:, :-1].values\n",
    "y_16 = X_16_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    97745.000000\n",
       "mean         1.091032\n",
       "std          0.069050\n",
       "min          0.884748\n",
       "25%          1.047925\n",
       "50%          1.073544\n",
       "75%          1.123171\n",
       "max          1.425089\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 968,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2_df['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5129.000000\n",
       "mean        1.067475\n",
       "std         0.072273\n",
       "min         0.791820\n",
       "25%         1.023399\n",
       "50%         1.061971\n",
       "75%         1.106317\n",
       "max         1.425089\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16_df['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.98408333e+01,   5.28288568e+00,   1.26100000e+01, ...,\n",
       "          4.44783710e-02,   1.11710935e+00,   1.00000000e+00],\n",
       "       [  8.59416667e+00,   2.68156148e+00,   4.90000000e+00, ...,\n",
       "          3.36152944e-02,   1.16117045e+00,   1.00000000e+00],\n",
       "       [  9.57916667e+00,   4.12912815e+00,   4.05000000e+00, ...,\n",
       "          2.08722085e-02,   1.76863352e+00,   1.00000000e+00],\n",
       "       ..., \n",
       "       [  3.84833333e+00,   7.56057457e+00,   3.00000000e-02, ...,\n",
       "         -1.84320184e-02,   2.15476923e+01,   1.00000000e+00],\n",
       "       [  7.37000000e+00,   8.97160267e+00,   1.15000000e+00, ...,\n",
       "          1.24468539e-02,   9.77659456e-01,   0.00000000e+00],\n",
       "       [  2.06333333e+00,   1.31138604e+00,   6.10000000e-01, ...,\n",
       "         -1.59529241e-02,   8.84642600e-01,   0.00000000e+00]])"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_per_column(X):\n",
    "    new_array = []\n",
    "    shape = X.shape\n",
    "    columns = shape[1]\n",
    "    for num in range(columns):\n",
    "        mx = X[:, num].max()\n",
    "        if mx == 0:\n",
    "            new_array.append(X[:, num])\n",
    "        else:\n",
    "            X[:, num] = X[:, num] / mx\n",
    "            new_array.append(X[:, num])\n",
    "    new_array = np.array(new_array)\n",
    "    new_array = np.transpose(new_array)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2 = norm_per_column(X_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_16 = norm_per_column(X_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5129, 540)"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 541), dtype=float64)"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12984.457456492662"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_16 = X_16 * 12900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_2 = X_2 / 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_16_df = X_16_df / 94650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-cb48261dd72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RegionName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'np'"
     ]
    }
   ],
   "source": [
    "train_features = train_features.orderby('RegionName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-31591fb11963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     51\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m    533\u001b[0m             raise ValueError(\n\u001b[1;32m    534\u001b[0m                 \u001b[0;34m'can not merge DataFrame with instance of '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                 'type {0}'.format(type(right)))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.merge(train_features, train_targets, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size= .2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_16, X_test_16, y_train_16, y_test_16 = train_test_split(X_16, y_16, test_size= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hold overfit model for now\n",
    "model = Sequential()\n",
    "model.add(Dense(3000, input_shape=(540,), activation='relu'))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(540,), activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4103 samples, validate on 1026 samples\n",
      "Epoch 1/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.9291e-04 - val_loss: 0.3209\n",
      "Epoch 2/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.0620e-04 - val_loss: 0.3204\n",
      "Epoch 3/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.8823e-05 - val_loss: 0.3209\n",
      "Epoch 4/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.0509e-04 - val_loss: 0.3214\n",
      "Epoch 5/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.9939e-04 - val_loss: 0.3217\n",
      "Epoch 6/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.1241e-04 - val_loss: 0.3213\n",
      "Epoch 7/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.4091e-04 - val_loss: 0.3196\n",
      "Epoch 8/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 6.1681e-05 - val_loss: 0.3184\n",
      "Epoch 9/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.4459e-05 - val_loss: 0.3186\n",
      "Epoch 10/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.6849e-05 - val_loss: 0.3199\n",
      "Epoch 11/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.3029e-05 - val_loss: 0.3223\n",
      "Epoch 12/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.9986e-05 - val_loss: 0.3244\n",
      "Epoch 13/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.7444e-05 - val_loss: 0.3257\n",
      "Epoch 14/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 8.6525e-05 - val_loss: 0.3257\n",
      "Epoch 15/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.4758e-04 - val_loss: 0.3242\n",
      "Epoch 16/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.2184e-04 - val_loss: 0.3231\n",
      "Epoch 17/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.0027e-04 - val_loss: 0.3216\n",
      "Epoch 18/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.6929e-04 - val_loss: 0.3192\n",
      "Epoch 19/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.9284e-04 - val_loss: 0.3191\n",
      "Epoch 20/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.3177e-04 - val_loss: 0.3211\n",
      "Epoch 21/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.0668e-04 - val_loss: 0.3240\n",
      "Epoch 22/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 8.5931e-05 - val_loss: 0.3277\n",
      "Epoch 23/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.4960e-05 - val_loss: 0.3315\n",
      "Epoch 24/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.0127e-05 - val_loss: 0.3347\n",
      "Epoch 25/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 8.2880e-05 - val_loss: 0.3371\n",
      "Epoch 26/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.8338e-04 - val_loss: 0.3405\n",
      "Epoch 27/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.6377e-04 - val_loss: 0.3426\n",
      "Epoch 28/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 6.3083e-04 - val_loss: 0.3448\n",
      "Epoch 29/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 8.5701e-04 - val_loss: 0.3437\n",
      "Epoch 30/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 6.6732e-04 - val_loss: 0.3380\n",
      "Epoch 31/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.8535e-04 - val_loss: 0.3327\n",
      "Epoch 32/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 7.5508e-05 - val_loss: 0.3307\n",
      "Epoch 33/100\n",
      "4103/4103 [==============================] - 7s 2ms/step - loss: 2.6369e-04 - val_loss: 0.3325\n",
      "Epoch 34/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.2573e-04 - val_loss: 0.3359\n",
      "Epoch 35/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.3890e-04 - val_loss: 0.3397\n",
      "Epoch 36/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.1182e-04 - val_loss: 0.3430\n",
      "Epoch 37/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.0603e-05 - val_loss: 0.3451\n",
      "Epoch 38/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.4720e-05 - val_loss: 0.3464\n",
      "Epoch 39/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.0812e-05 - val_loss: 0.3475\n",
      "Epoch 40/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.7035e-05 - val_loss: 0.3478\n",
      "Epoch 41/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.6391e-05 - val_loss: 0.3476\n",
      "Epoch 42/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.7083e-05 - val_loss: 0.3472\n",
      "Epoch 43/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.8130e-05 - val_loss: 0.3466\n",
      "Epoch 44/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.6012e-05 - val_loss: 0.3462\n",
      "Epoch 45/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.7203e-05 - val_loss: 0.3467\n",
      "Epoch 46/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 9.4670e-05 - val_loss: 0.3475\n",
      "Epoch 47/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.8606e-04 - val_loss: 0.3477\n",
      "Epoch 48/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.3652e-04 - val_loss: 0.3482\n",
      "Epoch 49/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 5.9426e-04 - val_loss: 0.3483\n",
      "Epoch 50/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 9.3332e-04 - val_loss: 0.3477\n",
      "Epoch 51/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 0.0011 - val_loss: 0.3469\n",
      "Epoch 52/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 7.1237e-04 - val_loss: 0.3430\n",
      "Epoch 53/100\n",
      "4103/4103 [==============================] - 7s 2ms/step - loss: 1.3002e-04 - val_loss: 0.3385\n",
      "Epoch 54/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.2869e-04 - val_loss: 0.3365\n",
      "Epoch 55/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 6.4630e-04 - val_loss: 0.3363\n",
      "Epoch 56/100\n",
      "4103/4103 [==============================] - 6s 2ms/step - loss: 5.8329e-04 - val_loss: 0.3358\n",
      "Epoch 57/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.8280e-04 - val_loss: 0.3380\n",
      "Epoch 58/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.3131e-05 - val_loss: 0.3410\n",
      "Epoch 59/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.5855e-04 - val_loss: 0.3443\n",
      "Epoch 60/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.1865e-04 - val_loss: 0.3465\n",
      "Epoch 61/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.7217e-04 - val_loss: 0.3476\n",
      "Epoch 62/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 8.0961e-05 - val_loss: 0.3483\n",
      "Epoch 63/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.4079e-05 - val_loss: 0.3494\n",
      "Epoch 64/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.4060e-05 - val_loss: 0.3507\n",
      "Epoch 65/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.1515e-04 - val_loss: 0.3513\n",
      "Epoch 66/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.0658e-04 - val_loss: 0.3517\n",
      "Epoch 67/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.6804e-04 - val_loss: 0.3523\n",
      "Epoch 68/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.5906e-04 - val_loss: 0.3536\n",
      "Epoch 69/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.6596e-04 - val_loss: 0.3543\n",
      "Epoch 70/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 7.4021e-05 - val_loss: 0.3546\n",
      "Epoch 71/100\n",
      "4103/4103 [==============================] - 6s 2ms/step - loss: 2.3494e-05 - val_loss: 0.3550\n",
      "Epoch 72/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.2352e-05 - val_loss: 0.3552\n",
      "Epoch 73/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 5.5009e-05 - val_loss: 0.3562\n",
      "Epoch 74/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.0336e-04 - val_loss: 0.3590\n",
      "Epoch 75/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.6590e-04 - val_loss: 0.3623\n",
      "Epoch 76/100\n",
      "4103/4103 [==============================] - 6s 2ms/step - loss: 2.5234e-04 - val_loss: 0.3644\n",
      "Epoch 77/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.0981e-04 - val_loss: 0.3662\n",
      "Epoch 78/100\n",
      "4103/4103 [==============================] - 6s 2ms/step - loss: 2.8956e-04 - val_loss: 0.3686\n",
      "Epoch 79/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.9111e-04 - val_loss: 0.3690\n",
      "Epoch 80/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 7.3875e-05 - val_loss: 0.3678\n",
      "Epoch 81/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.8717e-05 - val_loss: 0.3662\n",
      "Epoch 82/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.6650e-05 - val_loss: 0.3640\n",
      "Epoch 83/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.4553e-04 - val_loss: 0.3609\n",
      "Epoch 84/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 3.1402e-04 - val_loss: 0.3572\n",
      "Epoch 85/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 5.8125e-04 - val_loss: 0.3557\n",
      "Epoch 86/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 9.1594e-04 - val_loss: 0.3524\n",
      "Epoch 87/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 9.8862e-04 - val_loss: 0.3528\n",
      "Epoch 88/100\n",
      "4103/4103 [==============================] - 6s 2ms/step - loss: 5.0113e-04 - val_loss: 0.3589\n",
      "Epoch 89/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 7.1710e-05 - val_loss: 0.3668\n",
      "Epoch 90/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.5044e-04 - val_loss: 0.3738\n",
      "Epoch 91/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 5.5719e-04 - val_loss: 0.3769\n",
      "Epoch 92/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 5.2064e-04 - val_loss: 0.3756\n",
      "Epoch 93/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.2225e-04 - val_loss: 0.3725\n",
      "Epoch 94/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 4.2055e-05 - val_loss: 0.3699\n",
      "Epoch 95/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 5.8075e-05 - val_loss: 0.3685\n",
      "Epoch 96/100\n",
      "4103/4103 [==============================] - 6s 2ms/step - loss: 1.4015e-04 - val_loss: 0.3673\n",
      "Epoch 97/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 2.0067e-04 - val_loss: 0.3678\n",
      "Epoch 98/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.9146e-04 - val_loss: 0.3692\n",
      "Epoch 99/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 1.4364e-04 - val_loss: 0.3716\n",
      "Epoch 100/100\n",
      "4103/4103 [==============================] - 6s 1ms/step - loss: 7.2094e-05 - val_loss: 0.3739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a258e10>"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_features, y=train_targets, \n",
    "          batch_size=2500, \n",
    "          epochs=2000, \n",
    "          verbose=1, \n",
    "          validation_data=(test_features, testtargets),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss = .00001441 (overfit) = .0038\n",
    "\n",
    "#std = .072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding reviews. No overfit! \n",
    "model = Sequential()\n",
    "model.add(Dense(3000, input_shape=(541,), activation='relu'))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = keras.optimizers.SGD(lr=0.75)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78196 samples, validate on 19549 samples\n",
      "Epoch 1/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 4.7343 - val_loss: 0.0027\n",
      "Epoch 2/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 0.0937 - val_loss: 0.0046\n",
      "Epoch 3/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 4/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 3.2574e-04 - val_loss: 0.0059\n",
      "Epoch 5/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 7.3467e-05 - val_loss: 0.0059\n",
      "Epoch 6/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 4.5027e-05 - val_loss: 0.0060\n",
      "Epoch 7/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 3.2798e-05 - val_loss: 0.0061\n",
      "Epoch 8/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.5425e-05 - val_loss: 0.0062\n",
      "Epoch 9/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.0523e-05 - val_loss: 0.0063\n",
      "Epoch 10/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.7073e-05 - val_loss: 0.0064\n",
      "Epoch 11/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 1.4884e-05 - val_loss: 0.0064\n",
      "Epoch 12/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 1.2662e-05 - val_loss: 0.0065\n",
      "Epoch 13/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 1.1025e-05 - val_loss: 0.0065\n",
      "Epoch 14/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 9.6451e-06 - val_loss: 0.0066\n",
      "Epoch 15/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 8.4698e-06 - val_loss: 0.0066\n",
      "Epoch 16/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 7.5258e-06 - val_loss: 0.0066\n",
      "Epoch 17/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 6.6461e-06 - val_loss: 0.0067\n",
      "Epoch 18/1000\n",
      "78196/78196 [==============================] - 10895s 139ms/step - loss: 5.9375e-06 - val_loss: 0.0068\n",
      "Epoch 19/1000\n",
      "78196/78196 [==============================] - 7298s 93ms/step - loss: 5.2844e-06 - val_loss: 0.0068\n",
      "Epoch 20/1000\n",
      "78196/78196 [==============================] - 1125s 14ms/step - loss: 4.7737e-06 - val_loss: 0.0068\n",
      "Epoch 21/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 4.2512e-06 - val_loss: 0.0069\n",
      "Epoch 22/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 3.8335e-06 - val_loss: 0.0069\n",
      "Epoch 23/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 3.5044e-06 - val_loss: 0.0069\n",
      "Epoch 24/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 3.1160e-06 - val_loss: 0.0069\n",
      "Epoch 25/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.8295e-06 - val_loss: 0.0069\n",
      "Epoch 26/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.5962e-06 - val_loss: 0.0070\n",
      "Epoch 27/1000\n",
      "78196/78196 [==============================] - 112s 1ms/step - loss: 2.4202e-06 - val_loss: 0.0070\n",
      "Epoch 28/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.1700e-06 - val_loss: 0.0070\n",
      "Epoch 29/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.0196e-06 - val_loss: 0.0071\n",
      "Epoch 30/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.8961e-06 - val_loss: 0.0071\n",
      "Epoch 31/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.7118e-06 - val_loss: 0.0071\n",
      "Epoch 32/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.7906e-06 - val_loss: 0.0071\n",
      "Epoch 33/1000\n",
      "78196/78196 [==============================] - 112s 1ms/step - loss: 1.5684e-06 - val_loss: 0.0071\n",
      "Epoch 34/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.4365e-06 - val_loss: 0.0071\n",
      "Epoch 35/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.3009e-06 - val_loss: 0.0072\n",
      "Epoch 36/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.2430e-06 - val_loss: 0.0072\n",
      "Epoch 37/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.1966e-06 - val_loss: 0.0071\n",
      "Epoch 38/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.1307e-06 - val_loss: 0.0072\n",
      "Epoch 39/1000\n",
      "78196/78196 [==============================] - 125s 2ms/step - loss: 1.0713e-06 - val_loss: 0.0073\n",
      "Epoch 40/1000\n",
      "78196/78196 [==============================] - 114s 1ms/step - loss: 1.4843e-06 - val_loss: 0.0072\n",
      "Epoch 41/1000\n",
      "78196/78196 [==============================] - 121s 2ms/step - loss: 1.0901e-06 - val_loss: 0.0072\n",
      "Epoch 42/1000\n",
      "78196/78196 [==============================] - 128s 2ms/step - loss: 1.0082e-06 - val_loss: 0.0073\n",
      "Epoch 43/1000\n",
      "78196/78196 [==============================] - 123s 2ms/step - loss: 1.0009e-06 - val_loss: 0.0073\n",
      "Epoch 44/1000\n",
      "78196/78196 [==============================] - 116s 1ms/step - loss: 1.8567e-06 - val_loss: 0.0073\n",
      "Epoch 45/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 5.0224e-06 - val_loss: 0.0073\n",
      "Epoch 46/1000\n",
      "78196/78196 [==============================] - 7299s 93ms/step - loss: 9.5015e-07 - val_loss: 0.0074\n",
      "Epoch 47/1000\n",
      "78196/78196 [==============================] - 7298s 93ms/step - loss: 1.0685e-06 - val_loss: 0.0074\n",
      "Epoch 48/1000\n",
      "78196/78196 [==============================] - 3707s 47ms/step - loss: 1.0764e-05 - val_loss: 0.0075\n",
      "Epoch 49/1000\n",
      "78196/78196 [==============================] - 7301s 93ms/step - loss: 5.1339e-06 - val_loss: 0.0075\n",
      "Epoch 50/1000\n",
      "78196/78196 [==============================] - 7298s 93ms/step - loss: 1.1024e-06 - val_loss: 0.0074\n",
      "Epoch 51/1000\n",
      "78196/78196 [==============================] - 7298s 93ms/step - loss: 5.9525e-07 - val_loss: 0.0074\n",
      "Epoch 52/1000\n",
      "78196/78196 [==============================] - 7298s 93ms/step - loss: 5.8703e-07 - val_loss: 0.0074\n",
      "Epoch 53/1000\n",
      "78196/78196 [==============================] - 3413s 44ms/step - loss: 8.2009e-07 - val_loss: 0.0073\n",
      "Epoch 54/1000\n",
      "78196/78196 [==============================] - 112s 1ms/step - loss: 1.2467e-05 - val_loss: 0.0072\n",
      "Epoch 55/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.2361e-06 - val_loss: 0.0076\n",
      "Epoch 56/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.7377e-06 - val_loss: 0.0075\n",
      "Epoch 57/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.8206e-05 - val_loss: 0.0076\n",
      "Epoch 58/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 4.3757e-06 - val_loss: 0.0072\n",
      "Epoch 59/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 1.0779e-06 - val_loss: 0.0073\n",
      "Epoch 60/1000\n",
      "78196/78196 [==============================] - 111s 1ms/step - loss: 6.7133e-07 - val_loss: 0.0073\n",
      "Epoch 61/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 5.7408e-07 - val_loss: 0.0074\n",
      "Epoch 62/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 4.1388e-05 - val_loss: 0.0073\n",
      "Epoch 63/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 2.0241e-05 - val_loss: 0.0070\n",
      "Epoch 64/1000\n",
      "78196/78196 [==============================] - 111s 1ms/step - loss: 1.5835e-06 - val_loss: 0.0072\n",
      "Epoch 65/1000\n",
      "78196/78196 [==============================] - 109s 1ms/step - loss: 6.8067e-07 - val_loss: 0.0074\n",
      "Epoch 66/1000\n",
      "78196/78196 [==============================] - 110s 1ms/step - loss: 3.9125e-06 - val_loss: 0.0080\n",
      "Epoch 67/1000\n",
      "78196/78196 [==============================] - 116s 1ms/step - loss: 6.4615e-05 - val_loss: 0.0074\n",
      "Epoch 68/1000\n",
      "78196/78196 [==============================] - 111s 1ms/step - loss: 5.9912e-06 - val_loss: 0.0076\n",
      "Epoch 69/1000\n",
      "78196/78196 [==============================] - 200s 3ms/step - loss: 1.7642e-06 - val_loss: 0.0074\n",
      "Epoch 70/1000\n",
      "78196/78196 [==============================] - 202s 3ms/step - loss: 6.6541e-05 - val_loss: 0.0099\n",
      "Epoch 71/1000\n",
      "78196/78196 [==============================] - 195s 2ms/step - loss: 5.9380e-05 - val_loss: 0.0075\n",
      "Epoch 72/1000\n",
      "78196/78196 [==============================] - 192s 2ms/step - loss: 2.3167e-06 - val_loss: 0.0074\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78196/78196 [==============================] - 198s 3ms/step - loss: 8.0601e-07 - val_loss: 0.0074\n",
      "Epoch 74/1000\n",
      "78196/78196 [==============================] - 188s 2ms/step - loss: 4.5826e-07 - val_loss: 0.0073\n",
      "Epoch 75/1000\n",
      "78196/78196 [==============================] - 191s 2ms/step - loss: 1.0517e-04 - val_loss: 0.0065\n",
      "Epoch 76/1000\n",
      "78196/78196 [==============================] - 196s 3ms/step - loss: 3.7467e-05 - val_loss: 0.0073\n",
      "Epoch 77/1000\n",
      "78196/78196 [==============================] - 188s 2ms/step - loss: 2.5486e-06 - val_loss: 0.0073\n",
      "Epoch 78/1000\n",
      "67500/78196 [========================>.....] - ETA: 26s - loss: 4.9238e-07"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-980-85741cd0d845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train_2, y=y_train_2, \n",
    "          batch_size=2500, \n",
    "          epochs=1000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_2, y_test_2),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Val_loss = .00000306\n",
    "# loss = .000000415\n",
    "# STD = .069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(90,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5336 samples, validate on 1334 samples\n",
      "Epoch 1/2000\n",
      "5336/5336 [==============================] - 5s 906us/step - loss: 1.0742 - val_loss: 0.8664\n",
      "Epoch 2/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.8218 - val_loss: 0.6511\n",
      "Epoch 3/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.6200 - val_loss: 0.4853\n",
      "Epoch 4/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.4641 - val_loss: 0.3586\n",
      "Epoch 5/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.3450 - val_loss: 0.2620\n",
      "Epoch 6/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.2540 - val_loss: 0.1897\n",
      "Epoch 7/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.1856 - val_loss: 0.1357\n",
      "Epoch 8/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.1340 - val_loss: 0.0961\n",
      "Epoch 9/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0967 - val_loss: 0.0677\n",
      "Epoch 10/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0694 - val_loss: 0.0474\n",
      "Epoch 11/2000\n",
      "5336/5336 [==============================] - 3s 603us/step - loss: 0.0498 - val_loss: 0.0334\n",
      "Epoch 12/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0362 - val_loss: 0.0237\n",
      "Epoch 13/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0265 - val_loss: 0.0170\n",
      "Epoch 14/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0198 - val_loss: 0.0126\n",
      "Epoch 15/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0152 - val_loss: 0.0096\n",
      "Epoch 16/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0119 - val_loss: 0.0077\n",
      "Epoch 17/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0100 - val_loss: 0.0065\n",
      "Epoch 18/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0086 - val_loss: 0.0057\n",
      "Epoch 19/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0076 - val_loss: 0.0052\n",
      "Epoch 20/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0069 - val_loss: 0.0050\n",
      "Epoch 21/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0065 - val_loss: 0.0048\n",
      "Epoch 22/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0062 - val_loss: 0.0047\n",
      "Epoch 23/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0060 - val_loss: 0.0047\n",
      "Epoch 24/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0059 - val_loss: 0.0047\n",
      "Epoch 25/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0057 - val_loss: 0.0047\n",
      "Epoch 26/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0057 - val_loss: 0.0047\n",
      "Epoch 27/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 28/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 29/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 30/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 31/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 32/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 33/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 34/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 35/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 36/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 37/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 38/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 39/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 40/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 41/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 42/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 43/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 44/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 45/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 46/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 47/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 48/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 49/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 50/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 51/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 52/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 53/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 54/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 55/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 56/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 57/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 58/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 59/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 60/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 61/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 62/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 63/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 64/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 65/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 66/2000\n",
      "5336/5336 [==============================] - 3s 512us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 67/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 68/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 69/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 70/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 71/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 72/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 73/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 74/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 75/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 76/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 77/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 78/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 79/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 80/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 81/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 82/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 83/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 84/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 85/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 86/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 87/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 88/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 89/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 90/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 91/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 92/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 93/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 94/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 95/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 96/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 97/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 98/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 99/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 100/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 101/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 102/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 103/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 104/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 105/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 106/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 107/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 108/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 109/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 110/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 111/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 112/2000\n",
      "5336/5336 [==============================] - 3s 509us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 113/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 114/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 115/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 116/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 117/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 118/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 119/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 120/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 121/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 122/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 123/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 124/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 125/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 126/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 127/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 128/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 129/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 130/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 131/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 132/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 133/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 134/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 135/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 136/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 137/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 138/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 139/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 140/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 141/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 142/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 143/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 144/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 145/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 146/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 147/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 148/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 149/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 150/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 151/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 152/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 153/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 154/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 155/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 156/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 157/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 158/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 159/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 160/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 161/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 162/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 163/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 164/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 165/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 166/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 167/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 168/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 169/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 170/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 171/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 172/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 173/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 174/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 175/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 176/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 177/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 178/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 179/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 180/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 181/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 182/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 183/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 184/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 185/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 186/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 187/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 188/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 189/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 190/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 191/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 192/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 193/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 194/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 195/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 196/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 197/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 198/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 199/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 200/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 201/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 202/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 203/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 204/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 205/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 206/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 207/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 208/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 209/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 210/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 211/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 212/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 213/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 214/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 215/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 216/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 217/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 218/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 219/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 220/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 221/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 222/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 223/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 224/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 225/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 226/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 227/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 228/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 230/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 231/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 232/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 233/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 234/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 235/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 236/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 237/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 238/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 239/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 240/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 241/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 242/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 243/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 244/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 245/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 246/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 247/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 248/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 249/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 250/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 251/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 252/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 253/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 254/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 255/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 256/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 257/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 258/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 259/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 260/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 261/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 262/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 263/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 264/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 265/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 266/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 267/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 268/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 269/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 270/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 271/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 272/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 273/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 274/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 275/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 276/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 277/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 278/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 279/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 280/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 281/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 282/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 283/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 284/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 285/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 286/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 287/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 288/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 289/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 290/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 291/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 292/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 293/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 294/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 295/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 296/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 297/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 298/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 299/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 300/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 301/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 302/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 303/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 304/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 306/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 307/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 308/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 309/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 310/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 311/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 312/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 313/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 314/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 315/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 316/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 317/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 318/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 319/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 320/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 321/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 322/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 323/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 324/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 325/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 326/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 327/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 328/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 329/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 330/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 331/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 332/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 333/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 334/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 335/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 336/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 337/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 338/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 339/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 340/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 341/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 342/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 343/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 344/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 345/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 346/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 347/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 348/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 349/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 350/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 351/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 352/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 353/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 354/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 355/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 356/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 357/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 358/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 359/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 360/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 361/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 362/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 363/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 364/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 365/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 366/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 367/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 368/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 369/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 370/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 371/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 372/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 373/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 374/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 375/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 376/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 377/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 378/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 379/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 380/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 382/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 383/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 384/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 385/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 386/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 387/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 388/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 389/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 390/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 391/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 392/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 393/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 394/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 395/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 396/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 397/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 398/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 399/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 400/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 401/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 402/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 403/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 404/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 405/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 406/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 407/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 408/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 409/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 410/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 411/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 412/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 413/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 414/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 415/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 416/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 417/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 418/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 419/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 420/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 421/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 422/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 423/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 424/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 425/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 426/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 427/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 428/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 429/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 430/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 431/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 432/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 433/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 434/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 435/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 436/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 437/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 438/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 439/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 440/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 441/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 442/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 443/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 444/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 445/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 446/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 447/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 448/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 449/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 450/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 451/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 452/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 453/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 454/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 455/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 456/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 458/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 459/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 460/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 461/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 462/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 463/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 464/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 465/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 466/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 467/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 468/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 469/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 470/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 471/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 472/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0057 - val_loss: 0.0048\n",
      "Epoch 473/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 474/2000\n",
      "5336/5336 [==============================] - 3s 519us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 475/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 476/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 477/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 478/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 479/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 480/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 481/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 482/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 483/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 484/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 485/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 486/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 487/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 488/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 489/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 490/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 491/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 492/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 493/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 494/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 495/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 496/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 497/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 498/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 499/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 500/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 501/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 502/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 503/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 504/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 505/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 506/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 507/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 508/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 509/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 510/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 511/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 512/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 513/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 514/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 515/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 516/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 517/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 518/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 519/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 520/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 521/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 522/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 523/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 524/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 525/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 526/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 527/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 528/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 529/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 530/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 531/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 532/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 533/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 534/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 535/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 536/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 537/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 538/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 539/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 540/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 541/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 542/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 543/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 544/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 545/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 546/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 547/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 548/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 549/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 550/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 551/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 552/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 553/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 554/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 555/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 556/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 557/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 558/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 559/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 560/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 561/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 562/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 563/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 564/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 565/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 566/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 567/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 568/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 569/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 570/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 571/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 572/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 573/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 574/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 575/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 576/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 577/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 578/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 579/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 580/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 581/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 582/2000\n",
      "5336/5336 [==============================] - 3s 504us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 583/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 584/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 585/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 586/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 587/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 588/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 589/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 590/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 591/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 592/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 593/2000\n",
      "5336/5336 [==============================] - 3s 506us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 594/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 595/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 596/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 597/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 598/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 599/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 600/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 601/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 602/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 603/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 604/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 605/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 606/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 607/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 608/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 609/2000\n",
      "5336/5336 [==============================] - 3s 517us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 610/2000\n",
      "5336/5336 [==============================] - 3s 592us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 611/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 612/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 613/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 614/2000\n",
      "5336/5336 [==============================] - 3s 519us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 615/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 616/2000\n",
      "5336/5336 [==============================] - 3s 510us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 617/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 618/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 619/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 620/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 621/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 622/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 623/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 624/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 625/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 626/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 627/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 628/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 629/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 630/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 631/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 632/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 633/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 634/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 635/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 636/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 637/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 638/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 639/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 640/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 641/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 642/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 643/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 644/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 645/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 646/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 647/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 648/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 649/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 650/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 651/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 652/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 653/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 654/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 655/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 656/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 657/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 658/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 659/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 660/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 661/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 662/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 663/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 664/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 665/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 666/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 667/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 668/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 669/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 670/2000\n",
      "5336/5336 [==============================] - 3s 510us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 671/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 672/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 673/2000\n",
      "5336/5336 [==============================] - 3s 507us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 674/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 675/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 676/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 677/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 678/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 679/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 680/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 681/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 682/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 683/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 684/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 686/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 687/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 688/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 689/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 690/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 691/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 692/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 693/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 694/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 695/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 696/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 697/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 698/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 699/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 700/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 701/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 702/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 703/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 704/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 705/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 706/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 707/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 708/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 709/2000\n",
      "5336/5336 [==============================] - 3s 516us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 710/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 711/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 712/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 713/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 714/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 715/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 716/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 717/2000\n",
      "5336/5336 [==============================] - 3s 506us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 718/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 719/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 720/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 721/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 722/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 723/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 724/2000\n",
      "5336/5336 [==============================] - 3034s 569ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 725/2000\n",
      "5336/5336 [==============================] - 3s 653us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 726/2000\n",
      "5336/5336 [==============================] - 4s 680us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 727/2000\n",
      "5336/5336 [==============================] - 4s 767us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 728/2000\n",
      "5336/5336 [==============================] - 5s 891us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 729/2000\n",
      "5336/5336 [==============================] - 5s 930us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 730/2000\n",
      "5336/5336 [==============================] - 4s 750us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 731/2000\n",
      "5336/5336 [==============================] - 5s 853us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 732/2000\n",
      "5336/5336 [==============================] - 5s 959us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 733/2000\n",
      "5336/5336 [==============================] - 5s 951us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 734/2000\n",
      "5336/5336 [==============================] - 4s 704us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 735/2000\n",
      "5336/5336 [==============================] - 5s 859us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 736/2000\n",
      "5336/5336 [==============================] - 4s 835us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 737/2000\n",
      "5336/5336 [==============================] - 5s 989us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 738/2000\n",
      "5336/5336 [==============================] - 5s 950us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 739/2000\n",
      "5336/5336 [==============================] - 3s 561us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 740/2000\n",
      "5336/5336 [==============================] - 3s 540us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 741/2000\n",
      "5336/5336 [==============================] - 3s 527us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 742/2000\n",
      "5336/5336 [==============================] - 3s 569us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 743/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 744/2000\n",
      "5336/5336 [==============================] - 3s 519us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 745/2000\n",
      "5336/5336 [==============================] - 3s 628us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 746/2000\n",
      "5336/5336 [==============================] - 3s 577us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 747/2000\n",
      "5336/5336 [==============================] - 3s 566us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 748/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 749/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 750/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 751/2000\n",
      "5336/5336 [==============================] - 3s 635us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 752/2000\n",
      "5336/5336 [==============================] - 3s 546us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 753/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 754/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 755/2000\n",
      "5336/5336 [==============================] - 3s 515us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 756/2000\n",
      "5336/5336 [==============================] - 3s 526us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 757/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 758/2000\n",
      "5336/5336 [==============================] - 3s 537us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 759/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 760/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 761/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 762/2000\n",
      "5336/5336 [==============================] - 3s 513us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 763/2000\n",
      "5336/5336 [==============================] - 3s 553us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 764/2000\n",
      "5336/5336 [==============================] - 3s 557us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 765/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 766/2000\n",
      "5336/5336 [==============================] - 3s 523us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 767/2000\n",
      "5336/5336 [==============================] - 3s 528us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 768/2000\n",
      "5336/5336 [==============================] - 3s 516us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 769/2000\n",
      "5336/5336 [==============================] - 3s 509us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 770/2000\n",
      "5336/5336 [==============================] - 3s 505us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 771/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 772/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 773/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 774/2000\n",
      "5336/5336 [==============================] - 3s 515us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 775/2000\n",
      "5336/5336 [==============================] - 3s 515us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 776/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 777/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 778/2000\n",
      "5336/5336 [==============================] - 3s 510us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 779/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 780/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 781/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 782/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 783/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 784/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 785/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 786/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 787/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 788/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 789/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 790/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 791/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 792/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 793/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 794/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 795/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 796/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 797/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 798/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 799/2000\n",
      "5336/5336 [==============================] - 3s 519us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 800/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 801/2000\n",
      "5336/5336 [==============================] - 3s 504us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 802/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 803/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 804/2000\n",
      "5336/5336 [==============================] - 3s 529us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 805/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0048\n",
      "Epoch 806/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 807/2000\n",
      "5336/5336 [==============================] - 3s 505us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 808/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 809/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 810/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 811/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 812/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 813/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 814/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 815/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 816/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 817/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 818/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 819/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 820/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 821/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 822/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 823/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 824/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 825/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 826/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 827/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 828/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 829/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 830/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 831/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 832/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 833/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 834/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 835/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 836/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 837/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 838/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 839/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 840/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 841/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 842/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 843/2000\n",
      "5336/5336 [==============================] - 3s 520us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 844/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 845/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 846/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 847/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 848/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 849/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 850/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 851/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 852/2000\n",
      "5336/5336 [==============================] - 3s 524us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 853/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 854/2000\n",
      "5336/5336 [==============================] - 3s 506us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 855/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 856/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 857/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 858/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 859/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 860/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 861/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 862/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 863/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 864/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 865/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 866/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 867/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 868/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 869/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 870/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 871/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 872/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 873/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 874/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 875/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 876/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 877/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 878/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 879/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 880/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 881/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 882/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 883/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 884/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 885/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 886/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 887/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 888/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 889/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 890/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 891/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 892/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 893/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 894/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 895/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 896/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 897/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 898/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 899/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 900/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 901/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 902/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 903/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 904/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 905/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 906/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 907/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 908/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 909/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 910/2000\n",
      "5336/5336 [==============================] - 3s 510us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 911/2000\n",
      "5336/5336 [==============================] - 3s 526us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 912/2000\n",
      "5336/5336 [==============================] - 3s 517us/step - loss: 0.0055 - val_loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 913/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 914/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 915/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 916/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 917/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 918/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 919/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 920/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 921/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 922/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 923/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 924/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 925/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 926/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 927/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 928/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 929/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 930/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 931/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 932/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 933/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 934/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 935/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 936/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 937/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 938/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 939/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 940/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 941/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 942/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 943/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 944/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 945/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 946/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 947/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 948/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 949/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 950/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 951/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 952/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 953/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 954/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 955/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 956/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 957/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 958/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 959/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 960/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 961/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 962/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 963/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 964/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 965/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 966/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 967/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 968/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 969/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 970/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 971/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 972/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 973/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 974/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 975/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 976/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 977/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 978/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 979/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 980/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 981/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 982/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 983/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 984/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 985/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 986/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 987/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 988/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 989/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 990/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 991/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 992/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 993/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 994/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 995/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 996/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 997/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 998/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 999/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1000/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1001/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1002/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1003/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1004/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1005/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1006/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1007/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1008/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1009/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1010/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1011/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1012/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1013/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1014/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1015/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1016/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1017/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1018/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1019/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1020/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1021/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1022/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1023/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1024/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1025/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1026/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 1027/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1028/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1029/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1030/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1031/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1032/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1033/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1034/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1035/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1036/2000\n",
      "5336/5336 [==============================] - 3s 506us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1037/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1038/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1039/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1040/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1041/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1042/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1043/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1044/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1045/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1046/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1047/2000\n",
      "5336/5336 [==============================] - 3s 506us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1048/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1049/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1050/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1051/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1052/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1053/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1054/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1055/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1056/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1057/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1058/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1059/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1060/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1061/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1062/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1063/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1064/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1065/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1066/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1067/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1068/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1069/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1070/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1071/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1072/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1073/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1074/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1075/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1076/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1077/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1078/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1079/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1080/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1081/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1082/2000\n",
      "5336/5336 [==============================] - 3s 507us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1083/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1084/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1085/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1086/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1087/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1088/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1089/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1090/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1091/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1092/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1093/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1094/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1095/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1096/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1097/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1098/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1099/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1100/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1101/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1102/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1103/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1104/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1105/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1106/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1107/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1108/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1109/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1110/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1111/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1112/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1113/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1114/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1115/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1116/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1117/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1118/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1119/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1120/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1121/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1122/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1123/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1124/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1125/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1126/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1127/2000\n",
      "5336/5336 [==============================] - 3s 504us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1128/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1129/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1130/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1131/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1132/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1133/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1134/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1135/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1136/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1137/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1138/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1139/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1140/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1141/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1142/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1143/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1144/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1145/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1146/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1147/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1148/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1149/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1150/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1151/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1152/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1153/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1154/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1155/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1156/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1157/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1158/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1159/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1160/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1161/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1162/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1163/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1164/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1165/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1166/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1167/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1168/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1169/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1170/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1171/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1172/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1173/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1174/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1175/2000\n",
      "5336/5336 [==============================] - 3s 518us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1176/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1177/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1178/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1179/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1180/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1181/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1182/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1183/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1184/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1185/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1186/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1187/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1188/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1189/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1190/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1191/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1192/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1193/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1194/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1195/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1196/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1197/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1198/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1199/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1200/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1201/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1202/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1203/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1204/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1205/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1206/2000\n",
      "5336/5336 [==============================] - 3s 568us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1207/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1208/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1209/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1210/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1211/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1212/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1213/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1214/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1215/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1216/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1217/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1218/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1219/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1220/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1221/2000\n",
      "5336/5336 [==============================] - 3s 502us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1222/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1223/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1224/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1225/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1226/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1227/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1228/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1229/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1230/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1231/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1232/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1233/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1234/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1235/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1236/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1237/2000\n",
      "5336/5336 [==============================] - 3s 522us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1238/2000\n",
      "5336/5336 [==============================] - 3s 575us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1239/2000\n",
      "5336/5336 [==============================] - 3s 618us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1240/2000\n",
      "5336/5336 [==============================] - 3s 621us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1241/2000\n",
      "5336/5336 [==============================] - 3s 616us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1242/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1243/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1244/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1245/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1246/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1247/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1248/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1249/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1250/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1251/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1252/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1253/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1254/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1255/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1256/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1257/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1258/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1259/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1260/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1261/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1262/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1263/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1264/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1265/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1266/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1267/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1268/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1269/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1270/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1271/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1272/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1273/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1274/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1275/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1276/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1277/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1278/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1279/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1280/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1281/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1282/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1283/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1284/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1285/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1286/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1287/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1288/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1289/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1290/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1291/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1292/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1293/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1294/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1295/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1296/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1297/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1298/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1299/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1300/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1301/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1302/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1303/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1304/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1305/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1306/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1307/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1308/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1309/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1310/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1311/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1312/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1313/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1314/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1315/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1316/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1317/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1318/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1319/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1320/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1321/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1322/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1323/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1324/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1325/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1326/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1327/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1328/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1329/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1330/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1331/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1332/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1333/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1334/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1335/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1336/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1337/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1338/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1339/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1340/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1341/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1342/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1343/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1344/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1345/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1346/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1347/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1348/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1349/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1350/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1351/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1352/2000\n",
      "5336/5336 [==============================] - 3s 509us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1353/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1354/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1355/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1356/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1357/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1358/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1359/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1360/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1361/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1362/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1363/2000\n",
      "5336/5336 [==============================] - 3s 502us/step - loss: 0.0056 - val_loss: 0.0047\n",
      "Epoch 1364/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1365/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1366/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1367/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1368/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1369/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1370/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1371/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1372/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1373/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1374/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1375/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1376/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1377/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1378/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1379/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1380/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1381/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1382/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1383/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1384/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1385/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1386/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1387/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1388/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1389/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1390/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1391/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1392/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1393/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1394/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1395/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1396/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1397/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1398/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1399/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1400/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1401/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1402/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1403/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1404/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1405/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1406/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1407/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1408/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1409/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1410/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1411/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1412/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1413/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1414/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1415/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1416/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1417/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1418/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1419/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1420/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1421/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1422/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1423/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1424/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1425/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1426/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1427/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1428/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1429/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1430/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1431/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1432/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1433/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1434/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1435/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1436/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1437/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1438/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1439/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1440/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1441/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1442/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1443/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1444/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1445/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1446/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1447/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1448/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1449/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1450/2000\n",
      "5336/5336 [==============================] - 3s 505us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1451/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1452/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1453/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1454/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1455/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1456/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1457/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1458/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1459/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1460/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1461/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1462/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1463/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1464/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1465/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1466/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1467/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1468/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1469/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1470/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1471/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1472/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1473/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1474/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1475/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1476/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1477/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1478/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1479/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1480/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1481/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1482/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1483/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1484/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1485/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1486/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1487/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1488/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1489/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1490/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1491/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1492/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1493/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1494/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1495/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1496/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1497/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1498/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1499/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1500/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1501/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1502/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1503/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1504/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1505/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1506/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1507/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1508/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1509/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1510/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1511/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1512/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1513/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1514/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1515/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1516/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1517/2000\n",
      "5336/5336 [==============================] - 3s 480us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1518/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1519/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1520/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1521/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1522/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1523/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1524/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1525/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1526/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1527/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1528/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1529/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1530/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1531/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1532/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1533/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1534/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1535/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1536/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1537/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1538/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1539/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1540/2000\n",
      "5336/5336 [==============================] - 3s 511us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1541/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1542/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1543/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1544/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1545/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1546/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1547/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1548/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1549/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1550/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1551/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1552/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1553/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1554/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1555/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1556/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1557/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1558/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1559/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1560/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1561/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1562/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1563/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1564/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1565/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1566/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1567/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1568/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1569/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1570/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1571/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1572/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1573/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1574/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1575/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1576/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1577/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1578/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1579/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1580/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1581/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1582/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1583/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1584/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1585/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1586/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1587/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1588/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1589/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1590/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1591/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1592/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1593/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1594/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1595/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1596/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1597/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1598/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1599/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1600/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1601/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1602/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1603/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1604/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1605/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1606/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1607/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1608/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1609/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1610/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1611/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1612/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1613/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1614/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1615/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1616/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1617/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1618/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1619/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1620/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1621/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1622/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1623/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1624/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1625/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1626/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1627/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1628/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1629/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1630/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1631/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1632/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1633/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1634/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1635/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1636/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1637/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1638/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1639/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1640/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1641/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1642/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1643/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1644/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1645/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1646/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1647/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1648/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1649/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1650/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1651/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1652/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1653/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1654/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1655/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1656/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1657/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1658/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1659/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1660/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1661/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1662/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1663/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1664/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1665/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1666/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1667/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1668/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1669/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1670/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1671/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1672/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1673/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1674/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1675/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1676/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1677/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1678/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1679/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1680/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1681/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1682/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1683/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1684/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1685/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1686/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1687/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1688/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1689/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1690/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1691/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1692/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1693/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1694/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1695/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1696/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1697/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1698/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1699/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1700/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1701/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1702/2000\n",
      "5336/5336 [==============================] - 3s 526us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1703/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1704/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1705/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1706/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1707/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1708/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1709/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1710/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1711/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1712/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1713/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1714/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1715/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1716/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1717/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1718/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1719/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1720/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1721/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1722/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1723/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1724/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1725/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1726/2000\n",
      "5336/5336 [==============================] - 3s 504us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1727/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1728/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1729/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1730/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1731/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1732/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1733/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1734/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1735/2000\n",
      "5336/5336 [==============================] - 3s 502us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1736/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1737/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1738/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1739/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1740/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1741/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1742/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1743/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1744/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1745/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1746/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1747/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1748/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1749/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1750/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1751/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1752/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1753/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1754/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1755/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1756/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1757/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1758/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1759/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1760/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1761/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1762/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1763/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1764/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1765/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1766/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1767/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1768/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1769/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1770/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1771/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1772/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1773/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1774/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1775/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1776/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1777/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1778/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1779/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1780/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1781/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1782/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1783/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1784/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1785/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1786/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1787/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1788/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1789/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1790/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1791/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1792/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1793/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1794/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1795/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1796/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1797/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1798/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1799/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1800/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1801/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1802/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1803/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1804/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1805/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1806/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1807/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1808/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1809/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1810/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1811/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1812/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1813/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1814/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1815/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1816/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1817/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1818/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1819/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1820/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1821/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1822/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1823/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1824/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1825/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1826/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1827/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1828/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1829/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1830/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1831/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1832/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1833/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1834/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1835/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1836/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1837/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1838/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1839/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1840/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1841/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1842/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1843/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1844/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1845/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1846/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1847/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1848/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1849/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1850/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1851/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1852/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1853/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1854/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1855/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1856/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1857/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1858/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1859/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1860/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1861/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1862/2000\n",
      "5336/5336 [==============================] - 3s 499us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1863/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1864/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1865/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1866/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1867/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1868/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1869/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1870/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1871/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1872/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1873/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1874/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1875/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1876/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1877/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1878/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1879/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1880/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1881/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1882/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1883/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1884/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1885/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1886/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1887/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1888/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1889/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1890/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1891/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1892/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1893/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1894/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1895/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1896/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1897/2000\n",
      "5336/5336 [==============================] - 3s 501us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1898/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1899/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1900/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1901/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1902/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1903/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1904/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1905/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1906/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1907/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1908/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1909/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1910/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1911/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1912/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1913/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1914/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1915/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1916/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1917/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1918/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1919/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1920/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1921/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1922/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1923/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1924/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1925/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1926/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1927/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1928/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1929/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1930/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1931/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1932/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1933/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1934/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1935/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1936/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1937/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1938/2000\n",
      "5336/5336 [==============================] - 3s 503us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1939/2000\n",
      "5336/5336 [==============================] - 3s 509us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1940/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1941/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1942/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1943/2000\n",
      "5336/5336 [==============================] - 3s 514us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1944/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1945/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1946/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1947/2000\n",
      "5336/5336 [==============================] - 3597s 674ms/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1948/2000\n",
      "5336/5336 [==============================] - 3s 511us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1949/2000\n",
      "5336/5336 [==============================] - 3s 498us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1950/2000\n",
      "5336/5336 [==============================] - 3s 536us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1951/2000\n",
      "5336/5336 [==============================] - 3s 516us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1952/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1953/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1954/2000\n",
      "5336/5336 [==============================] - 3s 491us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1955/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1956/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1957/2000\n",
      "5336/5336 [==============================] - 3s 513us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1958/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1959/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1960/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1961/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1962/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1963/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1964/2000\n",
      "5336/5336 [==============================] - 3s 489us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1965/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1966/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1967/2000\n",
      "5336/5336 [==============================] - 3598s 674ms/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1968/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1969/2000\n",
      "5336/5336 [==============================] - 3s 518us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1970/2000\n",
      "5336/5336 [==============================] - 3s 516us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1971/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1972/2000\n",
      "5336/5336 [==============================] - 3s 493us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1973/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1974/2000\n",
      "5336/5336 [==============================] - 3s 514us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1975/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1976/2000\n",
      "5336/5336 [==============================] - 3s 482us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1977/2000\n",
      "5336/5336 [==============================] - 3s 485us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1978/2000\n",
      "5336/5336 [==============================] - 3s 497us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1979/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 1980/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1981/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1982/2000\n",
      "5336/5336 [==============================] - 3s 496us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1983/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1984/2000\n",
      "5336/5336 [==============================] - 3s 492us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1985/2000\n",
      "5336/5336 [==============================] - 3s 487us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 1986/2000\n",
      "5336/5336 [==============================] - 2135s 400ms/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1987/2000\n",
      "5336/5336 [==============================] - 3s 490us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1988/2000\n",
      "5336/5336 [==============================] - 3s 500us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1989/2000\n",
      "5336/5336 [==============================] - 3s 527us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1990/2000\n",
      "5336/5336 [==============================] - 3s 488us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1991/2000\n",
      "5336/5336 [==============================] - 3s 486us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1992/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1993/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1994/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1995/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1996/2000\n",
      "5336/5336 [==============================] - 3s 484us/step - loss: 0.0055 - val_loss: 0.0047\n",
      "Epoch 1997/2000\n",
      "5336/5336 [==============================] - 3s 494us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1998/2000\n",
      "5336/5336 [==============================] - 3s 495us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 1999/2000\n",
      "5336/5336 [==============================] - 3s 483us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 2000/2000\n",
      "5336/5336 [==============================] - 3s 481us/step - loss: 0.0055 - val_loss: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15455b550>"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train_s, y=y_train_s, \n",
    "          batch_size=2500, \n",
    "          epochs=2000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_s, y_test_s),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_16, X_test_16, y_train_16, y_test_16 = train_test_split(X_16, y_16, test_size= .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, test_size= .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05329405,  1.12662344,  1.08065202, ...,  0.99302723,\n",
       "        1.0074516 ,  1.07181999])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2498697469914781"
      ]
     },
     "execution_count": 796,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_16.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07416198487095663"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".0055 ** .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New datasets\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(690,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# best lr .02\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New datasets\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(1350,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "# Added\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# best lr .005\n",
    "sgd = keras.optimizers.SGD(lr=0.005)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New datasets\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(875,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "# Added\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# best lr .005\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 662 samples, validate on 166 samples\n",
      "Epoch 1/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0177 - val_loss: 0.0591\n",
      "Epoch 2/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0183 - val_loss: 0.0606\n",
      "Epoch 3/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0183 - val_loss: 0.0574\n",
      "Epoch 4/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0169 - val_loss: 0.0568\n",
      "Epoch 5/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0182 - val_loss: 0.0587\n",
      "Epoch 6/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0191 - val_loss: 0.0616\n",
      "Epoch 7/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0210 - val_loss: 0.0591\n",
      "Epoch 8/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0187 - val_loss: 0.0579\n",
      "Epoch 9/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0167 - val_loss: 0.0570\n",
      "Epoch 10/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0169 - val_loss: 0.0551\n",
      "Epoch 11/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0149 - val_loss: 0.0557\n",
      "Epoch 12/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0167 - val_loss: 0.0567\n",
      "Epoch 13/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0151 - val_loss: 0.0578\n",
      "Epoch 14/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0564\n",
      "Epoch 15/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0170 - val_loss: 0.0579\n",
      "Epoch 16/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0152 - val_loss: 0.0567\n",
      "Epoch 17/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0163 - val_loss: 0.0561\n",
      "Epoch 18/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0163 - val_loss: 0.0550\n",
      "Epoch 19/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0155 - val_loss: 0.0553\n",
      "Epoch 20/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0153 - val_loss: 0.0565\n",
      "Epoch 21/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0146 - val_loss: 0.0545\n",
      "Epoch 22/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0143 - val_loss: 0.0534\n",
      "Epoch 23/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0148 - val_loss: 0.0563\n",
      "Epoch 24/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0124 - val_loss: 0.0584\n",
      "Epoch 25/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0166 - val_loss: 0.0552\n",
      "Epoch 26/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0152 - val_loss: 0.0568\n",
      "Epoch 27/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0163 - val_loss: 0.0544\n",
      "Epoch 28/30\n",
      "662/662 [==============================] - 1s 2ms/step - loss: 0.0147 - val_loss: 0.0531\n",
      "Epoch 29/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0154 - val_loss: 0.0530\n",
      "Epoch 30/30\n",
      "662/662 [==============================] - 1s 1ms/step - loss: 0.0142 - val_loss: 0.0530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1661f8f98>"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train_16, y=y_train_16, \n",
    "          batch_size=300, \n",
    "          epochs=30, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_16, y_test_16),\n",
    "          callbacks=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Without new features\n",
    "root = .0040 ** .5\n",
    "# .071\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2400.000000\n",
       "mean        1.066686\n",
       "std         0.065818\n",
       "min         0.831398\n",
       "25%         1.024956\n",
       "50%         1.061385\n",
       "75%         1.101586\n",
       "max         1.425089\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_16['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new datasets with new features (big swing)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(450,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2186 samples, validate on 547 samples\n",
      "Epoch 1/2000\n",
      "2186/2186 [==============================] - 3s 2ms/step - loss: 1.1054 - val_loss: 1.0352\n",
      "Epoch 2/2000\n",
      "2186/2186 [==============================] - 1s 502us/step - loss: 1.0320 - val_loss: 0.9584\n",
      "Epoch 3/2000\n",
      "2186/2186 [==============================] - 1s 519us/step - loss: 0.9561 - val_loss: 0.8839\n",
      "Epoch 4/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.8826 - val_loss: 0.8130\n",
      "Epoch 5/2000\n",
      "2186/2186 [==============================] - 1s 498us/step - loss: 0.8131 - val_loss: 0.7462\n",
      "Epoch 6/2000\n",
      "2186/2186 [==============================] - 1s 506us/step - loss: 0.7475 - val_loss: 0.6837\n",
      "Epoch 7/2000\n",
      "2186/2186 [==============================] - 1s 508us/step - loss: 0.6858 - val_loss: 0.6255\n",
      "Epoch 8/2000\n",
      "2186/2186 [==============================] - 1s 505us/step - loss: 0.6284 - val_loss: 0.5714\n",
      "Epoch 9/2000\n",
      "2186/2186 [==============================] - 1s 500us/step - loss: 0.5750 - val_loss: 0.5213\n",
      "Epoch 10/2000\n",
      "2186/2186 [==============================] - 1s 497us/step - loss: 0.5258 - val_loss: 0.4750\n",
      "Epoch 11/2000\n",
      "2186/2186 [==============================] - 1s 540us/step - loss: 0.4802 - val_loss: 0.4323\n",
      "Epoch 12/2000\n",
      "2186/2186 [==============================] - 1s 537us/step - loss: 0.4374 - val_loss: 0.3929\n",
      "Epoch 13/2000\n",
      "2186/2186 [==============================] - 1s 507us/step - loss: 0.3984 - val_loss: 0.3566\n",
      "Epoch 14/2000\n",
      "2186/2186 [==============================] - 1s 500us/step - loss: 0.3623 - val_loss: 0.3233\n",
      "Epoch 15/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.3296 - val_loss: 0.2927\n",
      "Epoch 16/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.2987 - val_loss: 0.2646\n",
      "Epoch 17/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.2709 - val_loss: 0.2389\n",
      "Epoch 18/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.2446 - val_loss: 0.2155\n",
      "Epoch 19/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.2219 - val_loss: 0.1941\n",
      "Epoch 20/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.2004 - val_loss: 0.1746\n",
      "Epoch 21/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.1811 - val_loss: 0.1568\n",
      "Epoch 22/2000\n",
      "2186/2186 [==============================] - 1s 491us/step - loss: 0.1634 - val_loss: 0.1407\n",
      "Epoch 23/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.1468 - val_loss: 0.1261\n",
      "Epoch 24/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.1323 - val_loss: 0.1129\n",
      "Epoch 25/2000\n",
      "2186/2186 [==============================] - 1s 501us/step - loss: 0.1189 - val_loss: 0.1010\n",
      "Epoch 26/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.1068 - val_loss: 0.0902\n",
      "Epoch 27/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0957 - val_loss: 0.0805\n",
      "Epoch 28/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0864 - val_loss: 0.0718\n",
      "Epoch 29/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0774 - val_loss: 0.0640\n",
      "Epoch 30/2000\n",
      "2186/2186 [==============================] - 1s 510us/step - loss: 0.0694 - val_loss: 0.0570\n",
      "Epoch 31/2000\n",
      "2186/2186 [==============================] - 1s 514us/step - loss: 0.0621 - val_loss: 0.0507\n",
      "Epoch 32/2000\n",
      "2186/2186 [==============================] - 1s 678us/step - loss: 0.0556 - val_loss: 0.0451\n",
      "Epoch 33/2000\n",
      "2186/2186 [==============================] - 1s 599us/step - loss: 0.0500 - val_loss: 0.0401\n",
      "Epoch 34/2000\n",
      "2186/2186 [==============================] - 1s 500us/step - loss: 0.0452 - val_loss: 0.0356\n",
      "Epoch 35/2000\n",
      "2186/2186 [==============================] - 1s 497us/step - loss: 0.0402 - val_loss: 0.0317\n",
      "Epoch 36/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0363 - val_loss: 0.0282\n",
      "Epoch 37/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0326 - val_loss: 0.0251\n",
      "Epoch 38/2000\n",
      "2186/2186 [==============================] - 1s 498us/step - loss: 0.0293 - val_loss: 0.0223\n",
      "Epoch 39/2000\n",
      "2186/2186 [==============================] - 1s 524us/step - loss: 0.0262 - val_loss: 0.0199\n",
      "Epoch 40/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.0238 - val_loss: 0.0178\n",
      "Epoch 41/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0214 - val_loss: 0.0159\n",
      "Epoch 42/2000\n",
      "2186/2186 [==============================] - 1s 500us/step - loss: 0.0192 - val_loss: 0.0142\n",
      "Epoch 43/2000\n",
      "2186/2186 [==============================] - 1s 497us/step - loss: 0.0175 - val_loss: 0.0128\n",
      "Epoch 44/2000\n",
      "2186/2186 [==============================] - 1s 499us/step - loss: 0.0158 - val_loss: 0.0115\n",
      "Epoch 45/2000\n",
      "2186/2186 [==============================] - 1s 498us/step - loss: 0.0145 - val_loss: 0.0104\n",
      "Epoch 46/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0134 - val_loss: 0.0094\n",
      "Epoch 47/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0121 - val_loss: 0.0086\n",
      "Epoch 48/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0111 - val_loss: 0.0079\n",
      "Epoch 49/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0103 - val_loss: 0.0072\n",
      "Epoch 50/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0097 - val_loss: 0.0067\n",
      "Epoch 51/2000\n",
      "2186/2186 [==============================] - 1s 490us/step - loss: 0.0088 - val_loss: 0.0062\n",
      "Epoch 52/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0083 - val_loss: 0.0058\n",
      "Epoch 53/2000\n",
      "2186/2186 [==============================] - 1s 501us/step - loss: 0.0078 - val_loss: 0.0054\n",
      "Epoch 54/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0072 - val_loss: 0.0051\n",
      "Epoch 55/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0069 - val_loss: 0.0048\n",
      "Epoch 56/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0066 - val_loss: 0.0046\n",
      "Epoch 57/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0064 - val_loss: 0.0044\n",
      "Epoch 58/2000\n",
      "2186/2186 [==============================] - 1s 524us/step - loss: 0.0060 - val_loss: 0.0043\n",
      "Epoch 59/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0058 - val_loss: 0.0041\n",
      "Epoch 60/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0056 - val_loss: 0.0040\n",
      "Epoch 61/2000\n",
      "2186/2186 [==============================] - 1s 511us/step - loss: 0.0054 - val_loss: 0.0039\n",
      "Epoch 62/2000\n",
      "2186/2186 [==============================] - 1s 505us/step - loss: 0.0053 - val_loss: 0.0038\n",
      "Epoch 63/2000\n",
      "2186/2186 [==============================] - 1s 506us/step - loss: 0.0051 - val_loss: 0.0037\n",
      "Epoch 64/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0049 - val_loss: 0.0037\n",
      "Epoch 65/2000\n",
      "2186/2186 [==============================] - 1s 490us/step - loss: 0.0049 - val_loss: 0.0036\n",
      "Epoch 66/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0049 - val_loss: 0.0036\n",
      "Epoch 67/2000\n",
      "2186/2186 [==============================] - 1s 521us/step - loss: 0.0047 - val_loss: 0.0036\n",
      "Epoch 68/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.0048 - val_loss: 0.0036\n",
      "Epoch 69/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 70/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 71/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0044 - val_loss: 0.0035\n",
      "Epoch 72/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0045 - val_loss: 0.0035\n",
      "Epoch 73/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0044 - val_loss: 0.0035\n",
      "Epoch 74/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 75/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 76/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 77/2000\n",
      "2186/2186 [==============================] - 1s 490us/step - loss: 0.0043 - val_loss: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 79/2000\n",
      "2186/2186 [==============================] - 1s 491us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 80/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0042 - val_loss: 0.0035\n",
      "Epoch 81/2000\n",
      "2186/2186 [==============================] - 1s 489us/step - loss: 0.0042 - val_loss: 0.0035\n",
      "Epoch 82/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0043 - val_loss: 0.0035\n",
      "Epoch 83/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 84/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0043 - val_loss: 0.0036\n",
      "Epoch 85/2000\n",
      "2186/2186 [==============================] - 1s 508us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 86/2000\n",
      "2186/2186 [==============================] - 1s 499us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 87/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 88/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 89/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 90/2000\n",
      "2186/2186 [==============================] - 1s 491us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 91/2000\n",
      "2186/2186 [==============================] - 1s 490us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 92/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 93/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 94/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0044 - val_loss: 0.0036\n",
      "Epoch 95/2000\n",
      "2186/2186 [==============================] - 1s 522us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 96/2000\n",
      "2186/2186 [==============================] - 1s 494us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 97/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 98/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 99/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 100/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 101/2000\n",
      "2186/2186 [==============================] - 1s 490us/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 102/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 103/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 104/2000\n",
      "2186/2186 [==============================] - 1s 491us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 105/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 106/2000\n",
      "2186/2186 [==============================] - 1s 493us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 107/2000\n",
      "2186/2186 [==============================] - 1s 496us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 108/2000\n",
      "2186/2186 [==============================] - 1s 488us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 109/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 110/2000\n",
      "2186/2186 [==============================] - 1s 488us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 111/2000\n",
      "2186/2186 [==============================] - 1s 495us/step - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 112/2000\n",
      "2186/2186 [==============================] - 1s 489us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 113/2000\n",
      "2186/2186 [==============================] - 1s 513us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 114/2000\n",
      "2186/2186 [==============================] - 1s 492us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 115/2000\n",
      "2186/2186 [==============================] - 1s 502us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 116/2000\n",
      "2186/2186 [==============================] - 1s 586us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 117/2000\n",
      "2186/2186 [==============================] - 2s 726us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 118/2000\n",
      "2186/2186 [==============================] - 1s 628us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 119/2000\n",
      "2186/2186 [==============================] - 1s 655us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 120/2000\n",
      "2186/2186 [==============================] - 1s 634us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 121/2000\n",
      "2186/2186 [==============================] - 2s 723us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 122/2000\n",
      "2186/2186 [==============================] - 1s 594us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 123/2000\n",
      "2186/2186 [==============================] - 1s 592us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 124/2000\n",
      "2186/2186 [==============================] - 1s 555us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "Epoch 125/2000\n",
      "2186/2186 [==============================] - 1s 595us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 126/2000\n",
      "2186/2186 [==============================] - 1s 659us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 127/2000\n",
      "2186/2186 [==============================] - 1s 623us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 128/2000\n",
      "2186/2186 [==============================] - 1s 534us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "Epoch 129/2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-562-052235ad6671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_sm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_sm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=None)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Thinkful/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train_sm, y=y_train_sm, \n",
    "          batch_size=2500, \n",
    "          epochs=2000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_sm, y_test_sm),\n",
    "          callbacks=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new datasets with new features (big swing) and new layers\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(450,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(4000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.01)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2186 samples, validate on 547 samples\n",
      "Epoch 1/2000\n",
      "2186/2186 [==============================] - 9s 4ms/step - loss: 1.1104 - val_loss: 1.0365\n",
      "Epoch 2/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 1.0305 - val_loss: 0.9589\n",
      "Epoch 3/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.9543 - val_loss: 0.8822\n",
      "Epoch 4/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.8791 - val_loss: 0.8091\n",
      "Epoch 5/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.8070 - val_loss: 0.7405\n",
      "Epoch 6/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.7393 - val_loss: 0.6766\n",
      "Epoch 7/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.6760 - val_loss: 0.6172\n",
      "Epoch 8/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.6172 - val_loss: 0.5622\n",
      "Epoch 9/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.5626 - val_loss: 0.5114\n",
      "Epoch 10/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.5124 - val_loss: 0.4645\n",
      "Epoch 11/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.4658 - val_loss: 0.4213\n",
      "Epoch 12/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.4228 - val_loss: 0.3816\n",
      "Epoch 13/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.3833 - val_loss: 0.3451\n",
      "Epoch 14/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.3474 - val_loss: 0.3117\n",
      "Epoch 15/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.3142 - val_loss: 0.2811\n",
      "Epoch 16/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.2837 - val_loss: 0.2532\n",
      "Epoch 17/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.2559 - val_loss: 0.2277\n",
      "Epoch 18/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.2306 - val_loss: 0.2045\n",
      "Epoch 19/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.2076 - val_loss: 0.1834\n",
      "Epoch 20/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.1868 - val_loss: 0.1643\n",
      "Epoch 21/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.1672 - val_loss: 0.1470\n",
      "Epoch 22/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.1500 - val_loss: 0.1313\n",
      "Epoch 23/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.1343 - val_loss: 0.1171\n",
      "Epoch 24/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.1203 - val_loss: 0.1044\n",
      "Epoch 25/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.1076 - val_loss: 0.0929\n",
      "Epoch 26/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0959 - val_loss: 0.0826\n",
      "Epoch 27/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0856 - val_loss: 0.0734\n",
      "Epoch 28/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0765 - val_loss: 0.0652\n",
      "Epoch 29/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0681 - val_loss: 0.0578\n",
      "Epoch 30/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0607 - val_loss: 0.0513\n",
      "Epoch 31/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0542 - val_loss: 0.0454\n",
      "Epoch 32/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0481 - val_loss: 0.0403\n",
      "Epoch 33/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0428 - val_loss: 0.0357\n",
      "Epoch 34/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0382 - val_loss: 0.0316\n",
      "Epoch 35/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0341 - val_loss: 0.0280\n",
      "Epoch 36/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0302 - val_loss: 0.0249\n",
      "Epoch 37/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0270 - val_loss: 0.0221\n",
      "Epoch 38/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0243 - val_loss: 0.0197\n",
      "Epoch 39/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0217 - val_loss: 0.0175\n",
      "Epoch 40/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0195 - val_loss: 0.0156\n",
      "Epoch 41/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0175 - val_loss: 0.0140\n",
      "Epoch 42/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0158 - val_loss: 0.0125\n",
      "Epoch 43/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0142 - val_loss: 0.0113\n",
      "Epoch 44/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0129 - val_loss: 0.0102\n",
      "Epoch 45/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0118 - val_loss: 0.0092\n",
      "Epoch 46/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0107 - val_loss: 0.0084\n",
      "Epoch 47/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0099 - val_loss: 0.0077\n",
      "Epoch 48/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0089 - val_loss: 0.0071\n",
      "Epoch 49/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0084 - val_loss: 0.0065\n",
      "Epoch 50/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0077 - val_loss: 0.0061\n",
      "Epoch 51/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0073 - val_loss: 0.0057\n",
      "Epoch 52/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0069 - val_loss: 0.0053\n",
      "Epoch 53/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0064 - val_loss: 0.0050\n",
      "Epoch 54/2000\n",
      "2186/2186 [==============================] - 7s 3ms/step - loss: 0.0061 - val_loss: 0.0048\n",
      "Epoch 55/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0058 - val_loss: 0.0046\n",
      "Epoch 56/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0056 - val_loss: 0.0044\n",
      "Epoch 57/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 58/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0051 - val_loss: 0.0041\n",
      "Epoch 59/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 60/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0047 - val_loss: 0.0039\n",
      "Epoch 61/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 62/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 63/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 64/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0043 - val_loss: 0.0036\n",
      "Epoch 65/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 66/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0042 - val_loss: 0.0035\n",
      "Epoch 67/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0042 - val_loss: 0.0035\n",
      "Epoch 68/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 69/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 70/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0040 - val_loss: 0.0035\n",
      "Epoch 71/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0040 - val_loss: 0.0034\n",
      "Epoch 72/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 73/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 74/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 75/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 76/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 77/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 78/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 80/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 81/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 82/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 83/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 84/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 85/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 86/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 87/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 88/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 89/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 90/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 91/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 92/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 93/2000\n",
      "2186/2186 [==============================] - 7s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 94/2000\n",
      "2186/2186 [==============================] - 7s 3ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 95/2000\n",
      "2186/2186 [==============================] - 7s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 96/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 97/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 98/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 99/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 100/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 101/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 102/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 103/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 104/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 105/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 106/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 107/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 108/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 109/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 110/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 111/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 112/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 113/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 114/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 115/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 116/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 117/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 118/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 119/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 120/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 121/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 122/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 123/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 124/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 125/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 126/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 127/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 128/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 129/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 130/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 131/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 132/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 133/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 134/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 135/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 136/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 137/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 138/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 139/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 140/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 141/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 142/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 143/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 144/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 145/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 146/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 147/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 148/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 149/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 150/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 151/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 152/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 153/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 154/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 155/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 156/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 158/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 159/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 160/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 161/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 162/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 163/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 164/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 165/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 166/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 167/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 168/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 169/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 170/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 171/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 172/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 173/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 174/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 175/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 176/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 177/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 178/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 179/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 180/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 181/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 182/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 183/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 184/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 185/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 186/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 187/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 188/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 189/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 190/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 191/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 192/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 193/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 194/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 195/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 196/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 197/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 198/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 199/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 200/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 201/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 202/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 203/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 204/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 205/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 206/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 207/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 208/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 209/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 210/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 211/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 212/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 213/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 214/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 215/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 216/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 217/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 218/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 219/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 220/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 221/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 222/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 223/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 224/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 225/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 226/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 227/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 228/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 229/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 230/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 231/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 232/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 233/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 234/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 236/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 237/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 238/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 239/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 240/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 241/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 242/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 243/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 244/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 245/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 246/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 247/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 248/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 249/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 250/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 251/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 252/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 253/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 254/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 255/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 256/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 257/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 258/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 259/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 260/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 261/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 262/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 263/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 264/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 265/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 266/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 267/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 268/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 269/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 270/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 271/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 272/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 273/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 274/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 275/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 276/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 277/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 278/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 279/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 280/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 281/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 282/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 283/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 284/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 285/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 286/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 287/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 288/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 289/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 290/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 291/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 292/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 293/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 294/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 295/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 296/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 297/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 298/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 299/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 300/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 301/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 302/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 303/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 304/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 305/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 306/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 307/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 308/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 309/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 310/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 311/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 312/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 313/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 314/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 315/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 316/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 317/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 318/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 319/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 320/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 321/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 322/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 323/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 324/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 325/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 326/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 327/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 328/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 329/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 330/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 331/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 332/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 333/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 334/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 335/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 336/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 337/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 338/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 339/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 340/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 341/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 342/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 343/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 344/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 345/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 346/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 347/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 348/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 349/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 350/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 351/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 352/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 353/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 354/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 355/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 356/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 357/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 358/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 359/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 360/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 361/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 362/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 363/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 364/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 365/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 366/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 367/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 368/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 369/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 370/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 371/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 372/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 373/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 374/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 375/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 376/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 377/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 378/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 379/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 380/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 381/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 382/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 383/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 384/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 385/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 386/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 387/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 388/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 389/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 390/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 392/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 393/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 394/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 395/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 396/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 397/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 398/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 399/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 400/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 401/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 402/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 403/2000\n",
      "2186/2186 [==============================] - 7s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 404/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 405/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 406/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 407/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 408/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 409/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 410/2000\n",
      "2186/2186 [==============================] - 245s 112ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 411/2000\n",
      "2186/2186 [==============================] - 8s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 412/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 413/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 414/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 415/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 416/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 417/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 418/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 419/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 420/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 421/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 422/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 423/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 424/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 425/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 426/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 427/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 428/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 429/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 430/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 431/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 432/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 433/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 434/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 435/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 436/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 437/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 438/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 439/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 440/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 441/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 442/2000\n",
      "2186/2186 [==============================] - 6s 3ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 443/2000\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train_sm, y=y_train_sm, \n",
    "          batch_size=2500, \n",
    "          epochs=2000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_sm, y_test_sm),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new datasets with new features (big swing) and new layers\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(450,), activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(4000, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.0001)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=X_train_sm, y=y_train_sm, \n",
    "          batch_size=2500, \n",
    "          epochs=2000, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test_sm, y_test_sm),\n",
    "          callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
